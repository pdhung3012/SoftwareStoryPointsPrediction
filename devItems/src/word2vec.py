# -*- coding: utf-8 -*-
"""SoftwareEffortEstimation_Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqXklmCtt3j8S5TlpEY7-MEVjBW9t9sG
"""

import pandas as pd
# Load Data
url = 'https://raw.githubusercontent.com/SEAnalytics/datasets/master/storypoint/IEEE%20TSE2018/dataset/mulestudio.csv'
raw_data = pd.read_csv(url)
raw_data.columns
raw_data.head(6)

# Check for missing values
raw_data.isnull().sum()

# Examine storypoints summary statistics
raw_data.storypoint.describe()

# Visualize number of storypoints distribution
import matplotlib.pyplot as plt
# plt.hist(raw_data.storypoint, bins=20, alpha=0.6, color='b')
# plt.title("#Items per Point")
# plt.xlabel("Points")
# plt.ylabel("Count")
# plt.savefig('w2v_1.png')

# Zoom in on most common data representing 75% of the dataset
import numpy as np
frequent_data = raw_data.query('storypoint <= 8')
# plt.hist(frequent_data.storypoint, bins=20, alpha=0.6, color='b')
# plt.title("#Items per Point")
# plt.xlabel("Points")
# plt.ylabel("Count")
# plt.xticks(np.arange(0, 10, 1))
# plt.savefig('w2v_2.png')

raw_data.loc[raw_data.storypoint <= 3, 'storypoint'] = 0 #small
raw_data.loc[(raw_data.storypoint > 3) & (raw_data.storypoint <= 9), 'storypoint'] = 1 #medium
raw_data.loc[raw_data.storypoint > 9, 'storypoint'] = 2 #big

# Examine distribution in detail
raw_data.groupby('storypoint').size()

"""This shows that 5 the most common number of storypoints assigned to an issue/requirement."""

################### Data Preprocessing ###################################

import nltk
from nltk.corpus import stopwords 
# nltk.download('punkt')
# nltk.download('stopwords')

issue_titles = raw_data['title']
issue_descriptions = raw_data['description']

# Create a list of strings, one for each title
titles_list = [title for title in issue_titles]
descriptions_list = [description for description in issue_descriptions]

# Collapse the list of strings into a single long string for processing
big_title_string = ' '.join(titles_list)

from nltk.tokenize import word_tokenize

# Tokenize the string into words
tokens = word_tokenize(big_title_string)

# Remove non-alphabetic tokens, such as punctuation
words = [word.lower() for word in tokens if word.isalpha()]

# Filter out stopwords
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
words = [w for w in words if w not in stop_words]

import gensim
# Load word2vec model (trained on Google corpus)
model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary = True)

# Check dimension of word vectors
model.vector_size

# Filter the list of vectors to include only those that Word2Vec has a vector for
vector_list = [model[word] for word in words if word in model.vocab]

# Create a list of the words corresponding to these vectors
words_filtered = [word for word in words if word in model.vocab]

# Zip the words together with their vector representations
word_vec_zip = zip(words_filtered, vector_list)

# Cast to a dict so we can turn it into a DataFrame
word_vec_dict = dict(word_vec_zip)
df = pd.DataFrame.from_dict(word_vec_dict, orient='index')
df.head(3)

# Vocabulary
vocabulary = set(words) # list of unique words
vocabulary_size = len(vocabulary)
vocabulary_size

# Total Number of Words
len(words)

# Frequency Distribution of Words Plot
from nltk.probability import FreqDist
fdist = FreqDist(words)
# import matplotlib.pyplot as plt
# fdist.plot(30,cumulative=False)
# plt.savefig('w2v_3.png')

# Dimentionality Reduction
from sklearn.manifold import TSNE

# Initialize t-SNE
tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)
# Use only 400 rows to shorten processing time
tsne_df = tsne.fit_transform(df[:400])

# Plot
import seaborn as sns
sns.set()# Initialize figure
fig, ax = plt.subplots(figsize = (11.7, 8.27))
sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)

# Import adjustText, initialize list of texts
#from adjustText import adjust_text
texts = []
words_to_plot = list(np.arange(0, 400, 10))

# Append words to list
for word in words_to_plot:
    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))
    
# Plot text using adjust_text (because overlapping text is hard to read)
'''
adjust_text(texts, force_points = 0.4, force_text = 0.4, 
            expand_points = (2,1), expand_text = (1,2),
            arrowprops = dict(arrowstyle = "-", color = 'black', lw = 0.5))
'''
# plt.savefig('w2v_4.png')

raw_data['storypoint']

# Averaging Word Embeddings
def document_vector(word2vec_model, doc):
    # remove out-of-vocabulary words
    doc = [word for word in doc if word in model.vocab]
    return np.mean(model[doc], axis=0)

# Our earlier preprocessing was done when we were dealing only with word vectors
# Here, we need each document to remain a document 
def preprocess(text):
    text = text.lower()
    doc = word_tokenize(text)
    doc = [word for word in doc if word not in stop_words]
    doc = [word for word in doc if word.isalpha()] 
    return doc

# Function that will help us drop documents that have no word vectors in word2vec
def has_vector_representation(word2vec_model, doc):
    """check if at least one word of the document is in the
    word2vec dictionary"""
    return not all(word not in word2vec_model.vocab for word in doc)

# Filter out documents
def filter_docs(corpus, texts, condition_on_doc):
    """
    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.
    """
    number_of_docs = len(corpus)

    if texts is not None:
        texts = [text for (text, doc) in zip(texts, corpus)
                 if condition_on_doc(doc)]

    corpus = [doc for doc in corpus if condition_on_doc(doc)]

    # print("{} docs removed".format(number_of_docs - len(corpus)))

    return (corpus, texts)

raw_data['storypoint'][0]

# Preprocess the corpus
def preprocessing(data_list):
  corpus = [preprocess(title) for title in data_list]

  # Remove docs that don't include any words in W2V's vocab
  corpus, data_list = filter_docs(corpus, data_list, lambda doc: has_vector_representation(model, doc))

  # Filter out any empty docs
  corpus, data_list = filter_docs(corpus, data_list, lambda doc: (len(doc) != 0))

  y = []
  x = []
  for doc in corpus: # append the vector for each document
      print(corpus.index(doc))
      x.append(document_vector(model, doc))
      y.append(raw_data['storypoint'][corpus.index(doc)])
      
  X = np.array(x) # list to array
  y = np.array(y)
  return X, y

# Titles
X = preprocessing(titles_list)[0]
y = preprocessing(titles_list)[1]
print('Done preprocessing!')
# 80% of data goes to training
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)

"""XGBRegressor - using Titles of software issues/requests as data"""

from sklearn.metrics import mean_squared_error
import xgboost as xgb
# Instantiate an XGBRegressor
xgr = xgb.XGBRegressor(random_state=2)

# Fit the classifier to the training set
xgr.fit(X_train, y_train)

y_pred = xgr.predict(X_test)

mean_squared_error(y_test, y_pred)

# Accuracy
y_pred_rounded = [round(prediction,0) for prediction in y_pred ]
y_pred_rounded = [int(prediction) for prediction in y_pred_rounded]

from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix

acc_score = metrics.accuracy_score(y_test, y_pred_rounded)
print('Title Total accuracy classification score: {}'.format(acc_score))
print(str(confusion_matrix(y_test,y_pred_rounded)) + '\n')
print(str(classification_report( y_test,y_pred_rounded)) + '\n')

# """GaussianNB - - using Titles of software issues/requests as data"""
#
# # Build the classifier
# from sklearn.naive_bayes import GaussianNB
# clf = GaussianNB()
#
# # Fit the classifier to the training set
# clf.fit(X_train, y_train)
# y_pred = clf.predict(X_test)
#
# # Accuracy
# from sklearn import metrics
# acc_score = metrics.accuracy_score(y_test, y_pred)
# print('Total accuracy classification score: {}'.format(acc_score))

"""Using Descriptions of sotware issues/requests as data"""

# Descriptions
X = preprocessing(descriptions_list)[0]
y = preprocessing(descriptions_list)[1]
print('Done preprocessing!')
# 80% of data goes to training
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)

from sklearn.metrics import mean_squared_error
import xgboost as xgb
# Instantiate an XGBRegressor
xgr = xgb.XGBRegressor(random_state=2)

# Fit the classifier to the training set
xgr.fit(X_train, y_train)

y_pred = xgr.predict(X_test)

mean_squared_error(y_test, y_pred)

# Accuracy
y_pred_rounded = [round(prediction,0) for prediction in y_pred ]
y_pred_rounded = [int(prediction) for prediction in y_pred_rounded]

from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix

acc_score = metrics.accuracy_score(y_test, y_pred_rounded)
print('Description Total accuracy classification score: {}'.format(acc_score))
print(str(confusion_matrix(y_test,y_pred_rounded)) + '\n')
print(str(classification_report( y_test,y_pred_rounded)) + '\n')

# # Build the classifier
# from sklearn.naive_bayes import GaussianNB
# clf = GaussianNB()
#
# # Fit the classifier to the training set
# clf.fit(X_train, y_train)
# y_pred = clf.predict(X_test)
#
# # Accuracy
# from sklearn import metrics
# acc_score = metrics.accuracy_score(y_test, y_pred)
# print('Total accuracy classification score: {}'.format(acc_score))
#
# y_pred, y_test

