1,Allows CVS repo to timeout and report on locking issues,"Sometimes, when you perform a CVS action you get something like    {noformat}  cvs update: [01:38:32] waiting for mchai's lock in /cvsroot/atlassian/maven2test/bamboo  {noformat}    so Bamboo would probably just hang and become not so happy. We should allow Bamboo to timeout, or conditionally stop and tell the user how to dix the problem"
0,allow a build to be placed at the head of the build queue... (or edit the queue order),NULL
0,Result not saved when checkout fails,"Happened yesterday on our hosted bamboo: https://gdansk.bamboo2.atlassian.com/browse/INT-TST  Bamboo was upgraded from 2.0.5 to 2.15    I had build log messages like:  {noformat}  29-Jan-2009 03:12:23 	Build INT-TST-1067 started building on agent Default Agent  29-Jan-2009 03:12:23 	Updating source code to revision: 26986  29-Jan-2009 03:12:23 	Source found at  '/opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST'. Updating source...  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/test/java/com/atlassian/theplugin/idea  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/test/java/com/atlassian/theplugin  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/test/java/com/atlassian  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/test/java/com  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/test/java  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/test  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/main/java/com/atlassian/theplugin/idea/jira  29-Jan-2009 03:12:24 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/main/java/com/atlassian/theplugin/idea/crucible/tree  29-Jan-2009 03:12:24 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/main/java/com/atlassian/theplugin/idea/crucible/editor/OpenDiffToolAction.java  29-Jan-2009 03:12:24 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST/src/main/java/com/atlassian/theplugin/idea/crucible/editor/CrucibleEditorFactoryListener.java  29-Jan-2009 03:12:25 	Error occurred while executing the build for INT-TST-1067 : Unable to retrieve source code to '26986' for 'INT-TST'  29-Jan-2009 03:15:23 	Change detection found 11 changes  29-Jan-2009 03:15:23 	Build INT-TST-1068 started building on agent Default Agent  {noformat}    {noformat}  29-Jan-2009 03:30:13  	Build INT-TST2-1 started building on agent Default Agent  29-Jan-2009 03:30:13 	Updating source code to latest  29-Jan-2009 03:30:13 	Working directory '/opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2' is empty. Checking out SVN URL 'https://studio.atlassian.com/svn/PL/trunk/atlassian-intellij-connector'  29-Jan-2009 03:30:15 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/LICENSE  29-Jan-2009 03:30:15 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/atlassian-idea-plugin.ipr  29-Jan-2009 03:30:15 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/doc  29-Jan-2009 03:30:16 	Error occurred while executing the build for INT-TST2-1 : Failed to checkout source code to revision 'HEAD' for https://studio.atlassian.com/svn/PL/trunk/atlassian-intellij-connector  29-Jan-2009 03:34:06 	Manual build triggered by wseliga  29-Jan-2009 03:34:06 	Build INT-TST2-2 started building on agent Default Agent  29-Jan-2009 03:34:06 	Updating source code to latest  29-Jan-2009 03:34:06 	Source found at  '/opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2'. Updating source...  29-Jan-2009 03:34:09 	D         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/doc  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/docs  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian/theplugin  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian/theplugin/bamboo  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian/theplugin/bamboo/api  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian/theplugin/idea  29-Jan-2009 03:34:09 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian/theplugin/idea/GenericServerConfigurationFormTest.java  29-Jan-2009 03:34:09 	A         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-TST2/src/test/java/com/atlassian/theplugin/idea/bamboo  29-Jan-2009 03:34:10 	Error occurred while executing the build for INT-TST2-2 : Unable to retrieve source code to 'null' for 'INT-TST2  {noformat}    or    {noformat}  	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS/src/main/java/com/atlassian/theplugin/idea/crucible/CrucibleRevisionsNumber.java  29-Jan-2009 03:37:10 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS/src/main/java/com/atlassian/theplugin/idea/crucible/tree/ReviewItemTreePanel.java  29-Jan-2009 03:37:10 	          /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS/src/main/java/com/atlassian/theplugin/idea/crucible/tree  29-Jan-2009 03:37:10 	U         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS/src/main/java/com/atlassian/theplugin/idea/crucible/CrucibleCreatePostCommitReviewForm.java  29-Jan-2009 03:37:10 	D         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS/src/main/java/com/atlassian/theplugin/idea/crucible/editor/EditorDiffActionImpl.java  29-Jan-2009 03:37:10 	D         /opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS/src/main/java/com/atlassian/theplugin/idea/crucible/editor/EditorUpdateDiffActionImpl.java  29-Jan-2009 03:37:11 	Error occurred while executing the build for INT-CS-1177 : Unable to retrieve source code to '26987' for 'INT-CS'  29-Jan-2009 03:38:20 	Purging the checked out code from ""/opt/j2ee/domains/bamboo2.atlassian.com/gdansk/webapps/data/bamboo-2.0.2-attempt/xml-data/build-dir/INT-CS""  29-Jan-2009 03:38:30 	Manual build triggered by wseliga  29-Jan-2009 03:38:30 	Build INT-CS-1178 started building on agent Default Agent  29-Jan-2009 03:38:30 	Updating source code to revision: 26987  {noformat}    However executing svn co from command line on exactly the same host where Bamboo was installed (with the same credentials even) worked fine.    There were two problems:  * Bamboo did bump up build number and this build failed, but nowhere it was really shown. I.e. on dashboard (and via remote API) I still got last build as successful. And although my build number (in above logs) was now greater than 1000, Bamboo claimed on dashboard and via API that the last build is e.g. 500 (which happened to be the last one before these problems started)  * Cleaning working dir or full checkout did not help  * I solved this problem by full clean and using anonymous SVN access or my private account. Only then Bamboo finally showed new build on dashboard (with number > 1000) and its real status.    I suspect the problem may be also somehow connected to our SVN server which could have applied some security policies to avoid DOS attacks from Bamboo (Bamboo every 3 minutes - above 500 hundred times) was trying to checkout from svn the sources without success.  "
0,Talkback from elastic agent to bamboo server to include EBS volume mount results,When an elastic instance/agent is configured to mount a ebs volume containing a snapshot during startup the progress/success/fail should be fed back to bamboo server for display.  the log in /tmp/setupEbsSnapshot.log could be used to process progress and results.
1,"Users can see the maven module (groupid, artifactid, version) against each plan.",* Should respect the sub-working directory. Only look for poms under the sub-working directory  * Only read top level pom for modules  * Pom is read only when building.  * This is only read only  * Each plan may have multiple artifacts
0,PostBuildProcessor plugin to kick off the Maven pom.xml parsing,Consider running on remote agents.  Can functional test the PostBuilderProcessor plugin behavior.  
1,Get Bamboo functional tests passing in the cloud,NULL
1,Allow gadget server plugin to be incorporated into Bamboo.,"Does not include OAuth, nor SAL  Read this for more details:  http://extranet.atlassian.com/display/BAMBOO/Plugins+2+Estimate  "
1,Implement SAL part 1,The goal was Goal to get 100% compatible with the SAL compatibility kit however we are only tackling a subset of SAL implementation.  We should figure how much of the compatibility kit we are targeting.    NOTE - I changed this from 8 storypoints to 5 given that we are not tackling all of SAL.  
1,Implement SAL part 2,This is the remaining work for SAL.  See BAM-3631 for part 1.
0,Alert users of builds which can not be built by any agents,Bamboo should alert the user if a Plan can not be built for a period of time.  This should be set by default and overridden or removed by the user if necessary.    This will avoid the following:    !http://img.skitch.com/20090310-fk3547se8py75u3pr1g2g4mfux.jpg!
1,"Reliably Reproduce ""BAM-3244 Fix hanging agents""",NULL
1,Patch ActiveMQ to fix the code problem to fix hanging agent problem,"relies on BAM-3676.  this is the minimal patch that we could ship to customers.  this does NOT imply creating a activeMQ branch that Atlassian maintain, etc, etc."
2,Reproduce BAM-3643 - Extreme CPU spike on BEAC,We have little idea what this could be and have not had much success repro'ing it.
1,BAM-3495 Fixing borked status,NULL
1,Support bundled plugins in Bamboo,NULL
0,Add the ability to sleep before collecting artifacts to a bamboo agent,"When the build ends, bamboo sweeps up the artifacts.  I'd like a system property on the agent which could introducce a small sleep.  This would be to allow the build to settle down and save all its artifacts before the agent comes along to collect them."
1,Bamboo Specfic Functional Tests for SAL Intefaces,All the CTK testing plugin does is generic testing that the interfaces return something.   We need to write some Bamboo specific tests which ensure Bamboo is actually doing the right thing underneath those SAL calls.    
1,Implement SAL Interfaces Part 3,Implement the interfaces which didn't make it into the last milestone.  See this page for details on which items missed out   http://extranet.atlassian.com/display/BAMBOO/Bamboo+SAL+Compatability+-+Work+In+Progress
2,Spike - Deploy gadget server into Bamboo with a Hello world gadget to prove it,Estimates to include infrastructure  Assuming plugins 2 is done  depends on implement SAL pt 2  No Authentication mechanism yet  See where this can be consumed. Not sure if Ref App can consume.  Timebox this spike to 1 person iteration  
1,BAM-3674 (Regression) - Incorrect display of faiing since test,Investigation required  Keep Sam Berlin Happy  To be done on stable branch for 2.2.2  
0,BAM-3717 (Regression) - :: in test names throws error,Need to escape properly  To be done on stable branch for 2.2.2  
0,Implement Bundled Plugins Part 2,Delivery mechanism for the Gadget server.    Try plugin that is known to work (SAL)  
0,REST Module type plugin point,Drop in the plugin   
1,REST endpoint for build status gadget,Requires mockups done  
1,Gadget - User can authenticate through a gadget,This means manual username/password must be inputted. Should use basic auth    Not OAuth (will be obselete with OAuth)  
1,Gadget - User can see the status of favourite plans ,interactions are links back to the server only    Edwin to get UI teams to give some mock ups  
1,Gadget - User can see the status of the recent 5 builds of favourite plans in the existing status gadget,Just the display. REST should provide info    Need mockup for this - story assume that mockup is there already  
1,Gadget - User can select which plans they want to watch,"Selection based on read permissions of the authenticated user    Can pick between individual plans or their favourite plans    Whilst selecting, the favourited should still be indicated (up top)    Storage of configuraiton on the gadge consumer  "
0,Upgrade default AMI OS to newer version of Fedora Linux - see reduced scope of this issue,"For the current default AMI (ami-3c3dda55 in case of 2.2.1) there are many updates pending and it isn't wise to do it on each instance startup - see:    {noformat}  [root@domU-12-31-39-03-75-B2 bamboo-ebs]# yum update  Loading ""fastestmirror"" plugin  Loading mirror speeds from cached hostfile   * fedora: archive.fedoraproject.org   * updates: archive.fedoraproject.org  Setting up Update Process  Resolving Dependencies  --> Running transaction check  ---> Package fedora-release.noarch 0:8-6.transition set to be updated  --> Finished Dependency Resolution    Dependencies Resolved    =============================================================================   Package                 Arch       Version          Repository        Size  =============================================================================  Updating:   fedora-release          noarch     8-6.transition   updates            31 k    Transaction Summary  =============================================================================  Install      0 Package(s)  Update       1 Package(s)  Remove       0 Package(s)    Total download size: 31 k  Is this ok [y/N]: y  Downloading Packages:  (1/1): fedora-release-8-6 100% |=========================|  31 kB    00:00  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Updating  : fedora-release               ######################### [1/2]    Cleanup   : fedora-release               ######################### [2/2]    Updated: fedora-release.noarch 0:8-6.transition  Complete!  [root@domU-12-31-39-03-75-B2 bamboo-ebs]# yum update  Loading ""fastestmirror"" plugin  Loading mirror speeds from cached hostfile   * updates-newkey: archive.fedoraproject.org   * fedora: archive.fedoraproject.org   * updates: archive.fedoraproject.org  updates-newkey            100% |=========================| 2.3 kB    00:00  primary.sqlite.bz2        100% |=========================| 3.7 MB    00:07  Setting up Update Process  Resolving Dependencies  --> Running transaction check  ---> Package ntp.i386 0:4.2.4p4-1.fc8 set to be updated  ---> Package vim-enhanced.i386 2:7.1.245-1.fc8 set to be updated  ---> Package openldap.i386 0:2.3.39-4.fc8 set to be updated  ---> Package fuse-devel.i386 0:2.7.4-1.fc8 set to be updated  ---> Package bind-utils.i386 32:9.5.0-29.P2.fc8 set to be updated  ---> Package python-libs.i386 0:2.5.1-26.fc8.2 set to be updated  ---> Package pcre.i386 0:7.3-4.fc8 set to be updated  ---> Package yum.noarch 0:3.2.20-5.fc8 set to be updated  --> Processing Dependency: pygpgme for package: yum  ---> Package ruby.i386 0:1.8.6.287-2.fc8 set to be updated  ---> Package logwatch.noarch 0:7.3.6-22.fc8 set to be updated  ---> Package kernel-xen.i686 0:2.6.21.7-5.fc8 set to be installed  ---> Package util-linux-ng.i386 0:2.13.1-2.fc8 set to be updated  ---> Package vim-minimal.i386 2:7.1.245-1.fc8 set to be updated  ---> Package vixie-cron.i386 4:4.2-9.fc8 set to be updated  ---> Package alsa-lib.i386 0:1.0.16-3.fc8 set to be updated  ---> Package iptables.i386 0:1.4.1.1-2.fc8 set to be updated  ---> Package libxml2.i386 0:2.7.2-2.fc8 set to be updated  ---> Package libpng.i386 2:1.2.33-1.fc8 set to be updated  ---> Package nss.i386 0:3.12.2.0-1.1.fc8 set to be updated  ---> Package paps-libs.i386 0:0.6.8-8.fc8 set to be updated  ---> Package libacl.i386 0:2.2.39-14.fc8 set to be updated  ---> Package openobex.i386 0:1.3-12.fc8 set to be updated  ---> Package libnfnetlink.i386 0:0.0.39-3.fc8 set to be updated  ---> Package acl.i386 0:2.2.39-14.fc8 set to be updated  ---> Package microcode_ctl.i386 1:1.17-1.39.fc8 set to be updated  ---> Package hal-libs.i386 0:0.5.10-5.fc8 set to be updated  ---> Package pygobject2.i386 0:2.14.2-1.fc8 set to be updated  ---> Package rsync.i386 0:2.6.9-5.fc8 set to be updated  ---> Package ruby-irb.i386 0:1.8.6.287-2.fc8 set to be updated  ---> Package bluez-libs.i386 0:3.35-1.fc8 set to be updated  ---> Package bind-libs.i386 32:9.5.0-29.P2.fc8 set to be updated  ---> Package system-config-network-tui.noarch 0:1.5.10-1.fc8 set to be updated  ---> Package beecrypt.i386 0:4.1.2-17.fc8 set to be updated  ---> Package gtk2.i386 0:2.12.8-2.fc8 set to be updated  ---> Package fuse-libs.i386 0:2.7.4-1.fc8 set to be updated  ---> Package libedit.i386 0:2.11-1.20080712cvs.fc8 set to be updated  ---> Package ed.i386 0:1.1-1.fc8 set to be updated  ---> Package NetworkManager-glib.i386 1:0.7.0-0.12.svn4326.fc8 set to be updated  --> Processing Dependency: NetworkManager = 1:0.7.0-0.12.svn4326.fc8 for package: NetworkManager-glib  ---> Package db4.i386 0:4.6.21-3.fc8 set to be updated  ---> Package dhcpv6-client.i386 0:0.10-52.fc8 set to be updated  ---> Package freetype.i386 0:2.3.5-5.fc8 set to be updated  ---> Package python-urlgrabber.noarch 0:3.0.0-9.fc8 set to be updated  ---> Package coreutils.i386 0:6.9-19.fc8 set to be updated  ---> Package e2fsprogs.i386 0:1.40.4-3.fc8 set to be updated  ---> Package psmisc.i386 0:22.6-2.1.fc8 set to be updated  ---> Package libsysfs.i386 0:2.1.0-4.fc8 set to be updated  ---> Package audit-libs.i386 0:1.7.4-1.fc8 set to be updated  ---> Package libglade2.i386 0:2.6.2-4.fc8 set to be updated  ---> Package quota.i386 1:3.14-2.fc8 set to be updated  ---> Package cups-libs.i386 1:1.3.9-2.fc8 set to be updated  ---> Package perl-libs.i386 4:5.8.8-41.fc8 set to be updated  ---> Package fedora-gnome-theme.noarch 0:8.0.0-2.fc8 set to be updated  ---> Package libdhcp6client.i386 0:0.10-52.fc8 set to be updated  ---> Package iptables-ipv6.i386 0:1.4.1.1-2.fc8 set to be updated  ---> Package libtiff.i386 0:3.8.2-11.fc8 set to be updated  ---> Package autofs.i386 1:5.0.2-31 set to be updated  ---> Package ypbind.i386 3:1.20.4-4.fc8 set to be updated  ---> Package anacron.i386 0:2.3-58.fc8 set to be updated  ---> Package control-center-filesystem.i386 1:2.20.3-3.fc8 set to be updated  ---> Package nfs-utils.i386 1:1.1.1-2.fc8 set to be updated  ---> Package smartmontools.i386 1:5.38-1.fc8 set to be updated  ---> Package wget.i386 0:1.11.1-1.fc8 set to be updated  ---> Package pm-utils.i386 0:0.99.4-19.fc8 set to be updated  ---> Package sudo.i386 0:1.6.9p4-6.fc8 set to be updated  ---> Package popt.i386 0:1.13-4.fc8 set to be updated  ---> Package shadow-utils.i386 2:4.0.18.1-22.fc8 set to be updated  ---> Package dbus-glib.i386 0:0.73-8.fc8 set to be updated  ---> Package traceroute.i386 3:2.0.12-1.fc8 set to be updated  ---> Package kernel-headers.i386 0:2.6.26.8-57.fc8 set to be updated  ---> Package ruby-rdoc.i386 0:1.8.6.287-2.fc8 set to be updated  ---> Package info.i386 0:4.11-5.fc8 set to be updated  ---> Package pam_krb5.i386 0:2.2.18-2.fc8 set to be updated  ---> Package bluez-utils.i386 0:3.35-5.fc8 set to be updated  ---> Package curl.i386 0:7.18.2-7.fc8 set to be updated  --> Processing Dependency: libssh2.so.1 for package: curl  ---> Package gnupg.i386 0:1.4.7-10.fc8 set to be updated  ---> Package hal.i386 0:0.5.10-5.fc8 set to be updated  ---> Package file-libs.i386 0:4.21-6.fc8 set to be updated  ---> Package bzip2-libs.i386 0:1.0.4-13.fc8 set to be updated  ---> Package rubygems.noarch 0:1.2.0-2.fc8 set to be updated  ---> Package perl.i386 4:5.8.8-41.fc8 set to be updated  ---> Package PolicyKit.i386 0:0.6-2.fc8 set to be updated  ---> Package aspell.i386 12:0.60.5-4.fc8 set to be updated  ---> Package e2fsprogs-libs.i386 0:1.40.4-3.fc8 set to be updated  ---> Package ruby-libs.i386 0:1.8.6.287-2.fc8 set to be updated  ---> Package shared-mime-info.i386 0:0.23-2.fc8 set to be updated  ---> Package yum-fastestmirror.noarch 0:1.1.17-1.fc8 set to be updated  ---> Package ntfs-3g.i386 2:1.5012-4.fc8 set to be updated  ---> Package iproute.i386 0:2.6.26-2.fc8 set to be updated  ---> Package stunnel.i386 0:4.24-0.fc8 set to be updated  ---> Package fuse.i386 0:2.7.4-1.fc8 set to be updated  ---> Package elfutils-libelf.i386 0:0.137-3.fc8 set to be updated  ---> Package libxcb.i386 0:1.1-1.1.fc8 set to be updated  ---> Package wpa_supplicant.i386 1:0.5.10-5.fc8 set to be updated  ---> Package libxslt.i386 0:1.1.24-2.fc8 set to be updated  ---> Package libgnomecanvas.i386 0:2.20.1-3.fc8 set to be updated  ---> Package tzdata.noarch 0:2008i-1.fc8 set to be updated  ---> Package fuse-sshfs.i386 0:2.2-1.fc8 set to be updated  ---> Package gnutls.i386 0:1.6.3-5.fc8 set to be updated  ---> Package pciutils.i386 0:2.2.9-2.fc8 set to be updated  ---> Package glib2.i386 0:2.14.6-2.fc8 set to be updated  ---> Package nss-tools.i386 0:3.12.2.0-1.1.fc8 set to be updated  ---> Package mlocate.i386 0:0.21-1.fc8 set to be updated  ---> Package bzip2.i386 0:1.0.4-13.fc8 set to be updated  ---> Package fuse-encfs.i386 0:1.5-3.fc8 set to be updated  --> Processing Dependency: libboost_serialization-mt.so.3 for package: fuse-encfs  --> Processing Dependency: libboost_filesystem-mt.so.3 for package: fuse-encfs  ---> Package hal-info.noarch 0:20080607-2.fc8 set to be updated  ---> Package cups.i386 1:1.3.9-2.fc8 set to be updated  ---> Package eject.i386 0:2.1.5-8.fc8 set to be updated  ---> Package make.i386 1:3.81-11.fc8 set to be updated  ---> Package nspr.i386 0:4.7.3-1.fc8 set to be updated  ---> Package vim-common.i386 2:7.1.245-1.fc8 set to be updated  ---> Package file.i386 0:4.21-6.fc8 set to be updated  ---> Package paps.i386 0:0.6.8-8.fc8 set to be updated  ---> Package python.i386 0:2.5.1-26.fc8.2 set to be updated  --> Running transaction check  ---> Package libssh2.i386 0:0.18-5.fc8 set to be updated  ---> Package NetworkManager.i386 1:0.7.0-0.12.svn4326.fc8 set to be updated  --> Processing Dependency: ppp >= 2.2.4 for package: NetworkManager  --> Processing Dependency: dnsmasq for package: NetworkManager  --> Processing Dependency: avahi-autoipd for package: NetworkManager  ---> Package pygpgme.i386 0:0.1-6.fc8 set to be updated  --> Processing Dependency: libgpgme.so.11(GPGME_1.0) for package: pygpgme  --> Processing Dependency: libgpgme.so.11 for package: pygpgme  ---> Package boost.i386 0:1.34.1-7.fc8 set to be updated  --> Processing Dependency: libicuuc.so.38 for package: boost  --> Processing Dependency: libicui18n.so.38 for package: boost  --> Running transaction check  ---> Package gpgme.i386 0:1.1.5-4.fc8 set to be updated  --> Processing Dependency: gnupg2 for package: gpgme  --> Processing Dependency: libpth.so.20 for package: gpgme  ---> Package avahi-autoipd.i386 0:0.6.21-8.fc8 set to be updated  ---> Package libicu.i386 0:3.8-6.fc8 set to be updated  ---> Package ppp.i386 0:2.4.4-2 set to be updated  ---> Package dnsmasq.i386 0:2.40-1.fc8 set to be updated  --> Running transaction check  ---> Package gnupg2.i386 0:2.0.9-2.fc8 set to be updated  --> Processing Dependency: libksba.so.8(KSBA_0.9) for package: gnupg2  --> Processing Dependency: dirmngr for package: gnupg2  --> Processing Dependency: libksba.so.8 for package: gnupg2  --> Processing Dependency: pinentry for package: gnupg2  ---> Package pth.i386 0:2.0.7-3 set to be updated  --> Running transaction check  ---> Package dirmngr.i386 0:1.0.0-6.fc8 set to be updated  ---> Package libksba.i386 0:1.0.2-3.fc8 set to be updated  ---> Package pinentry.i386 0:0.7.4-1.fc8 set to be updated  --> Finished Dependency Resolution    Dependencies Resolved    =============================================================================   Package                 Arch       Version          Repository        Size  =============================================================================  Installing:   NetworkManager-glib     i386       1:0.7.0-0.12.svn4326.fc8  updates-newkey    167 k       replacing  NetworkManager.i386 1:0.7.0-0.6.7.svn3235.fc8     kernel-xen              i686       2.6.21.7-5.fc8   updates-newkey     17 M  Updating:   PolicyKit               i386       0.6-2.fc8        updates-newkey     76 k   acl                     i386       2.2.39-14.fc8    updates-newkey     69 k   alsa-lib                i386       1.0.16-3.fc8     updates-newkey    402 k   anacron                 i386       2.3-58.fc8       updates-newkey     39 k   aspell                  i386       12:0.60.5-4.fc8  updates-newkey    969 k   audit-libs              i386       1.7.4-1.fc8      updates-newkey     73 k   autofs                  i386       1:5.0.2-31       updates-newkey    865 k   beecrypt                i386       4.1.2-17.fc8     updates-newkey    144 k   bind-libs               i386       32:9.5.0-29.P2.fc8  updates-newkey    951 k   bind-utils              i386       32:9.5.0-29.P2.fc8  updates-newkey    187 k   bluez-libs              i386       3.35-1.fc8       updates-newkey     59 k   bluez-utils             i386       3.35-5.fc8       updates-newkey    469 k   bzip2                   i386       1.0.4-13.fc8     updates-newkey     49 k   bzip2-libs              i386       1.0.4-13.fc8     updates-newkey     37 k   control-center-filesystem  i386       1:2.20.3-3.fc8   updates-newkey     34 k   coreutils               i386       6.9-19.fc8       updates-newkey    3.3 M   cups                    i386       1:1.3.9-2.fc8    updates-newkey    3.5 M   cups-libs               i386       1:1.3.9-2.fc8    updates-newkey    197 k   curl                    i386       7.18.2-7.fc8     updates-newkey    293 k   db4                     i386       4.6.21-3.fc8     updates-newkey    587 k   dbus-glib               i386       0.73-8.fc8       updates-newkey    160 k   dhcpv6-client           i386       0.10-52.fc8      updates-newkey     85 k   e2fsprogs               i386       1.40.4-3.fc8     updates-newkey    610 k   e2fsprogs-libs          i386       1.40.4-3.fc8     updates-newkey    138 k   ed                      i386       1.1-1.fc8        updates-newkey     71 k   eject                   i386       2.1.5-8.fc8      updates-newkey     51 k   elfutils-libelf         i386       0.137-3.fc8      updates-newkey     58 k   fedora-gnome-theme      noarch     8.0.0-2.fc8      updates-newkey     10 k   file                    i386       4.21-6.fc8       updates-newkey     29 k   file-libs               i386       4.21-6.fc8       updates-newkey    309 k   freetype                i386       2.3.5-5.fc8      updates-newkey    331 k   fuse                    i386       2.7.4-1.fc8      updates-newkey     83 k   fuse-devel              i386       2.7.4-1.fc8      updates-newkey     27 k   fuse-encfs              i386       1.5-3.fc8        updates-newkey    373 k   fuse-libs               i386       2.7.4-1.fc8      updates-newkey     71 k   fuse-sshfs              i386       2.2-1.fc8        updates-newkey     49 k   glib2                   i386       2.14.6-2.fc8     updates-newkey    850 k   gnupg                   i386       1.4.7-10.fc8     updates-newkey    1.9 M   gnutls                  i386       1.6.3-5.fc8      updates-newkey    397 k   gtk2                    i386       2.12.8-2.fc8     updates-newkey    6.8 M   hal                     i386       0.5.10-5.fc8     updates-newkey    461 k   hal-info                noarch     20080607-2.fc8   updates-newkey    118 k   hal-libs                i386       0.5.10-5.fc8     updates-newkey     61 k   info                    i386       4.11-5.fc8       updates-newkey    166 k   iproute                 i386       2.6.26-2.fc8     updates-newkey    843 k   iptables                i386       1.4.1.1-2.fc8    updates-newkey    309 k   iptables-ipv6           i386       1.4.1.1-2.fc8    updates-newkey    192 k   kernel-headers          i386       2.6.26.8-57.fc8  updates-newkey    753 k   libacl                  i386       2.2.39-14.fc8    updates-newkey     20 k   libdhcp6client          i386       0.10-52.fc8      updates-newkey     79 k   libedit                 i386       2.11-1.20080712cvs.fc8  updates-newkey     79 k   libglade2               i386       2.6.2-4.fc8      updates-newkey     64 k   libgnomecanvas          i386       2.20.1-3.fc8     updates-newkey    228 k   libnfnetlink            i386       0.0.39-3.fc8     updates-newkey     23 k   libpng                  i386       2:1.2.33-1.fc8   updates-newkey    249 k   libsysfs                i386       2.1.0-4.fc8      updates-newkey     47 k   libtiff                 i386       3.8.2-11.fc8     updates-newkey    307 k   libxcb                  i386       1.1-1.1.fc8      updates-newkey    124 k   libxml2                 i386       2.7.2-2.fc8      updates-newkey    828 k   libxslt                 i386       1.1.24-2.fc8     updates-newkey    526 k   logwatch                noarch     7.3.6-22.fc8     updates-newkey    319 k   make                    i386       1:3.81-11.fc8    updates-newkey    477 k   microcode_ctl           i386       1:1.17-1.39.fc8  updates-newkey    437 k   mlocate                 i386       0.21-1.fc8       updates-newkey     75 k   nfs-utils               i386       1:1.1.1-2.fc8    updates-newkey    281 k   nspr                    i386       4.7.3-1.fc8      updates-newkey    119 k   nss                     i386       3.12.2.0-1.1.fc8  updates-newkey    1.2 M   nss-tools               i386       3.12.2.0-1.1.fc8  updates-newkey    1.2 M   ntfs-3g                 i386       2:1.5012-4.fc8   updates-newkey    192 k   ntp                     i386       4.2.4p4-1.fc8    updates-newkey    1.4 M   openldap                i386       2.3.39-4.fc8     updates-newkey    294 k   openobex                i386       1.3-12.fc8       updates-newkey     37 k   pam_krb5                i386       2.2.18-2.fc8     updates-newkey    128 k   paps                    i386       0.6.8-8.fc8      updates-newkey     32 k   paps-libs               i386       0.6.8-8.fc8      updates-newkey     23 k   pciutils                i386       2.2.9-2.fc8      updates-newkey     93 k   pcre                    i386       7.3-4.fc8        updates-newkey    137 k   perl                    i386       4:5.8.8-41.fc8   updates-newkey     12 M   perl-libs               i386       4:5.8.8-41.fc8   updates-newkey    570 k   pm-utils                i386       0.99.4-19.fc8    updates-newkey     44 k   popt                    i386       1.13-4.fc8       updates-newkey     39 k   psmisc                  i386       22.6-2.1.fc8     updates-newkey     72 k   pygobject2              i386       2.14.2-1.fc8     updates-newkey    104 k   python                  i386       2.5.1-26.fc8.2   updates-newkey    4.8 M   python-libs             i386       2.5.1-26.fc8.2   updates-newkey    568 k   python-urlgrabber       noarch     3.0.0-9.fc8      updates-newkey    112 k   quota                   i386       1:3.14-2.fc8     updates-newkey    357 k   rsync                   i386       2.6.9-5.fc8      updates-newkey    232 k   ruby                    i386       1.8.6.287-2.fc8  updates-newkey    525 k   ruby-irb                i386       1.8.6.287-2.fc8  updates-newkey    285 k   ruby-libs               i386       1.8.6.287-2.fc8  updates-newkey    1.7 M   ruby-rdoc               i386       1.8.6.287-2.fc8  updates-newkey    353 k   rubygems                noarch     1.2.0-2.fc8      updates-newkey    598 k   shadow-utils            i386       2:4.0.18.1-22.fc8  updates-newkey    1.1 M   shared-mime-info        i386       0.23-2.fc8       updates-newkey    166 k   smartmontools           i386       1:5.38-1.fc8     updates-newkey    323 k   stunnel                 i386       4.24-0.fc8       updates-newkey    125 k   sudo                    i386       1.6.9p4-6.fc8    updates-newkey    235 k   system-config-network-tui  noarch     1.5.10-1.fc8     updates-newkey    1.8 M   traceroute              i386       3:2.0.12-1.fc8   updates-newkey     50 k   tzdata                  noarch     2008i-1.fc8      updates-newkey    756 k   util-linux-ng           i386       2.13.1-2.fc8     updates-newkey    2.0 M   vim-common              i386       2:7.1.245-1.fc8  updates-newkey    6.6 M   vim-enhanced            i386       2:7.1.245-1.fc8  updates-newkey    862 k   vim-minimal             i386       2:7.1.245-1.fc8  updates-newkey    346 k   vixie-cron              i386       4:4.2..."
1,Gadget - User can view a chart of recent percentage of builds successful,"Select plans  Grouped by time period, configurable.  Can filter by time period (Last 7 days, Last 30 days, Last 90 days, Last 25 builds, Show all builds)  "
1,Prevent a build from running if its ancestors are in queue or running,"E.g.  B depends on A  if B is triggered whilst A building, B trigger is ignored.    This will mean that further checkins against B won't run.    Have to be optional.  Must only block if the parent is the same revision as the child that is about to build, otherwise, the child might never build.  "
0,User can configure dependency blocking,Applicable per child plan  User can configure whether dependency children are blocked from building when ancestors are running.
2,Check ancestors of a build when it is triggered and prevent building if changes found (in ancestors) Ancestors are kicked off,"E.g.  B triggered, whilst A is not building. Check A, if changes found in A, build A, don't build B.    What if A is a manual and dependent builds only? or a schedule build?    Ignore the trigger for now. Later optimizations needed.  "
0,Detect infinte cycles in dependent builds.,"Should not allow users to input cyclic dependency configurations    If customers have already done cyclic dependencies, we don't do anything in the upgrade task. We show an error when we try to show the tree.  "
1,User can see in text its ancestors and children for the build result,See all parents  See all children   
1,Get a gadget to authenticate end to end via Oauth,Takes away the need to manually input username/password    
1,Enable user configuraiton of OAuth in Bamboo.,NULL
0,Gadget - chart of queued time,NULL
1,Notfication when a build has been in the queue for x minutes,Similar to a build hung notification but for builds in the queue
1,"""Test failing since"" doesn't properly consider builds without test results","h3. Case A    Consider a series of builds:    BUILD-1  Test X passed  BUILD-2  Test X failed  BUILD-3  Build failure, no test results  BUILD-4  Test X failed  BUILD-5  Test X failed    In the results of BUILD-4 and BUILD-5, Bamboo reports that test X has been failing since BUILD-4.  From this, developers incorrectly infer that the test passed in BUILD-3 and the change that caused this failure was introduced in BUILD-4.  In this case, it would be better to report that the test has been failing since BUILD-2.    h3. Case B    There's another interesting case that needs to be considered:    BUILD-6  Test X passed  BUILD-7  Build failure, no test results  BUILD-8  Build failure, no test results  BUILD-9  Test X failed    Here, the failure of test X could have been introduced in BUILD-7, BUILD-8 or BUILD-9.  In this case, perhaps it's best to report that Test X has been failing since BUILD-7?"
1,Bandana import/export,NULL
1,Fix i18n for gadgets,NULL
1,Provide feedback in UI about builds being blocked (build dependencies),NULL
1,Tests for REST,NULL
1,Selenium tests (with Hamish),NULL
1,Bucket - Gadget issues,NULL
0,Dependency blocking for polling builds,NULL
1,Configure agent capabilities for each Elastic Configuration,NULL
0,"Allow ""at least"" and ""maximum #"" of agent on schedule",NULL
1,Make it possible (via instructions) for a customer to 'Bamboo enable' an AMI,As per Krystian's comments - http://jira.atlassian.com/browse/BAM-3766?focusedCommentId=160191&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_160191 we want to make it possible for a customer to follow instructions/install software so that an instances of an AMI can be spawned by Bamboo server and have elastic agents start up on them.    This is will get us out of the endless cycle of upgrading the default AMI.
1,Add support for EC2 Availability Zones,See here for what we are talking about - http://developer.amazonwebservices.com/connect/entry.jspa?externalID=1347    Availability zone support is a pre-requisite for [Reserved Instance|http://aws.amazon.com/ec2/faqs/#What_is_a_Reserved_Instance] support.    We should have a default availability zone (for the Bamboo install) and be able to specify the avail zone per Elastic Configuration.
0,Rename ElasticAgentSchedule to ElasticInstanceSchedule,Also rework associated UI text
0,Roundtrip error w/ input validation during EC2 instance startup,Round trip errors occur on the EC2/instance screen when an invalid number of instances are requested
0,Ensure that all commands when mounting EBS volumes handle failure recursively,"EBS volume creation, attachment & mounting is still depending on various variable timing from Amazon (especially when multiple volumes are stared simultaneously).    We should ensure that we cater for failure and perform retries for all commands."
0,Please make a Maven 2.1.0 builder available on the EC2 image,Confluence need Maven 2.1.0 specifically to build confluence.
0,"Set the ""Last Modified"" header for artifacts, so browsers can cache static resources","Since most (all?) build artifacts are static, and probably never change once they are created, Bamboo should set the ""Last Modified"" header when serving any artifact.  The timestamp from the file stored on disk could be used, or if that is unavailable - the time stamp of when the build finished?    !http://img.skitch.com/20090819-me3phuunma1um3acd6jpdddkxt.jpg!    Large artifacts such as the JSON needed for Clover's new Tree Map could greatly benefit from browsers doing the needful caching."
0,"The ""no more than"" instance schedule seems to shut down more instances than it should","Having a schedule configured as ""no more than"" e.g. ""no more than 4"" seems to kill all available instances, since it seems that it looks if the instance is on list (even if already scheduled to be shut down - agent stopped, but the instance is still on the list) instead of checking if given instance to be shut down has agent present or not."
0,Failed EBS volume creaion should start as disabled,"If startup of EBS volume fails for an instance, the Agent should start as disabled.    "
0,"Rename ""Bamboo Builds"" tab in JIRA Bamboo plugin to be ""Builds""",NULL
1,Allow a clover license to be entered via the Bamboo admin (ie a global license).  License can be overridden at the plan config level,"* on the build config screen if a ""global"" license exists then we display a message indicating that a license is already configured.  * on the build config screen if a ""global"" license exists AND the license is valid then we display a message indicating that a license is valid.  * on the build config screen if a ""global"" license exists AND the license is invalid then we display a message indicating that a license is NOT valid  * on the build config screen if NO ""global"" license exists then a text entry field should be available and a message indicating that a license is required should be displayed.  * a text entry field should be available on the build config screen to allow the license to be overridden at the plan level.  * If _any_ license is defined at a plan level, it takes precedence    * Check interaction with clover.license in the settings.xml*  Validation of lic. is optional nice to have iff extras supports decoding.  "
0,Add path to clover gadget to the list of gadgets available,NULL
0,Update Dependencies to match jira's (again),"once jira has finalised its own dependencies...    To ensure gadgets actually work we will probably need to ensure that our oauth, atlasian-plugins and atlassian-gadgets libraries are updated to the same version as jira 4.0 and then double check that all the gadgets work."
0,ship version 2.1.15 of nant builder plugin with bamboo 2.3.x,Changes in the plugin api for bamboo 2.3x have broken the nant builder plugin    http://developer.atlassian.com/jira/browse/NANT-24    A new version of the plugin is [available|http://confluence.atlassian.com/download/attachments/47448218/nant_builder-2.1.15.jar?version=1&modificationDate=1251978264907]
3,Spike: Concurrent chains running,NULL
1,Quick and dirty way to add a plan to a stage in a chain,"Suggestion here is to have a ""chain and stage"" selector just on the plan edit page.  No need to hide anything on the plan edit page."
3,Spike: Extract common behaviour from Plan and Chain,NULL
0,Quick and dirty list of chains,NULL
0,Package http://datejs.googlecode.com/files/date.js for gadgets,The gadgets should not use http://datejs.googlecode.com/files/date.js directly
1,Faster Setup: Minimize screens for setup wizard - express vs custom installs,NULL
0,Faster Setup: Immediately login as admin user after setup wizard,NULL
1,Faster Setup: Make it obvious where to go next after login,NULL
0,Faster Setup: Remove/Reword supportability wording of HSQLDB,NULL
1,External DB: Bundle database drivers,NULL
1,WIP Creation: Allow Save after builders entry,NULL
0,WIP Creation: Option to not build immediately after plan create,NULL
1,Faster Wizard: Setting up Builders/JDKs inline,NULL
0,Inline adding of IM,NULL
0,Faster Wizard: Inline create mail sever,NULL
1,Faster Setup: Visual indication for long running tasks in Setup Wizard,NULL
0,Re-enable the maven tab to see if it still works on single module projects.,NULL
0,"Show the snapshot dependencies at the top of the maven tab, ie update the maven dependency parser so that it can just provide snapshot dependencies.",NULL
1,"introduce a ""generated"" type of dependency (as opposed to a manually created).",NULL
1,generated dependencies are created after builds complete based on maven dependency info.,NULL
1,Check and make the maven tab work on multi module projects.,NULL
1,Clean up chain level Repo information,NULL
0,Installer not setting bamboo home,after installing bamboo using the 2.5 M2 windows installer bamboo does not start because bamboo.home is not set.  I checked bamboo-init.properties and it has not been updated at all.  See install4j logs
0,"Provide persistence to abstract ""artifacts/particles"" consumed and produced by plan.",NULL
0,Allow user to specify location of pom file,NULL
1,Spiking read pom before build occurs using maven 3 embedder - svn only,NULL
1,Unchecking use maven dependencies option should actually clear maven depedencies data. Confirmation for this should happen.,NULL
1,Miscellaneous UI ugliness,Remembering which tab you're on for configuraiton  Tooltips broken  Styling for tabs for build configuration  Dropdown build menu is too narrow.  Build Configuration tabs no longer check if there is data not saved before navigating away from the tab.
0,View a build that didn't run because of a failed prior stage in the chain,NULL
0,Clicking the run page should go to the live results page.,NULL
0,Redirect to last result of the chain if no chains currently running,NULL
0,Build blocking for chain dependencies,NULL
0,Automatically turn on build blocking when turning on maven dependencies with warning,NULL
3,Spike backend support up to 100 remote agents,Acceptance criteria:    * 100 remote agents can concurrently connect to one server.  * The 100 remote agents include both remote and elastic agents.  * 100 builds can concurrently run
0,Make mail server template more compact,NULL
1,update page inline rather than reload after adding builder/jdk/mail/im,NULL
1,User can specify a pom.xml for plans to be created from,NULL
1,Resolve concurrent chains issue with hack,NULL
1,Bamboo can parse the pom.xml plan details,NULL
0,Upgrade to AUI 2.0.3,"AUI 2.0 has changes that are not currently compatible with Bamboo's js, especially the restrictions they have imposed where the selector $ is no longer allowed    This is needed for 2009-11 Upgrade Pack."
1,Dependency block should block on parents as well,NULL
1,Styling for tabs for build configuration,NULL
0,Maven dependency on by default(?),NULL
1,Maven dependency tab clean up,NULL
1,Show REST & Gadgets URLs from within the UI,"* REST URL should appear where we enable the Remote API part of the description)  * Add new section with page ""List of Gadget URLs"" & OAuth  * Minimal req. URLS & descriptions, other stuff would be cool  "
1,Add a function to remove OAuth tokens in Bamboo,"Please add a function to remove OAuth tokens from Bamboo.    This is already implemented in JIRA, if that's a helpful reference.    http://confluence.atlassian.com/display/JIRA/Configuring+OAuth+Consumers#ConfiguringOAuthConsumers-RemovinganOAuthconsumer    "
1,REST: Incorrect representation of JSON data,JSON data which is returned from the builds rest api service is incorrect. Please see screenshots.    Example: https://bamboo.extranet.atlassian.com/rest/api/latest/build/CRUC.json?expand=builds.build
0,Plan requirements are not updated after builder JDK has been updated.,An update in a plan's builder spec is not reflected in the plan's requirements.  For example after changing the JDK from 1.5 to 1.6 in the builder spec the requirements tab will still show   JDK 1.5  
0,Removing build results from a menu should have a confirmation screen,"* Deleting from list page currently uses JS popup  * Prefer JS popup but fine to add confirmation screen (just be consistent with each other)  * Should work with the class=""confirmationRequired""  * Fix wording o be conssitent"
0,The contact administrators link should not be available is the setup footer,NULL
0,"When editing plans ""Done"" button in artifacts doesn't work",* Should go to view version of the artifacts screen
0,"Artifact ""Clover Report (System)"" is added even if the Clover is not enabled",NULL
1,Problem when importing a plan from Maven for a second time,"* Not sure if both plans were the same URL.    {noformat}  2009-11-29 18:45:41,330 INFO [1085098145@qtp-1013059835-7] [AccessLogFilter] admin http://xxxxgold.local:8085/bamboo/admin/importMavenPlanCreatePlan.action 64565kb  2009-11-29 18:45:41,363 WARN [1085098145@qtp-1013059835-7] [JDBCExceptionReporter] SQL Error: -104, SQLState: 23000  2009-11-29 18:45:41,364 ERROR [1085098145@qtp-1013059835-7] [JDBCExceptionReporter] Violation of unique constraint $$: duplicate value(s) for column(s) $$: SYS_CT_108 in statement [insert into BUILD_DEFINITION (BUILD_ID, CREATED_DATE, UPDATED_DATE, XML_DEFINITION_DATA, BUILD_DEFINITION_TYPE, BUILD_DEFINITION_ID) values (?, ?, ?, ?, 'BUILD', ?)]  2009-11-29 18:45:41,364 ERROR [1085098145@qtp-1013059835-7] [SessionImpl] Could not synchronize database state with session  2009-11-29 18:45:41,364 WARN [1085098145@qtp-1013059835-7] [HibernateObjectDao] Problems getting build by key  net.sf.hibernate.exception.GenericJDBCException: could not insert: [com.atlassian.bamboo.build.DefaultBuildDefinitionForBuild#819203]    net.sf.hibernate.exception.ErrorCodeConverter.handledNonSpecificException(ErrorCodeConverter.java:90)    net.sf.hibernate.exception.ErrorCodeConverter.convert(ErrorCodeConverter.java:79)    net.sf.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:29)    net.sf.hibernate.persister.AbstractEntityPersister.convert(AbstractEntityPersister.java:1331)    net.sf.hibernate.persister.EntityPersister.insert(EntityPersister.java:472)    net.sf.hibernate.persister.EntityPersister.insert(EntityPersister.java:436)    net.sf.hibernate.impl.ScheduledInsertion.execute(ScheduledInsertion.java:37)    net.sf.hibernate.impl.SessionImpl.execute(SessionImpl.java:2449)    net.sf.hibernate.impl.SessionImpl.executeAll(SessionImpl.java:2435)    net.sf.hibernate.impl.SessionImpl.execute(SessionImpl.java:2392)    net.sf.hibernate.impl.SessionImpl.autoFlushIfRequired(SessionImpl.java:1821)    net.sf.hibernate.impl.SessionImpl.find(SessionImpl.java:3652)    net.sf.hibernate.impl.CriteriaImpl.list(CriteriaImpl.java:238)    net.sf.hibernate.impl.CriteriaImpl.uniqueResult(CriteriaImpl.java:385)    com.atlassian.bamboo.build.BuildHibernateDao.getBuildByKey(BuildHibernateDao.java:40)    sun.reflect.GeneratedMethodAccessor116.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy7.getBuildByKey(Unknown Source)    com.atlassian.bamboo.build.HibernateBuildManager.getBuildByKey(HibernateBuildManager.java:77)    com.atlassian.bamboo.build.HibernateBuildManager.createBuild(HibernateBuildManager.java:248)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    com.atlassian.bamboo.security.acegi.intercept.aopalliance.AuthorityOverrideMethodSecurityInterceptor.invoke(AuthorityOverrideMethodSecurityInterceptor.java:30)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    com.atlassian.bamboo.security.acegi.intercept.aopalliance.AuthorityOverrideMethodSecurityInterceptor.invoke(AuthorityOverrideMethodSecurityInterceptor.java:30)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy13.createBuild(Unknown Source)    com.atlassian.bamboo.build.creation.BuildPlanCreationServiceImpl.createBuild(BuildPlanCreationServiceImpl.java:235)    com.atlassian.bamboo.build.creation.BuildPlanCreationServiceImpl.createBuild(BuildPlanCreationServiceImpl.java:207)    com.atlassian.bamboo.ww2.actions.admin.ImportMavenPlanCreatePlanAction.doExecute(ImportMavenPlanCreatePlanAction.java:120)    com.atlassian.bamboo.ww2.BambooActionSupport.execute(BambooActionSupport.java:705)  ...    Caused by: java.sql.SQLException: Violation of unique constraint $$: duplicate value(s) for column(s) $$: SYS_CT_108 in statement [insert into BUILD_DEFINITION (BUILD_ID, CREATED_DATE, UPDATED_DATE, XML_DEFINITION_DATA, BUILD_DEFINITION_TYPE, BUILD_DEFINITION_ID) values (?, ?, ?, ?, 'BUILD', ?)]    org.hsqldb.jdbc.Util.throwError(Unknown Source)    org.hsqldb.jdbc.jdbcPreparedStatement.executeUpdate(Unknown Source)    com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:105)    net.sf.hibernate.impl.NonBatchingBatcher.addToBatch(NonBatchingBatcher.java:22)    net.sf.hibernate.persister.EntityPersister.insert(EntityPersister.java:462)  	... 178 more    java.lang.IllegalArgumentException: object cannot be null    org.springframework.util.Assert.notNull(Assert.java:112)    com.atlassian.bamboo.security.acegi.acls.HibernateObjectIdentityImpl.(HibernateObjectIdentityImpl.java:58)    com.atlassian.bamboo.build.HibernateBuildManager.createBuild(HibernateBuildManager.java:249)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    com.atlassian.bamboo.security.acegi.intercept.aopalliance.AuthorityOverrideMethodSecurityInterceptor.invoke(AuthorityOverrideMethodSecurityInterceptor.java:30)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    com.atlassian.bamboo.security.acegi.intercept.aopalliance.AuthorityOverrideMethodSecurityInterceptor.invoke(AuthorityOverrideMethodSecurityInterceptor.java:30)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy13.createBuild(Unknown Source)    com.atlassian.bamboo.build.creation.BuildPlanCreationServiceImpl.createBuild(BuildPlanCreationServiceImpl.java:235)    com.atlassian.bamboo.build.creation.BuildPlanCreationServiceImpl.createBuild(BuildPlanCreationServiceImpl.java:207)    com.atlassian.bamboo.ww2.actions.admin.ImportMavenPlanCreatePlanAction.doExecute(ImportMavenPlanCreatePlanAction.java:120)  {noformat}    "
1,"System property to enable the ""unsupported database"" option in setup",{{-Dbamboo.enable.unsupported.db=true}} or something like that.    * Only affects UI on setup wizard  * Need to add to upgrade guide for people doing import export
1,Import from Maven should redirect the user to the builder configuration in edit mode and prevent the initial build,* Should not build immediately (current way is to Disable plan)  * Redirect to Builder config. section in Edit mode  * (Some risk with tabs recently broken)  * Q: re how to set tab from param  ** Should be a param that sets the cookied & the selectedTab
1,Upgrade to AUI 1.2.5 to resolve dialog size issues in ie7,"See https://studio.atlassian.com/browse/AJS-202    Under IE7 the buttons pane will over-run the size of the dialog (affects add JDK / add Builder)    * Need to test in all supported browsers.  * Inline JDK, Builder, Mail, IM  * Ping Anton for VM"
0,OAuth Consumer tab should not be visible,Test in all supported browsers    !http://img.skitch.com/20091201-xsixujr2hfjfhpq8dkc86dwhq8.png!
0,"Make ""Enable Plan"" link more prominent on the config screen",* Above (or below if that looks better)  * Use your judgement & trial it out! 
0,Link to Maven plan import from the first page of the create plan wizard,* Don't bother with the dashboard
1,should append pom.xml if not specified,"* Check repo if the SVN URL is a file or a directory first {{svn info}} and then append pom.xml if directory    * Does SVN kit have APIs to do SVN info?  ** Oui oui, but SVN does have it, but never done in Bamboo"
1,Retest all new gadget work in Test IE7 IE8 Safari,* Make sure we test all gadgets in JIRA & Connie  * In all supported browsers  * No IE6
1,Build process not shutting down EC2 instances,NULL
0,Changing from a JDK Label that's been deleted will not remove old Requirement,"# Create a plan with a JDK  # Go to admin section and remove the JDK  # Edit the plan, changing the JDK  # The old JDK should be removed but it's not    !http://img.skitch.com/20091211-m297tdrx9j28n3hisgjttgftfm.png! "
0,Test the tagger plugin,http://confluence.atlassian.com/display/BAMEXT/Bamboo+Tagger+plugin    
0,Ensure Clover gadget is working,"Nothing's changed in the gadget itself, just the config screen... We'll hold the Poles to it :)    "
0,Write developer docs for backend plan creation,NULL
1,StopBuildManager with abandon result does not abandon result,StopBuildManager with abandon result does not abandon result in 3.0. In 2.0 it just stopped the result from being persisted but that now happens at the start of the build.
0,Improve the Bamboo startup script,"There are several things wrong with the bamboo.sh startup script:    1) an option to specify which user to run Bamboo under  2) The startup script takes no care of CWD. Depending on which directory you're in the startup behaves differently.  3) if Bamboo stops abruptly or startup fails, it leaves a bamboo.pid file and this file causes future startup attempts to fail.  4) make use of the JAVA_HOME in bamboo.sh script, Bamboo defaults to the java executable in the System Path - we ask users to set JAVA_HOME, but we never use it in the script  5) Parameterize each of the startup option in RUN_CMD variable or read the options from wrapper.conf file.  "
0,SASL support with bundled svnkit,"The version of svnkit we bundle with bamboo doesn't have SASL support:    {noformat}[amyers@erdinger:svnkit]$ jar tf svnkit-1.3.0.5847.jar | grep Sasl  [amyers@erdinger:svnkit]$   {noformat}    If I download it from the website it does (this is a newer version, but apparently older ones have it too):    {noformat}  [amyers@erdinger:svnkit-1.3.2.6267]$ jar tf svnkit.jar | grep Sasl  org/tmatesoft/svn/core/internal/io/svn/sasl/SVNSaslAuthenticator$SVNCallbackHandler.class  org/tmatesoft/svn/core/internal/io/svn/sasl/SVNSaslAuthenticator.class  org/tmatesoft/svn/core/internal/io/svn/sasl/SaslInputStream.class  org/tmatesoft/svn/core/internal/io/svn/sasl/SaslOutputStream.class  [amyers@erdinger:svnkit-1.3.2.6267]$   {noformat}    Is there any technical reason we don't bundle the support for SASL authentication? "
1,"Importing without restart runs all upgrade tasks, if run after initial install.","* All upgrades are run rather than just the ones that are needed.  * This is only a problem if run after the initial install (where the upgrade task collections in UpgradeManagerImpl are already prepopulated with all upgrade tasks.)    This is fixed/avoided by:  * Do not do the import without restart.  * Stop and start Bamboo before the import.  * If it gets into a screwed state, delete your bamboo home and start again (don't accidentally delete the import!)."
0,Upgrade atlassian template renderer to 1.0.5,Needed by an upgrade pack
0,Exception in macro buildResult,"I've gotten this exception several times now:    2009-12-31 10:36:49,916 INFO [1300187729@qtp-380183900-10] [AccessLogFilter] bteh http://bteh.sydney.atlassian.com:8085/bamboo/api/rest/getLatestUserBuilds.action?auth=OmdvT9Cj0M 115615kb  2009-12-31 10:36:49,963 INFO [1300187729@qtp-380183900-10] [AccessLogFilter] bteh http://bteh.sydney.atlassian.com:8085/bamboo/api/rest/getLatestBuildResults.action?auth=OmdvT9Cj0M&buildKey=TEST-DEF 112273kb  2009-12-31 10:36:49,982 INFO [1300187729@qtp-380183900-10] [AccessLogFilter] bteh http://bteh.sydney.atlassian.com:8085/bamboo/api/rest/getLatestBuildResults.action?auth=OmdvT9Cj0M&buildKey=TEST-DEF2 111418kb  2009-12-31 10:36:50,013 INFO [1300187729@qtp-380183900-10] [AccessLogFilter] bteh http://bteh.sydney.atlassian.com:8085/bamboo/api/rest/getLatestBuildResults.action?auth=OmdvT9Cj0M&buildKey=TEST-DEF3 110540kb  2009-12-31 10:36:50,117 INFO [1300187729@qtp-380183900-10] [AccessLogFilter] bteh http://bteh.sydney.atlassian.com:8085/bamboo/api/rest/getRecentlyCompletedBuildResultsForBuild.action?auth=OmdvT9Cj0M&buildKey=TEST-DEF3 109878kb  2009-12-31 10:26:52,088 ERROR [543021934@qtp-380183900-0] [runtime]     Error executing macro: buildResult  required parameter: buildSummary is not specified.  The problematic instruction:  ----------  ==> macro buildResult [on line 1, column 1 in api/rest/macros.ftl]   in user-directive api.buildResult [on line 4, column 9 in api/rest/recentlyCompletedBuildResultsForBuild.ftl]  ----------    Java backtrace for programmers:  ----------  freemarker.template.TemplateException: Error executing macro: buildResult  required parameter: buildSummary is not specified.    freemarker.core.Macro$Context.sanityCheck(Macro.java:207)    freemarker.core.Macro$Context.runMacro(Macro.java:165)    freemarker.core.Environment.visit(Environment.java:602)    freemarker.core.UnifiedCall.accept(UnifiedCall.java:106)    freemarker.core.Environment.visit(Environment.java:209)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:209)    freemarker.core.IteratorBlock$Context.runLoop(IteratorBlock.java:167)    freemarker.core.Environment.visit(Environment.java:416)    freemarker.core.IteratorBlock.accept(IteratorBlock.java:102)    freemarker.core.Environment.visit(Environment.java:209)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:209)    freemarker.core.Environment.process(Environment.java:189)    freemarker.template.Template.process(Template.java:237)    com.opensymphony.webwork.views.freemarker.FreemarkerResult.doExecute(FreemarkerResult.java:214)    com.opensymphony.webwork.dispatcher.WebWorkResultSupport.execute(WebWorkResultSupport.java:143)    com.opensymphony.xwork.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:313)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:208)    com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:175)    com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)    com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:68)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:48)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.TestCaseAwareInterceptor.intercept(TestCaseAwareInterceptor.java:44)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:47)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.RemoteApiExceptionInterceptor.intercept(RemoteApiExceptionInterceptor.java:21)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.RemoteApiLoginInterceptor.intercept(RemoteApiLoginInterceptor.java:57)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)    com.opensymphony.webwork.dispatcher.DispatcherUtils.serviceAction(DispatcherUtils.java:274)    com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:202)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:61)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:61)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:206)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:133)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.spring.filter.FlushingSpringSessionInViewFilter.doFilterInternal(FlushingSpringSessionInViewFilter.java:29)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.planetj.servlet.filter.compression.CompressingFilter.handleDoFilter(CompressingFilter.java:203)    com.planetj.servlet.filter.compression.CompressingFilter.doFilter(CompressingFilter.java:174)    com.atlassian.bamboo.filter.CompressingFilter.doFilter(CompressingFilter.java:65)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter.doFilter(RequestCacheThreadLocalFilter.java:26)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:378)    org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)    org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)    org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)    org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)    org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)    org.mortbay.jetty.Server.handle(Server.java:324)    org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)    org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:865)    org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)    org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)    org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)    org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)"
1,Running a chain from the actions menu runs the chain twice,NULL
1,CVS deletes working copy when using tag/branch and ampersand modules after initial build,When using cvs ampersand modules on a tag/branch bamboo checks out the _initial_ working copy just fine.    For subsequent builds bamboo *deletes* all files in the working copy when updating.    https://support.atlassian.com/browse/BSP-2452  
0,Agent upload of artifacts fails to set Content-Length in HTTP/1.1 (on lighthttpd or nginx)," Bamboo Agent upload of artifacts fails to set Content-Length:, causes problems when using a proxy that implements strict HTTP/1.1    Our preferred reverse proxy solution is Lighttpd. However, use of Lighttpd resulted in failure of Bamboo Agents to upload artifacts to the Bamboo Server.    Upon investigation, we discovered the following error in the Lighttpd error logs:    2010-01-08 13:50:26: (request.c.1115) POST-request, but content-length missing -> 411     HTTP/1.1 - http://www.ietf.org/rfc/rfc2616.txt - Section 4.4:        ...      For compatibility with HTTP/1.0 applications, HTTP/1.1 requests      containing a message-body MUST include a valid Content-Length header      field unless the server is known to be HTTP/1.1 compliant. If a      request contains a message-body and a Content-Length is not given,      the server SHOULD respond with 400 (bad request) if it cannot      determine the length of the message, or with 411 (length required) if      it wishes to insist on receiving a valid Content-Length.    And below in section 10.4.12:        10.4.12 411 Length Required        The server refuses to accept the request without a defined Content-      Length. The client MAY repeat the request if it adds a valid      Content-Length header field containing the length of the message-body      in the request message.    And further below in section :        14.13 Content-Length        The Content-Length entity-header field indicates the size of the      entity-body, in decimal number of OCTETs, sent to the recipient or,      in the case of the HEAD method, the size of the entity-body that      would have been sent had the request been a GET.        Content-Length = ""Content-Length"" "":"" 1*DIGIT        An example is        Content-Length: 3495        Applications SHOULD use this field to indicate the transfer-length of      the message-body, unless this is prohibited by the rules in section      4.4.        Any Content-Length greater than or equal to zero is a valid value.      Section 4.4 describes how to determine the length of a message-body      if a Content-Length is not given.        Note that the meaning of this field is significantly different from      the corresponding definition in MIME, where it is an optional field      used within the ""message/external-body"" content-type. In HTTP, it      SHOULD be sent whenever the message's length can be determined prior      to being transferred, unless this is prohibited by the rules in      section 4.4.    According to my research, both Microsoft IIS and Lighttpd are strict on this. I have confirmed that Lighttpd does not have a configuration option to disable this level of strictness. Apache, however, is lax about enforcing this rule. As a result, Apache works and Lighttpd fails.    Please correct the Bamboo Agent upload of artifacts to specify a Content-Length according to the specification defined above so we can switch back to Lighttpd."
1,Can't create Plan from POM if the only Builder detected was the Elastic Agent,"To reproduce, make sure that the ""Maven 2"" Builder has only the Elastic Agent Builder in its list (Bamboo Console >> Administration >> Builders). Therefore, Bamboo has not detected any local or remote Maven installation.    When trying to create the Plan from a POM, after detecting the Repository and Providing the ProjectName and ProjectKey, the following stacktrace is displayed:    Version: 2.5  Build: 1711  Build Date: 29 Dec 2009  system.error.request.information:        * Request URL: http://localhost:8085/admin/importMavenPlanCreatePlan.action      * Scheme: http      * Server: localhost      * Port: 8085      * URI: /admin/importMavenPlanCreatePlan.action      * Context Path:      * Servlet Path: /admin/importMavenPlanCreatePlan.action      * Path Info:      * Query String:    Stack Trace:    com.atlassian.bamboo.build.PlanCreationException: No Maven 2 builder found, should not occur    com.atlassian.bamboo.ww2.actions.admin.ImportMavenPlanCreatePlanAction.createBuilder(ImportMavenPlanCreatePlanAction.java:184)    com.atlassian.bamboo.ww2.actions.admin.ImportMavenPlanCreatePlanAction.doExecute(ImportMavenPlanCreatePlanAction.java:123)    com.atlassian.bamboo.ww2.BambooActionSupport.execute(BambooActionSupport.java:705)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:592)    com.opensymphony.xwork.DefaultActionInvocation.invokeAction(DefaultActionInvocation.java:358)    com.opensymphony.xwork.DefaultActionInvocation.invokeActionOnly(DefaultActionInvocation.java:218)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:192)    com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:175)    com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)    com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.GlobalAdminInterceptor.intercept(GlobalAdminInterceptor.java:21)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:68)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:48)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.TestCaseAwareInterceptor.intercept(TestCaseAwareInterceptor.java:44)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:47)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildConfigurationInterceptor.intercept(BuildConfigurationInterceptor.java:52)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildConfigurationFromSessionInterceptor.intercept(BuildConfigurationFromSessionInterceptor.java:50)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)    com.opensymphony.webwork.dispatcher.DispatcherUtils.serviceAction(DispatcherUtils.java:274)    com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:202)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:61)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:61)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:204)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:138)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.spring.filter.FlushingSpringSessionInViewFilter.doFilterInternal(FlushingSpringSessionInViewFilter.java:29)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.planetj.servlet.filter.compression.CompressingFilter.handleDoFilter(CompressingFilter.java:203)    com.planetj.servlet.filter.compression.CompressingFilter.doFilter(CompressingFilter.java:193)    com.atlassian.bamboo.filter.CompressingFilter.doFilter(CompressingFilter.java:65)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter.doFilter(RequestCacheThreadLocalFilter.java:26)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)    org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:378)    org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)    org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)    org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)    org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)    org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)    org.mortbay.jetty.Server.handle(Server.java:324)    org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)    org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:880)    org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)    org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)    org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)    org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)"
1,Update Builders and Jdk's to allow for > 25 agents,"This view JDKs and view builders page needs rewriting.  A mock up can be found here: https://extranet.atlassian.com/display/BAMBOO/Supporting+more+than+25+remote+agents   We are trying to condense this information.    There is a bug on this page which also needs to be fixed, see screenshot: http://skitch.com/brydiemccoy/n8fbq/jdks-atlassian-bamboo    Investigate surrounding/similar pages and identify if any other work needs doing."
0,"${system.bamboo.agent.home} should be defined for all agents, not just remote ones","${system.bamboo.agent.home} needs to be defined for all agents, otherwise it makes running pre and post build scripts break."
1,Ability to delete build working directory after a suceessful build,Add a checkbox to delete the builds working directory after the build to control the amount of disk space in use    It would be excellent if this https://extranet.atlassian.com/display/IT/2010/01/11/Pre+and+Post+scripts+on+BEAC+agents+to+clean+up+builds was incorporated into the product at a per plan level
1,Bamboo Rest calls cannot find resources directly after install,"I started a brand new Bamboo Home (2.5).  After running the setup wizard in Bamboo and running a few builds, I added a new plan status gadget to jira (4.0) and got the error below (see screen shot also).  The error could also be seen when hitting the straight rest urls:     http://localhost:8085/bamboo/rest/api/latest/build/SVN/DEF?expand=builds[0%3A9].build&max-results=10    I then stopped and started Bamboo and the resources were found happily and everything worked.   Notifications are not affected (runs through similar code) just the REST calls.      ---------------    2010-01-12 13:51:45,070 INFO [10084294@qtp-11642082-31] [AccessLogFilter] admin http://172.20.7.225:8085/bamboo/rest/api/latest/build/SVN/DEF2.json?expand=builds%5B0%3A9%5D.build&max-results=10&nocache=1263264704955 174430kb  2010-01-12 13:51:45,132 ERROR [9682830@qtp-11642082-30] [BambooFreemarkerManager] Error resolving template  java.io.FileNotFoundException: Template freemarker-lib/ui.ftl not found.    freemarker.template.Configuration.getTemplate(Configuration.java:489)    freemarker.core.Environment.getTemplateForInclusion(Environment.java:1464)    freemarker.core.Environment.getTemplateForImporting(Environment.java:1518)    freemarker.core.Environment.importLib(Environment.java:1503)    freemarker.template.Configuration.doAutoImportsAndIncludes(Configuration.java:843)    freemarker.core.Configurable.doAutoImportsAndIncludes(Configurable.java:835)    freemarker.core.Configurable.doAutoImportsAndIncludes(Configurable.java:835)    freemarker.core.Environment.process(Environment.java:188)    freemarker.template.Template.process(Template.java:237)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPageWithNoContext(BambooFreemarkerManager.java:419)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:260)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:236)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.render(BambooFreemarkerManager.java:305)    com.atlassian.bamboo.v2.build.trigger.DefaultTriggerReasonRenderer.getShortDescriptionHtml(DefaultTriggerReasonRenderer.java:40)    com.atlassian.bamboo.resultsummary.AbstractBuildResultsSummary.getReasonSummary(AbstractBuildResultsSummary.java:135)    com.atlassian.bamboo.plugins.rest.model.build.AbstractRestBuild.expand(AbstractRestBuild.java:145)    com.atlassian.bamboo.plugins.rest.model.build.RestBuild.expand(RestBuild.java:68)    com.atlassian.bamboo.plugins.rest.model.build.RestBuildExpander.expandInternal(RestBuildExpander.java:14)    com.atlassian.bamboo.plugins.rest.model.build.RestBuildExpander.expandInternal(RestBuildExpander.java:8)    com.atlassian.plugins.rest.common.expand.AbstractRecursiveEntityExpander.expand(AbstractRecursiveEntityExpander.java:9)    com.atlassian.plugins.rest.common.expand.resolver.CollectionEntityExpanderResolver$ListExpander.expand(CollectionEntityExpanderResolver.java:65)    com.atlassian.plugins.rest.common.expand.resolver.CollectionEntityExpanderResolver$ListExpander.expand(CollectionEntityExpanderResolver.java:56)    com.atlassian.plugins.rest.common.expand.EntityCrawler.expandFields(EntityCrawler.java:94)    com.atlassian.plugins.rest.common.expand.EntityCrawler.crawl(EntityCrawler.java:35)    com.atlassian.plugins.rest.common.expand.resolver.ListWrapperEntityExpanderResolver$ListWrapperEntityExpander.expand(ListWrapperEntityExpanderResolver.java:58)    com.atlassian.plugins.rest.common.expand.resolver.ListWrapperEntityExpanderResolver$ListWrapperEntityExpander.expand(ListWrapperEntityExpanderResolver.java:32)    com.atlassian.plugins.rest.common.expand.EntityCrawler.expandFields(EntityCrawler.java:94)    com.atlassian.plugins.rest.common.expand.EntityCrawler.crawl(EntityCrawler.java:35)    com.atlassian.plugins.rest.common.expand.jersey.ExpandResponseFilter.filter(ExpandResponseFilter.java:39)    com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:675)    com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:612)    com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:603)    com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:309)    com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)    com.atlassian.plugins.rest.module.RestDelegatingServletFilter$JerseyOsgiServletContainer.doFilter(RestDelegatingServletFilter.java:198)    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:689)    com.atlassian.plugins.rest.module.RestDelegatingServletFilter.doFilter(RestDelegatingServletFilter.java:112)      "
1,Editing user and setting Source Repository Alias causes null pointer exception,"assigning a repository alias to an external (ldap) user throws NPE    2010-01-14 14:41:36,093 ERROR [4844447@qtp-6659511-0] [FiveOhOh] 500 Exception was thrown.  java.lang.NullPointerException    java.util.AbstractCollection.addAll(AbstractCollection.java:316)    com.atlassian.bamboo.ww2.actions.admin.user.ConfigureUser.execute(ConfigureUser.java:88)    com.atlassian.bamboo.ww2.actions.admin.user.ConfigureUser.doUpdateExternalUser(ConfigureUser.java:153)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:592)    "
1,When downsizing a license plans users are directed to delete on the _old_  server ,When a user has to downsize bamboo instance the use is directed to a delete plans screen.   The link points to the bamboo server the last export was made from.  Plans on the _previous_ server are deleted _not_ on the current. The previous server may still be active and eligible to run unlimited plans.     On that screen a neon warning should be displayed and the base url should be questioned (if not the same as the current hostname/ip address)
0,CVS module field doesn't store changes when cloning an existing build plan,"First: create a build plan using CVS as source repository and specify a module to be used.  Second: create another build plan and select ""Clone an existing build plan?"" then select the other plan. Click ""Next >>"" and change the CVS module to be used. Click ""Next >>"" until the new plan can be saved and tries to do it's initial build... you will notice that it uses the CVS module specified in the initial build plan instead of the new one specified.    Workaround: Abort the initial build and change the module again in the new plan's configuration... afterwards the module is set correctly."
1,Cannot undo Maven POM dependency management,"We have 200+ Maven plans configured on one Bamboo instance and some of them are nightly plans with a once-a-day trigger.     On upgrading to 2.5 I set all plans to use the Maven2 dependency management. The problem is that I don't want regular CI plans to trigger nightly plans(as they are resource intensive). When I remove POM dependency management from the nightly builds, it doesn't stop them from being triggered by regular CI plans. I have no choice now but to take the Maven 2 dep management off all plans.    Oh well, it was nice while it lasted.     So - any way to stop the Maven 2 dependencies from crossing from plans of one build strategy (polling) to plans of another (single daily build)?"
1,Fix up UI for builders/jdks admin pages,NULL
1,Return URL should be restricted to current server,NULL
1,Session timeout and refreshing of Latest Status bar causes decorated login screen to appear inside page rather than taking up whole page,Attached is a screenshot of what happens when your session has timed out and you go back then forward - appears after about 5-10 seconds
1,JIRA Bamboo Plugin creates deadlock in JIRA and Bamboo applications when starting them up in the same Tomcat server,"Steps to reproduce:    # Deploy JIRA 4.0.1 WAR into Tomcat 5.5.26  # Deploy Bamboo 2.5.1 WAR into same Tomcat  # Configure everything normally  # Start up JIRA. Default configuration. Create TST project.  # Start up Bamboo. Express Installation.  # In JIRA, configure a Bamboo Server for the Bamboo app  # Shutdown Tomcat  # Restart Tomcat    Bamboo gets this far:    {quote}  2010-02-04 16:06:03,522 INFO [main] [BambooSmackClient] No IM server specified. Bambo Smack client not initiated.  2010-02-04 16:06:03,573 INFO [main] [BambooContainer] Registered Event Listener: completedBuildNotificationEventListener  2010-02-04 16:06:03,591 INFO [main] [BambooContainer] Registered Event Listener: afterXFailedNotificationListener  2010-02-04 16:06:03,597 INFO [main] [BambooContainer] Registered Event Listener: buildCommentedNotificationListener  2010-02-04 16:06:03,601 INFO [main] [BambooContainer] Registered Event Listener: buildHungNotificationListener  2010-02-04 16:06:03,605 INFO [main] [BambooContainer] Registered Event Listener: buildQueueTimeoutNotificationListener  2010-02-04 16:06:03,608 INFO [main] [BambooContainer] Registered Event Listener: legacyNotificationEventListener  2010-02-04 16:06:03,612 INFO [main] [BambooContainer] Registered Event Listener: buildErrorNotificationListener  2010-02-04 16:06:03,616 INFO [main] [BambooContainer] Registered Event Listener: buildMissingCapableAgentNotificationListener  2010-02-04 16:06:03,620 INFO [main] [BambooContainer] Registered Event Listener: serverStartedEventListener  2010-02-04 16:06:03,620 INFO [main] [BambooContainer] Initialising builds...  2010-02-04 16:06:03,630 INFO [main] [BambooContainer] Checking build repository configurations...  2010-02-04 16:06:03,634 INFO [main] [BambooContainer] Bamboo version 2.5.1 initialised OK  2010-02-04 16:06:03,634 INFO [main] [BambooContainer] Using bamboo.home: /Users/mtokar/support/JSP-51856/bamboohome  2010-02-04 16:06:03,636 INFO [main] [AccessLogFilter] AccessLogFilter initialized. Format is: <user> <url> <starting memory free (kb)> +- <difference in free mem (kb)> <query time (ms)>  2010-02-04 16:06:03,642 INFO [main] [[/bamboo]] org.tuckey.web.filters.urlrewrite.utils.Log ERROR: logLevelConf: null  2010-02-04 16:06:03,665 INFO [main] [[/bamboo]] org.tuckey.web.filters.urlrewrite.UrlRewriteFilter INFO: loaded (conf ok)  2010-02-04 16:06:03,748 INFO [main] [[/bamboo]]  [CompressingFilter/1.4] CompressingFilter has initialized  {quote}    JIRA initialisation gets this far:    {quote}    Database configuration OK    ___ Database Configuration _______         Loading entityengine.xml from       : file:/Users/mtokar/support/JSP-51856/apache-tomcat-5.5.26/webapps/jira/WEB-INF/classes/entityengine.xml       Entity model data source name       : defaultDS       Entity model field type name        : hsql       Entity model schema name            : PUBLIC       Database Version                    : HSQL Database Engine - 1.8.0       Database Driver                     : HSQL Database Engine Driver - 1.8.0       Database URL                        : jdbc:hsqldb:/Users/mtokar/support/JSP-51856/atlassian-jira-enterprise-4.0.1/database/jiradb       Database JNDI address               : java:comp/env/jdbc/JiraDS  {quote}    Tomcat logs show that Tomcat has hung after the plugins system has been initialised:    {quote}  4/02/2010 16:06:17 com.atlassian.plugin.osgi.factory.UnrecognizedModuleDescriptorServiceTrackerCustomizer addingService  INFO: Turned plugin module com.atlassian.streams.streams-jira-plugin:streamsActivityProvider into module com.atlassian.streams.streams-jira-plugin:streamsActivityProvider (null)  4/02/2010 16:06:26 com.atlassian.plugin.loaders.ScanningPluginLoader loadAllPlugins  INFO: No plugins found to be deployed  4/02/2010 16:06:26 com.atlassian.plugin.manager.DefaultPluginManager init  INFO: Plugin system started in 0:00:19.183  {quote}    Neither JIRA or Bamboo webapps can be accessed through the browser. Thread dump of Tomcat shows the following stack trace:    {quote}  Thread t@81667: (state = IN_NATIVE)   - java.net.SocketInputStream.socketRead0(java.io.FileDescriptor, byte[], int, int, int) @bci=0 (Interpreted frame)   - java.net.SocketInputStream.read(byte[], int, int) @bci=84, line=129 (Interpreted frame)   - java.io.BufferedInputStream.fill() @bci=175, line=218 (Compiled frame)   - java.io.BufferedInputStream.read() @bci=12, line=235 (Compiled frame)   - org.apache.commons.httpclient.HttpParser.readRawLine(java.io.InputStream) @bci=36, line=77 (Interpreted frame)   - org.apache.commons.httpclient.HttpParser.readLine(java.io.InputStream, java.lang.String) @bci=11, line=105 (Interpreted frame)   - org.apache.commons.httpclient.HttpConnection.readLine(java.lang.String) @bci=19, line=1115 (Interpreted frame)   - org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.readLine(java.lang.String) @bci=12, line=1373 (Interpreted frame)   - org.apache.commons.httpclient.HttpMethodBase.readStatusLine(org.apache.commons.httpclient.HttpState, org.apache.commons.httpclient.HttpConnection) @bci=36, line=1832 (Interpreted frame)   - org.apache.commons.httpclient.HttpMethodBase.readResponse(org.apache.commons.httpclient.HttpState, org.apache.commons.httpclient.HttpConnection) @bci=16, line=1590 (Interpreted frame)   - org.apache.commons.httpclient.HttpMethodBase.execute(org.apache.commons.httpclient.HttpState, org.apache.commons.httpclient.HttpConnection) @bci=68, line=995 (Interpreted frame)   - org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(org.apache.commons.httpclient.HttpMethod) @bci=138, line=397 (Interpreted frame)   - org.apache.commons.httpclient.HttpMethodDirector.executeMethod(org.apache.commons.httpclient.HttpMethod) @bci=291, line=170 (Interpreted frame)   - org.apache.commons.httpclient.HttpClient.executeMethod(org.apache.commons.httpclient.HostConfiguration, org.apache.commons.httpclient.HttpMethod, org.apache.commons.httpclient.HttpState) @bci=115, line=396 (Interpreted frame)   - org.apache.commons.httpclient.HttpClient.executeMethod(org.apache.commons.httpclient.HttpMethod) @bci=14, line=324 (Interpreted frame)   - com.atlassian.jira.plugin.ext.bamboo.BambooServerManagerImpl.isGadgetAvailable(java.net.URI) @bci=17, line=413 (Interpreted frame)   - com.atlassian.jira.plugin.ext.bamboo.BambooServerManagerImpl.addGadgetsForServer(com.atlassian.jira.plugin.ext.bamboo.service.BambooServer) @bci=63, line=376 (Interpreted frame)   - com.atlassian.jira.plugin.ext.bamboo.BambooServerManagerImpl.start() @bci=36, line=117 (Interpreted frame)   - com.atlassian.sal.jira.lifecycle.JiraLifecycleManager.notifyOnStart() @bci=99, line=83 (Interpreted frame)   - com.atlassian.sal.core.lifecycle.DefaultLifecycleManager.start() @bci=15, line=50 (Interpreted frame)   - com.atlassian.sal.jira.lifecycle.JiraLifecycleManager.onJiraStart(com.atlassian.jira.extension.JiraStartedEvent) @bci=9, line=60 (Interpreted frame)  {quote}    So what we have is a deadlock. Bamboo is somewhat started up but not responding to HTTP requests. JIRA has started its plugins, and in the starting of the JIRA Bamboo Plugin, some code tries to eagerly instantiate something in {{BambooServerManagerImpl.java}}:    {code}      protected boolean isGadgetAvailable(URI uri)      {          HeadMethod method = new HeadMethod(uri.toString());          try          {              httpClient.executeMethod(method);              return method.getStatusCode() < HTTP_STATUS_CODE_ERRORS;          }          catch(IOException e)          {              return false;          }          finally          {             method.releaseConnection();          }      }  {code}    i.e. JIRA is waiting on Bamboo to respond.    This should be fixed so that the attempted HTTP connection at least has a sensible timeout (60seconds?) before it gives up and moves along. That, or instantiate things not on the startup of a plugin but at a different point in the lifecycle."
0,Name change from Build to Plan security aware broke nant plugin,"2010-02-04 13:40:32,839 ERROR [main] [DefaultPluginManager] There was an error loading the descriptor 'View NCover Result' of plugin 'com.atlassian.bamboo.plugin.builder.nant'. Disabling.  java.lang.NoClassDefFoundError: com/atlassian/bamboo/ww2/aware/permissions/BuildReadSecurityAware    java.lang.ClassLoader.defineClass1(Native Method)    java.lang.ClassLoader.defineClass(ClassLoader.java:620)    java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)    org.apache.catalina.loader.WebappClassLoader.findClassInternal(WebappClassLoader.java:1629)    org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:850)    org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1299)    org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1181)    com.opensymphony.util.ClassLoaderUtil.loadClass(ClassLoaderUtil.java:96)    com.opensymphony.xwork.ObjectFactory.getClassInstance(ObjectFactory.java:88)    com.opensymphony.xwork.spring.SpringObjectFactory.getClassInstance(SpringObjectFactory.java:177)    com.opensymphony.xwork.spring.SpringObjectFactory.buildBean(SpringObjectFactory.java:117)    com.opensymphony.xwork.ObjectFactory.buildAction(ObjectFactory.java:101)    com.atlassian.bamboo.plugin.xwork.PluginAwareObjectFactory.buildAction(PluginAwareObjectFactory.java:34)    com.atlassian.bamboo.plugin.xwork.XWorkModuleDescriptor.addAction(XWorkModuleDescriptor.java:282)    com.atlassian.bamboo.plugin.xwork.XWorkModuleDescriptor.getPackages(XWorkModuleDescriptor.java:115)    com.atlassian.bamboo.plugin.xwork.XWorkModuleDescriptor.getPackages(XWorkModuleDescriptor.java:500)    com.atlassian.bamboo.plugin.xwork.XWorkModuleDescriptor.init(XWorkModuleDescriptor.java:479)    com.opensymphony.xwork.config.impl.DefaultConfiguration.reload(DefaultConfiguration.java:159)    com.atlassian.bamboo.plugin.xwork.XWorkModuleDescriptor.enabled(XWorkModuleDescriptor.java:65)    com.atlassian.plugin.manager.DefaultPluginManager.notifyModuleEnabled(DefaultPluginManager.java:1168)    com.atlassian.plugin.manager.DefaultPluginManager.enablePluginModules(DefaultPluginManager.java:988)    com.atlassian.plugin.manager.DefaultPluginManager.addPlugins(DefaultPluginManager.java:558)    com.atlassian.plugin.manager.DefaultPluginManager.init(DefaultPluginManager.java:153)    com.atlassian.bamboo.container.BambooContainer.init(BambooContainer.java:181)    com.atlassian.bamboo.container.BambooContainer.initialise(BambooContainer.java:148)    com.atlassian.bamboo.upgrade.UpgradeLauncher.initialiseBambooContainer(UpgradeLauncher.java:140)    com.atlassian.bamboo.upgrade.UpgradeLauncher.upgradeAndStartBamboo(UpgradeLauncher.java:86)    com.atlassian.bamboo.upgrade.UpgradeLauncher.contextInitialized(UpgradeLauncher.java:32)    org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3669)    org.apache.catalina.core.StandardContext.start(StandardContext.java:4104)    org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:759)    org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)    org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)    org.apache.catalina.startup.HostConfig.deployDirectory(HostConfig.java:894)    org.apache.catalina.startup.HostConfig.deployDirectories(HostConfig.java:857)    org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:475)    org.apache.catalina.startup.HostConfig.start(HostConfig.java:1102)    org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:311)    org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)    org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1020)    org.apache.catalina.core.StandardHost.start(StandardHost.java:718)    org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1012)    org.apache.catalina.core.StandardEngine.start(StandardEngine.java:442)    org.apache.catalina.core.StandardService.start(StandardService.java:450)    org.apache.catalina.core.StandardServer.start(StandardServer.java:683)    org.apache.catalina.startup.Catalina.start(Catalina.java:537)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:585)    org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:271)    org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:409)"
1,Differences in the implementation of a BuildResultsSummary cause labels to be duplicated on the 'Related builds by date' tab of the Jira Bamboo Plugin,See BuildResultsSummaryDocument vs BuildResultsSummaryImpl for getLabelNames()
1,Delete multiple agents simultaneously,The lab manager plugin pollutes the agent space will thousands of agents that do no deregister properly. This ruins performance as the ui is affected by the product of plans * agent capabilities.     Please implement a way to delete inactive agents.
0,Large number of exceptions being thrown on JBAC,"ssh atlassian20.private nice tail -F /service/j2ee_jira.bamboo.atlassian.com/log/main/current       Sample:  @400000004b74ceb62c1587b4 2010-02-11 21:44:44,737 ERROR [TP-Processor29] [runtime]   @400000004b74ceb62c159754   @400000004b74ceb62c159b3c Error executing macro: displayReorderQueueActions  @400000004b74ceb62c159f24 required parameter: returnUrl is not specified.  @400000004b74ceb62c15a30c The problematic instruction:  @400000004b74ceb62c15a6f4 ----------  @400000004b74ceb62c15aadc ==> macro displayReorderQueueActions [on line 3, column 1 in fragments/showBuildQueue.ftl]  @400000004b74ceb62c15b2ac  in user-directive displayReorderQueueActions [on line 166, column 33 in fragments/showBuildQueue.ftl]  @400000004b74ceb62c1652d4 ----------  @400000004b74ceb62c1652d4   @400000004b74ceb62c1656bc Java backtrace for programmers:  @400000004b74ceb62c165aa4 ----------  @400000004b74ceb62c165e8c freemarker.template.TemplateException: Error executing macro: displayReorderQueueActions  @400000004b74ceb62c16665c required parameter: returnUrl is not specified.  @400000004b74ceb62c166a44   freemarker.core.Macro$Context.sanityCheck(Macro.java:207)  @400000004b74ceb62c167dcc   freemarker.core.Macro$Context.runMacro(Macro.java:165)  @400000004b74ceb62c16859c   freemarker.core.Environment.visit(Environment.java:602)  @400000004b74ceb62c168984   freemarker.core.UnifiedCall.accept(UnifiedCall.java:106)  @400000004b74ceb62c168d6c   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c169154   freemarker.core.ConditionalBlock.accept(ConditionalBlock.java:79)  @400000004b74ceb62c16a4dc   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c16a8c4   freemarker.core.MixedContent.accept(MixedContent.java:92)  @400000004b74ceb62c16d3bc   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c16d7a4   freemarker.core.IfBlock.accept(IfBlock.java:82)  @400000004b74ceb62c16df74   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c16e35c   freemarker.core.IteratorBlock$Context.runLoop(IteratorBlock.java:167)  @400000004b74ceb62c16e744   freemarker.core.Environment.visit(Environment.java:416)  @400000004b74ceb62c16facc   freemarker.core.IteratorBlock.accept(IteratorBlock.java:102)  @400000004b74ceb62c16feb4   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c170684   freemarker.core.IfBlock.accept(IfBlock.java:82)  @400000004b74ceb62c170a6c   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c170e54   freemarker.core.MixedContent.accept(MixedContent.java:92)  @400000004b74ceb62c1721dc   freemarker.core.Environment.visit(Environment.java:209)  @400000004b74ceb62c1725c4   freemarker.core.Environment.process(Environment.java:189)  @400000004b74ceb62c1729ac   freemarker.template.Template.process(Template.java:237)  @400000004b74ceb62c17317c   com.opensymphony.webwork.views.freemarker.FreemarkerResult.doExecute(FreemarkerResult.java:214)  @400000004b74ceb62c173564   com.opensymphony.webwork.dispatcher.WebWorkResultSupport.execute(WebWorkResultSupport.java:143)  @400000004b74ceb62c180c3c   com.opensymphony.xwork.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:313)  @400000004b74ceb62c18140c   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:208)  @400000004b74ceb62c181bdc   com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:175)  @400000004b74ceb62c1823ac   com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)  @400000004b74ceb62c183734   com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)  @400000004b74ceb62c183f04   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1842ec   com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)  @400000004b74ceb62c185a5c   com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)  @400000004b74ceb62c18622c   com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)  @400000004b74ceb62c1869fc   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c188d24   com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)  @400000004b74ceb62c1894f4   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c189cc4   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c18a0ac   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c18bc04   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c18c3d4   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c18c7bc   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c18cf8c   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c18e314   com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)  @400000004b74ceb62c18eae4   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c18eecc   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c1915dc   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c191dac   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c192194   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c192964   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c1940d4   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1944bc   com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)  @400000004b74ceb62c194c8c   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c195074   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c196bcc   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c196fb4   com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)  @400000004b74ceb62c197784   com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)  @400000004b74ceb62c19e4e4   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c19ecb4   com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:68)  @400000004b74ceb62c19f09c   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a0fdc   com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)  @400000004b74ceb62c1a17ac   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a1b94   com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:48)  @400000004b74ceb62c1a3304   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a36ec   com.atlassian.bamboo.ww2.interceptors.TestCaseAwareInterceptor.intercept(TestCaseAwareInterceptor.java:44)  @400000004b74ceb62c1a3ebc   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a61e4   com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)  @400000004b74ceb62c1a69b4   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a6d9c   com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:47)  @400000004b74ceb62c1a8124   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a88f4   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c1a8cdc   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1a94ac   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c1ab004   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1ab3ec   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  @400000004b74ceb62c1abbbc   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1ac38c   com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)  @400000004b74ceb62c1aee84   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  @400000004b74ceb62c1af654   com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)  @400000004b74ceb62c1afe24   com.opensymphony.webwork.dispatcher.DispatcherUtils.serviceAction(DispatcherUtils.java:274)  @400000004b74ceb62c1b020c   com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:202)  @400000004b74ceb62c1b214c   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1b2534   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1b2d04   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  @400000004b74ceb62c1b34d4   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  @400000004b74ceb62c1b4c44   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  @400000004b74ceb62c1b5414   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1b7f0c   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1b86dc   com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)  @400000004b74ceb62c1b8ac4   com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)  @400000004b74ceb62c1b9294   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1bf824   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1bfff4   com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)  @400000004b74ceb62c1c07c4   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1c0bac   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1c1f34   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  @400000004b74ceb62c1c2704   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  @400000004b74ceb62c1c4a2c   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  @400000004b74ceb62c1c51fc   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1c59cc   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1c6d54   com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:76)  @400000004b74ceb62c1c7524   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1c790c   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1c80dc   org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)  @400000004b74ceb62c1c9464   org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)  @400000004b74ceb62c1c9c34   org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)  @400000004b74ceb62c1ca01c   com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:61)  @400000004b74ceb62c1cc344   org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)  @400000004b74ceb62c1cc72c   org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)  @400000004b74ceb62c1ccefc   org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)  @400000004b74ceb62c1cd6cc   com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)  @400000004b74ceb62c1ce66c   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1cee3c   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1cf60c   com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)  @400000004b74ceb62c1cf9f4   com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)  @400000004b74ceb62c1d0d7c   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1d154c   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1d1934   com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)  @400000004b74ceb62c1d4044   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1d442c   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1d4bfc   com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:204)  @400000004b74ceb62c1d4fe4   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1da9bc   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1db18c   com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:138)  @400000004b74ceb62c1db574   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1dbd44   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1dd0cc   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  @400000004b74ceb62c1dd89c   com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)  @400000004b74ceb62c1de06c   com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)  @400000004b74ceb62c1e0394   com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)  @400000004b74ceb62c1e0b64   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)  @400000004b74ceb62c1e1334   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  @400000004b74ceb62c1e26bc   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  @400000004b74ceb62c1e2e8c   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1e3274   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1e45fc   org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)  @400000004b74ceb62c1e4dcc   com.atlassian.spring.filter.FlushingSpringSessionInViewFilter.doFilterInternal(FlushingSpringSessionInViewFilter.java:29)  @400000004b74ceb62c1e559c   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)  @400000004b74ceb62c1e7cac   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1e8094   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1e8864   com.planetj.servlet.filter.compression.CompressingFilter.handleDoFilter(CompressingFilter.java:203)  @400000004b74ceb62c1e9034   com.planetj.servlet.filter.compression.CompressingFilter.doFilter(CompressingFilter.java:193)  @400000004b74ceb62c1e9fd4   com.atlassian.bamboo.filter.CompressingFilter.doFilter(CompressingFilter.java:65)  @400000004b74ceb62c1ea7a4   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1eaf74   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1eb35c   com.atlassian.bamboo.filter.NonCachingHeaderFilter.doFilter(NonCachingHeaderFilter.java:22)  @400000004b74ceb62c1ec6e4   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1eceb4   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1ed684   com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter.doFilter(RequestCacheThreadLocalFilter.java:26)  @400000004b74ceb62c1ef5c4   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1efd94   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1f0564   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  @400000004b74ceb62c1f094c   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  @400000004b74ceb62c1f5384   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  @400000004b74ceb62c1f5b54   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1f6324   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @400000004b74ceb62c1f76ac   org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)  @400000004b74ceb62c1f7e7c   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)  @400000004b74ceb62c1fa1a4   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)  @400000004b74ceb62c1fa974   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)  @40..."
0,Persist some calculated data for ArtifactLink,"From: https://atlaseye.atlassian.com/cru/CR-BAM-997#c17472    Chai:  Is there any reason why we wouldn't simply store the calculated values? One of the things was that size was not calculated for directories in fear that it was going to be too expensive (which it can be if artifacts are big). Thoughts?    Krystian:  If we decide to persist the url then we shall remember to update it once the buildKey will get changed (build move) which in my opinion adds complexity. We can however store a path with some kind of token to be replaced by planKey while property is being read (""/browse/%s/artifacts/build-123/whatever"")  If you want to persist artifact size as well, then we might calculate directory size (as it would be done once).    Chai:  Yeah for URL we could just store the calculated suffix (ie the thing that requires us to browse for an index.html etc) and leave out the prefix (that can just be apended, it's not expensive)  + we should store the full size for the directory, though it'd be good to do some testing to see if how much more time it adds (say for a Clover report over JIRA is a good test)"
1,Dynamic EC2 agents, Related to BAM-3344
0,Adding correct AWS account credentials gives a stack trace if the account does not have an EC2 subscription,"It should instead give a regular error message informing the user of the problem.    The stack trace is:  {noformat}  A communication error occurred while trying to validate the AWS account credentials.  com.atlassian.aws.AWSException: Failed to determine validity of AWS credentials.    com.atlassian.aws.AWSAccountImpl.validate(AWSAccountImpl.java:79)    com.atlassian.bamboo.agent.elastic.server.ElasticInstanceManagerImpl.validateAwsCredentials(ElasticInstanceManagerImpl.java:167)    com.atlassian.bamboo.ww2.actions.admin.elastic.ConfigureElasticCloudAction.testAWSCredentials(ConfigureElasticCloudAction.java:215)    com.atlassian.bamboo.ww2.actions.admin.elastic.ConfigureElasticCloudAction.validate(ConfigureElasticCloudAction.java:129)    com.atlassian.bamboo.ww2.actions.admin.elastic.ConfigureElasticCloudAction.doSave(ConfigureElasticCloudAction.java:167)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:592)    com.opensymphony.xwork.DefaultActionInvocation.invokeAction(DefaultActionInvocation.java:358)    com.opensymphony.xwork.DefaultActionInvocation.invokeActionOnly(DefaultActionInvocation.java:218)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:192)    com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:30)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:30)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:83)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:63)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:68)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:48)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.TestCaseAwareInterceptor.intercept(TestCaseAwareInterceptor.java:44)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:47)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)    com.opensymphony.webwork.dispatcher.DispatcherUtils.serviceAction(DispatcherUtils.java:274)    com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:202)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:61)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:61)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:204)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:138)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.security.auth.trustedapps.filter.TrustedApplicationsFilter.doFilter(TrustedApplicationsFilter.java:100)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.spring.filter.FlushingSpringSessionInViewFilter.doFilterInternal(FlushingSpringSessionInViewFilter.java:29)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.bamboo.filter.CompressingFilter.doFilter(CompressingFilter.java:69)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter.doFilter(RequestCacheThreadLocalFilter.java:26)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:210)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:174)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:151)    org.apache.coyote.ajp.AjpAprProcessor.process(AjpAprProcessor.java:444)    org.apache.coyote.ajp.AjpAprProtocol$AjpConnectionHandler.process(AjpAprProtocol.java:472)    org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1286)    java.lang.Thread.run(Thread.java:595)  Caused by: com.xerox.amazonws.ec2.EC2Exception: Client error : You are not subscribed to this service. Please go to <span class=""nobr""><a href=""http://aws.amazon.com"">http://aws.amazon.com<sup><img class=""rendericon"" src=""/images/icons/linkext7.gif"" height=""7"" width=""7"" align=""absmiddle"" alt="""" border=""0""/></sup></a></span> to subscribe.    com.xerox.amazonws.ec2.Jec2.makeRequestInt(Jec2.java:1680)    com.xerox.amazonws.ec2.Jec2.describeAvailabilityZones(Jec2.java:1284)    com.atlassian.aws.AWSAccountImpl.validate(AWSAccountImpl.java:69) ... 151 more  {noformat}    This bug was actually found on Studio 2.1 which is running Bamboo 2.5-m5 (long story)."
0,REST API missing / in url for build log artifact ,"Problem is only with the *Build Log*, normal artifacts have the correct URL.    https://bamboo.community.atlassian.com/rest/api/latest/build/CONFTRIGGER-TRUNK?expand=builds[0].build.artifacts    Note missing / before *download*:  {code:xml}  <artifact>  <name>Build log</name>  <link rel=""self"" href=""http://bamboo.community.atlassian.comdownload/CONFTRIGGER-TRUNK/buildSummary_logs/CONFTRIGGER-TRUNK-1553.log""/>  </artifact>  {code}"
1,Elastic Images for Windows 2003 with IE7 and IE8 for Selenium,It would be excellent to have EC2 Images with IE7 and IE8 for Selenium testing.    A few concerns:  * Build system changes  * Security on the Windows hosts  ** Need some way to RDP to the host without the ec2 images having the same user/pass  * Windows 2003 Server security needs to be tuned to Windows XP level (Selenium will break if the Security Zones are not configured correctly)  * Windows software updates  * Should we install Firefox 2/3 and Google Chrome?
1,Expire build logs,NULL
1,CAPABILITY  and NOTIFICATIONS table access with Empty values causes (Oracle) Deadlock in larger instances,"Frequent deadlocks are reported by (Oracle) DB when accessing the capability table.  Databases wait to actually write (flush) data until someone wants to read it.    The hibernate session is being flushed and for some reason the capability object is marked as dirty.  Hibernate issues an update to persist the capability. Not usually a big deal but with a large number of plans and the fact that this is invoked from an event, we could have events that try to lock competing resources.    "
0,Broken link to the dashboard on the /error/error.action,"In case there're no errors the /error/error.action contains following:    {code}  There are no errors accessing Bamboo at present. Return to <a href=""//bamboo"">dashboard</a>.  {code}"
1,Ensure all reports work with new test infrastructure,"Reports = those on the reports page.    * Some of the reports relied on the test count information being available on the buildresult  * The tests report relied on the test data being indexed.  Now it is not.   * We should index the testsummary object    Reports that use it  * Test report    In the build data table, it may need test summary  "
1,'view' a plan config tab and click 'edit' - you land on a tab that you've last edited (and not the one you just viewed),annoying cookie crumble here.    If I view a config tab and click edit - where do I want to land? -- exactly you got it - I want to edit the tab that I just viewed and not some random tab that I might have edited in the past.    
1,Upgrade task fails on MS SQL Server due to deadlock,Upgrade task 1825 reports deadlock and fails. It works correctly only if run as single thread.
2,Deadlocks reported during change detection on MS SQL Server,Observed on installation created from J2BAC export.
1,Brute force protection,Login access needs to present a captcha
0,invalid backup cron expression leads to NPE when trying to edit.,there is no remedy other than manually editing <bamboo-home>/xml-data/configuration/administration.xml    If an invalid cron expression is detected the edit screen  should be accessible with the default cron expression.    
0,JCaptcha throwing occasional exception on OS X,"{noformat}  2010-04-22 12:02:26,281 ERROR [1173503085@qtp-1259610518-2] [FiveOhOh:46] 500 Exception was thrown.  com.octo.captcha.CaptchaException: word is too tall: try to use less letters, smaller font or bigger background:  text bounds = {text=stoden  	java.awt.geom.Rectangle2D$Float[x=0.0,y=-32.871094,w=19.22461,h=40.04297] ascent=32.871094 descent=7.171875 leading=0.0   	java.awt.geom.Rectangle2D$Float[x=39.22461,y=-29.357,w=7.9051514,h=38.409] ascent=29.357 descent=9.052 leading=0.0   	java.awt.geom.Rectangle2D$Float[x=67.12976,y=-21.811523,w=15.0,h=30.703125] ascent=21.811523 descent=8.891602 leading=0.0   	java.awt.geom.Rectangle2D$Float[x=102.12976,y=-25.410645,w=20.157715,h=33.0] ascent=25.410645 descent=7.5893555 leading=0.0   	java.awt.geom.Rectangle2D$Float[x=142.28748,y=-60.0,w=16.399902,h=108.08] ascent=60.0 descent=48.08 leading=0.0   	java.awt.geom.Rectangle2D$Float[x=178.68738,y=-60.0,w=20.879883,h=108.08] ascent=60.0 descent=48.08 leading=0.0   	} with fonts {java.awt.Font[family=PMingLiU,name=PMingLiU,style=plain,size=32]  	java.awt.Font[family=Dialog,name=BrushScriptMT,style=bold,size=34]  	java.awt.Font[family=Bodoni SvtyTwo OS ITC TT,name=BodoniSvtyTwoOSITCTT-BookIt,style=italic,size=31]  	java.awt.Font[family=New Peninim MT,name=NewPeninimMT-Bold,style=bolditalic,size=30]  	java.awt.Font[family=Helvetica,name=Helvetica-Bold,style=bolditalic,size=33]  	java.awt.Font[family=Zapfino,name=Zapfino,style=bolditalic,size=32]  	} versus image width = 200, height = 100    com.octo.captcha.image.gimpy.GimpyFactory.getImageCaptcha(GimpyFactory.java:79)    com.octo.captcha.engine.image.ListImageCaptchaEngine.getNextImageCaptcha(ListImageCaptchaEngine.java:128)    com.octo.captcha.engine.image.ImageCaptchaEngine.getNextCaptcha(ImageCaptchaEngine.java:52)    com.octo.captcha.service.AbstractCaptchaService.generateAndStoreCaptcha(AbstractCaptchaService.java:148)    com.octo.captcha.service.AbstractManageableCaptchaService.generateCountTimeStampAndStoreCaptcha(AbstractManageableCaptchaService.java:340)    com.octo.captcha.service.AbstractManageableCaptchaService.generateAndStoreCaptcha(AbstractManageableCaptchaService.java:329)    com.octo.captcha.service.AbstractCaptchaService.getChallengeForID(AbstractCaptchaService.java:74)    com.octo.captcha.service.image.AbstractManageableImageCaptchaService.getImageChallengeForID(AbstractManageableImageCaptchaService.java:62)    com.atlassian.bamboo.servlet.BambooCaptchaServlet.doGet(BambooCaptchaServlet.java:38)    {noformat}    http://jcaptcha.octo.com/jira/browse/FWK-58"
1,Column AUTHOR_NAME in AUTHOR table should be unique,NULL
0,Link is escaped on Elastic Bamboo Configuration scrren,see screenshot (link to the enable remote agents page inside the error box is escaped)
0,"After installing Bamboo (on Windows), Bamboo automatically detects and assigns the JRE executables of a JDK installation rather than the JDK executables.","When you install a JDK on windows, it usually installs a JDK and JRE component to the following paths:  * {{C:\Java\jdk1.6.0_10}}  * {{C:\Java\jre6}}    When Bamboo automatically detects the presence of local JDKs (after having installed a JDK and Bamboo on Windows), Bamboo appears to assign these locally detected JDKs the path to the JRE executables of the JDK installation (for example, {{C:\Java\jre6}}), rather than the path to the JDK executables instead (i.e. {{C:\Java\jdk1.6.0_10}}).    I'm not sure if this happens on other operating systems.  "
1,Labmanager should remove agents properly,From https://studio.atlassian.com/browse/LABMAN-26
0,Incorrect state shown on remote agent page,"After a remote  agent is stopped the state shown in the page cycles between Disabled (correct) and ""will be disabled when the build finishes"" (incorrect as agent process is long dead)  "
1,Support MSSQL with Unicode,[Confluence supports MSSQL Unicode|http://confluence.atlassian.com/display/DOC/Known+Issues+For+SQL+Server#KnownIssuesForSQLServer-UnicodeCharactersNotSupportedByDefault]    Bamboo should do as well. Include net/sf/hibernate/dialect/SQLServerIntlDialect.java (we can steal it from confluence)    I have verified that it works.    
1,SaveBuildTask & LocalBuildProcessor should be run as a single tranasactional call,"SaveBuildTask needs some refactoring:  1. It should not be a task, instead, it should be a service.  2. At least database access for BRS and test classes should be made in a single transaction.  3. Consider moving updateBuildSummaryFromBuildResults from BRSManager to the newly created service."
0,Elastic bamboo agent (bamboo-agent-home) uses small 10G root partition of elastic instance.,"the elastic agent home is located in the small (10G) root partition of the el instance.  the rest of the disk space is available on /mnt which is not used at all!!!    bamboo-agent-home should be linked to use /mnt/bamboo/bamboo-agent-home for bamboo data  /home/bamboo _could_ be linked to  /mnt/bamboo/. for agent logs etc. but this is not urgent,    This issue occurs if no ebs volumes are used.    "
1,Create reusable progress bar component,NULL
1,Hibernate Exception when hitting dashboard,"2010-06-10 11:08:45,082 ERROR [304703545@qtp-46675819-10] [LazyInitializer] Exception initializing proxy  net.sf.hibernate.HibernateException: Could not initialize proxy - the owning Session was closed    net.sf.hibernate.proxy.LazyInitializer.initialize(LazyInitializer.java:47)    net.sf.hibernate.proxy.LazyInitializer.initializeWrapExceptions(LazyInitializer.java:60)    net.sf.hibernate.proxy.LazyInitializer.getImplementation(LazyInitializer.java:164)    net.sf.hibernate.proxy.CGLIBLazyInitializer.intercept(CGLIBLazyInitializer.java:108)    com.atlassian.bamboo.project.DefaultProject$$EnhancerByCGLIB$$ed17e412.getKey(<generated>)    com.atlassian.bamboo.project.ProjectStatusHelperImpl.<init>(ProjectStatusHelperImpl.java:39)    com.atlassian.bamboo.webwork.StarterAction.getProjectStatusHelper(StarterAction.java:146)    sun.reflect.GeneratedMethodAccessor358.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    ognl.OgnlRuntime.invokeMethod(OgnlRuntime.java:778)    ognl.OgnlRuntime.getMethodValue(OgnlRuntime.java:1297)    ognl.ObjectPropertyAccessor.getPossibleProperty(ObjectPropertyAccessor.java:60)    ognl.ObjectPropertyAccessor.getProperty(ObjectPropertyAccessor.java:147)    com.opensymphony.xwork.util.OgnlValueStack$ObjectAccessor.getProperty(OgnlValueStack.java:57)    ognl.OgnlRuntime.getProperty(OgnlRuntime.java:1940)    com.opensymphony.xwork.util.CompoundRootAccessor.getProperty(CompoundRootAccessor.java:334)    ognl.OgnlRuntime.getProperty(OgnlRuntime.java:1940)    ognl.ASTProperty.getValueBody(ASTProperty.java:114)    ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:212)    ognl.SimpleNode.getValue(SimpleNode.java:258)    ognl.Ognl.getValue(Ognl.java:494)    ognl.Ognl.getValue(Ognl.java:458)    com.opensymphony.xwork.util.OgnlUtil.getValue(OgnlUtil.java:287)    com.opensymphony.xwork.util.OgnlValueStack.findValue(OgnlValueStack.java:224)    com.opensymphony.webwork.views.freemarker.ScopesHashModel.get(ScopesHashModel.java:51)    freemarker.core.Environment.getGlobalVariable(Environment.java:1063)    freemarker.core.Environment.getVariable(Environment.java:1049)    freemarker.core.Identifier._getAsTemplateModel(Identifier.java:70)    freemarker.core.Expression.getAsTemplateModel(Expression.java:89)    freemarker.core.Dot._getAsTemplateModel(Dot.java:74)    freemarker.core.Expression.getAsTemplateModel(Expression.java:89)    freemarker.core.MethodCall._getAsTemplateModel(MethodCall.java:86)    freemarker.core.Expression.getAsTemplateModel(Expression.java:89)    freemarker.core.Expression.getStringValue(Expression.java:93)    freemarker.core.DollarVariable.accept(DollarVariable.java:76)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.IfBlock.accept(IfBlock.java:82)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.IfBlock.accept(IfBlock.java:82)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.IteratorBlock$Context.runLoop(IteratorBlock.java:167)    freemarker.core.Environment.visit(Environment.java:417)    freemarker.core.IteratorBlock.accept(IteratorBlock.java:102)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.Macro$Context.runMacro(Macro.java:172)    freemarker.core.Environment.visit(Environment.java:603)    freemarker.core.UnifiedCall.accept(UnifiedCall.java:106)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.Environment.process(Environment.java:190)    freemarker.template.Template.process(Template.java:237)    com.opensymphony.webwork.views.freemarker.FreemarkerResult.doExecute(FreemarkerResult.java:214)    com.opensymphony.webwork.dispatcher.WebWorkResultSupport.execute(WebWorkResultSupport.java:143)    com.opensymphony.xwork.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:313)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:208)    com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:175)    com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)    com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:84)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:49)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ChainAwareInterceptor.intercept(ChainAwareInterceptor.java:93)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:46)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PlanAwareInterceptor.intercept(PlanAwareInterceptor.java:53)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)    com.opensymphony.webwork.components.ActionComponent.executeAction(ActionComponent.java:229)    com.opensymphony.webwork.components.ActionComponent.end(ActionComponent.java:127)    com.opensymphony.webwork.views.freemarker.tags.CallbackWriter.afterBody(CallbackWriter.java:65)    freemarker.core.Environment.visit(Environment.java:301)    freemarker.core.UnifiedCall.accept(UnifiedCall.java:130)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.Environment.process(Environment.java:190)    freemarker.template.Template.process(Template.java:237)    com.opensymphony.webwork.views.freemarker.FreemarkerResult.doExecute(FreemarkerResult.java:214)    com.opensymphony.webwork.dispatcher.WebWorkResultSupport.execute(WebWorkResultSupport.java:143)    com.opensymphony.xwork.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:313)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:208)    com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:175)    com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)    com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)    com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:84)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:49)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.ChainAwareInterceptor.intercept(ChainAwareInterceptor.java:93)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:46)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.PlanAwareInterceptor.intercept(PlanAwareInterceptor.java:53)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)    com.opensymphony.webwork.components.ActionComponent.executeAction(ActionComponent.java:229)    com.opensymphony.webwork.components.ActionComponent.end(ActionComponent.java:127)    com.opensymphony.webwork.views.freemarker.tags.CallbackWriter.afterBody(CallbackWriter.java:65)    freemarker.core.Environment.visit(Environment.java:301)    freemarker.core.UnifiedCall.accept(UnifiedCall.java:130)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.IfBlock.accept(IfBlock.java:82)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.IfBlock.accept(IfBlock.java:82)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.IfBlock.accept(IfBlock.java:82)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.MixedContent.accept(MixedContent.java:92)    freemarker.core.Environment.visit(Environment.java:210)    freemarker.core.Environment.process(Environment.java:190)    freemarker.template.Template.process(Template.java:237)    com.opensymphony.webwork.views.freemarker.FreemarkerResult.doExecute(FreemarkerResult.java:214)    com.opensymphony.webwork.dispatcher.WebWorkResultSupport.execute(WebWorkResultSupport.java:143)    com.opensymphony.xwork.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:313)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:208)    com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:175)    com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)    com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)    com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)    com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)    com.opensymphony.xwork.DefaultActionI..."
0,"Implicit 'Build Requirements' from 'Builders' are not removed when changing 'Builders', causes builds to not execute","When creating/copying a plan, there are implicit build requirements derived from the selected builder. These requirements can not be managed by the user, but are added and deleted on their behalf by Bamboo when selecting a builder.    In 2.6 build 1901 - 27 May 10, when one changed the builder for a plan; the old builder requirement is not removed from the plan:    !Build Requirements.png!    This causes the build to no be able to schedule on desired agents, leaving it in the build queue.    Once saved, the requirements can not be removed; cloning the build plan doesn't work around this issue.    This will only affect plans that change builders."
0,Label of test cases are incorrect,NULL
0,Fail producer job if required chain artifacts do not get produced, https://extranet.atlassian.com/display/BAMBOO/Copy+in+place+artifact+subscriptions
0,"""Block build if parent plans have unbuilt changes"" doesn't block when parent builds are queued","I have multiple component builds set up in bamboo, with the actual dependencies indicated in the build dependencies for those builds.   Set with a blocking strategy of ""Block build if parent plans have unbuilt changes"" - but I don't think this affects the issue.    Because of the graph of dependencies, and the current methodology of triggering builds in bamboo, the components at the end of the dependency tree can get build multiple times for the same root reason/submission.    Consider the builds:  || Build || Parent Plans ||  | parent | |  | child-A | parent |  | grandchild-B | parent \\  child-A |    If a change is submitted to parent source, then on a successful build, both child-A and grandchild-B builds are queued. Once child-A successfully builds, grandchild-B is queued.    Grandchild-b gets built twice (with non-deterministic results on the first build since it uses the current parent results and the previous child-A results)."
0,phpunit --log-xml option deprecated in phpunit 3.4.3+,"When configuring the PhpUnit Builder,  under the option:  ""Log test execution to XML file  Relative path to file where PHPUnit will store log test execution in XML format (option --log-xml).""    fails in PhpUnit 3.4.3+ since --log-xml is now deprecated option in PhpUnit.  Message: The --log-xml option is deprecated, please use --log-junit instead    Therefore the only way to successfully build with PhpUnit 3.4.3+ is to have ""Log test execution to XML file"" disabled."
0,custom.svn.lastchange.revision.number is omitted in build result metadata when reprository advanced option 'quiet period' is enabled,reproduce:    enable quiet period in plan (advanced repository options)  trigger build via commit or manual.
0,PHPUnit builder fails by default on Windows,"I installed Babmoo 2.6.1. It found PHPUnit and added PHPUnit builder correctly without any user action.  However when I created and started a plan using the PHPUnit builder it fails with following error:    {code}  Failed to execute build: Cannot run program ""C:\Program Files (x86)\PHP\phpunit"" (in directory ""C:\Users\Jacek Jaroczynski\bamboo-home\xml-data\build-dir\TEST-TEST""): CreateProcess error=193, %1 is not a valid Win32 application  {code}    The solution was to explicitly configure PHPUnit builder to point at *phpunit.bat* instead of *phpunit*"
1,Amazon EC2 commands do not timeout,"    {noformat}  QuartzScheduler_Worker-2 [RUNNABLE, IN_NATIVE] CPU time: 0:12  java.net.SocketInputStream.read(byte[], int, int)  com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputStream, byte[], int, int)  com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputStream, OutputStream)  com.sun.net.ssl.internal.ssl.InputRecord.read(InputStream, OutputStream)  com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(InputRecord, boolean)  com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(InputRecord)  com.sun.net.ssl.internal.ssl.AppInputStream.read(byte[], int, int)  org.apache.commons.httpclient.HttpClient.executeMethod(HttpMethod)  com.xerox.amazonws.common.AWSQueryConnection.makeRequest(HttpMethodBase, String, Map, Class)  com.xerox.amazonws.ec2.Jec2.makeRequestInt(HttpMethodBase, String, Map, Class)  com.xerox.amazonws.ec2.Jec2.describeSecurityGroups(List)  com.atlassian.aws.AWSAccountImpl.getEC2SecurityGroups()  com.atlassian.bamboo.agent.elastic.server.ElasticFunctionalityFacadeImpl.ensureSecurityGroupExists(AWSAccount)  com.atlassian.bamboo.agent.elastic.server.ElasticFunctionalityFacadeImpl.startupAgents(Collection)  com.atlassian.bamboo.agent.elastic.schedule.ElasticInstancesMonitorJob.execute(JobExecutionContext)  org.quartz.core.JobRunShell.run()  com.atlassian.bamboo.quartz.SystemAuthorizedThreadPool$1.run()  org.quartz.simpl.SimpleThreadPool$WorkerThread.run()  {noformat}    this can lead to some fairly serious thread starvation issues like http://skitch.com/markchai/dknaw/bootstrap-yourkit-java-profiler-8.0.24"
0,Maven Import doesn't work if the Maven builder isn't labelled 'Maven 2',"Bamboo doesn't seem to pick up the Maven builder if its using a non-default label, it complains about {{Bamboo is currently not configured to build Maven 2 projects. Proceed to builder configuration before importing Maven plan. }}    "
0,Cannot import Maven POM from repositories requiring authentication,"When I try to import a Maven POM from my CVS repository I get the error ""Cannot connect to CVS root: Wrong Password.""    (see attachment)    However the password specified is exactly the same than the one I specify in another plan to retrieve files from my CVS, and in this case it can connect succesfully to the CVS server.    Furthermore the error generates the following stacktrace in bamboo.log :    INFO   | jvm 1    | 2010/07/09 15:13:11 | 2010-07-09 15:13:11,079 INFO [1619340@qtp-6161922-7] [AccessLogFilter] system http://vbsbuiltin.ivb.victor-buck.com:8085/admin/importMavenPlanExecutePomCheckout.action 136127kb  INFO   | jvm 1    | 2010/07/09 15:13:11 | 2010-07-09 15:13:11,094 INFO [1619340@qtp-6161922-7] [PlanAwareInterceptor] /admin/importMavenPlanExecutePomCheckout.action  INFO   | jvm 1    | 2010/07/09 15:13:11 | 2010-07-09 15:13:11,735 ERROR [1619340@qtp-6161922-7] [runtime] Expression repository.p4Executable is undefined on line 4, column 26 in templates/plugins/repository/p4RepositoryMavenPomCheckoutAccessEdit.ftl.  INFO   | jvm 1    | 2010/07/09 15:13:11 |   INFO   | jvm 1    | 2010/07/09 15:13:11 | Expression repository.p4Executable is undefined on line 4, column 26 in templates/plugins/repository/p4RepositoryMavenPomCheckoutAccessEdit.ftl.  INFO   | jvm 1    | 2010/07/09 15:13:11 | The problematic instruction:  INFO   | jvm 1    | 2010/07/09 15:13:11 | ----------  INFO   | jvm 1    | 2010/07/09 15:13:11 | ==> ${repository.p4Executable} [on line 4, column 24 in templates/plugins/repository/p4RepositoryMavenPomCheckoutAccessEdit.ftl]  INFO   | jvm 1    | 2010/07/09 15:13:11 |  in user-directive ww.param [on line 4, column 13 in templates/plugins/repository/p4RepositoryMavenPomCheckoutAccessEdit.ftl]  INFO   | jvm 1    | 2010/07/09 15:13:11 |  in user-directive ww.text [on line 3, column 9 in templates/plugins/repository/p4RepositoryMavenPomCheckoutAccessEdit.ftl]  INFO   | jvm 1    | 2010/07/09 15:13:11 | ----------  INFO   | jvm 1    | 2010/07/09 15:13:11 |   INFO   | jvm 1    | 2010/07/09 15:13:11 | Java backtrace for programmers:  INFO   | jvm 1    | 2010/07/09 15:13:11 | ----------  INFO   | jvm 1    | 2010/07/09 15:13:11 | freemarker.core.InvalidReferenceException: Expression repository.p4Executable is undefined on line 4, column 26 in templates/plugins/repository/p4RepositoryMavenPomCheckoutAccessEdit.ftl.  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.TemplateObject.assertNonNull(TemplateObject.java:124)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Expression.getStringValue(Expression.java:118)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Expression.getStringValue(Expression.java:93)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.DollarVariable.accept(DollarVariable.java:76)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:299)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.UnifiedCall.accept(UnifiedCall.java:130)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:299)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.UnifiedCall.accept(UnifiedCall.java:130)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.IfBlock.accept(IfBlock.java:82)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.process(Environment.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.template.Template.process(Template.java:237)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:310)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:250)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.BambooFreemarkerManager.render(BambooFreemarkerManager.java:319)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.repository.MavenPomAccessorAbstract.getMavenPomCheckoutAccessEditHtml(MavenPomAccessorAbstract.java:36)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   sun.reflect.GeneratedMethodAccessor612.invoke(Unknown Source)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   java.lang.reflect.Method.invoke(Unknown Source)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.ext.beans.BeansWrapper.invokeMethod(BeansWrapper.java:841)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.ext.beans.SimpleMethodModel.exec(SimpleMethodModel.java:106)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MethodCall._getAsTemplateModel(MethodCall.java:93)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Expression.getAsTemplateModel(Expression.java:89)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.DefaultToExpression._getAsTemplateModel(DefaultToExpression.java:100)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Expression.getAsTemplateModel(Expression.java:89)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Expression.getStringValue(Expression.java:93)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.DollarVariable.accept(DollarVariable.java:76)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:395)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.BodyInstruction.accept(BodyInstruction.java:93)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Macro$Context.runMacro(Macro.java:172)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:603)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.UnifiedCall.accept(UnifiedCall.java:106)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.IteratorBlock$Context.runLoop(IteratorBlock.java:167)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:417)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.IteratorBlock.accept(IteratorBlock.java:102)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:299)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.UnifiedCall.accept(UnifiedCall.java:130)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.IfBlock.accept(IfBlock.java:82)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.MixedContent.accept(MixedContent.java:92)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.visit(Environment.java:210)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.core.Environment.process(Environment.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   freemarker.template.Template.process(Template.java:237)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.webwork.views.freemarker.FreemarkerResult.doExecute(FreemarkerResult.java:214)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.webwork.dispatcher.WebWorkResultSupport.execute(WebWorkResultSupport.java:143)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:313)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:208)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.GlobalAdminInterceptor.intercept(GlobalAdminInterceptor.java:21)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:84)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:41)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:49)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.ChainAwareInterceptor.intercept(ChainAwareInterceptor.java:93)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.BuildAwareInterceptor.intercept(BuildAwareInterceptor.java:46)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.PlanAwareInterceptor.intercept(PlanAwareInterceptor.java:53)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.ww2.interceptors.BuildConfigurationInterceptor.intercept(BuildConfigurationInterceptor.java:54)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.webwork.dispatcher.DispatcherUtils.serviceAction(DispatcherUtils.java:274)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:202)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:66)  INFO   | jvm 1    | 2010/07/09 15:13:11 |   org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  INFO   | jvm 1    | 2010/07/09 15:13:11 |  ..."
1,Activity Stream atom feed throws NPE,"{noformat}  2010-07-12 11:58:12,434 ERROR [http-8085-22] [BambooFreemarkerManager] Error resolving template  java.lang.NullPointerException    org.apache.catalina.connector.Request.setAttribute(Request.java:1424)    org.apache.catalina.connector.RequestFacade.setAttribute(RequestFacade.java:503)    org.apache.catalina.core.ApplicationHttpRequest.setAttribute(ApplicationHttpRequest.java:279)    javax.servlet.ServletRequestWrapper.setAttribute(ServletRequestWrapper.java:284)    com.opensymphony.webwork.views.freemarker.FreemarkerManager.buildScopesHashModel(FreemarkerManager.java:192)    com.opensymphony.webwork.views.freemarker.FreemarkerManager.buildTemplateModel(FreemarkerManager.java:324)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:324)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:274)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.render(BambooFreemarkerManager.java:340)    com.atlassian.bamboo.v2.build.trigger.DefaultTriggerReasonRenderer.getShortDescriptionHtml(DefaultTriggerReasonRenderer.java:40)    com.atlassian.bamboo.resultsummary.AbstractResultsSummary.getReasonSummary(AbstractResultsSummary.java:175)    com.atlassian.streams.bamboo.BuildToSyndTransformer.buildDescriptionHtml(BuildToSyndTransformer.java:127)    com.atlassian.streams.bamboo.BuildToSyndTransformer.apply(BuildToSyndTransformer.java:67)    com.atlassian.streams.bamboo.BuildToSyndTransformer.apply(BuildToSyndTransformer.java:24)    com.google.common.collect.Lists$TransformingRandomAccessList.get(Lists.java:431)    com.sun.syndication.feed.synd.impl.ConverterForAtom10.createAtomEntries(ConverterForAtom10.java:416)    com.sun.syndication.feed.synd.impl.ConverterForAtom10.createRealFeed(ConverterForAtom10.java:397)    com.sun.syndication.feed.synd.SyndFeedImpl.createWireFeed(SyndFeedImpl.java:207)    com.sun.syndication.feed.synd.SyndFeedImpl.createWireFeed(SyndFeedImpl.java:189)    com.sun.syndication.io.SyndFeedOutput.outputString(SyndFeedOutput.java:61)    com.atlassian.streams.servlet.StreamsActivityServlet.doGet(StreamsActivityServlet.java:167)    javax.servlet.http.HttpServlet.service(HttpServlet.java:617)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    com.atlassian.plugin.servlet.DelegatingPluginServlet.service(DelegatingPluginServlet.java:42)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    com.atlassian.plugin.servlet.ServletModuleContainerServlet.service(ServletModuleContainerServlet.java:52)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:66)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:66)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:219)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:140)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.bamboo.persistence.BambooSessionInViewFilter.doFilterInternal(BambooSessionInViewFilter.java:31)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)    org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:845)    org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)    org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)    java.lang.Thread.run(Thread.java:637)  2010-07-12 11:58:12,439 INFO [BAM::Events:pool-1-thread-35] [DependencyBuildListener] Finished checking children plans of 'Bamboo Plugins - Pre and Post Build Command Plugin' for build BPLUG-BPBC-247  2010-07-12 11:58:12,468 ERROR [http-8085-22] [BambooFreemarkerManager] Error resolving template  java.lang.NullPointerException    org.apache.catalina.connector.Request.setAttribute(Request.java:1424)    org.apache.catalina.connector.RequestFacade.setAttribute(RequestFacade.java:503)    org.apache.catalina.core.ApplicationHttpRequest.setAttribute(ApplicationHttpRequest.java:279)    javax.servlet.ServletRequestWrapper.setAttribute(ServletRequestWrapper.java:284)    com.opensymphony.webwork.views.freemarker.FreemarkerManager.buildScopesHashModel(FreemarkerManager.java:202)    com.opensymphony.webwork.views.freemarker.FreemarkerManager.buildTemplateModel(FreemarkerManager.java:324)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:324)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.renderPage(BambooFreemarkerManager.java:274)    com.atlassian.bamboo.ww2.BambooFreemarkerManager.render(BambooFreemarkerManager.java:340)    com.atlassian.bamboo.v2.build.trigger.DefaultTriggerReasonRenderer.getShortDescriptionHtml(DefaultTriggerReasonRenderer.java:40)    com.atlassian.bamboo.resultsummary.AbstractResultsSummary.getReasonSummary(AbstractResultsSummary.java:175)    com.atlassian.streams.bamboo.BuildToSyndTransformer.buildDescriptionHtml(BuildToSyndTransformer.java:127)    com.atlassian.streams.bamboo.BuildToSyndTransformer.apply(BuildToSyndTransformer.java:67)    com.atlassian.streams.bamboo.BuildToSyndTransformer.apply(BuildToSyndTransformer.java:24)    com.google.common.collect.Lists$TransformingRandomAccessList.get(Lists.java:431)    com.sun.syndication.feed.synd.impl.ConverterForAtom10.createAtomEntries(ConverterForAtom10.java:416)    com.sun.syndication.feed.synd.impl.ConverterForAtom10.createRealFeed(ConverterForAtom10.java:397)    com.sun.syndication.feed.synd.SyndFeedImpl.createWireFeed(SyndFeedImpl.java:207)    com.sun.syndication.feed.synd.SyndFeedImpl.createWireFeed(SyndFeedImpl.java:189)    com.sun.syndication.io.SyndFeedOutput.outputString(SyndFeedOutput.java:61)    com.atlassian.streams.servlet.StreamsActivityServlet.doGet(StreamsActivityServlet.java:167)    javax.servlet.http.HttpServlet.service(HttpServlet.java:617)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    com.atlassian.plugin.servlet.DelegatingPluginServlet.service(DelegatingPluginServlet.java:42)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    com.atlassian.plugin.servlet.ServletModuleContainerServlet.service(ServletModuleContainerServlet.java:52)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:66)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:66)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:219)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:140)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.bamboo.persistence.BambooSessionInViewFilter.doFilterInternal(BambooSessionInViewFilter.java:31)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)    org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:845)    org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)    org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)    java.lang.Thread.run(Thread.java:637)    {noformat}"
0,Scheduled backups are written to Bamboo Installation directory,Bamboo writes scheduled backups to the Bamboo-Install directory (or the location it was started from) instead of <Bamboo-Home>/export directory.
0,Allow admins to shutdown instances that are not connected to bamboo,There were some reports that bamboo leaves 'orphaned' elastic instances and detached ebs volumes.   Add functionality to allow admins to view/shutdown instances that are not currently connected to bamboo.
1,User can set up chain completed notification,NULL
0,User can set up chain failed + 1st success notification,NULL
0,Ensure notifications aren't run / throw exceptions for SubPlan,NULL
0,Drag and drop for moving SubPlan between stages,NULL
0,Live Job  Result Summary REST does not lookup Buildable,This causes the page to be devoid of useful content.
0,Test for SASL support with bundled svnkit,"The version of svnkit we bundle with bamboo doesn't have SASL support:    {noformat}[amyers@erdinger:svnkit]$ jar tf svnkit-1.3.0.5847.jar | grep Sasl  [amyers@erdinger:svnkit]$   {noformat}    If I download it from the website it does (this is a newer version, but apparently older ones have it too):    {noformat}  [amyers@erdinger:svnkit-1.3.2.6267]$ jar tf svnkit.jar | grep Sasl  org/tmatesoft/svn/core/internal/io/svn/sasl/SVNSaslAuthenticator$SVNCallbackHandler.class  org/tmatesoft/svn/core/internal/io/svn/sasl/SVNSaslAuthenticator.class  org/tmatesoft/svn/core/internal/io/svn/sasl/SaslInputStream.class  org/tmatesoft/svn/core/internal/io/svn/sasl/SaslOutputStream.class  [amyers@erdinger:svnkit-1.3.2.6267]$   {noformat}    Is there any technical reason we don't bundle the support for SASL authentication? "
0,Ability to Stop a Job,NULL
1,User can clone another chain's job configuration,NULL
0,Hide configuration tabs of Job,NULL
1,Chain configuration should use static tabs,NULL
1,Ensure dependencies work for chains,"Functional tests, validation, description, chains & plans in the same list."
0,New failure periods tab for Chains,NULL
0,Update bread crumbs to be the title,NULL
1,Ensure basic charts work for chains,NULL
0,System Variables does not work in Source repository,When we try to use System envirenment variables in the Repository URL we get an error : This is not a valid Subversion Repository.  We think the system property is not replaced
0,Servlet api 2.5 broken,"NoSuchMethodError: javax.servlet.ServletContext.getContextPath()Ljava/lang/String when installing the JavaMelody plugin.  This issue is based on the initial issue submitted at http://code.google.com/p/javamelody/issues/detail?id=37      Steps to reproduce:  1. Download the JavaMelody plugin from https://plugins.atlassian.com/plugin/details/20909  2. Copy the jar into the webapp/WEB-INF/lib folder  3. Start Bamboo (2.6.1)  4. Tail the bamboo log file    The Bamboo logs show this stack trace when JavaMelody is installed in Bamboo 2.6.1:    2010-07-16 14:40:46,712 ERROR [796347291@qtp-745957924-0] [DefaultServletModuleManager] Unable to create filter  com.atlassian.plugin.servlet.util.LazyLoadedReference$InitializationException: java.lang.NoSuchMethodError: javax.servlet.ServletContext.getContextPath()Ljava/lang/String;          at com.atlassian.plugin.servlet.util.LazyLoadedReference.get(LazyLoadedReference.java:94)          at com.atlassian.plugin.servlet.DefaultServletModuleManager.getFilter(DefaultServletModuleManager.java:321)          at com.atlassian.plugin.servlet.DefaultServletModuleManager.getFilters(DefaultServletModuleManager.java:188)          at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:53)          at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)          at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)          at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)          at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)          at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)          at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:378)          at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)          at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)          at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)          at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)          at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)          at org.mortbay.jetty.Server.handle(Server.java:324)          at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)          at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:865)          at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)          at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)          at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)          at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)          at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)  Caused by: java.lang.NoSuchMethodError: javax.servlet.ServletContext.getContextPath()Ljava/lang/String;          at net.bull.javamelody.Parameters.getContextPath(Parameters.java:328)          at net.bull.javamelody.MonitoringFilter.init(MonitoringFilter.java:94)          at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.init(DelegatingPluginFilter.java:43)          at com.atlassian.plugin.servlet.DefaultServletModuleManager$LazyLoadedFilterReference.create(DefaultServletModuleManager.java:371)          at com.atlassian.plugin.servlet.DefaultServletModuleManager$LazyLoadedFilterReference.create(DefaultServletModuleManager.java:356)          at com.atlassian.plugin.servlet.util.LazyLoadedReference$1.call(LazyLoadedReference.java:62)          at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)          at java.util.concurrent.FutureTask.run(Unknown Source)          at com.atlassian.plugin.servlet.util.LazyLoadedReference.get(LazyLoadedReference.java:74)    My analysis:  This ServletContext.getContextPath() method is called by the javamelody plugin only if the servlet api version is 2.5 or more.  So the fact that the method is called in Bamboo v2.6.1 Build 1902 / jetty v6.1.15 implies that ServletContext.getMajorVersion() has returned 2 and that ServletContext.getMinorVersion() has returned 5.  But the method call in the plugin throws ""java.lang.NoSuchMethodError: javax.servlet.ServletContext.getContextPath()Ljava/lang/String;"",  even if this method is supposed to exists in the servlet api 2.5.    This issue implies that in this setup (in a plugin with bamboo 2.6.1 Build 1902 and jetty v6.1.15), the servlet api 2.5 was broken somehow: the method must exists.  This is probably related to the fact that there are both servlet-api-2.4.jar and servlet-api-2.5.jar in ""Bamboo/webapp/WEB-INF/lib/"".    Workaround:  The following workaround was tested with success:  1. stop the bamboo server  2. backup and delete the file ""Bamboo/webapp/WEB-INF/lib/servlet-api-2.4.jar"" from your bamboo setup (but keep the servlet-api-2.5.jar)  3. restart the bamboo server  4. install the javamelody plugin    Thanks anyway for a great plugin architecture.    PS: The JavaMelody plugin goal is to monitor JIRA, Confluence and Bamboo servers.  "
0,"Allow ""Import With Maven"" for ""Restricted Administrators""","We differentiate between System Administrators and Administrators in Studio. System Administrators are Atlassian and Contegix, Administrators may be customers. I was told that these groups roughly map to the notion of Administrators (Studio: System Administrators) and Restricted Administrators (Studio: Administrators) in Bamboo.     The customer is allowed to perform certain administration tasks in Studio's Bamboo, amongst them f.e. to configure the build expiry (BuildExpiryAction). These actions define no interceptor ref and thus use the defaultStack (default-interceptor-ref, xwork.xml). The defaultStack contains as far as I can see only one interceptor for authorisation, that is the permissionCheck (com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy) interceptor which delegates to an extension of Acegi's org.acegisecurity.intercept.AbstractSecurityInterceptor, com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor. The AbstractSecurityInterceptor is configured to check, amongst other things,  if principals trying to access   com.atlassian.bamboo.ww2.aware.permissions.GlobalAdminSecurityAware instances possess one of these authorities: WW_ADMIN,WW_RESTRICTED_ADMIN,GLOBAL_READ (applicationContextAcegiSecurity.xml). The  administrative actions currently configured in Studio to be accessible by Restricted Administrators (Studio: Administrators) implement this interface and thus they are allowed to access the action.    My current task is to allow customers to create build plans from POMs (https://studio.atlassian.com/browse/JST-2328). This action, importMavenPlan (com.atlassian.bamboo.ww2.actions.admin.ImportMavenPlanCheckoutPomAction) uses a different interceptor stack, buildConfig and globalAdminStackAlwaysValidate. Regarding permissions, as far as I can see, the WebworkSecurityInterceptor is still used due to globalAdminStackAlwaysValidate → basicAdminStackPreValidate →  permissionCheck, but the principal is only checked against these authorities: WW_READ,GLOBAL_READ due to the instance being ""only"" a com.atlassian.bamboo.ww2.aware.permissions.GlobalReadSecurityAware (BambooActionSupport, see screenshot). The Restricted Administrators pass this test. However, a second permission check is applied in this configuration, the globalAdminStackAlwaysValidate → basicAdminStackPreValidate → globalAdmin (com.atlassian.bamboo.ww2.interceptors.GlobalAdminInterceptor). This interceptor uses the com.atlassian.bamboo.security.acegi.acls.HibernateMutableAclServiceImpl in order to check if the principal has the ADMINISTRATION permission, which he has not (RESTRICTEDADMINISTRATION instead).    My request is to change the configuration for the importMavenPlan action in order to allow it to be used by Restricted Administrators. One way would be to use the default stack and implement GlobalAdminSecurityAware. I haven't checked what other implications that would have regarding the interceptor configuration. Thx for the help."
0,Cannot create entry in AUTHORS table when running build from Mercurial repository,"Don't forget to clean AUTHORS table to reproduce it.    Sample log  {noformat}  2010-09-04 13:14:49,553 ERROR [BAM::Default Agent::Agent:pool-4-thread-1] [BuildAgentControllerImpl] Unknown exception occurred on 'Default Agent'. Agent will attempt to recover its normal operation...  java.lang.RuntimeException: java.lang.Exception: Could not save the build results BuildResults: RAN88534083-MAIN88534083-DEFAULT-1. Data could be in an inconsistent state.    com.atlassian.bamboo.v2.build.agent.LocalBuildResultProcessor.processBuildResult(LocalBuildResultProcessor.java:146)    com.atlassian.bamboo.v2.build.agent.BuildAgentControllerImpl.waitAndPerformBuild(BuildAgentControllerImpl.java:115)    com.atlassian.bamboo.v2.build.agent.DefaultBuildAgent$1.run(DefaultBuildAgent.java:102)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:47)    java.lang.Thread.run(Thread.java:595)  Caused by: java.lang.Exception: Could not save the build results BuildResults: RAN88534083-MAIN88534083-DEFAULT-1. Data could be in an inconsistent state.    com.atlassian.bamboo.v2.build.agent.BuildResultsSummaryPersisterImpl.saveBuild(BuildResultsSummaryPersisterImpl.java:112)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:592)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy39.saveBuild(Unknown Source)    com.atlassian.bamboo.v2.build.agent.LocalBuildResultProcessor.processBuildResult(LocalBuildResultProcessor.java:141)  	... 4 more  Caused by: java.lang.RuntimeException: AuthorCreatorService must not be called within an existing transaction.    com.atlassian.bamboo.utils.BambooValidate.notInsideTransaction(BambooValidate.java:39)    com.atlassian.bamboo.author.AuthorCreatorServiceImpl.createAuthorIfMissing(AuthorCreatorServiceImpl.java:31)    com.atlassian.bamboo.author.AuthorCreatorServiceImpl.createMissingAuthors(AuthorCreatorServiceImpl.java:62)    com.atlassian.bamboo.v2.build.agent.BuildResultsSummaryPersisterImpl.saveBuild(BuildResultsSummaryPersisterImpl.java:76)  	... 16 more  {noformat}"
0,Comment Tooltip does not show text of the comment anymore On JBAC,"When you put you mouse over the comment icon you used to be able to view the content of the comment withour having to click on it to be take to the comments tab of that build.    Now, this gets shortened so that you are only able to see the commenter and the comment's date. See screenshot.    I am marking this bug as critical, because it's extremely annoying to browse build comments by clicking on them, as you have to wait for a page load and be taken out of the plan view, which you then will have to reload again once you finish reading the comment."
0,IMPORT/EXPORT of login failure info is broken,This error occurs when importing a tardigrade export.  export_tardigradeserver_2014_20100824.zip    Caused by: org.springframework.orm.hibernate.HibernateSystemException: not-null property references a null or transient value: com.atlassian.bamboo.user.LoginInformationImpl.userName; nested exception is net.sf.hibernate.PropertyValueException: not-null property references a null or transient value: com.atlassian.bamboo.user.LoginInformationImpl.userName    org.springframework.orm.hibernate.SessionFactoryUtils.convertHibernateAccessException(SessionFactoryUtils.java:597)    org.springframework.orm.hibernate.HibernateAccessor.convertHibernateAccessException(HibernateAccessor.java:353)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:375)    org.springframework.orm.hibernate.HibernateTemplate.saveOrUpdate(HibernateTemplate.java:601)    com.atlassian.hibernate.HibernateObjectDao.saveRaw(HibernateObjectDao.java:148)    com.atlassian.hibernate.HibernateObjectDao.save(HibernateObjectDao.java:118)    com.atlassian.bamboo.user.LoginInformationHibernateDao.save(LoginInformationHibernateDao.java:49)    com.atlassian.bamboo.user.LoginInformationManagerImpl.injectLoginInformation(LoginInformationManagerImpl.java:79)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy16.injectLoginInformation(Unknown Source)    com.atlassian.bamboo.migration.stream.LoginInformationMapper.afterImportListItem(LoginInformationMapper.java:96)    com.atlassian.bamboo.migration.stream.LoginInformationMapper.afterImportListItem(LoginInformationMapper.java:20)    com.atlassian.bamboo.migration.BambooStAXListImportStrategy.importListItemXml(BambooStAXListImportStrategy.java:268)    com.atlassian.bamboo.migration.BambooStAXListImportStrategy.access$100(BambooStAXListImportStrategy.java:16)    com.atlassian.bamboo.migration.BambooStAXListImportStrategy$1.importListItems(BambooStAXListImportStrategy.java:36)    com.atlassian.bamboo.migration.BambooStAXListImportStrategy.importListXml(BambooStAXListImportStrategy.java:234)
0,HG Changeset detection is totally wrong,"The build https://bamboo-extranet.atlassian.com/browse/CRUCHG-BR22-3 triggered at the right time, but since https://bamboo-extranet.atlassian.com/browse/CRUCHG-BR22-2, there is only 1 change, but https://bamboo-extranet.atlassian.com/browse/CRUCHG-BR22-3/commit lists loads of changes, none of which were even on the 2.2 branch. The real diffs:    {code}  $ hg log -b 2.2 -r a780b00521e8e6464de1c3509f330b4c7d2ffedc:2a4ec7c3fc9db7a907bdf42601ab431615aeca48  changeset:   17456:a780b00521e8  branch:      2.2  parent:      17427:545beab760a6  user:        Seb Ruiz <sruiz@atlassian.com>  date:        Wed Jul 07 04:31:37 2010 +0000  files:       src/java/com/cenqua/fisheye/web/Watch.java  description:  NONE: Fix NPE when comparing invalid watches      changeset:   17628:2a4ec7c3fc9d  branch:      2.2  tag:         tip  parent:      17456:a780b00521e8  user:        Matthew Watson <mwatson@atlassian.com>  date:        Tue Sep 07 17:05:13 2010 +1000  files:       src/tests/com/cenqua/fisheye/license/LicenseTestStrings.java  description:  NONE: Upgrade license Keys for Tests  {code}  Note, the first is the commit from build 2, the second is the commit that triggered build 3.    Note the builds trigger correctly only when changes are made for their branch, but the list of changes is almost always rubbish."
1,Inline Importer Causes Upgrade Tasks To Fail,"When doing an import either via the setup wizard (or checking the checkbox to not restart bamboo from the admin section) the upgrades are run within the same thread as the action so therefore maintain the same session object (via the sessionInViewFilter thingy) for every upgrade task.  This causes some nasty exceptions - the tasks assume they have a clean slate to work off.    We need to make the upgrades run in a separate thread which does not have the overarching session available to it, and just make the action thread hang around waiting for the other thread to finish.    I have attached an export which reproduces this problem. "
0,Can not delete image configuration if used in instance scheduling ,"   2010-09-09 14:36:15,156 INFO [1921423913@qtp-1196121930-3] [LocalAgentManagerImpl] Attempting to remove build agent...   2010-09-09 14:36:15,182 WARN [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] SQL Error: 0, SQLState: null  2010-09-09 14:36:15,182 ERROR [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] Batch entry 0 delete from ELASTIC_IMAGE where ELASTIC_IMAGE_ID=131075 was aborted.  Call getNextException to see the cause.  2010-09-09 14:36:15,182 WARN [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] SQL Error: 0, SQLState: 23503  2010-09-09 14:36:15,182 ERROR [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] ERROR: update or delete on table ""elastic_image"" violates foreign key constraint ""fk1c6b30e1581fb731"" on table ""elastic_schedule""    Detail: Key (elastic_image_id)=(131075) is still referenced from table ""elastic_schedule"".  2010-09-09 14:36:15,184 WARN [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] SQL Error: 0, SQLState: null  2010-09-09 14:36:15,184 ERROR [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] Batch entry 0 delete from ELASTIC_IMAGE where ELASTIC_IMAGE_ID=131075 was aborted.  Call getNextException to see the cause.  2010-09-09 14:36:15,184 WARN [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] SQL Error: 0, SQLState: 23503  2010-09-09 14:36:15,184 ERROR [1921423913@qtp-1196121930-3] [JDBCExceptionReporter] ERROR: update or delete on table ""elastic_image"" violates foreign key constraint ""fk1c6b30e1581fb731"" on table ""elastic_schedule""    Detail: Key (elastic_image_id)=(131075) is still referenced from table ""elastic_schedule"".  2010-09-09 14:36:15,184 ERROR [1921423913@qtp-1196121930-3] [SessionImpl] Could not synchronize database state with session"
0,You can edit the project name on the job level,Not sure if it was intentional but it just feel strange
0,"""Move Plans"" should not show jobs","http://skitch.com/brydiemccoy/dsaqe/configure-new-plan-details-atlassian-bambooooo    Currently it lists all subplans on this page which is kind of stupid.  The reason I haven't just ""fixed it"" is the interfaces on the DefaultProject object are outdated and need review.  I dont want to add to the mess."
0,Upgrade aspectj to 1.6.9,http://mirrors.ibiblio.org/pub/mirrors/maven2/org/aspectj/aspectjrt/1.6.9/    Will solve https://bugs.eclipse.org/bugs/show_bug.cgi?id=269867.  We'll need to update the groupId and add a block for the old groupId
0,Make sure that hg errors are properly added to the plan log,"Currently we just throw RepositoryExceptions.  Some errors - esp 'authentication failure' - should be explicitly added to the log so the user sees it.    Blocker - if it happens in change detection phase, the log is not visible at the moment - difficult to test."
0,"If AJS.conglomerate.cookie gets too big, web servers will report HTTP error 413","As an Administrator edits the plans, the *AJS.conglomerate.cookie* stores the UI history. However, Bamboo is not resetting the cookie value often enough to avoid it getting too big.    This causes the web/app server to report HTTP error 413.  "
0,Bamboo upgrade process fails on Windows,"I had Bamboo 2.6.1 installed on my Windows 7 machine. I tried to install 2.6.2 without manually uninstalling the previous version. The installer found previous version, uninstalled it and installed new one. When I tried to run the Bamboo it failed saying that duplicated plugins were found. Indeed in the WEB-INF/lib directory there were plenty of duplicated jars but in different versions."
0,Wallboard Re-Design,The wallboard looks dated and not inline with our JIRA wallboards that we've presented at Atlassian Summit. 
0,Changing the builder type during cloning adds multiple builder requirements,"When cloning a plan: if you change the builder type from Ant to Maven then Bamboo adds Ant and Maven as a requirement to the plan.    Ideally, Bamboo should detect that the builder has changed and set the requirement to Maven from ant in the next screen."
0,Discrepancy in time estimations in a single-job chain between job and chain,NULL
0,Job/Chain Name duplication throws exception,"My Project Name: AAAA  My Chain Name: AAAA    In one instance I tried to update an existing job to have the name AAAA.  I got a validation error. I should theoretically be able to name my plan like this shouldn't I?    During an attempt to reproduce the above behaviour, I tried to create a _new_ Job with the name AAAA.  This submitted successfully.  However now I get a whole bunch of other exceptions:    {code}  2010-09-30 10:17:41,755 ERROR [1638360365@qtp-1196121930-6] [FiveOhOh] 500 Exception was thrown.  org.springframework.dao.IncorrectResultSizeDataAccessException: query did not return a unique result: 2    org.springframework.orm.hibernate.SessionFactoryUtils.convertHibernateAccessException(SessionFactoryUtils.java:590)    org.springframework.orm.hibernate.HibernateAccessor.convertHibernateAccessException(HibernateAccessor.java:353)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:375)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:337)    com.atlassian.bamboo.plan.PlanHibernateDao.getPlanByName(PlanHibernateDao.java:83)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)  {code}    The getPlanByName method only looks at the project name and plan name in the build table.  And hence will return both the chain and the job.     So we need to either  a) prevent people from creating jobs with the same name as a plan (though this behaves the opposite to how we handle the keys)  b) fix the getPlanByName method, and any other use cases which die because of the duplication."
1,VariableSubstitutionBean cannot be used in V2 plugins,"Any plugin that requires variable substitution via the VariableSubstitutionBean has to be a V1 plugin because the VSB is not exposed to V2 plugins.    This was discovered during: https://studio.plugins.atlassian.com/browse/BPBC-41    Emails were exchanged regarding this as well:    Mark Chai:  -------------  The VSB is a prototype scoped bean so it can't be made available in OSGI.    I'm not 100% sure if it _has_ to be a prototype (it does access the AdminConfigManager but doesn't seem to cache anything).     If you have Bamboo building from source, you could try to switch the prototype flag off on the application context definition and plugin:available=""true"" and see if that works.    Jonathan Doklovic (response):  -----------------------------  I just updated the applicationContextCommon.xml inside of bamboo-agent-core within the container that amps uses.  Removed the prototype scope and added plugins:available=""true"".      This has no effect. When I run my plugin, the VSB still does not get injected into my action.  :(  http://www.sadtrombone.com/   "
1,Allow import of Plans from Maven poms stored in Mercurial repositories,"Currently Bamboo allows out of the box import of pom.xml from CVS/Perforce/Subversion.  With Mercurial support in place, Mercurial should be added to the list of repositories available for plan import."
0,Apparent deadlock in Axis,"We had one of our builds hang, at a point where it was running a functional test against Bamboo.    It looks like there might be a deadlock within Bamboo, but it's not totally obvious to me what's going on. I'm attaching the log with a thread dump at the end."
1,Reports for tests don't have any data points,Issue reported in https://support.atlassian.com/browse/BSP-3259
0,Hide file filter pattern options when configuraing job repository,These options are only used for change detection.  So should not be configurable at the job level.
0,Quiet Period and Max Retries Should be hidden for jobs,This only relates to change detection 
0,Invalid Build log artifact URL in REST reponse,"<server>/rest/api/latest/build/<build>?expand=artifacts.artifact returns an URL to build log, which does not exist (response is a chain)"
1,Add admin action that will blow away all Mercurial cache directories.,"[https://atlaseye.atlassian.com/cru/CR-BAM-1971#c32842]    currently if I change the repo URL, then the cache is effectively ""orphaned"" and can't be cleared through plan view.  "
0,Maven 3.0 or 3.x Builder,"Hi all,    It would be nice if I would have the possibility to add a Maven 3.x (or Maven 3.0) Builder.  When I want to build with Maven 3 I have to choose Maven 2.x or Maven Builder.  That's not nice ;(    Regards,  Florian"
0,Stop build on dashboard when multiple builds redirects to plan page,Stopping a single build from the dashboard happens async. You stay on the dashboard.  If there are multiple builds running you get the dialog.  After confirming deletion you get redirected to the plan page.  You should return to the dashboard instead
0,Errors not showing up on result summary page,"  The job-result-error is showing on the job and chain summary page, but not on the chain result or job result page.  Both results are ""Not Built"""
0,Bulk Actions wizard has sorting issues,"In the second/third/fourth step of bulk action, the chains are grouped at the top, and jobs grouped at the bottom.  The jobs should be grouped underneath their respective chain."
0,Bulk Action change svn info should indicate if a job is inheriting its parents,"You should still be able to bulk edit jobs to something new.  But, in the preview list showing existing config, it should show either just (e.g.) ""Inheriting information from parent"" or still show the url but e.g. in brackets next to it show (inherited)."
0,Mercurial is instantly triggering another build right after the initial build.,"Creating new plan (enabled) triggers initial build. Mercurial plugin will instantly trigger another build (if configured to poll repository for the changes) - because it thinks last revision was 'null'.    How to reproduce:  1) just create a plan using valid Mercurial repository, enable it (during creation), set 'poll repository for changes' for example to 10s  2) wait till first build (initial) finishes  3a) instantly second build will be triggered (showing all the changesets from the repository root)  3b) where I expected the build won't be triggered until new commits will be pushed to that repository"
0,Cannot load x64 wrapper dll on Windows XP x64,NULL
0,Web repository isn't validating when configuring Plan.,"Giles Gaskell (at Bamboo DEV2 chatroom, 11/08/2010):  {quote}  Btw - just out of interest for the Docs - when configuring the 'Common repository configuration' options of an existing Plan or Job, I am able to select the option 'Mercurial Web Repository' in the 'Web Repository' field for Plan. However, this option is not available when configuring the equivalent field for a Job.  Is there a reason for that? Or is this a bug?  Does anyone have an answer as to why the 'Mercurial Web Repository' option is not available when configuring a Job? (whereas it is for a Plan)  Aha - I see what the issue is - when you're configuring a Plan, the 'Web Repository' field does not appear to be validating your selected repository and the 'Mercurial Web Repository' option appears no matter what repository type you select.  {quote}    Chai:  {quote}  Ah okay. That sounds like a bug  {quote}    Giles:  {quote}  Yeah - I thought so - fyi - the Job configuration page appears to be working okay in this regard.  {quote}    -----    Worth investigating. Maybe it's a Mercurial Web Repository issue itself. Maybe not."
0,${system.PATH} or ${system.ALLUSERSPROFILE} variables are not being replaced,* I have a builder argument that uses ${system.ALLUSERSPROFILE} but now it is not being replaced by the %ALLUSERSPROFILE% environment variable  * I also add extra paths to the default PATH environment variable using the following    {noformat}  PATH=${system.PATH};tools/bin  {noformat}    but the ${system.PATH} variable is also not being replaced
0,Mail Server can't get the SMTP or JNDI values if the server is Tested before Saving it,"If the Mail Server is tested (""Test"" button is clicked) before being saved (""Save"" button is clicked), the following stack is reported:    {noformat}  Version: 2.7  Build: 2100  Build Date: 05 Nov 2010    system.error.request.information:  Request URL: http://domain:8085/admin/saveMailServer.action  Scheme: http  Server: domain  Port: 8085  URI: /admin/saveMailServer.action  Context Path:  Servlet Path: /admin/saveMailServer.action  Path Info:  Query String:  Stack Trace:    java.lang.NullPointerException: Name is null  at java.lang.Enum.valueOf(Unknown Source)  at com.atlassian.bamboo.ww2.actions.admin.mail.ConfigureMailServer$MailSetting.valueOf(ConfigureMailServer.java:25)  at com.atlassian.bamboo.ww2.actions.admin.mail.ConfigureMailServer.validate(ConfigureMailServer.java:88)  at com.opensymphony.xwork.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:154)  at com.atlassian.bamboo.ww2.interceptors.BambooWorkflowInterceptor.doIntercept(BambooWorkflowInterceptor.java:33)  at com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:116)  at com.atlassian.bamboo.ww2.interceptors.BambooValidationInterceptor.doIntercept(BambooValidationInterceptor.java:33)  at com.opensymphony.xwork.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.JSONValidationInterceptor.intercept(JSONValidationInterceptor.java:78)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.webwork.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:174)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:151)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.GlobalAdminInterceptor.intercept(GlobalAdminInterceptor.java:21)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptor.intercept(WebworkSecurityInterceptor.java:55)  at com.atlassian.bamboo.security.acegi.intercept.web.WebworkSecurityInterceptorProxy.intercept(WebworkSecurityInterceptorProxy.java:30)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.PaginationAwareInterceptor.intercept(PaginationAwareInterceptor.java:81)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.StatisticsAwareInterceptor.intercept(StatisticsAwareInterceptor.java:42)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.ResultsListAwareInterceptor.intercept(ResultsListAwareInterceptor.java:45)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.BuildResultsSummaryAwareInteceptor.intercept(BuildResultsSummaryAwareInteceptor.java:66)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.ResultsSummaryAwareInteceptor.intercept(ResultsSummaryAwareInteceptor.java:67)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.ChainAwareInterceptor.intercept(ChainAwareInterceptor.java:110)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.atlassian.bamboo.ww2.interceptors.PlanAwareInterceptor.intercept(PlanAwareInterceptor.java:71)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.AroundInterceptor.intercept(AroundInterceptor.java:31)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:186)  at com.opensymphony.xwork.DefaultActionInvocation.invoke(DefaultActionInvocation.java:190)  at com.opensymphony.xwork.DefaultActionProxy.execute(DefaultActionProxy.java:116)  at com.opensymphony.webwork.dispatcher.DispatcherUtils.serviceAction(DispatcherUtils.java:274)  at com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:202)  at com.atlassian.bamboo.ww2.BambooFilterDispatcher.doFilter(BambooFilterDispatcher.java:30)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)  at com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:66)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)  at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)  at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)  at com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:66)  at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)  at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)  at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)  at com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)  at com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:72)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:219)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:140)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)  at com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:69)  at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)  at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)  at com.atlassian.bamboo.persistence.BambooSessionInViewFilter.doFilterInternal(BambooSessionInViewFilter.java:31)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.planetj.servlet.filter.compression.CompressingFilter.handleDoFilter(CompressingFilter.java:203)  at com.planetj.servlet.filter.compression.CompressingFilter.doFilter(CompressingFilter.java:193)  at com.atlassian.bamboo.filter.CompressingFilter.doFilter(CompressingFilter.java:69)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter.doFilter(RequestCacheThreadLocalFilter.java:31)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:55)  at com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:41)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1139)  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:378)  at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)  at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)  at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)  at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)  at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)  at org.mortbay.jetty.Server.handle(Server.java:324)  at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)  at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:865)  at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)  at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)  at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)  at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)  at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)  {noformat}"
0,Git Repo should work with gitosis,So bamboo user can at plan configuration pass an ssh keyfile with passphrase and JGit will talk to remote repository correctly.
0,Remove delayed changeset detection from Mercurial,"If I use mercurial repository and choose ""delay changeste detection' option then I will have ""Unknown Changes"" listed under ""changes"" tab. However, after complete build those placeholder changes should be swapped with real changesets. This is not occuring.    {color:red}edit: remove this functionality from Mercurial Repository. See comments below{color}"
0,ManualBuildTriggerReason.getUserName() returns null.,"I've been trying out the Artifactory Bamboo plugin.  The plugin currently fails with a null pointer exception caused by ManualBuildTriggerReason.getUserName() returning null.    I've filed a bug report and suggested workaround at JFrog, but I think the root cause is probably a Bamboo bug.  https://issues.jfrog.org/jira/browse/BAP-26        http://docs.atlassian.com/bamboo/2.7/com/atlassian/bamboo/v2/build/trigger/ManualBuildTriggerReason.html#getUserName()"
0,Confirm/Fix Plan Navigation For Configuration,Where should you get taken when moving between plan and job within the configuration pages.  Currently it works like this  1) if there is an equivalent tab take you to that tab  2) when moving to the chain (with no equiv) it takes you to the stages tab  3) when moving to the job (with no equiv) it take you to whatever tab you have saved in the cookie    Alternatives  * Replace #3 always taking you to the builder tab of the job.   * Replace #2 and #3 to take you to the plan/job details page.  * Replace #2 to also take you to the cookied page.  * Replace #1 and #2 to always take you to the cookied page.
0,Ensure that tabs for Plans and Jobs are consistent,"In the Result Summary section we should ensure that the tabs that are common between for Plans and Jobs are in the same order.     In the Configuration, we should ensure that the tabs that are common between Plans and Jobs have the same name (E.g. Source Repository)    "
0,Decimal-Point missing in Clover integration,"When showing the ""Clover code coverage summary"" Page the coverage is multiplied by 10 (ok, may be a feature :-) ). See attached Screenshot 1.    When opening the clover artifact the coverage is ok (29.1 % to 291%). "
0,Mercurial quiet period,Mercurial is missing quiet period support (BAM-1178).
0,bamboo sent emails  containing unreadable info,"I am a east-asian user, and i setup my bamboo server to send email after every completed build. however, there's unreadable part in the email, like the following:  *****  [INFO] Compilation failure  \...\iskControlFacadeImpl.java:[50,38] ????????  ???? ???? findNaireByCustomerType(java.lang.String)  *****    I googled for this issue, and I get the following info:  *****  it's a bamboo's bug. the utf-8 charset parameter's missing in htmlpart.setContent method  in com.atlassian.bamboo.notification.transports/MultipartEmailTransport.class:    htmlContent = notification.getHtmlEmailContent();  if(StringUtils.isNotBlank(htmlContent))  {    BodyPart htmlPart = new MimeBodyPart();       htmlPart.setContent(htmlContent, ""text/html;charset=utf-8"");       mp.addBodyPart(htmlPart);  }  *****    is that true? how can i solve the problem?    THANKS!"
0,Overwrite Existing Data option is not effective ,"The *Overwrite Existing Data* from the DB Setup installation step is not being completely effective. Data is not deleted and after the Admin User Creation step, the installation process fails when it tries to create DB entries that already exist.    It should also be tested with other DBs (not only MySQL)."
0,Git Repository should properly detect code has changed on arbitrary branch,I was testing Git on 'master' branch. I should test if it work on other branches as well. Probably this works already. However we need to write up some tests.
1,Git repo should use shallow clones instead of full clones.,"Currently we use full clones during:  * collecting changes, on bamboo server  * code retrieval, on agents, during performing build    In both cases we don't need the whole history, so if it is possible we should do shallow clones.     Shallow clones are not implemented in JGit yet.    Also - both server and agents should be able to fetch afterwards properly. Corner test case:  * detecting new rev A -> add to queue  * detecting new rev B (grandchild of A) -> add to queue (concurrent builds are on)  * agent -> takes rev B, does shallow clone on it (so it won't have rev A)  * agent -> takes rev A, does fetch - in such case agent should somehow obtain rev A"
0,Initial Git build should ignore history in 'changes' tab.,Instead of showing 10 753 changeset.  See: [http://groups.google.com/group/bamboogitplugin/browse_thread/thread/532384d62f62a930#]  Git Repository should behave as other (CVS/SVN) repositories with regard to first build.
0,Git repository should recover from failures during checkout.,Like in other repositories - if something fail during code checkout/fetch then Git repository should try to recover - in worst case remove working directory and clone remote repository from scratch.    P.S. watch out for lock files in .git directory (they lock 'checkout' operation) - teamcity had problem with that too AFAIK.
0,Git Repository should work on remote agents,ensure it does!  * remote agent  * elastic one
0,Investigate whether shallow clones (Git Repo) are fast enough,Before supporting shallow clones we would like to confirm that shallow clones are enough for us. So that we do not find out down the track that they are actually slow and we need caches on agents nevertheless.
0,Importing Git Repository from maven,"like in other SCM's, like HgRepository"
0,Plan Navigator - general progress bar improvements,I believe that plan was to not show the progress bar for the currently selected job.    If we do want to show it we need to make sure the text is readable (currently text is white on light blue background)
0,Ignore compatibility mode not working in IE8 (breaking breadcrumbs),The new character used for breadcrumbs seems to show twice in IE8.    
0,Change dashboard plan result keys to #buildNumber,NULL
0,Test basic Maven artifact passing workflow works with remote agents,NULL
0,Test basic Maven artifact passing workflow works with elastic agents,We need to change the Command decorator to use the tunnel if available.
1,Implement disk space management for maven artifact passing,https://extranet.atlassian.com/display/BAMBOO/Maven+Artifact+Passing#MavenArtifactPassing-DiskspaceManagement
1,implement artifact sharing for maven ,https://extranet.atlassian.com/display/BAMBOO/Maven+Artifact+Passing#MavenArtifactPassing-ArtifactSharing    Only implementing simple groupid/artifactid selection and not choosing from a list of produced artifacts (latter requires dependency tracing).
0,errors while checkout/clone should fail the build in Git Plugin,http://tardigrade.sydney.atlassian.com:8085/bamboo/browse/GP-IDEAMASTER-JOB1-1    the error during code checkout should fail the build I reckon. Investigate what is the problem.
0,Drop down menu is 'transparent',NULL
0,BuildManager still used in core Bamboo functionality,"I've found some error messages in Tardigrade logs about build failures of builds that seemed to be completed just fine.    Following that, I've found that BuildManager is still used in some places in Bamboo, causing problems with some of the functionality. For example, BuildCompletedEventListener operates on Buildables but uses BuildManager for build key validation.  This which never works, since valid job keys are not valid build keys - effectively disabling the BuildCompleted plugins.    There are not that many places that use BuildManager, about half of them are old UpgradeTasks. This should be fixed, and potentially backported to 2.7 too (since it breaks plugins in a way that cannot be fixed by developers).  "
0,Create Plan Level Result Log Tab,"To make it easier to navigate to the logs, we should offer a Plan level log tab which provides a summary table with links to the individual Job log pages as well as a link to download the job log files from the Plan level log page.  "
0,Handle subscriptions for Job move across stages,NULL
0,Remove pre-2.0 upgrade tasks and provide protection against upgrade from pre 2.0 version,"Krystian:  In the history of Bamboo we had several versions which were required to be used to upgrade to the newer version. Ie. to upgrade from Bamboo 1.0 to Bamboo 2.1 you need to execute upgrades to 1.1.2, 1.2 and 2.0.  The latest of this ""milestone-upgrade"" versions is 2.0. This is why I propose to drop all the upgrade tasks which were introduced by pre-2.0 versions. The main benefit would be dropping the last usages of old and usually deprecated APIs  According to https://extranet.atlassian.com/pages/viewpage.action?pageId=636158922 it means all pre 801 upgrade tasks (18 of them)    Przemek:  + make it impossible to use new Bamboo version to upgrade from build number of pre-2.0.x version.  That check should have been included in 2.0.x anyway...    Marek:  +1, those upgrade tasks are in sources too long. For historical stuff we got subversion    Anton:  Yes, if we do this, Bamboo *must* lockup upon starting if it detects that it is being brought up against a version which is before 2.0.  We should also stop the user importing XML backups from pre 2.0 versions.  In all these cases the user should be given a very clear error message and no data should be changed.  Can I ask why do you bring this up now? What caused you to do this? If we do this, will this simplify some other work we need to do for 3.0?  How long will the work of stopping a pre 2.0 upgrade/import take?    Krystian:  Adding protection against upgrade/import from pre-2.0 version is rather simple and would require ~4h of work (including testing).  I'm bringing this now cause last week Przemek has spotted a small bug which was caused by internal use of deprecated API (BuildManager). I thought that releasing 3.0 is a good moment to drop this (and maybe other) legacy APIs which were deprecated in 2.7 and previous versions.  A lot of uses of deprecated API is buried in old upgrade tasks which led to idea I presented in my original email.  This will probably not simplify other work for 3.0 but has a potential to help us find forgotten usages of legacy code that should have been updated some time ago."
0,Ensure we only show 100 changes in Git Repository and display skipped amount (for 3.0),... and mention in the interface that only the first 100 are shown if there are more than 100 changes.
0,Ability to stop a queued job via action menu,For stop button for a job in the action menu only shows up when the job is actually running not while it is queued.  It should show while its queued.    The fix needs to ensure that the button doesn't show up when the job's stage hasn't been kicked off yet.
1,Fix Bulk Edit Administration Screen,The bulk edit screen is pretty broken right now. We will need to fix this up before we release 3.0.     http://tardigrade.sydney.atlassian.com:8085/bamboo/admin/chooseBulkAction.action
0,Support for include/exclude files in Git Repository,AbstactRepository implements IncludeExcludeAwareRepository interface so the Git Plugin should handle that.
0,Import/Export of Audit Logs,NULL
1,Support Bitbucket repositories,"Currently we support any Mercurial Repository, but it would be great if we could specifically support BitBucket and make it easier to setup new plans using the BB API.    The underlying infrastructure would be the same as for Mercurial. However, the setup would provide a BitBucket option in addition or as part of the Mercurial Repository and allow for retrieving the users repositories and branches.     h4. Specification  https://extranet.atlassian.com/display/BAMBOO/Bitbucket+Support+Spec  "
0,Remove Build Queue Report ,"Since it's not working right now (BAM-7557), we should remove the report from the UI. Once we've fixed it, we will add it again. "
0,Stage Configuration UX Update,"The Stages configuration requires some UX changes to make it easier to use:    * *Make the Job name link to the Edit Job Configuration*  * Remove the Configuration Dropdown (and link)  * Add separate ""Disable"" and ""Delete"" links instead of the dropdown  * Replace ""Add Job"" with a button and move to the right"
0,Fix Test Case Overview Page UI,We need to clean this page up... at least a little bit:    * Remove the big colour box  * Use the same key value layout we use on the summary  * Move the stats below the table  * Make the top right stats look the same as the Plan summary stats (once we have them)    http://tardigrade.sydney.atlassian.com:8085/bamboo/browse/BAM-FC-DEP/test/case/148635675    
0,Change hg clone into hg init/hg pull (to get rid of credentials in .hgrc),"[forums|http://forums.atlassian.com/message.jspa?messageID=257362974]  bq. I went through the cache directories in the server and noticed also slight security issue. Bamboo seems to register the repository path with username/password into the hgrc file in plain text (in the url) in the cache directory and the file permissions are such that it is readable by all. Not that the source code and builds itself might have something sensitive, but atleast I find it good to know that the username/password is not secure within the file system.  "
0,Remove 'clean cache directory' checkbox from Mercurial configuration screen,"[forums|http://forums.atlassian.com/thread.jspa?messageID=257363207&#257363207]    I think we should remove that checkbox as it is hazardous to use. Details in mail(anton,jens,sginter,pstefaniak)."
0,Share Mercurial cache among the server and local agents (if concurrent builds are on),"Like in Summary - if concurrent builds are on then each local agent will grab its own copy of cache during code retrieval. And hopefully it doesn't need to, instead it should use bamboo server's cache, like in 'no concurrent build' mode."
0,Git Plugin should detect if remote repository support 'shallow' protocol,NULL
0,Git Repository shouldn't display user/pass + sshKey/pass together,"as it mislead user thinking user/pass may impact ssh connection. Ideally there should be combobox ""authenticationType"" with options ""none;http auth;sshKeyfile"" which would display exclusively either user/pass, either ssh+passphrase, or nothing."
0,Sharing an artifact via quick link doesn't work when artifact name is used by other Jobs,"This, of course, is correct behaviour. However, it currently doesn't provide any feedback. The link simply doesn't work. Instead, can we display a popup with the following message:    ""To share an Artifact, it's name must be unique within the Plan. Please rename the Artifact to share it""    Then allow the user to edit the Artifacts name with a textbox. (just like in edit without all the other fields)    The buttons at the bottom should say: Save and Cancel  "
0,Ensure Artifacts upgrade task gives decent errors and can be rerun,This may already be catered for but:    Its possible that moving artifact files will run out of disk space in the middle.  We should make sure that the upgrade task dies elegantly and its possible to recover. e.g. (just re-run the upgrade task and it picks up where it left off.)
0,Git Repository doesn't log messages on remote agents,when run on remote agent no log from GitRepository are shown. Probably textProvider is not properly initialized/loaded - as it was in MercurialRepository.
0,Git repo shallow clones should work via http(s),"Build TEST-TESTHTTPS-JOB1-18 started building on agent Default Agent  Updating source code to revision: 05b86d941d9f61f9236d7743c13821d6fdc4d416  Fetching branch 'master'. Will try to do a shallow fetch.  Git: Requesting shallow fetch  {color:red}Cannot fetch 'https://cixot@github.com/cixot/test.git', branch 'master' to source directory '/home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/131073/TEST-TESTHTTPS-JOB1'. https://cixot@github.com/cixot/test.git: Starting read stage without written request data pending is not supported{color}    {code}  com.atlassian.bamboo.repository.RepositoryException: Cannot fetch 'https://cixot@github.com/cixot/test.git', branch 'master' to source directory '/home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-645f5f1-1/repository'. https://cixot@github.com/cixot/test.git: Starting read stage without written request data pending is not supported    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:193)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:157)  ...  Caused by: org.eclipse.jgit.errors.TransportException: https://cixot@github.com/cixot/test.git: Starting read stage without written request data pending is not supported    org.eclipse.jgit.transport.BasePackFetchConnection.doFetch(BasePackFetchConnection.java:292)    org.eclipse.jgit.transport.TransportHttp$SmartHttpFetchConnection.doFetch(TransportHttp.java:613)    org.eclipse.jgit.transport.BasePackFetchConnection.fetch(BasePackFetchConnection.java:230)    org.eclipse.jgit.transport.FetchProcess.fetchObjects(FetchProcess.java:217)    org.eclipse.jgit.transport.FetchProcess.executeImp(FetchProcess.java:152)    org.eclipse.jgit.transport.FetchProcess.execute(FetchProcess.java:114)    org.eclipse.jgit.transport.Transport.fetch(Transport.java:904)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:188)  	... 23 more  Caused by: org.eclipse.jgit.errors.TransportException: https://cixot@github.com/cixot/test.git: Starting read stage without written request data pending is not supported    org.eclipse.jgit.transport.TransportHttp$Service.execute(TransportHttp.java:698)    org.eclipse.jgit.transport.TransportHttp$Service$HttpExecuteStream.read(TransportHttp.java:772)    org.eclipse.jgit.util.io.UnionInputStream.read(UnionInputStream.java:144)    java.io.FilterInputStream.read(FilterInputStream.java:133)    org.eclipse.jgit.util.io.TimeoutInputStream.read(TimeoutInputStream.java:111)    org.eclipse.jgit.util.IO.readFully(IO.java:175)    org.eclipse.jgit.transport.PacketLineIn.readLength(PacketLineIn.java:141)    org.eclipse.jgit.transport.PacketLineIn.readString(PacketLineIn.java:108)    org.eclipse.jgit.transport.PacketLineIn.readACK(PacketLineIn.java:86)    org.eclipse.jgit.transport.BasePackFetchConnection.negotiate(BasePackFetchConnection.java:575)    org.eclipse.jgit.transport.BasePackFetchConnection.doFetch(BasePackFetchConnection.java:278)  	... 30 more  {code}    "
0,Git Repository server cache should be thread-safe (during fetches),"h3. System Error Details  Build test - testmasterclean : Unable to detect changes   (com.atlassian.bamboo.repository.RepositoryException : Cannot fetch 'git://github.com/cixot/test.git', branch 'master' to source directory '/home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository'. Cannot move pack to /home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository/.git/objects/pack/pack-9debbf5763a2286524c616934e72bf0cb8b71dba.pack)    Occurred: 05 Jan 2011, 5:12:03 PM   com.atlassian.bamboo.repository.RepositoryException: Cannot fetch 'git://github.com/cixot/test.git', branch 'master' to source directory '/home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository'. Cannot move pack to /home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository/.git/objects/pack/pack-9debbf5763a2286524c616934e72bf0cb8b71dba.pack  {code}    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:193)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:157)    com.atlassian.bamboo.plugins.git.GitRepository.collectChangesSinceLastBuild(GitRepository.java:154)    com.atlassian.bamboo.v2.trigger.DefaultChangeDetectionManager.collectChangesSinceLastBuild(DefaultChangeDetectionManager.java:92)    com.atlassian.bamboo.v2.trigger.ChangeDetectionListenerAction.process(ChangeDetectionListenerAction.java:75)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.createChainState(ChainExecutionManagerImpl.java:164)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.start(ChainExecutionManagerImpl.java:107)    sun.reflect.GeneratedMethodAccessor178.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:616)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:198)    $Proxy41.start(Unknown Source)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.startWithLockTaken(PlanExecutionManagerImpl.java:174)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.access$100(PlanExecutionManagerImpl.java:39)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl$2.call(PlanExecutionManagerImpl.java:155)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.doWithProcessLock(PlanExecutionManagerImpl.java:306)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.start(PlanExecutionManagerImpl.java:148)    com.atlassian.bamboo.v2.trigger.ChangeDetectionListener.handleEvent(ChangeDetectionListener.java:50)    com.atlassian.bamboo.event.EventListenerRunnable.run(EventListenerRunnable.java:22)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:636)  Caused by: org.eclipse.jgit.errors.TransportException: Cannot move pack to /home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository/.git/objects/pack/pack-9debbf5763a2286524c616934e72bf0cb8b71dba.pack    org.eclipse.jgit.transport.BasePackFetchConnection.doFetch(BasePackFetchConnection.java:292)    org.eclipse.jgit.transport.BasePackFetchConnection.fetch(BasePackFetchConnection.java:230)    org.eclipse.jgit.transport.FetchProcess.fetchObjects(FetchProcess.java:217)    org.eclipse.jgit.transport.FetchProcess.executeImp(FetchProcess.java:152)    org.eclipse.jgit.transport.FetchProcess.execute(FetchProcess.java:114)    org.eclipse.jgit.transport.Transport.fetch(Transport.java:904)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:188)  	... 23 more  Caused by: java.io.IOException: Cannot move pack to /home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository/.git/objects/pack/pack-9debbf5763a2286524c616934e72bf0cb8b71dba.pack    org.eclipse.jgit.transport.IndexPack.renameAndOpenPack(IndexPack.java:1163)    org.eclipse.jgit.transport.BasePackFetchConnection.receivePack(BasePackFetchConnection.java:674)    org.eclipse.jgit.transport.BasePackFetchConnection.doFetch(BasePackFetchConnection.java:285)  	... 29 more  {code}    ----    h3. System Error Details  Build test - testmaster : Unable to detect changes   (com.atlassian.bamboo.repository.RepositoryException : Cannot fetch 'git://github.com/cixot/test.git', branch 'master' to source directory '/home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository'. Unknown repository format ""null""; expected ""0"".)    Occurred: 05 Jan 2011, 5:12:03 PM   com.atlassian.bamboo.repository.RepositoryException: Cannot fetch 'git://github.com/cixot/test.git', branch 'master' to source directory '/home/pstefaniak/bamboo/atlassian-bamboo-2.7.3-standalone/home/xml-data/build-dir/_git-repositories-cache/0x-7300b201-1/repository'. Unknown repository format ""null""; expected ""0"".  {code}    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:193)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:157)    com.atlassian.bamboo.plugins.git.GitRepository.collectChangesSinceLastBuild(GitRepository.java:154)    com.atlassian.bamboo.v2.trigger.DefaultChangeDetectionManager.collectChangesSinceLastBuild(DefaultChangeDetectionManager.java:92)    com.atlassian.bamboo.v2.trigger.ChangeDetectionListenerAction.process(ChangeDetectionListenerAction.java:75)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.createChainState(ChainExecutionManagerImpl.java:164)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.start(ChainExecutionManagerImpl.java:107)    sun.reflect.GeneratedMethodAccessor178.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:616)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:198)    $Proxy41.start(Unknown Source)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.startWithLockTaken(PlanExecutionManagerImpl.java:174)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.access$100(PlanExecutionManagerImpl.java:39)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl$2.call(PlanExecutionManagerImpl.java:155)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.doWithProcessLock(PlanExecutionManagerImpl.java:306)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.start(PlanExecutionManagerImpl.java:148)    com.atlassian.bamboo.v2.trigger.ChangeDetectionListener.handleEvent(ChangeDetectionListener.java:50)    com.atlassian.bamboo.event.EventListenerRunnable.run(EventListenerRunnable.java:22)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:636)  Caused by: java.io.IOException: Unknown repository format ""null""; expected ""0"".    org.eclipse.jgit.storage.file.FileRepository.(FileRepository.java:180)    org.eclipse.jgit.storage.file.FileRepository.(FileRepository.java:126)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:169)  	... 23 more  {code}    h3. System Error Details  Build gp - linuskernelhttp3 - Default Job 4 : Error occurred while executing the build for GP-LINUSKERNELHTTP3-JOB1-4   (com.atlassian.bamboo.repository.RepositoryException : null Could not delete file /home/hgrepo/bamboo-agent-home/xml-data/build-dir/GP-LINUSKERNELHTTP3-JOB1/.git/objects/incoming_4443714216792466821.idx)    Occurred: 05 Jan 2011, 5:29:19 PM   Agent: lucyfer (2)   com.atlassian.bamboo.repository.RepositoryException: null Could not delete file /home/hgrepo/bamboo-agent-home/xml-data/build-dir/GP-LINUSKERNELHTTP3-JOB1/.git/objects/incoming_4443714216792466821.idx  {code}    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:193)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetchAndCheckout(GitOperationHelper.java:151)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetchAndCheckout(GitOperationHelper.java:142)    com.atlassian.bamboo.plugins.git.GitRepository.retrieveSourceCode(GitRepository.java:203)    com.atlassian.bamboo.v2.build.task.CheckoutUpdateForBuild.doCall(CheckoutUpdateForBuild.java:136)    com.atlassian.bamboo.v2.build.task.CheckoutUpdateForBuild.call(CheckoutUpdateForBuild.java:41)    com.atlassian.bamboo.v2.build.agent.DefaultBuildAgent.build(DefaultBuildAgent.java:189)    com.atlassian.bamboo.v2.build.agent.BuildAgentControllerImpl.waitAndPerformBuild(BuildAgentControllerImpl.java:90)    com.atlassian.bamboo.v2.build.agent.DefaultBuildAgent$1.run(DefaultBuildAgent.java:102)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:636)  Caused by: org.eclipse.jgit.errors.TransportException: Could not delete file /home/hgrepo/bamboo-agent-home/xml-data/build-dir/GP-LINUSKERNELHTTP3-JOB1/.git/objects/incoming_4443714216792466821.idx    org.eclipse.jgit.transport.BasePackFetchConnection.doFetch(BasePackFetchConnection.java:292)    org.eclipse.jgit.transport.TransportHttp$SmartHttpFetchConnection.doFetch(TransportHttp.java:613)    org.eclipse.jgit.transport.BasePackFetchConnection.fetch(BasePackFetchConnection.java:230)    org.eclipse.jgit.transport.FetchProcess.fetchObjects(FetchProcess.java:217)    org.eclipse.jgit.transport.FetchProcess.executeImp(FetchProcess.java:152)    org.eclipse.jgit.transport.FetchProcess.execute(FetchProcess.java:114)    org.eclipse.jgit.transport.Transport.fetch(Transport.java:904)    com.atlassian.bamboo.plugins.git.GitOperationHelper.fetch(GitOperationHelper.java:188)  	... 10 more  Caused by: java.io.IOException: Could not delete file /home/hgrepo/bamboo-agent-home/xml-data/build-dir/GP-LINUSKERNELHTTP3-JOB1/.git/objects/incoming_4443714216792466821.idx    org.eclipse.jgit.util.FileUtils.delete(FileUtils.java:133)    org.eclipse.jgit.util.FileUtils.delete(FileUtils.java:91)    org.eclipse.jgit.transport.IndexPack.index(IndexPack.java:473)    org.eclipse.jgit.transport.BasePackFetchConnection.receivePack(BasePackFetchConnection.java:673)    org.eclipse.jgit.transport.BasePackFetchConnection.doFetch(BasePackFetchConnection.java:285)  	... 17 more  {code}"
0,"Rather than error out when a git ssh URL is entered without a username, automatically embed it in the URL for the user","See BAM-7665 for background information.    Git needs the username to be in the URL for ssh.    By implementing BAM-7665, we error out whenever a git ssh URL is entered without a username.    I think it would be better if we provided a separate UI textfield for the username and embeded the username into the ssh URL.    In this case if the user enters a username into the URL we should error out and ask the user to enter the username into a separate UI field."
0,Update Plan Statistics UI,"The blue box doesn't fit in with Bamboo's new style. We should update it to look similar to the Test Summary. See attached mockup.     We can definitely play with the size of the numbers. For Plans with a lot of builds, the first number might grow too long at the current size. "
0,Update Bamboo Header,"To look more Atlassian, our header requires a few changes. Since another project is underway to unify headers in general, we don't want to spend much time on this, but a few small changes can have a big impact here:     *Change the font:*  * Not Bold  * Remove the underlines for the shortcut keys (not needed very often anyway)  * Change the font size to 13 pixels    *Background*  * Remove the background image  * Use a new logo that works on a plain background (to be provided by Jens)"
0,Bamboo caches the Dashboard Plan permissions for the first user that accessed Bamboo ,"1. Considering that anonymous users can't see the plans in the dashboard;    2. If the first access is done in anonymous mode and then the Admin logs in, the Admin Dashboard will be empty, because the anonymous permission for the dashboard is still cached. All the other screens will show the plans and permissions without problem."
0,Bundle the Git Bamboo Repository Plugin with Bamboo 3.0,{quote}  We absolutely need to bundle the Git Bamboo Plugin into Bamboo 3.0.    We are actually hoping to do the bundling for the milestone 4 of Bamboo 3.0.   {quote}
1,OAUTH TokenExpiry never starts in Bamboo 3.0,"When upgrading to the new Atlassian Plaform, we've changed the expiry scheduler to be triggered by Bamboo start instead of plugin system startup event.  This is a good idea, but the plugin was not wired properly to receive the startup event. As the result, expiry never runs on Bamboo 3.0.  Priority major since the exposure is limited, but this has to be fixed fast."
0,User can see live logs for a jobs on the plan summary screen,NULL
0,Webwork library not compatible with JDK 1.5,NULL
1,Share Git cache among the server and local agents (if concurrent builds are on),As in BAM-7635 but for Git:  Like in Summary - if concurrent builds are on then each local agent will grab its own copy of cache during code retrieval.  Cache should be reused locally for checkouts - conurrent builds or not.    This will require changing cache lock strategy to read/write - see [comment|https://atlaseye.atlassian.com/cru/CR-BAM-2388#c39662]  
1,Build plan failed to run - it was marked as queued but was not present in the queue,"See https://bamboo.extranet.atlassian.com/browse/CONFFUNC5-SMOKE-33/    It has failed to run with error:    {noformat}Confluence Trunk Staging and Dumping - Confluence Smoke Tests - Extranet Staging - Smoke Tests > CONFFUNC5-SMOKE-SMOKE-33 : Build CONFFUNC5-SMOKE-SMOKE-33 had to be cancelled: it was marked as queued but was not present in the queue.  (19 Jan 2011, 5:47:05 PM)  Remove error from the list Clear error from log   {noformat}"
0,"Instant message notifications no longer say ""X new test failures"", just ""2/486 tests failed.""",I seem to recall that the Bamboo IM notifications used to tell me how many test failures were new. This seems to have gone missing in recent milestones on BEAC.    It's really useful information because it often tells you whether you broke the build or someone else did.  
0,"Move ""Plan Logs"" to be under the configuration screen",Doesn't make a lot of sense but it's the least bad option.    Have a link at the bottom of the View Configuration to the Plan Logs. Have a button on the top left to go back to the View Configuration
0,In the stages tab make the Job name link to the Job Configuration (not the Job Summary),NULL
0,Users can't reset passwords if the Global Anonymous Permission is disabled,"If the Global Anonymous Permission to *Access* is disabled, the user still have to login in order to Reset his password.    Steps to reproduce:  1. Disable the *Access* permission for Anonymous Users at the Admin => Global Permissions  2. Logout and try to reset your password using the *Can't access your account?* link.  3. You will receive an email with a link.  4. Click on the link.    Result: You will be redirected to a Login page, not a Reset Password Page. If you login with the correct password the Reset Page is then displayed, but this is not useful when the user forgot his password.    "
0,Source checkout on local agent should not fail if cache is deleted between change collection & source checkout,"This may happen when the user manually deletes the cache directory between change collection & source checkout.  Workaround: wait for next build.    We may detect such case (MissingObjectException), refill the cache and retry the checkout.  We do something similar already when source directory is messed up between checkouts."
0,Update atlassian-util-concurrent to 2.0,NULL
0,Errors during source checkout not correctly propagated causing plans to pass when they shouldn't,"https://bamboo.extranet.atlassian.com/browse/CRUCHG-CHAIN-FUNCFE-865/log    All jobs in this build actually failed which repository exceptions (i.e it didn't even get to running).  However this error is not being propagated to Bamboo, so depending on which builder you use the plan may or may not fail.    Exception That Cause This particular Failure: http://jira.atlassian.com/browse/BAM-7921    Build Logs Shown For Ant (Build Failed):    {code}  03-Feb-2011 17:05:21  	Build CRUCHG-CHAIN-INT-865 started building on agent bamboo-agent-78.private.atlassian.com (2)  03-Feb-2011 17:05:21 	Build always requires a clean checkout  03-Feb-2011 17:05:21 	Cleaning build directory '/opt/bamboo-agent/xml-data/build-dir/CRUCHG-CHAIN-INT'  03-Feb-2011 17:05:21 	Updating source code to revision: 7557776da2a4bd7b6ef8194f7b2f57e69bcb0e8d  03-Feb-2011 17:05:47 	Error occurred while executing the build for CRUCHG-CHAIN-INT-865 : null  03-Feb-2011 17:05:47 	Build failed since text 'BUILD SUCCESS' was not found in the last 250 lines of the output.  03-Feb-2011 17:05:47 	Running post build plugin on server 'JMeter Aggregator Build Processor Server'  03-Feb-2011 17:05:47 	Running post build plugin on server 'Clover Delta Calculator'  03-Feb-2011 17:05:47 	Running post build plugin on server 'Build Hanging Detection Configuration'  03-Feb-2011 17:05:47 	Running post build plugin on server 'Post Command Server Runner'  03-Feb-2011 17:05:47 	Running post build plugin on server 'NCover Results Collector'  03-Feb-2011 17:05:47 	Running post build plugin on server 'FindBugs Server Build Processor'  03-Feb-2011 17:05:47 	Generating build results summary...  03-Feb-2011 17:05:47 	Saving build results to disk...  03-Feb-2011 17:05:47 	Indexing build results...  03-Feb-2011 17:05:48 	Finished building CRUCHG-CHAIN-INT-865.  {code}    Build Logs Shown for Script (Build Passed):    {code}  03-Feb-2011 17:05:22  	Build CRUCHG-CHAIN-FUNCFE-865 started building on agent bamboo-agent-70.private.atlassian.com (2)  03-Feb-2011 17:05:22 	Build always requires a clean checkout  03-Feb-2011 17:05:22 	Cleaning build directory '/opt/bamboo-agent/xml-data/build-dir/CRUCHG-CHAIN-FUNCFE'  03-Feb-2011 17:05:22 	Updating source code to revision: 7557776da2a4bd7b6ef8194f7b2f57e69bcb0e8d  03-Feb-2011 17:06:20 	Error occurred while executing the build for CRUCHG-CHAIN-FUNCFE-865 : null  03-Feb-2011 17:06:20 	Build successful since return code was 0  03-Feb-2011 17:06:20 	Running post build plugin on server 'JMeter Aggregator Build Processor Server'  03-Feb-2011 17:06:20 	Running post build plugin on server 'Clover Delta Calculator'  03-Feb-2011 17:06:20 	Running post build plugin on server 'Build Hanging Detection Configuration'  03-Feb-2011 17:06:20 	Running post build plugin on server 'Post Command Server Runner'  03-Feb-2011 17:06:20 	Running post build plugin on server 'NCover Results Collector'  03-Feb-2011 17:06:20 	Running post build plugin on server 'FindBugs Server Build Processor'  03-Feb-2011 17:06:20 	Generating build results summary...  03-Feb-2011 17:06:20 	Saving build results to disk...  03-Feb-2011 17:06:20 	Indexing build results...  03-Feb-2011 17:06:20 	Finished building CRUCHG-CHAIN-FUNCFE-865.  {code}"
0,AccessLogFilter + Maven Artifact Sharing = too much logging,Essentially every request for a mvn resource gets logged. We should exclude the maven urls from the access log filter and just make sure the plugin logs e.g. actual hits (when the artifact was served).
0,Enable button should be change to a run button ,"For https://gdansk.bamboo2.atlassian.com  1. I have disabled plan on dashboard (INT-DPL).  2. I Clicked on ""Enable"" to enable plan.  3. Button hasn't changed. Button should change to run button.    See attached screenshot."
0,Make Bamboo Windows installer work nicer with default permissions," At the moment the default permissions on Windows 7 really do screw us over since we write atlassian-bamboo.log to running dir & for services, the tmp directory"
1,User can configure key value pairs as parameters at the Plan level,Currently we only have global parameters. With this story we allow the user to define parameters on the Plan level and expose these to all Jobs within the Plan
0,User can override key value pairs when running a manual build,"When triggering a build manually, the user will be asked if he wants to overwrite any of the existing parameters for the Plan. "
0,Allow a user to kick of a build via REST and provide the parameters as part of the REST call,NULL
0,Null repository, A dummy repository that basically doesn't do anything. 
1,Make variables available as environment variables, So script could use it more easily.
1,Upgrade ActiveMQ to 5.4.2,NULL
0,New dialog forms revert to initial data on validation error,"This might actually be happening in all forms in Bamboo.    To reproduce:  * go to Stage configuration on a plan  * edit a stage  * remove the ""name"" but add a description (or just something that will cause a validation error), then submit  * notice that validation causes the data you entered to be wiped and the original data to be restored"
1,Build hung never finish in the UI even after killing agent and all the related process,"Hi, we got this build:    https://bamboo-extranet.atlassian.com/build/result/viewBuildResults.action?buildKey=JST-NONGAPPS-FFWDRV&buildNumber=176 that was hung. A manual stop didn't work so we killed the process and the agents.    In spite of doing that the UI still thinks that is building something, even when the agent (2@atlassian25) was idle and after that building other plans.    You can check the thread-dump of the server here: https://extranet.atlassian.com/jira/browse/ADM-12681    The plan was executed again, but the UI still thinks it is building 176. Maybe it is because ""Concurrency builds"" is set to 2.         "
0,MovePlans does not invalidate the caches,It doesn't invalidate the build definition cace and may not invalidate the dashboard cache either.
0,Make ImportExportManager available to plugins,"I've been trying to create a cross-product way of doing restores through a REST resource.  This would make life much simpler for doing integration tests in that we could put applications into a ""known"" state before a test is run.  I tried writing a REST resource to do this for Bamboo, but found that I could not get the [ImportExportManager|http://docs.atlassian.com/atlassian-bamboo/3.0/com/atlassian/bamboo/migration/ImportExportManager.html] injected.    Any chance of we can make this available to plugins for uses such as this?"
0,Maven Artifact Sharing breaks when server is configured to use SSL and Job is running on ec2,"Bamboos tunnel on ec2 is at 127.0.0.1 which is not valid for any SSL certificate.    {noformat}  build	15-Mar-2011 00:22:36	Downloading: https://maven.atlassian.com/repository/internal//opensymphony/xwork/1.2.5-atlassian-1/xwork-1.2.5-atlassian-1.pom  build	15-Mar-2011 00:22:37	11K downloaded  (xwork-1.2.5-atlassian-1.pom)  build	15-Mar-2011 00:22:37	Downloading: https://127.0.0.1:46593/plugins/servlet/maven/subscribe/JST-NIGHTLY-6/JST-NIGHTLY-BUILD//ognl/ognl/2.7.1/ognl-2.7.1.pom  build	15-Mar-2011 00:22:37	[WARNING] Unable to get resource 'ognl:ognl:pom:2.7.1' from repository atlassian-bamboo-artifact-sharing (https://127.0.0.1:46593/plugins/servlet/maven/subscribe/JST-NIGHTLY-6/JST-NIGHTLY-BUILD/): Error transferring file: java.security.cert.CertificateException: No subject alternative names matching IP address 127.0.0.1 found  build	15-Mar-2011 00:22:37	Downloading: https://maven.atlassian.com/repository/internal//ognl/ognl/2.7.1/ognl-2.7.1.pom  build	15-Mar-2011 00:22:37	5K downloaded  (ognl-2.7.1.pom)  build	15-Mar-2011 00:22:37	Downloading: https://127.0.0.1:46593/plugins/servlet/maven/subscribe/JST-NIGHTLY-6/JST-NIGHTLY-BUILD//jboss/javassist/3.6.ga/javassist-3.6.ga.pom  build	15-Mar-2011 00:22:37	[WARNING] Unable to get resource 'jboss:javassist:pom:3.6.ga' from repository atlassian-bamboo-artifact-sharing (https://127.0.0.1:46593/plugins/servlet/maven/subscribe/JST-NIGHTLY-6/JST-NIGHTLY-BUILD/): Error transferring file: java.security.cert.CertificateException: No subject alternative names matching IP address 127.0.0.1 found  build	15-Mar-2011 00:22:37	Downloading: https://maven.atlassian.com/repository/internal//jboss/javassist/3.6.ga/javassist-3.6.ga.pom  build	15-Mar-2011 00:22:37	1008b downloaded  (javassist-3.6.ga.pom)  build	15-Mar-2011 00:22:37	Downloading: https://127.0.0.1:46593/plugins/servlet/maven/subscribe/JST-NIGHTLY-6/JST-NIGHTLY-BUILD//org/rifers/rife-continuations/0.0.2/rife-continuations-0.0.2.pom  build	15-Mar-2011 00:22:37	[WARNING] Unable to get resource 'org.rifers:rife-continuations:pom:0.0.2' from repository atlassian-bamboo-artifact-sharing (https://127.0.0.1:46593/plugins/servlet/maven/subscribe/JST-NIGHTLY-6/JST-NIGHTLY-BUILD/): Error transferring file: java.security.cert.CertificateException: No subject alternative names matching IP address 127.0.0.1 found  {noformat}"
0,Upgrade to gadgets 3.0.4,To get up to date with the February Common Upgrade Pack:  https://extranet.atlassian.com/display/DEV/Common+Module+Upgrade+Pack+-+2011-02  we need to upgrade to Gadgets 3.0.4.    Should be easy as the fixes are minor and we are on gadgets 3.0.2.
0,Upgrade to Atlassian XWork 1.12,To get up to date with the February Common Upgrade Pack:  https://extranet.atlassian.com/display/DEV/Common+Module+Upgrade+Pack+-+2011-02  we need to upgrade to XWork 1.12
0,context contains arrays when it expects strings on validation failure,"In our task code on validation failure, we re-copy everything that can in on the request back on to the stack for re-rendering (so the values entered by the user remain).    The problem is that the actionContext puts everything in an Array, where it usually would just be a string.  Whilst some of our front end code accomodates this, some of it doesn't (e.g. select/show/hide stuff)    I was originally going to convert everything over to a string when i put it on the stack, but I'm not sure how many assumptions I can make (are they all supposed to be strings, what if the front end really does want an array)"
0,Creating a task without a executable selected dies badly,"When you click save on a maven task at the moment, and you haven't selected an actual builder from the drop down, it dies badly, rather than just returning a nice validation error above the drop down: ""Please select a builder""."
0,Finalise Task wording on Plan/Job Creation screens,and also Task editing screens    Wording will be available soon
0,Create a popup for available variables,A popup to display available global variables should be accessible from the Plan and Job configuration pages. The popup should have 2 tab (Variables).  
0,MsTest Conversion,NULL
0,Categories for tasks backend,NULL
0,Categories for tasks UI,NULL
0,"Jobs with different Subversion repositories on the same host: ""Failed to checkout source code to revision""","We tried to use the following jobs with different Subversion locations in a plan in Bamboo:    - *Main job*: https://svn.atlassian.com/svn/private/atlassian/confluence/branches/confluence-project-3.5-stable  - *Plugin trigger job*: https://svn.atlassian.com/svn/public/atlassian/bamboo/plugins/build-trigger-scripts/trunk    Notice that the first job uses {{/svn/private/}} and the second uses {{/svn/public/}} -- different paths on the same host.    Bamboo fails when running the second job because it attempts to check out the first job's revision on the second job's Subversion repository, which does not work:    bq. Failed to checkout source code to revision '148195' ...    Here's an example on our internal Bamboo server:    * https://bamboo.extranet.atlassian.com/browse/CONFSTAB-MAIN-76  "
1,Git Repository locks index.lock on Windows agents,Whenever an error occurs on windows agents during the checkout phase(GitOperationHelper.checkout) the .git/index.lock file handle became locked by the system - disallowing any further operations for that particular plan.    Details: [BSP-4370|https://support.atlassian.com/browse/BSP-4370]
0,Using the git repository type can not connect to github using ssh keys,When attempting to use a key to log into my github account using my private key i get an authentication error. Specifying the same key with ssh -i i can connect to github fine. Using a username/password is a temporary work around.  
0,Release Management Plugin - Release Build  PostChainAction (java.lang.NullPointerException),"I've taken the new Jar from here   http://jira.atlassian.com/browse/BAM-8064    I still get the same erro    System Error Details  Build LGXT Test - LGXT Release 9 : PostChainAction   (java.lang.NullPointerException : )    Occurrences: 2   First Occurred: 11 Apr 2011, 4:49:14 PM   Last Occurred: 11 Apr 2011, 4:55:16 PM  java.lang.NullPointerException    com.atlassian.bamboo.build.VariableBuilderBeanImpl.getVariablesForSubscriptionsContext(VariableBuilderBeanImpl.java:125)    com.atlassian.bamboo.build.VariableBuilderBeanImpl.filterVariablesByName(VariableBuilderBeanImpl.java:61)    com.atlassian.bamboo.build.VariableSubstitutionBeanImpl.substituteBambooVariables(VariableSubstitutionBeanImpl.java:57)    com.atlassian.bamboo.build.VariableSubstitutionBeanImpl.substituteBambooVariables(VariableSubstitutionBeanImpl.java:42)    com.sysbliss.bamboo.plugins.brmp.action.postrunners.TaggerRunner.runTagger(TaggerRunner.java:74)    com.atlassian.bamboo.plugins.brmp.action.BRMPChainCompleteAction.execute(BRMPChainCompleteAction.java:61)    com.atlassian.bamboo.chains.ChainPluginSupportImpl.chainCompleted(ChainPluginSupportImpl.java:94)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.finaliseChainStateIfChainExecutionHasCompleted(ChainExecutionManagerImpl.java:525)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.access$600(ChainExecutionManagerImpl.java:49)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl$1.run(ChainExecutionManagerImpl.java:281)    com.atlassian.util.concurrent.ManagedLocks$ManagedLockImpl.withLock(ManagedLocks.java:322)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.execute(ChainExecutionManagerImpl.java:240)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.access$1000(ChainExecutionManagerImpl.java:49)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl$2.run(ChainExecutionManagerImpl.java:327)    com.atlassian.util.concurrent.ManagedLocks$ManagedLockImpl.withLock(ManagedLocks.java:322)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.handleEvent(ChainExecutionManagerImpl.java:307)    sun.reflect.GeneratedMethodAccessor374.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.orm.hibernate.HibernateInterceptor.invoke(HibernateInterceptor.java:117)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy48.handleEvent(Unknown Source)    com.atlassian.event.legacy.LegacyListenerHandler$LegacyListenerInvoker.invoke(LegacyListenerHandler.java:55)    com.atlassian.event.internal.AsynchronousAbleEventDispatcher$2.run(AsynchronousAbleEventDispatcher.java:60)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:662)    I've a very simple devenv build."
0,JDK Label should default to JDK,At the moment it's just the first one
0,Fix up the logs pages for Plans & Jobs,"This still wasn't fixed in 3.0. Table should be 100%. Get rid of view, link should go to the logs tab in the Job. Maybe even dispaly the last 25 lines in the logs inline with the hide / show twixie"
1,Bamboo fails to check out git code due to 'Missing object' but command line git clones the repo just fine,"While checking out: https://github.com/krosenvold/bgit.git  Checkout to revision 67fc88bf41aa2ddea0e50159868764c82bed5bad has failed due to Missing object 3a450411d6868221ae290bc0c17695de2990d5d8. Note this might happen if the repository contains submodules, as Bamboo does not currently support git submodules.    Is it a bug? How can I find out if the repo contains submodules?"
0,Tests that are recorded as error cause the build not to display test results.,See the following failure: http://tardigrade.sydney.atlassian.com:8085/bamboo/browse/BAM-FC-CLN-3947  
0,"execution of accesskey=""E"" on a build instance results in ""page not found""","h3. steps  # open a build instance   # hit ur browsers hotkey for edting  -> page not found    h3. cause  looking @ the generates HTML the fault is obvious:  {code:html}    <a class=""hidden"" href=""/build/admin/edit/editBuildConfiguration.action?buildKey=ECC-ECC21-18"" accesskey=""E"">Edit Plan</a>  {code}      h3. suggestion    i like the possibility to directly edit the config from a build instance and hence i would not want this to be dropped. after all this is very useful when debugging builds.  hence, just fix the link in case of build instances.    "
0,Maven 2 Auto Dependency parent checkbox not disabled,"Maven 2 dependencies determined in the 2.5 bamboo instance still exist, however only the child checkbox is greyed out, not the parent."
1,Update Bamboo Plugin SDK default project to simple task,We should include two sample tasks:  * Builder Task with Executable UI  * Test Collector    Things we should showcase:  * Executables - TaskRequirementSupport and CapabilityDefaultsHelper  * ProcessService  * TestCollationService with custom TestReportCollector and TestCollectionResult. Should implement and explain TaskTestResultsSupport.  * Task Categories     
0,Update Context Sensitive Help for Bamboo 3.1,as per the release process here: https://extranet.atlassian.com/display/BAMBOO/Context+Sensitive+Help    I have already updated the help-paths.properties to be docs-031
0,Simplify capability message on Tasks,"The message currently reads:     ""8 agents are capable of building this Job. However, those agents are currently offline or disabled. This Job is capable of building in the Elastic Cloud on 8 images.""     This is way to long for this section and distracts with the red color from the actual task at hand. We should shorten this to:    ""8 agents and 8 images have the _capabilities_ to run this job""    Link capabilities to the requirements tab. The numbers are for agents (whether online or offline) people can click through to the requirements tab where they can see the more detailed message    We should also align the text on the right, see screenshot."
0,Some Tasks fail on Windows because they exceed the MAX_PATH limit of 260 chars,"You might see something like the following:  {noformat}  The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.  {noformat}"
0,User can specify on stage creation it is manual,NULL
0,User can change manual flag for stage,NULL
0,REST API should provide manual stages information,* manual flag on stage definition  * manual stage in stage result  * all data related to manual stage lifecycle    Required for proper displaying in navigator
0,Build results with incomplete Manual stages should be clearly marked in all views,Stage only  Stage result and jobs result - for results    * plan configuration navigator  * plan navigator  * stages configuration page  * plan result navigator    Mockups to come
0,Plan execution should stop at manual stage,# only object that are necessary to execute current lap should be created for execution  # results for ALL stages and jobs should be created when chain execution is started  # jobs than will be not executed in current lap should have additional status flag (new ExecutionState enum in resultSummary)  # chain result should be marked as waiting for continue (the same ExecutionState enum in resultsSummary)  # Plan status is SUCCESSFUL if no failures in executed jobs
0,User can resume execution of plan stopped on manual stage,"# The only stage that could be resumed is the stage where chain executions stopped  ## Bamboo should know what stage to execute  # New Actions menu item ""Continue"" (plan result and job result) - automatically resumes on manual stage, visible only if execution stopped on manual stage  # Play button in Navigator on manual stage Tick"
0,User can execute manual stages automatically on specific conditions,# Run Parameterized Build dialog should have new tab  ## Same UI on the JIRA side (see mockups from the Release Management Spec)  # REST API parameters to allow run up to a stage  ## Name of the manual stage to automatically run up to and including  ## This is automatically run any previous manual stages as well  # The same url parameters should be accepted by action and REST API (/)  
0,A user can see on Dashboard plans with manual stages waiting for execution,# Only if latest executed plan stopped on manual stage  # Only marker - no direct action available to continue
0,Plan configuration should be validated before resuming plan execution,"Plan should be validated against stored result (with partial job results)  Error (friendly :) )  should be presented to the user when:  # If stage or job meant to be executed has been removed from plan  # If plan structure (stages, jobs) has been changed"
0,Events for manual stages/rerun stage,* stop on manual stage  * result manual stage  * restart failed stage
0,Plugin points for manual stages/rerun stage,* stop on manual stage  * resume manual stage  * restart failed stage
0,Ability to hit the REST endpoint to trigger a build in Bamboo from the JIRA plugin,NULL
0,Bamboo not updating Mercurial sub-repositories when getting source on build server,"From SAC:  ""We use Mercurial and our project has a sub-repository (in a folder UI\Elegant) which is specified in the .hgsub file. The project is setup correctly, we can get it to any PC, commit, push, pull changes etc.. However, when changes to the sub-repository are made the Bamboo build fails. The hg.exe command appears to work in getting the project to the cache folder, though the sub-repository (UI\Elegant) is missing, but when it then tries to update the working folder (Bamboo\xml-data\build-dir\AIE-DEV-JOB1) it fails saying no working folder is specified for UI\Elegant.    How can we resolve this?  ""  https://support.atlassian.com/browse/BSP-4674"
0,Build time for statistics should be sum of job times for rerun builds,"When manual stage is waiting for execution - it changes build time statistics.  Sum of job execution should be counted, not chain stop - chain start times."
0,Whilelist Bamboo REST proxy in Bamboo JIRA Plugin,Not done for the moment since we don't know what URLS we want to hit
0,Git Repository should check out symbolic links correctly,"see: http://forums.atlassian.com/thread.jspa?threadID=54198&tstart=0    {quote}  Hey folks,     I'm rolling out a new instance of Bamboo 3.1 using a git repository. When I clone the repo using my local git client, symbolic links are checked out correctly, but when Bamboo checks them out the files only contain the name of the files they should be linked to.  {quote}"
0,A user can define application link to Jira application,NULL
0,A user can define link between Bamboo project and Jira project,NULL
0,Jira issue information should be asynchronously retrieved from Jira on view - ajax,NULL
0,REST endpoint to retrieve possible variables to override.,NULL
0,Ability to pick future manual stages to run when triggering a previous build.,NULL
0,Handle Authentication Errors returned by Proxy,"When the JIRA UI polls bamboo it could require the oauth handshake, we need to make sure this works, and flows nicely."
1,Strange errors with disappearing OAuth consumers on JBAC when talking to JDOG,"So I was trying to make some gadgets work on JDOG.    I was getting the ""Login & Approve"" button on JDOG.  I clicked it and got redirected to JBAC where I approved access.  Once back in JDOG I'd still get the Login & Approve button.    I tried to then reduce the JDOG OAuth consumer from JBAC but when I clicked delete I'd get an error popup with something along the lines of 'Failed to remove OAuth consumer.  I tried this several times with the same result. Refreshed the page and the consumer was still there.    I then visited  https://jira.bamboo.atlassian.com/plugins/servlet/oauth/users/access-tokens to look at my access tokens but got this:  {noformat}  com.atlassian.templaterenderer.RenderingException: java.lang.NullPointerException: consumer    com.atlassian.templaterenderer.velocity.one.six.internal.VelocityTemplateRendererImpl.render(VelocityTemplateRendererImpl.java:99)    sun.reflect.GeneratedMethodAccessor2870.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:307)    org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.doInvoke(ServiceInvoker.java:58)    org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.invoke(ServiceInvoker.java:62)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)    org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invokeUnprivileged(ServiceTCCLInterceptor.java:56)    org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invoke(ServiceTCCLInterceptor.java:39)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.osgi.service.importer.support.LocalBundleContextAdvice.invoke(LocalBundleContextAdvice.java:59)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)    org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy208.render(Unknown Source)    com.atlassian.oauth.serviceprovider.internal.servlet.user.AccessTokensServlet.doGet(AccessTokensServlet.java:60)    javax.servlet.http.HttpServlet.service(HttpServlet.java:690)    javax.servlet.http.HttpServlet.service(HttpServlet.java:803)    com.atlassian.plugin.servlet.DelegatingPluginServlet.service(DelegatingPluginServlet.java:42)    javax.servlet.http.HttpServlet.service(HttpServlet.java:803)    com.atlassian.plugin.servlet.ServletModuleContainerServlet.service(ServletModuleContainerServlet.java:52)    javax.servlet.http.HttpServlet.service(HttpServlet.java:803)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:269)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.applinks.core.rest.context.ContextFilter.doFilter(ContextFilter.java:25)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:66)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:74)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:71)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:211)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:150)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:71)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.security.auth.trustedapps.filter.TrustedApplicationsFilter.doFilter(TrustedApplicationsFilter.java:98)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.bamboo.persistence.BambooSessionInViewFilter.doFilterInternal(BambooSessionInViewFilter.java:31)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:210)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:174)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:151)    org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:200)    org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:283)    org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:773)    org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:703)    org.apache.jk.common.ChannelSocket$SocketConnection.runIt(ChannelSocket.java:895)    org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:685)    java.lang.Thread.run(Thread.java:662)  Caused by: java.lang.NullPointerException: consumer    com.atlassian.oauth.util.Check.notNull(Check.java:37)    com.atlassian.oauth.Token$TokenBuilder.consumer(Token.java:178)    com.atlassian.oauth.serviceprovider.sal.PluginSettingsServiceProviderTokenStore.get(PluginSettingsServiceProviderTokenStore.java:92)    com.atlassian.oauth.serviceprovider.sal.PluginSettingsServiceProviderTokenStore$KeyToToken.apply(PluginSettingsServiceProviderTokenStore.java:232)    com.atlassian.oauth.serviceprovider.sal.PluginSettingsServiceProviderTokenStore$KeyToToken.apply(PluginSettingsServiceProviderTokenStore.java:221)    com.google.common.collect.Iterators$8.next(Iterators.java:751)    org.apache.velocity.runtime.directive.Foreach.render(Foreach.java:346)    org.apache.velocity.runtime.parser.node.ASTDirective.render(ASTDirective.java:175)    org.apache.velocity.runtime.parser.node.ASTBlock.render(ASTBlock.java:72)    org.apache.velocity.runtime.parser.node.ASTIfStatement.render(ASTIfStatement.java:87)    org.apache.velocity.runtime.parser.node.SimpleNode.render(SimpleNode.java:336)    org.apache.velocity.Template.merge(Template.java:328)    org.apache.velocity.Template.merge(Template.java:235)    com.atlassian.templaterenderer.velocity.one.six.internal.VelocityTemplateRendererImpl.render(VelocityTemplateRendererImpl.java:90)  	... 113 more  {noformat}    When I went back to the Oauth consumers page the consumer was gone. I re-added it and now https://jira.bamboo.atlassian.com/plugins/servlet/oauth/users/access-tokens works again. I can also get the gadgets to authenticate.  Very weird I know and not the best of bug reports but it happend :)."
0,"If an older version of Bamboo is trying to run a newer version of the data, we should stop the server","If a Bamboo 3.1.1 instance is shutdown and then a 2.6.2 is run pointing to the 3.1.1 Home-Dir, the 2.6.2 will change the bamboo.cfg.xml to an old Build Number.    The Build number should be checked and if it is clear that it is higher than the one for the installation, Bamboo should not even start.    This can avoid potential changes that version 2.6.2 would make to the 3.1.1 database and avoid data corruption."
0,Ability for the Test Collater to aggregate test results,"If more than one Task creates test results in a Job, Bamboo will not be able to save all the test results for all the Tasks.    Only the last task will have the test results saved and presented on the UI.    *Desired behavior:* Any Task can create test results and the final Test Report will include results from all Tasks that created tests."
0,"Restricted administrators cannot delete, move or create plans","Users that have restricted administrator permissions (ie, Studio customers) are able to see the screens to move and delete plans, but when they try to perform the action, they are prevented from doing so due to method level permission checks on PlanManager.    It appears to me that {{ACL_BUILD_ADMIN}} in {{applicationContextAcegiAuthorization}} should be granted to restricted administrators, not just normal administrators."
1,Git Repository should work with command 'git log'.,"from [BSP-4755|https://support.atlassian.com/browse/BSP-4755]    Steps to reproduce:  1) set up a Git plan, for example pointing at ""git://github.com/atlassian/homebrew.git""  1.1) set up branch != ""master"", i.e. ""atlassian-sdk""  1.2) create script task containing ""git log""  2) tick ""clean working directory after each build"" (or ""Force Clean Build"" - doesn't really matter for this issue)  3) run plan (you might need to run it twice to observe the problem)  4a) build will fail with ""fatal: bad default revision 'HEAD'""  4b) where expected behavior is to see git changesets in the logs of the *successful* build"
0,Applink type is not recognised if annonymous access is turned off, We need to white list /rest/applinks/1.0/manifest. Should also check with integration guys what other URLS need to be publicly available
0,Creation of trusted apps fails with hibernate exceptions,"Hibernate operation: Could not execute JDBC batch update; SQL []; Batch  entry 0 insert into TRUSTED_APPS (APP_ID, PUBLIC_KEY, APP_NAME, TIMEOUT,  TRUSTED_APPS_ID) values ('jira:15489595',  'MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5wwkHI85p7czkTQCdoDkcA4TKkcF3VC6AHSrXZoMUfIQFE8QRetWipfAob0CMnG+iX0BWn/aPCJkkpHyt7xUNwAsrtHMISdtHovbBWRnE923r6tafzjwnu4sy2O82ogWN0HhSsA/2C15S7IC7rxiFhnpHNrt2UzERgonBbYbYBqvkP/2xz697UwD82Y8Wh7zk3IdPhCiWX6hcHzVLxtzxCj34O+toqLMnHOwW3HDcTue0Qba3nlwZ4fpRwEHZNI1mbDRaOmPRnoAb+jpr9XpO6iM7fDij277P30R6AiJ19oy+YA6z6vkDdVmQYAs7y+qAq0QxtLh4B50lX4oJAp/pwIDAQAB',  NULL, '10000', '327682') was aborted.  Call getNextException to see the  cause.; nested exception is java.sql.BatchUpdateException: Batch entry 0  insert into TRUSTED_APPS (APP_ID, PUBLIC_KEY, APP_NAME, TIMEOUT,  TRUSTED_APPS_ID) values ('jira:15489595',  'MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5wwkHI85p7czkTQCdoDkcA4TKkcF3VC6AHSrXZoMUfIQFE8QRetWipfAob0CMnG+iX0BWn/aPCJkkpHyt7xUNwAsrtHMISdtHovbBWRnE923r6tafzjwnu4sy2O82ogWN0HhSsA/2C15S7IC7rxiFhnpHNrt2UzERgonBbYbYBqvkP/2xz697UwD82Y8Wh7zk3IdPhCiWX6hcHzVLxtzxCj34O+toqLMnHOwW3HDcTue0Qba3nlwZ4fpRwEHZNI1mbDRaOmPRnoAb+jpr9XpO6iM7fDij277P30R6AiJ19oy+YA6z6vkDdVmQYAs7y+qAq0QxtLh4B50lX4oJAp/pwIDAQAB',  NULL, '10000', '327682') was aborted.  Call getNextException to see the  cause.  "
0,Expiry for shared artifacts,"For us it seems like the global artifact expiry is not affecting the shared artifacts. Because our jobs are producing shared artifacts that are around 1GB big, we are constantly running out of diskspace on the server.     Is there a possibility to outdate the shared artifacts which are stored on the server?"
0,The contents of atlassian-bundled-plugins.zip are retained in memory,"I'm seeing in a heap dump of Bamboo from studio.atlassian.com, that the content of {{atlassian-bundled-plugins.zip}} is retained in memory by the Tomcat {{WebappClassLoader}}.    This is a problem that has already been addressed in Confluence (CONF-17511) and JIRA ([JRADEV-6137|https://jdog.atlassian.com/browse/JRADEV-6137]), so a similar solution should be implemented in Bamboo to avoid using the classloader to access the file. This file isn't as large in Bamboo as it is in JIRA and Confluence, but it still is a fairly significant amount of heap space to be wasting, and it is actually considerably bigger on Studio than in a standalone Bamboo (10M in Studio vs. 5.2M normally)."
0,Release status is not synced via rest status call in JIRA Bamboo plugin,"Whats happening:  * The OAuth token has expired during a release  * Once the user has done the 3LO dance they are returned to the tab panel  * Tab Panel queries Bamboo, detects that the build has finished (success or failure), releases the jira version  * The frontend code does not update the fact that the version has been released even though the status (released or not) has been sent back via the JSON response"
0,"Bamboo returns a JSON response when I submit the ""Edit Artifact Dependency"" dialog using Enter","Steps to reproduce:  # Click Edit on an artifact dependency  # Focus the Destination Directory field and change the text  # Press Enter  # Bamboo briefly shows ""An unknown error has occurred"", and then returns a JSON response (!)    See this video:    http://screencast.com/t/DHniQgeSp"
0,Running a Bamboo Remote Agent service on Windows as a user other than System the parameter passed to the java.io.tmpdir fails the build,"When the agent runs as the system, the param for java.io.tmpdir defaults to *C:\WINDOWS\TEMP\<BUILD-KEY>* and the build works just fine.  When I run the agent as a user (trying to get around permissions issues with writing) the param for java.io.tmpdir defaults to *""C:\Documents and Settings\<user-name>\Local Settings\temp\<BUILD-KEY>""*    When the build runs, the first task that runs an Ant script I get a message that says:  {color:red}  BUILD FAILED  Target ""and"" does not exist in project ""<project-name>""{color}    Even though I see that the param being passed is in quotes so the space in the path should be taken care of, that's the only ""and"" I can find in my log.  In the build script ""and"" does not appear anywhere, not even embedded in some other word."
0,"Change the ""not run"" icon in manual stages",NULL
0,agent.error.wrongState is not defined in BambooActionSupport.properties,when removing agent that is building currently
0,"Race condition can create duplicate plan keys, causing Bamboo to fail to start","Looks like a double-post of a create plan form resulted in duplicate plan key creation:    {noformat}  bamboo=# select build_id, buildkey from build;   build_id | buildkey   ----------+----------     393217 | TRUNK    1507329 | JSTMVN    1507330 | JSTMVN     393218 | JOB1  {noformat}    Excerpt from the Bamboo logs at that time:    {noformat}  @400000004e166cf406b6c1f4 2011-07-07 21:35:22,112 INFO [TP-Processor15] [AccessLogFilter] sysadmin POST https://sdog.jira.com/builds/build/admin/create/createPlan.action 103093kb  @400000004e166d0d0c95cb4c 2011-07-07 21:35:47,211 INFO [TP-Processor17] [AccessLogFilter] sysadmin POST https://sdog.jira.com/builds/build/admin/create/createPlan.action 114482kb  @400000004e166d1429151cdc 2011-07-07 21:35:54,685 INFO [TP-Processor15] [PlanCreationTemplate] Creating plan with key: JSTDEV-JSTMVN  @400000004e166d14301380b4 2011-07-07 21:35:54,803 INFO [TP-Processor15] [PlanCreationTemplate] New build created with key JSTDEV-JSTMVN  @400000004e166d1432779bec 2011-07-07 21:35:54,844 INFO [TP-Processor17] [PlanCreationTemplate] Creating plan with key: JSTDEV-JSTMVN  @400000004e166d1434823ce4 2011-07-07 21:35:54,852 ERROR [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-6] [ChainExecutionManagerImpl] Could not start executing Plan 'JSTDEV-JSTMVN' as the plan did not have any stages defined  @400000004e166d14348244b4 2011-07-07 21:35:54,878 ERROR [TP-Processor17] [FiveOhOh] 500 Exception was thrown.  @400000004e166d143482489c org.springframework.dao.IncorrectResultSizeDataAccessException: query did not return a unique result: 2  @400000004e166d143482489c       at org.springframework.orm.hibernate.SessionFactoryUtils.convertHibernateAccessException(SessionFactoryUtils.java:590)  @400000004e166d143482600c       at org.springframework.orm.hibernate.HibernateAccessor.convertHibernateAccessException(HibernateAccessor.java:353)  @400000004e166d14348263f4       at org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:375)  @400000004e166d14348267dc       at org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:337)  @400000004e166d1434826bc4       at com.atlassian.bamboo.plan.PlanHibernateDao.getPlanByKey(PlanHibernateDao.java:43)  @400000004e166d1434826fac       at sun.reflect.GeneratedMethodAccessor225.invoke(Unknown Source)  @400000004e166d1434826fac       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004e166d143482871c       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004e166d1434828b04       at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)  @400000004e166d1434828b04       at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)  @400000004e166d1434828eec       at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)  @400000004e166d14348296bc       at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)  @400000004e166d14348296bc       at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)  @400000004e166d1434829aa4       at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)  @400000004e166d1434829e8c       at $Proxy7.getPlanByKey(Unknown Source)  @400000004e166d1434829e8c       at com.atlassian.bamboo.plan.PlanManagerImpl.getPlanByKey(PlanManagerImpl.java:91)  @400000004e166d143482a274       at com.atlassian.bamboo.plan.PlanManagerImpl.getPlanByKey(PlanManagerImpl.java:85)  {noformat}    This completely breaks Bamboo. It can no longer even start up without throwing the {{IncorrectResultSizeDataAccessException}}.    Ideally, the uniqueness of this key should be enforced via a database constraint."
0,Modify Bamboo and Bamboo agents to first use the JAVA_HOME variable when starting,"Some users report that Bamboo starts using the first java command on the PATH, rather than respecting the exported JAVA_HOME varible.    This is inconsistent iwth other Atlassian tools, for example, JIRA, which look at the JAVA_HOME variable when starting.    We need to test this behaviour for:  # Bamboo server itself  # Bamboo agents.    For more information that caused this issue see:  https://jira.atlassian.com/browse/BAM-9284?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=258775#comment-258775"
0,JBAM Builds Versions tab panel breaks when no issues associated with version,"The related builds by date filter when looking at the builds for a specific version causes null pointer exceptions in bamboo when there are no issues with that version.  Fix is required on Bamboo side to avoid null pointers, and maybe on jira side to prevent it from making an unecessary call in the first place.      {code}  2011-07-22 14:59:23,616 INFO [qtp1872676077-17] [AccessLogFilter] admin POST http://brydie:8085/bamboo/ajax/build/viewBuildResultsByJiraKey.action 25253kb  2011-07-22 14:59:23,620 ERROR [qtp1872676077-17] [FiveOhOh] 500 Exception was thrown.  java.lang.NullPointerException    com.atlassian.bamboo.plugins.jiraPlugin.actions.ajax.ViewBuildResultsByJiraKey.execute(ViewBuildResultsByJiraKey.java:42)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)  {code}"
1,Maven dependency task cannot handle Maven3 projects,Bamboo embeds Maven 2.1.0 to read project metadata for the Automatic Dependency task. This version of Maven has problems reading POM and metadata files built with Maven 3.0.x.    This change cannot be back ported to 3.2.x as changing the Maven version has certain risks attached to it that can only be mitigated by a full release of Bamboo by running through teams quality control process.
0,"""An unexpected error has occurred"" after clicking a link to a JIRA issue",NULL
0,Cannot install plugins onto JBAC using the UPM in Chrome,"Firefox works fine, but when I try to install a plugin using Chrome the installer popup appears and then never finishes    JBAC is currently using: Atlassian Bamboo version 3.2 build 2600 - 20 Jul 11"
0,Stop distributing the user management files at <Bamboo-Install>/webapp/WEB-INF/classes/,Bamboo 3.2 users the Crowd/LDAP files located at *<Bamboo-Home-Dir>/xml-data/configuration/*    However we are still distributing them at *<Bamboo-Install>/webapp/WEB-INF/classes/*
0,Webresources from servlets leak onto the next page,"When the UPM UI is viewed, followed by any other UI the UPM resources are also included on the next page. This can be seen quite easily by looking at the html that is output.    From a bit of investigation com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter should be run for any request that returns html to clear the webResource cache. It is only shown for .action or /s/*     Fragment from web.xml    {code:xml}    <filter-mapping>      <filter-name>requestCache</filter-name>      <url-pattern>/s/*</url-pattern>      <dispatcher>REQUEST</dispatcher>      <dispatcher>FORWARD</dispatcher>    </filter-mapping>      <filter-mapping>      <filter-name>requestCache</filter-name>      <url-pattern>*.action</url-pattern>      <dispatcher>REQUEST</dispatcher>      <dispatcher>FORWARD</dispatcher>    </filter-mapping>    <filter>       <filter-name>requestCache</filter-name>      <filter-class>com.atlassian.bamboo.filter.RequestCacheThreadLocalFilter</filter-class>    </filter>  {code}    Adding /plugins/servlet/* as a filter mapping would solve the issue I was running into (the same script being included multiple times)"
1,Bamboo does not start on OpenJDK 7,"Captcha depends on SUN packages.    {noformat}  2011-08-01 11:06:01,314 INFO [main] [BrokerService] ActiveMQ JMS Message Broker (bamboo, ID:barnard.local-62860-1312160747729-0:1) stopped  2011-08-01 11:06:01,316 INFO [main] [ConfigurableLocalSessionFactoryBean] Closing Hibernate SessionFactory  2011-08-01 11:06:01,320 ERROR [main] [ContextLoader] Context initialization failed  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'imageCaptchaService' defined in class path resource [applicationContextCaptcha.xml]: Cannot resolve reference to bean 'captchaEngine' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'captchaEngine' defined in class path resource [applicationContextCaptcha.xml]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.atlassian.bamboo.captcha.AtlassianGimpyEngine]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException  Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'captchaEngine' defined in class path resource [applicationContextCaptcha.xml]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.atlassian.bamboo.captcha.AtlassianGimpyEngine]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException  Caused by: org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.atlassian.bamboo.captcha.AtlassianGimpyEngine]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException  Caused by: java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException    java.lang.Class.forName0(Native Method)    java.lang.Class.forName(Class.java:186)    com.octo.captcha.image.gimpy.GimpyFactory.class$(GimpyFactory.java:30)    com.octo.captcha.image.gimpy.GimpyFactory.<clinit>(GimpyFactory.java:30)    com.atlassian.bamboo.captcha.AtlassianGimpyEngine.buildInitialFactories(AtlassianGimpyEngine.java:59)    com.octo.captcha.engine.image.ListImageCaptchaEngine.<init>(ListImageCaptchaEngine.java:24)    com.atlassian.bamboo.captcha.AtlassianGimpyEngine.<init>(AtlassianGimpyEngine.java:28)    sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    java.lang.reflect.Constructor.newInstance(Constructor.java:525)    org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:85)    org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:61)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateBean(AbstractAutowireCapableBeanFactory.java:752)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:717)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:386)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:249)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:155)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:246)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:160)    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:267)    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:110)    org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:389)    org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:120)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:795)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:713)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:386)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:249)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:155)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:246)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:160)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:291)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:352)    org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:245)    org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:188)    org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:49)    com.atlassian.spring.container.ContainerContextLoaderListener.contextInitialized(ContainerContextLoaderListener.java:32)    org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:633)    org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:228)    org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1160)    org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:576)    org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:492)    org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)    org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:93)    org.eclipse.jetty.server.Server.doStart(Server.java:243)    org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)    com.atlassian.bamboo.server.Server.main(Server.java:77)  Caused by: java.lang.ClassNotFoundException: com.sun.image.codec.jpeg.ImageFormatException    java.net.URLClassLoader$1.run(URLClassLoader.java:366)    java.net.URLClassLoader$1.run(URLClassLoader.java:355)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:354)    java.lang.ClassLoader.loadClass(ClassLoader.java:423)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)    java.lang.ClassLoader.loadClass(ClassLoader.java:356)  	... 47 more  2011-08-01 11:06:01,326 WARN [main] [log] Failed startup of context WebAppContext@2ba42999@2ba42999/bamboo,file:/Users/jdumay/code/bamboo/components/bamboo-web-app/src/main/webapp/,/Users/jdumay/code/bamboo/components/bamboo-web-app/src/main/webapp  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'imageCaptchaService' defined in class path resource [applicationContextCaptcha.xml]: Cannot resolve reference to bean 'captchaEngine' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'captchaEngine' defined in class path resource [applicationContextCaptcha.xml]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.atlassian.bamboo.captcha.AtlassianGimpyEngine]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException  Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'captchaEngine' defined in class path resource [applicationContextCaptcha.xml]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.atlassian.bamboo.captcha.AtlassianGimpyEngine]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException  Caused by: org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [com.atlassian.bamboo.captcha.AtlassianGimpyEngine]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException  Caused by: java.lang.NoClassDefFoundError: com/sun/image/codec/jpeg/ImageFormatException    java.lang.Class.forName0(Native Method)    java.lang.Class.forName(Class.java:186)    com.octo.captcha.image.gimpy.GimpyFactory.class$(GimpyFactory.java:30)    com.octo.captcha.image.gimpy.GimpyFactory.<clinit>(GimpyFactory.java:30)    com.atlassian.bamboo.captcha.AtlassianGimpyEngine.buildInitialFactories(AtlassianGimpyEngine.java:59)    com.octo.captcha.engine.image.ListImageCaptchaEngine.<init>(ListImageCaptchaEngine.java:24)    com.atlassian.bamboo.captcha.AtlassianGimpyEngine.<init>(AtlassianGimpyEngine.java:28)    sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    java.lang.reflect.Constructor.newInstance(Constructor.java:525)    org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:85)    org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:61)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateBean(AbstractAutowireCapableBeanFactory.java:752)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:717)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:386)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:249)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:155)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:246)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:160)    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:267)    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:110)    org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:389)    org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:120)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:795)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:713)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:386)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:249)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:155)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:246)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:160)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:291)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:352)    org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:245)    org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:188)    org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:49)    com.atlassian.spring.container.ContainerContextLoaderListener.contextInitialized(ContainerContextLoaderListener.java:32)    org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:633)    org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:228)    org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1160)    org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:576)    org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:492)    org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)    org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:93)    org.eclipse.jetty.server.Server.doStart(Server.java:243)    org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)    com.atlassian.bamboo.server.Server.main(Server.java:77)  Caused by: java.lang.ClassNotFoundException: com.sun.image.codec.jpeg.ImageFormatException    java.net.URLClassLoader$1.run(URLClassLoader.java:366)    java.net.URLClassLoader$1.run(URLClassLoader.java:355)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:354)    java.lang.ClassLoader.loadClass(ClassLoader.java:423)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)    java.lang.ClassLoader.loadClass(ClassLoader.java:356)  	... 47 more  2011-08-01 11:06:01,343 INFO [main] [log] Started SelectChannelConnector@0.0.0.0:9087  ****************************************************************  *                                                              *  * You can see Bamboo by visiting http://localhost:9087/bamboo  *                                                              *  ****************************************************************  {noformat}"
0,Prevent failure of the 2511 upgrade task,"{code}  2011-07-30 11:54:53,085 INFO [1-UpgradeTaskBackgroundThread:pool-7-thread-1] [AbstractUpgradeManager] ---------------------------------------------------------------------------  2011-07-30 11:54:53,085 INFO [1-UpgradeTaskBackgroundThread:pool-7-thread-1] [AbstractUpgradeManager] Running upgrade task 2511 : Upgrade JIRA server configuration to UAL config  2011-07-30 11:54:53,085 INFO [1-UpgradeTaskBackgroundThread:pool-7-thread-1] [AbstractUpgradeManager] ---------------------------------------------------------------------------  2011-07-30 11:54:54,164 ERROR [1-UpgradeTaskBackgroundThread:pool-7-thread-1] [AbstractUpgradeManager] java.lang.IllegalArgumentException: Application with server ID 3c74886c-6074-30a4-a22d-dcc12346ba68 is already configured  java.lang.IllegalArgumentException: Application with server ID 3c74886c-6074-30a4-a22d-dcc12346ba68 is already configured  at com.atlassian.applinks.core.DefaultApplicationLinkService.addApplicationLink(DefaultApplicationLinkService.java:201)  at com.atlassian.applinks.core.DefaultApplicationLinkService.createApplicationLink(DefaultApplicationLinkService.java:638)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at com.atlassian.applinks.host.OsgiServiceProxyFactory$DynamicServiceInvocationHandler.invoke(OsgiServiceProxyFactory.java:111)  at $Proxy32.createApplicationLink(Unknown Source)  at com.atlassian.bamboo.applinks.JiraApplinksServiceImpl.createJiraApplinkWithBasicAuthentication(JiraApplinksServiceImpl.java:77)  at com.atlassian.bamboo.upgrade.tasks.UpgradeTask2511UpgradeJiraServerConfig.doUpgrade(UpgradeTask2511UpgradeJiraServerConfig.java:65)  at com.atlassian.bamboo.upgrade.tasks.AbstractInHibernateTransactionUpgradeTask$1.doInHibernateTransaction(AbstractInHibernateTransactionUpgradeTask.java:40)  at com.atlassian.bamboo.persistence.BambooConnectionTemplate$1$1.doInHibernate(BambooConnectionTemplate.java:51)  at org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:370)  at com.atlassian.bamboo.persistence.BambooConnectionTemplate$1.doInTransactionWithoutResult(BambooConnectionTemplate.java:44)  at org.springframework.transaction.support.TransactionCallbackWithoutResult.doInTransaction(TransactionCallbackWithoutResult.java:33)  at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:127)  at com.atlassian.bamboo.persistence.BambooConnectionTemplate.execute(BambooConnectionTemplate.java:39)  at com.atlassian.bamboo.upgrade.tasks.AbstractInHibernateTransactionUpgradeTask.doUpgrade(AbstractInHibernateTransactionUpgradeTask.java:36)  at com.atlassian.bamboo.upgrade.AbstractUpgradeManager.runUpgradeTask(AbstractUpgradeManager.java:175)  at com.atlassian.bamboo.upgrade.UpgradeManagerImpl.doUpgrade(UpgradeManagerImpl.java:72)  at com.atlassian.bamboo.upgrade.UpgradeLauncher$1.call(UpgradeLauncher.java:115)  at com.atlassian.bamboo.upgrade.UpgradeLauncher$1.call(UpgradeLauncher.java:112)  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)  at java.util.concurrent.FutureTask.run(FutureTask.java:138)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)  at java.util.concurrent.FutureTask.run(FutureTask.java:138)  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)  at com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)  at java.lang.Thread.run(Thread.java:662)  2011-07-30 11:54:54,165 INFO [1-UpgradeTaskBackgroundThread:pool-7-thread-1] [AbstractUpgradeManager] Completed upgrade task 2511 with errors.  2011-07-30 11:54:54,166 FATAL [main] [UpgradeLauncher] Upgrade task error: Upgrade task for build 2511 failed with exception: Application with server ID 3c74886c-6074-30a4-a22d-dcc12346ba68 is already configured  {code}    Currently, we know that the recommendation from [this KB Article|http://confluence.atlassian.com/display/BAMKB/Bamboo+3.2+Upgrade+Fails+on+UAL+Upgrade+Task] can fix the problem."
1,BRMP incompatible with Bamboo 3.2,"Following error is recorded when loading the BRMP plugin (Version 2.0.4 or 2.0.1)  in Bamboo 3.2:    {code}  2011-08-01 11:49:35,400 ERROR [qtp744758938-11] [DefaultPluginManager] Exception when retrieving plugin module brmpPlanConfiguration, will disable plugin com.atlassian.bamboo.plugins.bamboo-release-management-plugin  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'com.atlassian.bamboo.plugins.brmp.config.ReleaseManagementConfigurator': Instantiation of bean failed; nested exception is java.lang.NoClassDefFoundError: com/atlassian/bamboo/jira/jiraserver/JiraServerManager  Caused by: java.lang.NoClassDefFoundError: com/atlassian/bamboo/jira/jiraserver/JiraServerManager          at java.lang.Class.getDeclaredMethods0(Native Method)          at java.lang.Class.privateGetDeclaredMethods(Class.java:2427)          at java.lang.Class.privateGetPublicMethods(Class.java:2547)          .....          .....  Caused by: java.lang.ClassNotFoundException: com.atlassian.bamboo.jira.jiraserver.JiraServerManager          at java.net.URLClassLoader$1.run(URLClassLoader.java:202)          at java.security.AccessController.doPrivileged(Native Method)          at java.net.URLClassLoader.findClass(URLClassLoader.java:190){code}  "
0,Configuring OAuth Consumers document is out of date,"In Bamboo 3.2, in order to setup an OAuth consumer you need to first trust 2 applications, and then you'll able to add an OAuth consumer.    This document is out of date:     http://confluence.atlassian.com/display/BAMBOO/Configuring+OAuth+Consumers    It says on Step 3: _In the left-hand menu, under the title 'Gadgets', click the 'OAuth Consumers' link. The 'OAuth Administration' page will be displayed, showing a list of configured OAuth Consumers (if any exist)._    There is no such option under the Gadgets section, only ""Available Gadgets"", and that page has no OAuth links either."
1,Release feature in  JIRA  and Bamboo with trusted application not working properly,When User configure trusted application to communicate between JIRA and Bamboo. The feature release version will not work properly. The JIRA will not show the following in the UI   !bamboojira.png|thumbnail!    This problem does not happen when using basic authentication in application link. As a workaround user can try with basic authentication.jira
0,Mercurial Repository should correctly filter changesets since last build (even if there was more than 100 changesets),"http://tardigrade.sydney.atlassian.com:8085/bamboo/browse/BAM-EXT-4049    on bamboo-hg repository,  lastVcsRevisionKey = 1c969a8270b7b6b70ff7f855a31fde22d3abae0c (Marcin Gardias code change)  currentRevisionKey = 06998bc525a291de042309b8972e5d743d491d8b (Krystian's merge)    between those two revisions Bamboo has found 106 code changes. Due to hard limit (100 changeset parsed for the build, no more) the Mercurial Repository falsely filters out the not-relevant changesets to current branch:  bq. The Mercurial Plugin might incorrectly report the changesets triggering the build *if* there was more than 100 (HgRepository.CHANGESET_LIMIT) changes between the last build. The Mercurial Plugin will just have not enough information to traverse the changeset graph properly and will in defense report all changesets between, even those non-related to the built branch.  Code responsible is near here: [HgCommandProcessor.filterOutUnreachableChangesets|https://atlaseye.atlassian.com/browse/bamboo-hg/components/bamboo-plugins/bamboo-plugin-mercurial/src/main/java/com/atlassian/bamboo/plugins/hg/HgCommandProcessor.java?r=d8b981d129d0#to384]    I'm raising this issue so we can have a look at in in better detail."
0,Error summary does not match actual error log when rerunning,"The Error Summary shows a build failure, but when I click through the build was actually successful. Looks like the job was retried but the old error summary still shows.        "
0,JBAM - Socket time outs after 10 seconds,When talking to tardigrade from a JIRA Standalone instance I keep getting socket time out exceptions after 10 seconds. This is happening a lot.  Not sure if the core problem is Tardigrade fro taking too long or the Plugin should be waiting longer than that for a response.    The error message is not that friendly either.    
0,On restart non default project -> Bamboo servers links aren't used,"If you have two links to Bamboo and you have a project that points to the non-default App Link, this will not be respected on a restart.    You need to go to the Bamboo config screen and resave the configuration for it work again"
0,Handle empty destination directories correctly for artifacts when using Oracle,"Currently, the Destination directory option in Artifact Dependency can be left as empty.    This does not work when using Oracle.    Producing artifacts works well, but when a job from a later stage tries to use the artifacts and error occurs.    As Oracle cannot handle storing empty strings, the directory is stored as NULL instead.    It looks like Bamboo fails to deal with NULLs for a destination directory when retrieving and using the value:    {code}  java.lang.NullPointerException  at java.io.File.<init>(File.java:305)  at com.atlassian.bamboo.build.artifact.LocalArtifactManager.retrieve(LocalArtifactManager.java:74)  at com.atlassian.bamboo.build.pipeline.tasks.PrepareBuildTask.prepareArtifacts(PrepareBuildTask.java:100)  at com.atlassian.bamboo.build.pipeline.tasks.PrepareBuildTask.call(PrepareBuildTask.java:50)  {code}    The problem happens during execution of the second stage, even though no actual build action is being performed. The bamboo-home/artifacts directory is present, and the artifact is created correctly.    If the directory the artifact should be referenced from in stage 2 is changed, another nullpointerexception happens:    {code}  2011-08-29 17:14:30,361 ERROR [qtp470279362-11] [FiveOhOh] 500 Exception was thrown.  java.lang.NullPointerException  at com.atlassian.bamboo.ww2.actions.build.admin.config.ConfigureArtifactSubscription.isSubscriptionUnique(ConfigureArtifactSubscription.java:158)  at com.atlassian.bamboo.ww2.actions.build.admin.config.ConfigureArtifactSubscription.validateSubscription(ConfigureArtifactSubscription.java:147)  {code}       "
0,"When retrying a release with the release management plugin, default plan variables displayed are not used in the build","We have a bamboo plan for our ""release"" which runs the maven-release-plugin to actually release our software.  We define the plan with two plan variables: ""thisVersion"" and ""nextVersion"".      Steps to reproduce:  # Go to a JIRA version for the project we want to release, go to the ""release"" tab, and click to release this JIRA version using the bamboo release plugin  # select ""release with new build"" and choose the correct bamboo plan (the release plan with the custom variables)  # Click the link to type a value for the variable ""thisVersion"" and enter a value of 1.1.0  # Click the link to type a value for the variable ""nextVersion"" and enter a value of 1.2.0.M1-SNAPSHOT  # Click Release  \\  The plan will fire.  If the plan fails, then it is displayed as failing in JIRA and there is a button to ""retry""  \\  # Click Retry and the same release plugin popup will be displayed with the previously selected values already filled out except for these problems:    * *while there are two rows for each of the two plan variables we previously input, the variable name in the dropdown is the same for each*.  It lists both variable names as ""nextVersion"", even though we originally selected the drop downs as ""thisVersion"" for the first, and ""nextVersion"" for the second.  ** Note that the correct values, however, are still present in the text boxes.  Thus when you see the screen (and I forgot to get a screenshot) you see in the first row a select box with ""nextVersion"" but with a value of 1.1.0.  And the second row has (again) ""nextVersion"" in the drop down and the text box value of 1.2.0.M1-SNAPSHOT.  Thus, it persisted the text box values correctly but not the drop down selections correctly.  * If you only change the value of the first drop down -- i.e. change the first drop down from ""nextVersion"" to ""thisVersion"" (which is how we originally filled it out) and don't touch the second drop down box at all -- then it will _only_ set the ""thisVersion"" plan variable (the drop down that we physically clicked and changed).  It will _not_ submit the ""nextVersion"" variable value to the plan, despite appearing exactly as it did when we originally filled out the form.    This of course made my plan fail a second time as the maven-release plugin needs the ""nextVersion"" value, and since I don't have a default maven fails because the next version is """" (empty string).    I went back to the Jira Version -> release page and ""retried"" a third time.  Again I was presented with a form that had both drop downs displayed as ""nextVersion"".  I clicked the first row drop down and changed it to to the correct ""thisVersion"" variable.  Then I clicked the second row (currently ""nextVersion""), deliberately changed it to ""thisVersion"" and then clicked again to change it back to ""nextVersion"".  I left the text box values alone -- as they were already correct.    I submitted this and it worked perfectly.    Thus, by clicking the second row dropdown and changing it and changing it back is a valid workaround.  This seems pretty minor as there is an easy workaround and it seems to only be a problem whenever a release build fails, which is probably rare."
1,Oauth Access Token and Plugin Manager option is broken when Bamboo integrate with LDAP with the cache attribute is set to false,"Currently, if Bamboo integrated with LDAP using the atlassian-user.xml with the LDAP repository cache is set to false example shown below:  {code}   <ldap key=""adRepository"" name=""AD Repository"" cache=""false"">  {code}    With the cache is turn off, when user click on the *Oauth Access Token* and *Plugin Manage* option will throw the following exception and user will be direct to the login page:  {code}  2011-09-02 18:04:56,842 ERROR [qtp33513127-17] [LDAPUserManagerReadOnly] Error retrieving user: 'zed2' from LDAP.  com.atlassian.user.impl.RepositoryException: javax.naming.NoInitialContextException: Cannot instantiate class: com.sun.jndi.ldap.LdapCtxFactory [Root exception is java.lang.ClassNotFoundException: com.sun.jndi.ldap.LdapCtxFactory not found from bundle [com.atlassian.oauth.atlassian-oauth-service-provider-plugin]]    com.atlassian.user.impl.ldap.repository.DefaultLdapContextFactory.getLDAPContext(DefaultLdapContextFactory.java:91)    com.atlassian.user.impl.ldap.search.DefaultLDAPUserAdaptor.search(DefaultLDAPUserAdaptor.java:70)    com.atlassian.user.impl.ldap.search.DefaultLDAPUserAdaptor.search(DefaultLDAPUserAdaptor.java:54)    com.atlassian.user.impl.ldap.LDAPUserManagerReadOnly.getUser(LDAPUserManagerReadOnly.java:70)    com.atlassian.user.impl.delegation.DelegatingListUserManager.getUser(DelegatingListUserManager.java:71)    sun.reflect.GeneratedMethodAccessor177.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)    java.lang.reflect.Method.invoke(Unknown Source)    com.atlassian.crowd.integration.atlassianuser.DynamicDelegationAccessor$DelegatingInvocationHandler.invoke(DynamicDelegationAccessor.java:115)    $Proxy16.getUser(Unknown Source)    bucket.user.DefaultUserAccessor.getUser(DefaultUserAccessor.java:147)    com.atlassian.bamboo.user.BambooUserManagerImpl.getUser(BambooUserManagerImpl.java:103)    sun.reflect.GeneratedMethodAccessor178.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)    java.lang.reflect.Method.invoke(Unknown Source)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy25.getUser(Unknown Source)    com.atlassian.bamboo.user.authentication.BambooAuthenticator.getUser(BambooAuthenticator.java:48)    com.atlassian.seraph.auth.DefaultAuthenticator.refreshPrincipalObtainedFromSession(DefaultAuthenticator.java:365)    com.atlassian.seraph.auth.DefaultAuthenticator.getUserFromSession(DefaultAuthenticator.java:404)    com.atlassian.seraph.auth.DefaultAuthenticator.getUser(DefaultAuthenticator.java:318)    com.atlassian.seraph.auth.AbstractAuthenticator.getUser(AbstractAuthenticator.java:45)    com.atlassian.seraph.auth.AbstractAuthenticator.getRemoteUser(AbstractAuthenticator.java:33)    com.atlassian.sal.bamboo.user.BambooSalUserManager.getRemoteUsername(BambooSalUserManager.java:69)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)    java.lang.reflect.Method.invoke(Unknown Source)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:307)    org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.doInvoke(ServiceInvoker.java:58)    org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.invoke(ServiceInvoker.java:62)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)    org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invokeUnprivileged(ServiceTCCLInterceptor.java:56)    org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invoke(ServiceTCCLInterceptor.java:39)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.osgi.service.importer.support.LocalBundleContextAdvice.invoke(LocalBundleContextAdvice.java:59)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)    org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy239.getRemoteUsername(Unknown Source)    com.atlassian.oauth.serviceprovider.internal.servlet.user.AccessTokensServlet.doGet(AccessTokensServlet.java:49)    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)    com.atlassian.plugin.servlet.DelegatingPluginServlet.service(DelegatingPluginServlet.java:42)    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)    com.atlassian.plugin.servlet.ServletModuleContainerServlet.service(ServletModuleContainerServlet.java:52)    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:534)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1351)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.applinks.core.rest.context.ContextFilter.doFilter(ContextFilter.java:25)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)    com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.bamboo.filter.AccessLogFilter.doFilter(AccessLogFilter.java:66)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:265)    org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    com.atlassian.bamboo.filter.SeraphLoginFilter.doFilter(SeraphLoginFilter.java:74)    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:275)    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:149)    org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)    com.atlassian.bamboo.filter.BambooAcegiProxyFilter.doFilter(BambooAcegiProxyFilter.java:25)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.bamboo.filter.LicenseFilter.doFilter(LicenseFilter.java:73)    com.atlassian.core.filters.AbstractHttpFilter.doFilter(AbstractHttpFilter.java:31)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.johnson.filters.AbstractJohnsonFilter.doFilter(AbstractJohnsonFilter.java:71)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.seraph.filter.SecurityFilter.doFilter(SecurityFilter.java:211)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.seraph.filter.BaseLoginFilter.doFilter(BaseLoginFilter.java:150)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.oauth.serviceprovider.internal.servlet.OAuthFilter.doFilter(OAuthFilter.java:71)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)    com.atlassian.security.auth.trustedapps.filter.TrustedApplicationsFilter.doFilter(TrustedApplicationsFilter.java:98)    com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    org.springframework.orm.hibernate.support.OpenSessionInViewFilter.doFilterInternal(OpenSessionInViewFilter.java:170)    com.atlassian.bamboo.persistence.BambooSessionInViewFilter.doFilterInternal(BambooSessionInViewFilter.java:31)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:77)    com.atlassian.plugin.servlet.filter.ServletFilterModuleContainerFilter.doFilter(ServletFilterModuleContainerFilter.java:63)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:96)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:75)    org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1322)    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:473)    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)    org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:516)    org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:921)    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:403)    org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:184)    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:856)    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:114)    org.eclipse.jetty.server.Server.handle(Server.java:352)    org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)    org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1052)    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:590)    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:212)    org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:426)    org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:510)    org.eclipse.jetty.io.nio.SelectChannelEndPoint.access$000(SelectChannelEndPoint.java:34)    org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:40)    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:450)    java.lang.Thread.run(Unknown Source)  Caused by: javax.naming.NoInitialContextException: Cannot instantiate class: com.sun.jndi.ldap.LdapCtxFactory [Root exception is java.lang.ClassNotFoundException: com.sun.jndi.ldap.LdapCtxFactory not found from bundle [com.atlassian.oauth.atlassian-oauth-service-provider-plugin]]    javax.naming.spi.NamingManager.getInitialContext(Unknown Source)    javax.naming.InitialContext.getDefaultInitCtx(Unknown Source)    javax.naming.InitialContext.init(Unknown Source)    javax.naming.InitialContext.<init>(Unknown Source)    javax.naming.directory.InitialDirContext.<init>(Unknown Source)    com.atlassian.user.impl.ldap.repository.DefaultLdapContextFactory.getLDAPContext(DefaultLdapContextFactory.java:87)  	... 137 more  Caused by: java.lang.ClassNotFoundException: com.sun.jndi.ldap.LdapCtxFactory not found from bundle [com.atlassian.oauth.atlassian-oauth-service-provider-plugin]    org.springframework.osgi.util.BundleDelegatingClassLoader.findClass(BundleDelegatingClassLoader.java:103)    org.springframework.osgi.util.BundleDelegatingClassLoader.loadClass(BundleDelegatingClassLoader.java:156)    java.lang.ClassLoader.loadClass(Unknown Source)    java.lang.Class.forName0(Native Method)    java.lang.Class.forName(Unknown Source)    com.sun.naming.internal.VersionHelper12.loadClass(Unknown Source)  	... 143 more  Caused by: java.lang.ClassNotFoundException: com.sun.jndi.ldap.LdapCtxFactory    org.apache.felix.framework.ModuleImpl.findClassOrResourceByDelegation(ModuleImpl.java:772)    org.apache.felix.framework.ModuleImpl.access$200(ModuleImpl.java:73)    org.apache.felix.framework.ModuleImpl$ModuleClassLoader.loadClass(ModuleImpl.java:1690)    java.lang.ClassLoader.loadClass(Unknown Source)    org.apache.felix.framework.ModuleImpl.getClassByDelegation(ModuleImpl.java:634)    org.apache.felix.framework.Felix.loadBundleClass(Felix.java:1594)    org.apache.felix.framework.BundleImpl.loadClass(BundleImpl.java:887)    org.springframework.osgi.util.BundleDelegatingClassLoader.findClass(BundleDelegatingClassLoader.java:99)  	... 148 more  {code}    The current workaround is to set the LDAP repository cache=""true"" as shown below:  {code}  <ldap key=""adRepository"" name=""AD Repository"" cache=""true"">  {code}    The problem only happens with LDAP user only"
0,Possible race condition for build label addition,"A couple of days ago we started getting error messages back from many of our builds along the lines of the following:  {noformat}  System Error Details  xxx - 6. yyy - Default Job 10 : Build Labeller custom build post complete action failed to run   (java.lang.RuntimeException : Cannot add label '15_0_05_0917' to build results 'xxxxx-yyy-JOB1-10')    Occurred: 02 Sep 2011, 6:02:53 AM  java.lang.RuntimeException: Cannot add label '15_0_05_0917' to build results 'xxx-yyy-JOB1-10'    com.atlassian.bamboo.labels.LabelManagerImpl.addLabel(LabelManagerImpl.java:92)    sun.reflect.GeneratedMethodAccessor1147.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy42.addLabel(Unknown Source)    sun.reflect.GeneratedMethodAccessor1147.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    com.atlassian.bamboo.security.acegi.intercept.aopalliance.AuthorityOverrideMethodSecurityInterceptor.invoke(AuthorityOverrideMethodSecurityInterceptor.java:30)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy42.addLabel(Unknown Source)    com.atlassian.bamboo.plugins.labeller.BuildLabeller.labelBuildResult(BuildLabeller.java:304)    com.atlassian.bamboo.plugins.labeller.BuildLabeller.labelBuildResult(BuildLabeller.java:291)    com.atlassian.bamboo.plugins.labeller.BuildLabeller.call(BuildLabeller.java:85)    sun.reflect.GeneratedMethodAccessor712.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.orm.hibernate.HibernateInterceptor.invoke(HibernateInterceptor.java:117)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy362.call(Unknown Source)    com.atlassian.bamboo.v2.build.events.PostBuildCompletedEventListener$1.run(PostBuildCompletedEventListener.java:71)    com.atlassian.bamboo.variable.CustomVariableContextRunnerImpl.execute(CustomVariableContextRunnerImpl.java:25)    com.atlassian.bamboo.v2.build.events.PostBuildCompletedEventListener.performAction(PostBuildCompletedEventListener.java:52)    com.atlassian.bamboo.v2.build.events.PostBuildCompletedEventListener.handleEvent(PostBuildCompletedEventListener.java:41)    com.atlassian.event.legacy.LegacyListenerHandler$LegacyListenerInvoker.invoke(LegacyListenerHandler.java:55)    com.atlassian.event.internal.AsynchronousAbleEventDispatcher$2.run(AsynchronousAbleEventDispatcher.java:60)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:662)  Caused by: org.springframework.dao.IncorrectResultSizeDataAccessException: query did not return a unique result: 2    org.springframework.orm.hibernate.SessionFactoryUtils.convertHibernateAccessException(SessionFactoryUtils.java:590)    org.springframework.orm.hibernate.HibernateAccessor.convertHibernateAccessException(HibernateAccessor.java:353)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:375)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:337)    com.atlassian.bamboo.labels.LabelHibernateDao.findLabelByNameAndNamespace(LabelHibernateDao.java:35)    sun.reflect.GeneratedMethodAccessor1269.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy13.findLabelByNameAndNamespace(Unknown Source)    com.atlassian.bamboo.labels.LabelManagerImpl.addLabel(LabelManagerImpl.java:392)    com.atlassian.bamboo.labels.LabelManagerImpl$1.call(LabelManagerImpl.java:80)    com.atlassian.bamboo.labels.LabelManagerImpl$1.call(LabelManagerImpl.java:75)    com.atlassian.bamboo.labels.LabelManagerImpl.withWriteLock(LabelManagerImpl.java:207)    com.atlassian.bamboo.labels.LabelManagerImpl.addLabel(LabelManagerImpl.java:74)  	... 43 more  {noformat}    This happened for two separate and widely-used labels.  It was pretty obvious from the hibernate exception that Bamboo had created duplicate labels in the DB.  We ended up renaming the duplicate label and telling any builds that referenced it to use the original label.  The relevant database contents prior to the fix were as follows:  {noformat}  mysql> select * from LABEL where NAME=""14_0_36_21"";  +----------+------------+-----------+---------------------+---------------------+  | LABEL_ID | NAME       | NAMESPACE | CREATED_DATE        | UPDATED_DATE        |  +----------+------------+-----------+---------------------+---------------------+  | 55410738 | 14_0_36_21 | label     | 2011-09-02 05:08:56 | 2011-09-02 05:08:56 |  | 55410739 | 14_0_36_21 | label     | 2011-09-02 05:08:56 | 2011-09-02 05:08:56 |  +----------+------------+-----------+---------------------+---------------------+  2 rows in set (0.00 sec)    mysql> select * from BUILDRESULTSUMMARY_LABEL where LABEL_ID=""55410738"";  +----------+----------+-----------------------+----------+------------+-----------+---------------------+---------------------+  | ID       | LABEL_ID | BUILDRESULTSUMMARY_ID | BUILD_ID | PROJECT_ID | USER_NAME | CREATED_DATE        | UPDATED_DATE        |  +----------+----------+-----------------------+----------+------------+-----------+---------------------+---------------------+  | 55345686 | 55410738 |              55054288 | 44367885 |   43155458 | NULL      | 2011-09-02 05:08:56 | 2011-09-02 05:08:56 |  +----------+----------+-----------------------+----------+------------+-----------+---------------------+---------------------+  1 row in set (0.00 sec)    mysql> select * from BUILDRESULTSUMMARY_LABEL where LABEL_ID=""55410739"";  +----------+----------+-----------------------+----------+------------+-----------+---------------------+---------------------+  | ID       | LABEL_ID | BUILDRESULTSUMMARY_ID | BUILD_ID | PROJECT_ID | USER_NAME | CREATED_DATE        | UPDATED_DATE        |  +----------+----------+-----------------------+----------+------------+-----------+---------------------+---------------------+  | 55345687 | 55410739 |              55054290 | 51183804 |   43155458 | NULL      | 2011-09-02 05:08:56 | 2011-09-02 05:08:56 |  +----------+----------+-----------------------+----------+------------+-----------+---------------------+---------------------+  1 row in set (0.00 sec)      mysql> select * from BUILDRESULTSUMMARY where BUILDRESULTSUMMARY_ID=""55054288"";  +-----------------------+---------------------+---------------------+----------------------+--------------+-------------+-----------------------+-------------------+---------------------+----------------------+----------+-------------+--------------------------------------------------------------------------+--------------+-------------+----------------+---------------------+---------------------+--------------------+--------------------+------------------+-------------------+---------------------+------------------+---------------------+----------------+------------+------------------+----------------------+-----------------------+--------------+  | BUILDRESULTSUMMARY_ID | CREATED_DATE        | UPDATED_DATE        | BUILD_KEY            | BUILD_NUMBER | BUILD_STATE | SUCCESSFUL_TEST_COUNT | FAILED_TEST_COUNT | BUILD_DATE          | BUILD_COMPLETED_DATE | DURATION | TIME_TO_FIX | TRIGGER_REASON                                                           | REVISION_KEY | DELTA_STATE | BUILD_AGENT_ID | VCS_UPDATE_TIME     | QUEUE_TIME          | FAILURE_TEST_COUNT | SUCCESS_TEST_COUNT | TOTAL_TEST_COUNT | BROKEN_TEST_COUNT | EXISTING_TEST_COUNT | FIXED_TEST_COUNT | TOTAL_TEST_DURATION | STAGERESULT_ID | BUILD_TYPE | LIFE_CYCLE_STATE | BUILD_CANCELLED_DATE | SKIPPED_COMMITS_COUNT | CHAIN_RESULT |  +-----------------------+---------------------+---------------------+----------------------+--------------+-------------+-----------------------+-------------------+---------------------+----------------------+----------+-------------+--------------------------------------------------------------------------+--------------+-------------+----------------+---------------------+---------------------+--------------------+--------------------+------------------+-------------------+---------------------+------------------+---------------------+----------------+------------+------------------+----------------------+-----------------------+--------------+  |              55054288 | 2011-09-02 05:01:09 | 2011-09-02 05:08:55 | yyy-JOB1 |         3149 | Successful  |                  NULL |              NULL | 2011-09-02 05:02:22 | 2011-09-02 05:03:30  |    68367 |        NULL | com.atlassian.bamboo.plugin.system.triggerReason:DependencyTriggerReason | 17035        | NONE        |       41353237 | 2011-09-02 05:01:09 | 2011-09-02 05:01:09 |                  0 |                  0 |                0 |                 0 |                   0 |                0 |                   0 |       55085118 | BUILD      | Finished         | NULL                 |                     0 |     55054287 |  +-----------------------+---------------------+---------------------+----------------------+--------------+-------------+-----------------------+-------------------+---------------------+----------------------+----------+-------------+--------------------------------------------------------------------------+--------------+-------------+----------------+---------------------+---------------------+--------------------+--------------------+------------------+-------------------+---------------------+------------------+---------------------+----------------+------------+------------------+----------------------+-----------------------+--------------+  1 row in set (0.00 sec)    mysql> select * from BUILDRESULTSUMMARY where BUILDRESULTSUMMARY_ID=""55054290"";  +-----------------------+---------------------+---------------------+--------------------------+--------------+-------------+-----------------------+-------------------+---------------------+----------------------+----------+-------------+--------------------------------------------------------------------------+--------------+-------------+----------------+---------------------+---------------------+--------------------+--------------------+------------------+-------------------+---------------------+------------------+---------------------+----------------+------------+------------------+----------------------+-----------------------+--------------+  | BUILDRESULTSUMMARY_ID | CREATED_DATE        | UPDATED_DATE        | BUILD_KEY                | BUILD_NUMBER | BUILD_STATE | SUCCESSFUL_TEST_COUNT | FAILED_TEST_COUNT | BUILD_DATE          | BUILD_COMPLETED_DATE | DURATION | TIME_TO_FIX | TRIGGER_REASON                                                           | REVISION_KEY | DELTA_STATE | BUILD_AGENT_ID | VCS_UPDATE_TIME     | QUEUE_TIME          | FAILURE_TEST_COUNT | SUCCESS_TEST_COUNT | TOTAL_TEST_COUNT | BROKEN_TEST_COUNT | EXISTING_TEST_COUNT | FIXED_TEST_COUNT | TOTAL_TEST_DURATION | STAGERESULT_ID | BUILD_TYPE | LIFE_CYCLE_STATE | BUILD_CANCELLED_DATE | SKIPPED_COMMITS_COUNT | CHAIN_RESULT |  +-----------------------+---------------------+---------------------+--------------------------+--------------+-------------+-----------------------+-------------------+---------------------+----------------------+----------+-------------+--------------------------------------------------------------------------+--------------+-------------+----------------+---------------------+---------------------+--------------------+--------------------+------------------+-------------------+---------------------+------------------+---------------------+----------------+------------+------------------+----------------------+-----------------------+--------------+  |              55054290 | 2011-09-02 05:02:17 | 2011-09-02 05:08:55 | xxx-JOB1 |          667 | Successful  |                  NULL |              NULL | 2011-09-02 05:03:40 | 2011-09-02 05:05:58  |   137913 |        NULL | com.atlassian.bamboo.plugin.system.triggerReason:DependencyTriggerReason | 17035        | NONE        |       33128455 | 2011-09-02 05:02:18 | 2011-09-02 05:02:17 |                  0 |                  0 |                0 |                 0 |                   0 |                0 |                   0 |       55085119 | BUILD      | Finished         | NULL                 |                     0 |     55054289 |  +-----------------------+---------------------+---------------------+--------------------------+--------------+-------------+-----------------------+-------------------+---------------------+----------------------+----------+-------------+--------------------------------------------------------------------------+--------------+-------------+----------------+---------------------+---------------------+--------------------+--------------------+------------------+-------------------+---------------------+------------------+---------------------+----------------+------------+------------------+----------------------+-----------------------+--------------+  1 row in set (0.01 sec)    mysql> select * from BUILD where BUILD_ID=""44367885"";  +----------+---------------------+---------------------+----------------------+----------+-------------+--------------+--------------------+---------------------+-------------------+-------------------------+---------------------+------------+------------------+-----------------+------------+-------------+----------+  | BUILD_ID | CREATED_DATE        | UPDATED_DATE        | FULL_KEY             | BUILDKEY | TITLE       | REVISION_KEY | FIRST_BUILD_NUMBER | LATEST_BUILD_NUMBER | NEXT_BUILD_NUMBER | SUSPENDED_FROM_BUILDING | MARKED_FOR_DELETION | PROJECT_ID | NOTIFICATION_SET | REQUIREMENT_SET | BUILD_TYPE | DESCRIPTION | STAGE_ID |  +----------+---------------------+---------------------+----------------------+----------+-------------+--------------+--------------------+---------------------+-------------------+-------------------------+---------------------+------------+------------------+-----------------+------------+-------------+----------+  | 44367885 | 2011-07-03 06:10:37 | 2011-09-02 13:01:18 | yyy-JOB1 | JOB1     | Default Job | 17073        |                  1 |                3157 |              3158 |                         |                     |   43155458 |         44466189 |        44400653 | JOB        |             | 44335117 |  +----------+---------------------+---------------------+----------------------+----------+-------------+--------------+--------------------+---------------------+-------------------+-------------------------+---------------------+------------+------------------+-----------------+------------+-------------+----------+  1 row in set (0.00 sec)    mysql> select * from BUILD where BUILD_ID=""51183804"";  +----------+---------------------+---------------------+--------------------------+----------+-------------+--------------+--------------------+---------------------+-------------------+-------------------------+---------------------+------------+------------------+-----------------+------------+-------------+----------+  | BUILD_ID | CREATED_DATE        | UPDATED_DATE        | FULL_KEY                 | BUILDKEY | TITLE       | REVISION_KEY | FIRST_BUILD_NUMBER | LATEST_BUILD_NUMBER | NEXT_BUILD_NUMBER | SUSPENDED_FROM_BUILDING | MARKED_FOR_DELETION | PROJECT_ID | NOTIFICATION_SET | REQUIREMENT_SET | BUILD_TYPE | DESCRIPTION | STAGE_ID |  +----------+---------------------+---------------------+--------------------------+----------+-------------+--------------+--------------------+---------------------+-------------------+-------------------------+---------------------+------------+------------------+-----------------+------------+-------------+----------+  | 51183804 | 2011-08-05 02:01:37 | 2011-09-02 13:01:19 | xxx-JOB1 | JOB1     | Default Job | 17073        |                330 |                 675 |               676 |                         |                     |   43155458 |         51249340 |        51216523 | JOB        |             | 51150929 |  +----------+---------------------+---------------------+--------------------------+----------+-------------+--------------+--------------------+---------------------+-------------------+-------------------------+---------------------+------------+------------------+-----------------+------------+-------------+----------+  1 row in set (0.00 sec)  {noformat}    We hit a very similar issue with a plan creation race condition (BAM-9231) recently.   This appears to be caused by a separate race condition affecting the creation of labels."
0,Test (and fix is required) agent and server wrapper with openjdk7,NULL
0,Update checkout task definition on repository update and clone,NULL
0,Add server pause button to UPM decorator,"We need to add the server pause button to the UPM decorator and a warning that if you do not pause the server and upgrade a plugin, builds may fail."
0,My Bamboo looks weird when more than one commit is involved in a build :-),It links build only to the latest user commit. There's a bug in getResultSummariesByChangeSetId method query that returns result summaries only if the changeset is the latest changeset.
0,Bamboo should use default application link to retrieve issues +from jira 4.2.x+ when no project entity link is set up for a plan.,"Steps to reproduce:  1) grab {color:red}jira 4.2{color} (we can't reproduce problem on jira 4.3+)  2) grab bamboo 3.2.2  3) set up an application link from bamboo to jira, use outgoing ""basic access""  4) set up a test project with a test issue on jira instance  5) set up a test project with a test plan on bamboo instance  5.1) force that plan to detect changes on a repository that will contain commit with jira-issue comment  5.2) for example:  {noformat}  mkdir test ; cd test  hg init ; echo ""aaa"" >> aaa.txt ; hg add aaa.txt ; hg commit --user ""user"" -m ""initial""  echo ""aaa"" >> aaa.txt ; hg commit --user ""user"" -m ""BAM-9867 message message""  # repeat above line when you need to trigger bamboo build  {noformat}  6) go to build result page of the plan, build should have contain commits with ""BAM-9867"" comments. (replace ""BAM-9867"" with your jira instance test issue)  7.1) when you click on ""BAM-9867"" you will get ""unexpected error has occured""  7.2) while you should be redirected to the jira page  8.1) on the JIRA issues panel you should have see your issue, with it's icon, and with link to your jira instance  8.2) instead you will see something like ""Can't retrieve issues"" or similar error, I don't remember.  9) obviosly ""Issues"" tab is borked as well.    "
0,ActiveMQ JMS data file no longer found causing builds not able to launch,"{noformat:title=BEAC server log JMS summary snipppet}  2011-10-09 18:34:29,222 ERROR [BrokerService[bamboo] Task] [Queue] Failed to page in more queue messages  java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: Could not locate data file /opt/j2ee/domains/bamboo.atlassian.com/confluence/webapps/atlassian-bamboo/data/current/jms-store/bamboo/KahaDB/db-1782.log          at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.reset(AbstractStoreCursor.java:113)          at org.apache.activemq.broker.region.cursors.StoreQueueCursor.reset(StoreQueueCursor.java:157)          at org.apache.activemq.broker.region.Queue.doPageIn(Queue.java:1677)          at org.apache.activemq.broker.region.Queue.pageInMessages(Queue.java:1897)          at org.apache.activemq.broker.region.Queue.iterate(Queue.java:1424)          at org.apache.activemq.thread.PooledTaskRunner.runTask(PooledTaskRunner.java:122)          at org.apache.activemq.thread.PooledTaskRunner$1.run(PooledTaskRunner.java:43)          at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)          at java.lang.Thread.run(Thread.java:662)  Caused by: java.lang.RuntimeException: java.io.IOException: Could not locate data file /opt/j2ee/domains/bamboo.atlassian.com/confluence/webapps/atlassian-bamboo/data/current/jms-store/bamboo/KahaDB/db-1782.log          at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:265)          at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.reset(AbstractStoreCursor.java:110)          ... 9 more  Caused by: java.io.IOException: Could not locate data file /opt/j2ee/domains/bamboo.atlassian.com/confluence/webapps/atlassian-bamboo/data/current/jms-store/bamboo/KahaDB/db-1782.log          at org.apache.kahadb.journal.Journal.getDataFile(Journal.java:345)          at org.apache.kahadb.journal.Journal.read(Journal.java:592)          at org.apache.activemq.store.kahadb.MessageDatabase.load(MessageDatabase.java:786)          at org.apache.activemq.store.kahadb.KahaDBStore.loadMessage(KahaDBStore.java:956)          at org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore$5.execute(KahaDBStore.java:494)          at org.apache.kahadb.page.Transaction.execute(Transaction.java:728)          at org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore.recoverNextMessages(KahaDBStore.java:485)          at org.apache.activemq.store.ProxyMessageStore.recoverNextMessages(ProxyMessageStore.java:88)          at org.apache.activemq.broker.region.cursors.QueueStorePrefetch.doFillBatch(QueueStorePrefetch.java:97)          at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:262)          ... 10 more  2011-10-09 18:34:29,351 ERROR [BrokerService[bamboo] Task] [Journal] Looking for key 1782 but not found in fileMap: {1783=db-1783.log number = 1783 , length = 33554836, 1786=db-1786.log number = 1786 , length = 34109378, 1787=db-1787.log number = 1787 , length = 33556132, 1280=db-1280.log number = 1280 , length = 33556132, 1784=db-1784.log number = 1784 , length = 33554712, 1790=db-1790.log number = 1790 , length = 33606358, 1791=db-1791.log number = 1791 , length = 35664711, 1788=db-1788.log number = 1788 , length = 33581527, 1789=db-1789.log number = 1789 , length = 33557879, 1806=db-1806.log number = 1806 , length = 33592825, 1807=db-1807.log number = 1807 , length = 16003513, 1804=db-1804.log number = 1804 , length = 33635071, 1805=db-1805.log number = 1805 , length = 33554607, 1802=db-1802.log number = 1802 , length = 33580009, 1803=db-1803.log number = 1803 , length = 33554674, 1800=db-1800.log number = 1800 , length = 33567294, 1801=db-1801.log number = 1801 , length = 33819685, 1798=db-1798.log number = 1798 , length = 33971508, 1279=db-1279.log number = 1279 , length = 33554625, 1799=db-1799.log number = 1799 , length = 33630766, 1796=db-1796.log number = 1796 , length = 33651278, 1797=db-1797.log number = 1797 , length = 45623364, 1794=db-1794.log number = 1794 , length = 33666339, 1795=db-1795.log number = 1795 , length = 34512258, 1792=db-1792.log number = 1792 , length = 35143715, 1793=db-1793.log number = 1793 , length = 33776959}      {noformat}    {noformat:title=remote agents unable to send heartbeats}  2011-10-10 09:55:43,655 INFO [QuartzScheduler_Worker-9] [AgentHeartBeatJob] Not sending a new heartbeat since an old one is still being sent, last successful transmission time was 13 seconds ago, dropping the current heartbeat...  2011-10-10 10:05:43,655 INFO [QuartzScheduler_Worker-6] [AgentHeartBeatJob] Not sending a new heartbeat since an old one is still being sent, last successful transmission time was 7 seconds ago, dropping the current heartbeat..  {noformat}    Likely caused by these ActiveMQ Issues:    https://issues.apache.org/jira/browse/AMQ-3310  or:  https://issues.apache.org/jira/browse/AMQ-3470    Reported internally in:  https://extranet.atlassian.com/jira/browse/BUILDENG-649  https://extranet.atlassian.com/jira/browse/BUILDENG-651      The JMS spam caused the server logs to roll  over so I was unable to capture the beginning of the problem.    Confirmed server side that {{db-1782.log}} in the JMS repo was not present"
1,Scheduled backup fails randomly while expiration is running at the same time.,As reported in https://support.atlassian.com/browse/BSP-5570    We should provide a check:  - backup should wait for running expiration to finish  - expiration should wait for running backup to finish
0,Bamboo AMPS setup should hit the updateDefaultsLocalCapability link after import,"Its an absolute pain when developing amps plugins because everytime bamboo starts up it doesn't have and jdk's or executables set up.  If it could click the updateDefaultsLocalCapability action after import, all this stuff would get pulled in from your local environment."
0,"POMs from 3.3.2 Source code released to customers is still using ""3.3-SNAPSHOT""","POMs from 3.3.2 Source code released to customers is still using ""3.3-SNAPSHOT""."
0,Streams 5 aggregation for Studio,Chai knows more about this than I do. Ill get him to comment here.
0,Icons in Currently Building list are cut off,NULL
0,Test summary should show how many tests have been Quarentined (if any),NULL
0,Different table for Quarantined test results on Result Summary,NULL
0,Quarantine table on Tests tab should have column to show which user quarantined the test,NULL
0,Button for enabling and disabling a test in Quarantine on the Result summary,NULL
0,Make test Quarantine work for all Tasks that have support for Tests,NULL
1,Styling for new combined Plan Status and History Navigator,NULL
0,Improved email templates,Merge in email work from ShipIt. Test across common email clients (using [http://litmus.com] would make this a lot quicker). Polish the designs.    Screenshots of the new (already done) emails:    !https://img.skitch.com/20111028-ca1jwycmp9ph2aytuwi2jwiigm.jpg!  !https://img.skitch.com/20111028-8fg75bbkucqw618pt7qu9qtndx.jpg!
1,Improvements to EBS mounting,Includes support for Windows.
0,Indexing problem in table USER_COMMIT because of column COMMIT_REVISION text type,"This error happens because the column COMMIT_REVISION in the table USER_COMMIT is set to TEXT,     {code}  2011-10-24 12:06:05,071 ERROR [main] [SchemaUpdate] Unsuccessful: create index commit_rev_idx on USER_COMMIT (COMMIT_REVISION)  2011-10-24 12:06:05,071 ERROR [main] [SchemaUpdate] BLOB/TEXT column 'COMMIT_REVISION' used in key specification without a key length  {code}    There is a limitation on indexing [TEXT type fields|http://stackoverflow.com/questions/1827063/mysql-error-key-specification-without-a-key-length]. As [this post|http://forums.mysql.com/read.php?98,37000,37052] states, we can have VARCHAR type with maximum 65,532 bytes after MySQL 5.0.3.      We should change the column type from TEXT to VARCHAR."
1,Bamboo 3.3 does not respect @RequiresRestart ,"I tried bumping UPM to support Bamboo 3.3 from 3.1, and we got some failing tests with regards to the tests of the @RequiresRestart. Everything worked in Bamboo 3.1, but there's something wrong in Bamboo 3.3.    Note that this only happens when I'm installing a new plugin with a module that requires restart. But it works as expected when ""upgrading"" a plugin (i.e., @RequiresRestart is honored)"
0,Newer Bamboo versions are not shipping the svnkit for the jsvn script.,"Bamboo 3.3.X versions are not being shipped with all the svnkit necessary for the jsvn app.    Currently, only svnkit-cli-1.2.1.5297.jar is available at /scripts/lib. However, svnkit-1.2.1.5297.jar is also necessary.    Since the svnkit lib is missing, the following exception is reported when running jsvn:  {code}  Exception in thread ""main"" java.lang.NoClassDefFoundError: org/tmatesoft/svn/core/SVNException    org.tmatesoft.svn.cli.SVN.main(SVN.java:22)  Caused by: java.lang.ClassNotFoundException: org.tmatesoft.svn.core.SVNException    java.net.URLClassLoader$1.run(URLClassLoader.java:202)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:190)    java.lang.ClassLoader.loadClass(ClassLoader.java:306)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)    java.lang.ClassLoader.loadClass(ClassLoader.java:247)  	... 1 more  {code}"
0,An administrator should be able to enable or disable Agent Authentication,NULL
1,An administrator must be able to approve an agent when it connects for the first time,NULL
0,An administrator should be able to provide a wild card mask to validate an agents IP address,NULL
0,An administrator should be given the appropriate directions in the Agent log to approve the agent if it is not approved,NULL
0,An administrator should be provided a way to approve all connected agents,* Should have a confirm screen so the admin can review the agents about to be approved
0,Repository Isolation not available via the UI in Bamboo 3.3,Followed documentation to configure repository isolation:  http://confluence.atlassian.com/display/BAMBOO/Configuring+Repository+Isolation+for+Maven+Executables    but it doesn't seem to be present for Maven 2/3 builder
0,Can't access artifacts due to permission error,NULL
0,An administrator should be able to auto accept connected agents that are connected when the agent auth is enabled,NULL
0,An administrator should be able to revoke approved keys,NULL
0,Merge and add Functional test for Agent Authentication,NULL
0,HelpPathResolver shouldn't throw an NPE when called with a null argument,"During BAM-10121, I noticed:    HelpPathResolverTest#helpPathResolverAcceptsNullKey fails with an NPE.    To reproduce:  * In CtkIntegrationTest, remove 'excludes'  * Execute it.    When this issue is resolved, you need to remove the exclusion from the Ctk test."
0,Validate and properly used repository name on Plan create wizard,NULL
0,Failed build email should include build errors iff only one Job failed and no tests failed,"We should show the ""Build Errors"" iff only one Job failed, like the UI    At the moment one has to click through to determine why the build failed."
0,CSS images do not load after base url changes,NULL
0,User can override Plan Branch Build Strategy,NULL
0,Update breadcrumbs ,NULL
1,Tabbed content re-design,NULL
0,New Plan Navigator,Stages and Job list only  
0,New Navigation Sidebar Layout,New layout for the sidebar  
1,New Branch List Component,Needs to work on:    * Plan Summary  * Plan Configuration (sidebar)  * Branch Selector Drop-down 
0,Non-Tabbed content re-design,NULL
0,New Plan Statistics component,NULL
0,Update Status Ribbon for new UI,NULL
1,Global Admin Re-Design,Chrome and navigation bar changes  
0,Event Listeners,NULL
0,Build Process plugin points,NULL
0,SAL Plugin,"Should branches be considered a ""Project"""
1,Random config changes to master dont cause branch to blow up,NULL
0,Functional test TestReportingTest tests are not being run and must be updated for the new source repository code,"As above, TestReportingTest is not being run in the CI tests. It still uses all the old source repo code so must be updated before it is re-included."
0,Remote triggering for branches,Triggering the Master plan should also trigger the relevent child plans
0,Bamboo page constantly refreshes,Note: this actually affects {{3.4-rc2}} on CBAC.    https://confluence-bamboo.atlassian.com/browse/CONFREL-CONFRELSTABLESTUDIO-22    It only kept refreshing the page while the build was scheduled to run and had no log output.
0,Validation for when the default repository is changed on a Plan with Plan Branches,What happens to the branches when the repository changes to a repository that does not contain the same branches or does not support Plan Branches at all?    We need PM input for this.
0,Html email show as text after gmail rewrites email,"In any circumstance where gmail actually rewrites the email content e.g. to distribute to group members.  It will rewrite the mulipart parts in an incorrect order, resulting in the html part being first instead of last.     We have tracked this down to a superfluous part which atlassian-mail adds to the top of the emails.  When removed the emails get rewriten correctly.    "
0,NPE in bamboo sandbox plugin,java.lang.NullPointerException    com.atlassian.sandbox.bamboo.SandboxContext.isEnabled(SandboxContext.java:151)    com.atlassian.sandbox.bamboo.actions.SandboxPreChainAction.execute(SandboxPreChainAction.java:21)    com.atlassian.bamboo.chains.ChainPluginSupportImpl$1.run(ChainPluginSupportImpl.java:83)    com.atlassian.bamboo.variable.CustomVariableContextRunnerImpl.execute(CustomVariableContextRunnerImpl.java:27)    com.atlassian.bamboo.chains.ChainPluginSupportImpl.chainStarted(ChainPluginSupportImpl.java:71)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.start(ChainExecutionManagerImpl.java:133)    sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy73.start(Unknown Source)    sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:198)    $Proxy74.start(Unknown Source)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.startWithLockTaken(PlanExecutionManagerImpl.java:249)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.access$100(PlanExecutionManagerImpl.java:40)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl$1.call(PlanExecutionManagerImpl.java:236)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl$1.call(PlanExecutionManagerImpl.java:230)    com.atlassian.util.concurrent.ManagedLocks$ManagedLockImpl.withLock(ManagedLocks.java:324)    com.atlassian.bamboo.plan.PlanExecutionLockServiceImpl.lock(PlanExecutionLockServiceImpl.java:68)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.doWithProcessLock(PlanExecutionManagerImpl.java:360)    com.atlassian.bamboo.plan.PlanExecutionManagerImpl.start(PlanExecutionManagerImpl.java:229)    com.atlassian.bamboo.v2.trigger.ChangeDetectionListener.handleEvent(ChangeDetectionListener.java:47)    sun.reflect.GeneratedMethodAccessor231.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    com.atlassian.event.internal.SingleParameterMethodListenerInvoker.invoke(SingleParameterMethodListenerInvoker.java:36)    com.atlassian.event.internal.AsynchronousAbleEventDispatcher$2.run(AsynchronousAbleEventDispatcher.java:60)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:680)
0,"Bamboo executes all ""tries"", even if no new code is commited during the Quiet Period in Subversion","If no new commits are done during the Quite Period, Bamboo should not run one more try. However, even if no code is committed, Bamboo is always waiting the maximum number of tries:  {code}  2012-01-19 19:54:29,054 INFO [qtp1761506447-530] [AccessLogFilter] 127.0.0.1 GET http://nastro-azzuro:8085/updateAndBuild.action?buildKey=MSFAADMIN-MAX 88578kb  2012-01-19 19:54:29,180 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [SvnRepository] Collecting changes for 'PROJ-MAX' on path 'https://svn.atlassian.com/svn/private/atlassian/bambootest/trunk/successBuild' from version 169163 to 169163  2012-01-19 19:54:29,191 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [DefaultChangeDetectionManager] Found one change for Plan 'PROJ-MAX', waiting 30 seconds to see if there are any more... (4 retries remaining)  2012-01-19 19:54:59,218 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [SvnRepository] Collecting changes for 'PROJ-MAX' on path 'https://svn.atlassian.com/svn/private/atlassian/bambootest/trunk/successBuild' from version 169163 to 169163  2012-01-19 19:54:59,231 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [DefaultChangeDetectionManager] Found one change for Plan 'PROJ-MAX', waiting 30 seconds to see if there are any more... (3 retries remaining)  2012-01-19 19:55:29,259 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [SvnRepository] Collecting changes for 'PROJ-MAX' on path 'https://svn.atlassian.com/svn/private/atlassian/bambootest/trunk/successBuild' from version 169163 to 169163  2012-01-19 19:55:29,272 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [DefaultChangeDetectionManager] Found one change for Plan 'PROJ-MAX', waiting 30 seconds to see if there are any more... (2 retries remaining)  2012-01-19 19:55:59,299 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [SvnRepository] Collecting changes for 'PROJ-MAX' on path 'https://svn.atlassian.com/svn/private/atlassian/bambootest/trunk/successBuild' from version 169163 to 169163  2012-01-19 19:55:59,313 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [DefaultChangeDetectionManager] Found one change for Plan 'PROJ-MAX', waiting 30 seconds to see if there are any more... (one retry remaining)  2012-01-19 19:56:29,342 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [SvnRepository] Collecting changes for 'PROJ-MAX' on path 'https://svn.atlassian.com/svn/private/atlassian/bambootest/trunk/successBuild' from version 169163 to 169163  2012-01-19 19:56:29,352 INFO [AtlassianEvent::0-BAM::EVENTS:pool-1-thread-2] [ChangeDetectionListenerAction] Change detection found 5 changes  {code}"
1,Allow Bamboo to ignore commits with certain commit messages,"This is a useful feature, but it's infrastructure to allow us to ignore merge commits by Bamboo."
0,Official Support OpenJDK 1.7,NULL
1,Dependencies for Plan Branches,"Branches will trigger off dependencies of the child plan, iff the child plan has a branch name that exactly matches the branch. Configuration for this will be enabled in the Plan level's Branch configuration"
0,Tab label is wrong for Build Summary,NULL
1,Detect Branches should show dialog where user can pick branches to create,NULL
0,Branch variables page should have the key be a select list with optgroups for global vs plan,!https://extranet.atlassian.com/download/attachments/1964972659/branch_variables_add.png?version=1&modificationDate=1329107925466!
0,NPE in Branch detection when timeout occurs,"{noformat}  2012-02-14 10:05:31,489 WARN [StreamsCompletionService::thread-110] [ActivityProviderConnectionMonitorImpl$ActivityMonitorJob] Could not reach provider Your Company JIRA; it will be omitted from the stream. The connection will be retried in 5 minutes  2012-02-14 10:07:16,931 ERROR [1-BranchDetectionBackgroundThread:pool-4-thread-2] [BranchDetectionServiceImpl] Repository error while detecting branches for plan RAILS-RAILS. https://github.com/rails/rails.git: cannot open git-upload-pack  com.atlassian.bamboo.repository.RepositoryException: https://github.com/rails/rails.git: cannot open git-upload-pack    com.atlassian.bamboo.plugins.git.GitOperationHelper.getOpenBranches(GitOperationHelper.java:284)    com.atlassian.bamboo.plugins.git.GitRepository.getOpenBranches(GitRepository.java:385)    com.atlassian.bamboo.repository.RepositoryCachingFacade$1.load(RepositoryCachingFacade.java:31)    com.atlassian.bamboo.repository.RepositoryCachingFacade$1.load(RepositoryCachingFacade.java:26)    com.google.common.cache.CustomConcurrentHashMap$ComputingValueReference.compute(CustomConcurrentHashMap.java:3426)    com.google.common.cache.CustomConcurrentHashMap$Segment.compute(CustomConcurrentHashMap.java:2322)    com.google.common.cache.CustomConcurrentHashMap$Segment.getOrCompute(CustomConcurrentHashMap.java:2291)    com.google.common.cache.CustomConcurrentHashMap.getOrCompute(CustomConcurrentHashMap.java:3802)    com.google.common.cache.ComputingCache.get(ComputingCache.java:46)    com.atlassian.bamboo.repository.RepositoryCachingFacade.getOpenBranches(RepositoryCachingFacade.java:49)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.getOpenBranches(BranchDetectionServiceImpl.java:328)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.createNewBranchesForChainNoLock(BranchDetectionServiceImpl.java:252)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.access$200(BranchDetectionServiceImpl.java:50)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl$BranchDetector.run(BranchDetectionServiceImpl.java:168)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:680)  Caused by: org.shaded.eclipse.jgit.errors.TransportException: https://github.com/rails/rails.git: cannot open git-upload-pack    org.shaded.eclipse.jgit.transport.TransportHttp.connect(TransportHttp.java:466)    org.shaded.eclipse.jgit.transport.TransportHttp.openFetch(TransportHttp.java:276)    com.atlassian.bamboo.plugins.git.GitOperationHelper.getOpenBranches(GitOperationHelper.java:266)  	... 15 more  Caused by: java.net.SocketTimeoutException: Read timed out    java.net.SocketInputStream.socketRead0(Native Method)    java.net.SocketInputStream.read(SocketInputStream.java:129)    com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)    com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)    com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)    com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:830)    com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:787)    com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)    java.io.BufferedInputStream.fill(BufferedInputStream.java:218)    java.io.BufferedInputStream.read1(BufferedInputStream.java:258)    java.io.BufferedInputStream.read(BufferedInputStream.java:317)    sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:695)    sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:640)    sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1195)    java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)    sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:318)    org.shaded.eclipse.jgit.util.HttpSupport.response(HttpSupport.java:167)    org.shaded.eclipse.jgit.transport.TransportHttp.connect(TransportHttp.java:429)  	... 17 more  2012-02-14 10:07:16,931 ERROR [1-BranchDetectionBackgroundThread:pool-4-thread-3] [BranchDetectionServiceImpl] Repository error while detecting branches for plan JOYENT-NODE. https://github.com/joyent/node.git: cannot open git-upload-pack  com.atlassian.bamboo.repository.RepositoryException: https://github.com/joyent/node.git: cannot open git-upload-pack    com.atlassian.bamboo.plugins.git.GitOperationHelper.getOpenBranches(GitOperationHelper.java:284)    com.atlassian.bamboo.plugins.git.GitRepository.getOpenBranches(GitRepository.java:385)    com.atlassian.bamboo.repository.RepositoryCachingFacade$1.load(RepositoryCachingFacade.java:31)    com.atlassian.bamboo.repository.RepositoryCachingFacade$1.load(RepositoryCachingFacade.java:26)    com.google.common.cache.CustomConcurrentHashMap$ComputingValueReference.compute(CustomConcurrentHashMap.java:3426)    com.google.common.cache.CustomConcurrentHashMap$Segment.compute(CustomConcurrentHashMap.java:2322)    com.google.common.cache.CustomConcurrentHashMap$Segment.getOrCompute(CustomConcurrentHashMap.java:2291)    com.google.common.cache.CustomConcurrentHashMap.getOrCompute(CustomConcurrentHashMap.java:3802)    com.google.common.cache.ComputingCache.get(ComputingCache.java:46)    com.atlassian.bamboo.repository.RepositoryCachingFacade.getOpenBranches(RepositoryCachingFacade.java:49)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.getOpenBranches(BranchDetectionServiceImpl.java:328)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.createNewBranchesForChainNoLock(BranchDetectionServiceImpl.java:252)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.access$200(BranchDetectionServiceImpl.java:50)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl$BranchDetector.run(BranchDetectionServiceImpl.java:168)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:680)  Caused by: org.shaded.eclipse.jgit.errors.TransportException: https://github.com/joyent/node.git: cannot open git-upload-pack    org.shaded.eclipse.jgit.transport.TransportHttp.connect(TransportHttp.java:466)    org.shaded.eclipse.jgit.transport.TransportHttp.openFetch(TransportHttp.java:276)    com.atlassian.bamboo.plugins.git.GitOperationHelper.getOpenBranches(GitOperationHelper.java:266)  	... 15 more  Caused by: java.net.SocketTimeoutException: Read timed out    java.net.SocketInputStream.socketRead0(Native Method)    java.net.SocketInputStream.read(SocketInputStream.java:129)    com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)    com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)    com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)    com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:830)    com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:787)    com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)    java.io.BufferedInputStream.fill(BufferedInputStream.java:218)    java.io.BufferedInputStream.read1(BufferedInputStream.java:258)    java.io.BufferedInputStream.read(BufferedInputStream.java:317)    sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:695)    sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:640)    sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1195)    java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)    sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:318)    org.shaded.eclipse.jgit.util.HttpSupport.response(HttpSupport.java:167)    org.shaded.eclipse.jgit.transport.TransportHttp.connect(TransportHttp.java:429)  	... 17 more  2012-02-14 10:07:16,933 INFO [1-BranchDetectionBackgroundThread:pool-4-thread-3] [BranchDetectionServiceImpl] [BRANCH_DETECTION] No branches for plan JOYENT-NODE found in VCS  2012-02-14 10:07:16,932 INFO [1-BranchDetectionBackgroundThread:pool-4-thread-2] [BranchDetectionServiceImpl] [BRANCH_DETECTION] No branches for plan RAILS-RAILS found in VCS  2012-02-14 10:07:16,937 ERROR [1-BranchDetectionBackgroundThread:pool-4-thread-3] [NamedThreadFactory] Uncaught exception in thread 1-BranchDetectionBackgroundThread:pool-4-thread-3  java.lang.NullPointerException    com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)    com.google.common.collect.Lists.newArrayList(Lists.java:117)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.newBranches(BranchDetectionServiceImpl.java:339)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.createNewBranchesForChainNoLock(BranchDetectionServiceImpl.java:283)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.access$200(BranchDetectionServiceImpl.java:50)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl$BranchDetector.run(BranchDetectionServiceImpl.java:168)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:680)  2012-02-14 10:07:16,939 ERROR [1-BranchDetectionBackgroundThread:pool-4-thread-2] [NamedThreadFactory] Uncaught exception in thread 1-BranchDetectionBackgroundThread:pool-4-thread-2  java.lang.NullPointerException    com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)    com.google.common.collect.Lists.newArrayList(Lists.java:117)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.newBranches(BranchDetectionServiceImpl.java:339)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.createNewBranchesForChainNoLock(BranchDetectionServiceImpl.java:283)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl.access$200(BranchDetectionServiceImpl.java:50)    com.atlassian.bamboo.plan.branch.BranchDetectionServiceImpl$BranchDetector.run(BranchDetectionServiceImpl.java:168)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:680)  ^C2012-02-14 10:07:52,905 INFO [ActiveMQ ShutdownHook] [BrokerService] ActiveMQ Message Broker (bamboo, ID:shackle.sydney.atlassian.com-57424-1329086386684-0:1) is shutting down  2012-02-14 10:07:53,688 INFO [ActiveMQ ShutdownHook] [TransportConnector] Connector tcp://shackle.sydney.atlassian.com:54663?wireFormat.maxInactivityDuration=300000 Stopped  2012-02-14 10:07:53,696 INFO [ActiveMQ ShutdownHook] [TransportConnector] Connector vm://bamboo Stopped  2012-02-14 10:07:53,705 INFO [ActiveMQ ShutdownHook] [PListStore] PListStore:/Users/jdumay/bamboo-home/jms-store/bamboo/tmp_storage stopped  2012-02-14 10:07:53,705 INFO [ActiveMQ ShutdownHook] [KahaDBStore] Stopping async queue tasks  2012-02-14 10:07:53,705 INFO [ActiveMQ ShutdownHook] [KahaDBStore] Stopping async topic tasks  2012-02-14 10:07:53,705 INFO [ActiveMQ ShutdownHook] [KahaDBStore] Stopped KahaDB  2012-02-14 10:07:54,016 INFO [ActiveMQ ShutdownHook] [BrokerService] ActiveMQ JMS Message Broker (bamboo, ID:shackle.sydney.atlassian.com-57424-1329086386684-0:1) stopped  {noformat}"
0,Include SSH example in shipped jetty.xml,"Currently, user is unable to startup Bamboo server  with jetty.xml due to the default setting being ship with Bamboo installation:  {code}  <SystemProperty name=""bamboo.webapp"" default=""./webapp""/>  {code}     Where the above tag should be:  {code}  <SystemProperty name=""bamboo.webapp"" default=""../webapp""/>  {code}    It will be great if we could change the above tag in the installation distribution before distribute to user.    Lastly, it would be great if the default jetty.xml comes with SSL configuration as mentioned in the following documentation:  * http://confluence.atlassian.com/display/BAMBOO/Running+Bamboo+over+HTTPS  "
1,Shallow clones should be disabled when integration is enabled,"Merging won't work with shallow clones. We need to make sure that user is notified, agrees to disable it, cannot enable etc.  Just disabling the shallow clones ""should work"", but needs to be tested."
0,User reported as code updater not consistent in Bamboo e-mail notifications,Jared didn't change the code in Bonfire / for this BON BON BON build on JBAC.    https://jira.bamboo.atlassian.com/browse/BON-BON-BON2DAC-409
0,Author and Committer have to be made configurable (at least for Git),Otherwise merges and commits won't work.
0,Prettier handling of integration errors,"The attached example is a short error, so it's almost readable. When we have merge conflicts, it's much worse."
0,Validate that Task plugins without a view template do not make the Task unusable,We are no longer displaying the 'view' template for a Task. Check that its possible to create a Task without a view template (e.g. we do not validate that you need a 'view' template).
1,Make first commit search asynchronous.,NULL
1,Bamboo Git Plugin (native mode) shouldn't hang when using ssh protocol on windows.,"Steps to reproduce:  1) grab a windows machine  2) install there minimal git: [http://code.google.com/p/msysgit/]  3) grab Bamboo 3.4.3 (I think this error will still occur on newer versions too)  4) set up a Plan that uses Git as repository and uses ssh:// protocol, for example:  4.1) url = ssh://git@github.com/atlassian/bamboo-git-plugin.git  (use SSH keyfile as ""authentication type"" - set up your github account accordingly)  5) use the windows machine from point (1) as a remote agent, connect to bamboo  6) in bamboo administration - set up the remote agent to use Git capability (so it will use native-git checkout instead of jgit checkout)  7) run the plan  8a) observed behavior is that the build hangs at code checkout (possibly to localhost proxy-SSH fingerprint host key checking)  8b) while expected behavior is that the build will... build.    from: [BSP-6565|https://support.atlassian.com/browse/BSP-6565]  and: [https://answers.atlassian.com/questions/39371/ssh-port-on-git-repository-polling]"
0,Show X out of Y branches on Branch Creation dialog,NULL
0,Disable buttons when creating branch.,NULL
0,Integration Strategy UI is broken for longer field names,NULL
0,Rename Integration Strategy to Merge Strategy,NULL
0,"Merge Strategy ""default"" should be labeled default",NULL
0,Git revision not shown for plan,Bamboo fails to show the Git revision:    !bamboo rev.png!    This is on 4.0-m2 build 2903
0,When a plan uses 2 repositories and you clone the plan and change one repository the other will often error out,See  https://jira.bamboo.atlassian.com/browse/J50SHIPIT-ODAT-8  or  https://jira.bamboo.atlassian.com/browse/J50SHIPIT-DTU-6  as an example of the error.
0,"""Run"" menu in configuration confusing",You can't tell if its going to run your branch or your plan.  I have clicked on it incorrectly a couple of times now
0,mvn:deploy a final version of all EC2 images on prod account before release,NULL
1,HipChat build notifications,NULL
0,Created issue should be linked in Bamboo,"Issue created should show up in the ""Related Issues"" of a result."
1,Import/Export for ActiveObjects,* time box this to 2 days  * This is apparently straight forward to hookup according to Sam and Adrian  * Backs up rows to xml  
1,Improve AMI customisation process and documentation ,"We need to run through the entire customisation process for both Linux and Windows and update the documentation to reflect the process.    If we can write any scripts (and host them on bitbucket) that can improve EC2 customisation, we should do this.    Currently its difficult to find out what version of bamboo-elastic-image you need - is it safe to always use the latest? How can the customer find this out?    Please try the process out using the Ubuntu LTS to see if we can shake out any problems we do not see on CENTOS/Fedora"
0,Make all committers automatically responsible when build fails,Make all committers to a failing build responsible for the failure
0,User can claim all responsibility for a build failure,NULL
0,JIRA login screen displayed in dialog looks weird,"I've created a reciprocal application link, clicked configure and got this."
1,Rewrite JIRA issue details retrieval to use REST api not RSS.,"[roots|https://atlaseye.atlassian.com/cru/CR-BAM-4183#CFR-219552]    Currently we use RSS to retrieve details of an issue (their status, icon, etc) - the details visibile on Issue summary tab/panel. It just seems wrong, so we should rewrite it to use REST properly."
0,Bamboo artifacts don't get collected if they are located into directories,"{code}  I created a file called ""as.zip"" located in directory ""a"" which is located in directory ""test"".  I used ""test/a"" as Location, and I used ""*.zip"" as Copy Pattern - the same way as it is described in [this document|http://confluence.atlassian.com/display/BAMBOO40/Configuring+a+job%27s+build+artifacts].  {code}    However, the artifact didn't get published."
1,Bamboo sometimes kills processes not belonging to it on Windows,NULL
1,Database deadlock in MySQL,"From http://tardigrade.sydney.atlassian.com:8085/bamboo/browse/BDT-MYSQL-578/artifact/JOB1/Bamboo-Log/output.log    But it could be related to what BEAC and JBACV sees occasionaly as well    {code}  2012-05-06 15:39:19,181 INFO [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [ChainExecutionManagerImpl] Plan RAN119295352-MAIN1192953520-3: Random Project119295352 - Plan119295352 - branch_no_1 has finished executing  2012-05-06 15:39:19,221 INFO [11-BAM::Test Agent 1::Agent:pool-18-thread-1] [CvsRepositoryManager] Getting list of commits for Tue Jan 31 15:39:07 UTC 2012  2012-05-06 15:39:19,296 INFO [http-9087-Processor18] [CvsRepositoryManager] Getting list of commits for Sun Nov 27 15:39:02 UTC 2011  2012-05-06 15:39:19,468 WARN [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [JDBCExceptionReporter] SQL Error: 1213, SQLState: 40001  2012-05-06 15:39:19,468 ERROR [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [JDBCExceptionReporter] Deadlock found when trying to get lock; try restarting transaction  2012-05-06 15:39:19,483 WARN [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [JDBCExceptionReporter] SQL Error: 1213, SQLState: 40001  2012-05-06 15:39:19,483 ERROR [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [JDBCExceptionReporter] Deadlock found when trying to get lock; try restarting transaction  2012-05-06 15:39:19,484 ERROR [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [SessionImpl] Could not synchronize database state with session  2012-05-06 15:39:19,588 INFO [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [BrokenBuildPostProcessor] RAN119295352-MAIN1192953520-3 was rerun and still failed. Tracking entry not updated.  Rerun builds not currently supported  2012-05-06 15:39:19,605 INFO [http-9087-Processor14] [AccessLogFilter] admin GET http://localhost:9087/bamboo/start.action 239259kb  2012-05-06 15:39:19,619 INFO [http-9087-Processor14] [AccessLogFilter] admin GET http://localhost:9087/bamboo/legacyDashboard.action 238856kb  2012-05-06 15:39:19,752 INFO [11-BAM::Test Agent 1::Agent:pool-18-thread-1] [CvsRepositoryManager] Getting list of commits for Thu Jan 26 15:39:07 UTC 2012  2012-05-06 15:39:20,038 INFO [http-9087-Processor18] [CvsRepositoryManager] Getting list of commits for Tue Nov 22 15:39:02 UTC 2011  2012-05-06 15:39:20,113 INFO [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [DashboardCacheListener] event: com.atlassian.bamboo.event.ChainCompletedEvent[source=com.atlassian.bamboo.chains.ChainExecutionManagerImpl@1d5ba63] plan=RAN119295352-MAIN1192953520  2012-05-06 15:39:20,198 INFO [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [DashboardCachingManagerImpl] Plan cache updated for 1 plans in 0:00:00.018  2012-05-06 15:39:20,204 INFO [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-14] [DependencyChainListener] Dependant builds not checked since plan is not a master plan  2012-05-06 15:39:20,206 ERROR [AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13] [NamedThreadFactory] Uncaught exception in thread AtlassianEvent::0-BAM::EVENTS:pool-2-thread-13  org.springframework.dao.ConcurrencyFailureException: Hibernate operation: Could not execute JDBC batch update; SQL []; Deadlock found when trying to get lock; try restarting transaction; nested exception is java.sql.BatchUpdateException: Deadlock found when trying to get lock; try restarting transaction  Caused by: java.sql.BatchUpdateException: Deadlock found when trying to get lock; try restarting transaction    com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1998)    com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1443)    com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeBatch(NewProxyPreparedStatement.java:1723)    net.sf.hibernate.impl.BatchingBatcher.doExecuteBatch(BatchingBatcher.java:54)    net.sf.hibernate.impl.BatcherImpl.executeBatch(BatcherImpl.java:128)    net.sf.hibernate.impl.SessionImpl.executeAll(SessionImpl.java:2436)    net.sf.hibernate.impl.SessionImpl.execute(SessionImpl.java:2391)    net.sf.hibernate.impl.SessionImpl.autoFlushIfRequired(SessionImpl.java:1819)    net.sf.hibernate.impl.SessionImpl.find(SessionImpl.java:3650)    net.sf.hibernate.impl.CriteriaImpl.list(CriteriaImpl.java:238)    net.sf.hibernate.impl.CriteriaImpl.uniqueResult(CriteriaImpl.java:385)    com.atlassian.bamboo.resultsummary.BuildResultsSummaryHibernateDao$29.doInHibernate(BuildResultsSummaryHibernateDao.java:1082)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:370)    org.springframework.orm.hibernate.HibernateTemplate.execute(HibernateTemplate.java:337)    com.atlassian.bamboo.resultsummary.BuildResultsSummaryHibernateDao.findFirstResultAfter(BuildResultsSummaryHibernateDao.java:1078)    sun.reflect.GeneratedMethodAccessor983.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy41.findFirstResultAfter(Unknown Source)    com.atlassian.bamboo.resultsummary.BuildResultsSummaryManagerImpl.findFirstResultAfter(BuildResultsSummaryManagerImpl.java:1120)    sun.reflect.GeneratedMethodAccessor982.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    com.atlassian.bamboo.security.acegi.intercept.aopalliance.AuthorityOverrideMethodSecurityInterceptor.invoke(AuthorityOverrideMethodSecurityInterceptor.java:29)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy43.findFirstResultAfter(Unknown Source)    com.atlassian.bamboo.plan.PlanStatePersisterImpl.checkAndUpdateDeltaStatesOfFollowingBuild(PlanStatePersisterImpl.java:232)    com.atlassian.bamboo.plan.PlanStatePersisterImpl.persistChainState(PlanStatePersisterImpl.java:172)    sun.reflect.GeneratedMethodAccessor981.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    com.atlassian.bamboo.author.AuthorCreatorServiceInterceptor.invoke(AuthorCreatorServiceInterceptor.java:45)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy55.persistChainState(Unknown Source)    com.atlassian.bamboo.plan.PlanStatePersisterServiceImpl$3.run(PlanStatePersisterServiceImpl.java:62)    com.atlassian.util.concurrent.ManagedLocks$ManagedLockImpl.withLock(ManagedLocks.java:333)    com.atlassian.bamboo.plan.PlanStatePersisterServiceImpl.persistChainState(PlanStatePersisterServiceImpl.java:58)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.finaliseChainStateIfChainExecutionHasCompleted(ChainExecutionManagerImpl.java:894)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.access$400(ChainExecutionManagerImpl.java:88)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl$1.run(ChainExecutionManagerImpl.java:443)    com.atlassian.util.concurrent.ManagedLocks$ManagedLockImpl.withLock(ManagedLocks.java:333)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.execute(ChainExecutionManagerImpl.java:361)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.access$1300(ChainExecutionManagerImpl.java:88)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl$2.run(ChainExecutionManagerImpl.java:491)    com.atlassian.util.concurrent.ManagedLocks$ManagedLockImpl.withLock(ManagedLocks.java:333)    com.atlassian.bamboo.chains.ChainExecutionManagerImpl.handleEvent(ChainExecutionManagerImpl.java:471)    sun.reflect.GeneratedMethodAccessor688.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.orm.hibernate.HibernateInterceptor.invoke(HibernateInterceptor.java:117)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy82.handleEvent(Unknown Source)    sun.reflect.GeneratedMethodAccessor688.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    org.springframework.orm.hibernate.HibernateInterceptor.invoke(HibernateInterceptor.java:117)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    $Proxy83.handleEvent(Unknown Source)    com.atlassian.event.legacy.LegacyListenerHandler$LegacyListenerInvoker.invoke(LegacyListenerHandler.java:55)    com.atlassian.bamboo.event.spi.EventInvokerRunnable.run(EventInvokerRunnable.java:23)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)    com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    java.lang.Thread.run(Thread.java:662)  {code}"
1,ViewAgent action slow when there's a lot of data,"ViewAgent action slow when there's a lot of data (either in ""Offline Remote Agents"" or in ""Elastic Agent History""). Most probably missing DB index.  "
0,Create reciprocal link from JIRA to Bamboo when manually linking result to issue,"* Creating a manual link is done from the issues tab  * Instead of ""Created from"" use ""Linked from"""
0,Create reciprocal link from JIRA to Bamboo when manually linking result to issue,"* Creating a manual link is done from the issues tab  * Instead of ""Created from"" use ""Linked from"""
0,Some reponsibility not being shown on the Wallboard,http://tardigrade.sydney.atlassian.com:8085/bamboo/browse/BDT-PGQSL-573 has Przemek as responsible but the Wallboard doesn't show him.    Needs to investigate.
0,Show X lines of logs on the error log summary if no error log summary has been set,NULL
0,Notification when responsibility is manually changed,"* Add notification for when the responsibility changes * only fire when humans make the change (not when Bamboo assigns responsibility) * Name ""Change in Responsibilities"" * Should have the failure colouring and status * Check the IM text * Remove the notifications enabled/disabled for Responsibilities out of the admin and the build admin.  You have been made responsible for this build X has been made responsible for this build / X has been made not responsible for this build"
0,Add Linked JIRA Issue should be shown in dialog,* Add to Actions menu  * Under Create Issue  * Group Create and Link issue together  * remove the ability to set a type
0,Add Linked JIRA Issue should be shown in dialog,* Add to Actions menu  * Under Create Issue  * Group Create and Link issue together  * remove the ability to set a type
0,Keyboard shortcut for create issue,Pressing C then I on a result summary should show the create issue for build dialog
0,Make HipChat plugin work for change in responsibilities notification type,And possibly make it more extensible in general so we dont have to edit the plugin for every notification type? i.e could it just use the IM content? - Maybe if we only send html IMs
0,Going directly to Bamboo's configureGlobalPermissions.action erases all permissions,"Create a user and give them access to use restricted admin in Bamboo. As that user go directly to the path:    /builds/build/admin/configureGlobalPermissions.action    If you refresh the page or go to the view page, you won't have any permissions to do that because they will have been erased.  Effect: Data loss (in the form of who can do what), DoS (people can't use Bamboo), support costs (fixing it will be a PITA for our supporters).    This was first picked up in OnDemand, but I think it is a bamboo + webwork problem. Tim Moore was able to reproduce this on a bamboo 4 standalone instance."
0,Null environment variable values prevent builds from running,{noformat}  com.atlassian.bamboo.task.TaskException: Failed to execute task    at com.atlassian.bamboo.plugins.maven.task.Maven3BuildTask.execute(Maven3BuildTask.java:110)    at com.atlassian.bamboo.task.TaskExecutorImpl.executeTasks(TaskExecutorImpl.java:180)    at com.atlassian.bamboo.task.TaskExecutorImpl.execute(TaskExecutorImpl.java:88)    at com.atlassian.bamboo.build.pipeline.tasks.ExecuteBuildTask.call(ExecuteBuildTask.java:84)    at sun.reflect.GeneratedMethodAccessor510.invoke(Unknown Source)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    at java.lang.reflect.Method.invoke(Method.java:597)    at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:304)    at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)    at org.springframework.orm.hibernate.HibernateInterceptor.invoke(HibernateInterceptor.java:117)    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)    at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)    at $Proxy563.call(Unknown Source)    at com.atlassian.bamboo.v2.build.agent.DefaultBuildAgent.build(DefaultBuildAgent.java:202)    at com.atlassian.bamboo.v2.build.agent.BuildAgentControllerImpl.waitAndPerformBuild(BuildAgentControllerImpl.java:103)    at com.atlassian.bamboo.v2.build.agent.DefaultBuildAgent$1.run(DefaultBuildAgent.java:109)    at com.atlassian.bamboo.build.pipeline.concurrent.NamedThreadFactory$2.run(NamedThreadFactory.java:50)    at java.lang.Thread.run(Thread.java:662)    Caused by: java.lang.NullPointerException    at java.lang.ProcessEnvironment.validateValue(ProcessEnvironment.java:102)    at java.lang.ProcessEnvironment.access$400(ProcessEnvironment.java:44)    at java.lang.ProcessEnvironment$Value.valueOf(ProcessEnvironment.java:185)    at java.lang.ProcessEnvironment$StringEnvironment.put(ProcessEnvironment.java:224)    at java.lang.ProcessEnvironment$StringEnvironment.put(ProcessEnvironment.java:203)    at java.util.AbstractMap.putAll(AbstractMap.java:256)    at com.atlassian.utils.process.ExternalProcess.createProcess(ExternalProcess.java:237)    at com.atlassian.utils.process.ExternalProcess.start(ExternalProcess.java:283)    at com.atlassian.utils.process.ExternalProcess.execute(ExternalProcess.java:408)    at com.atlassian.bamboo.process.ProcessServiceImpl.executeProcess(ProcessServiceImpl.java:176)    at sun.reflect.GeneratedMethodAccessor951.invoke(Unknown Source)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    at java.lang.reflect.Method.invoke(Method.java:597)    at com.atlassian.plugin.osgi.hostcomponents.impl.DefaultComponentRegistrar$ContextClassLoaderSettingInvocationHandler.invoke(DefaultComponentRegistrar.java:129)    at $Proxy141.executeProcess(Unknown Source)    at sun.reflect.GeneratedMethodAccessor951.invoke(Unknown Source)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    at java.lang.reflect.Method.invoke(Method.java:597)    at com.atlassian.plugin.osgi.bridge.external.HostComponentFactoryBean$DynamicServiceInvocationHandler.invoke(HostComponentFactoryBean.java:154)    at $Proxy141.executeProcess(Unknown Source)    at com.atlassian.bamboo.plugins.maven.task.Maven3BuildTask.execute(Maven3BuildTask.java:75)    ... 18 more  {noformat}    We should probably change nulls to empty strings in ProcessUtils whenever ProcessBuilder.environment().put/putAll() is called.
0,Exception on My Bamboo -> My Broken Plans,"Seen on Tardigrade  {code}  Error in action: com.atlassian.bamboo.brokenbuildtracker.ui.ViewResponsibilitiesForUser@20674f89 Expression fn.hasPlanPermission('BUILD', build) is undefined on line 89, column 10 in fragments/plan/displayBuildPlansList.ftl. The problematic instruction: ---------- ==> if fn.hasPlanPermission('BUILD', build) [on line 89, column 5 in fragments/plan/displayBuildPlansList.ftl] in user-directive displayPlanListOperations [on line 77, column 13 in fragments/plan/displayBuildPlansList.ftl] in user-directive planList.displayBuildPlansList [on line 4, column 5 in panels/myResponsibilities.ftl] ---------- Java backtrace for programmers: ---------- freemarker.core.InvalidReferenceException: Expression fn.hasPlanPermission('BUILD', build) is undefined on line 89, column 10 in fragments/plan/displayBuildPlansList.ftl. at freemarker.core.TemplateObject.assertNonNull(TemplateObject.java:124) at freemarker.core.Expression.isTrue(Expression.java:145) at freemarker.core.ConditionalBlock.accept(ConditionalBlock.java:77) at freemarker.core.Environment.visit(Environment.java:210) at freemarker.core.MixedContent.accept(MixedContent.java:92) at freemarker.core.Environment.visit(Environment.java:210) at freemarker.core.Macro$Context.runMacro(Macro.java:183) at freemarker.core.Environment.visit(Environment.java:603) at freemarker.core.UnifiedCall.accept(UnifiedCall.java:118) at freemarker.core.Environment.visit(Environment.java:210) at freemarker.core.MixedContent.accept(MixedContent.java:92) at freemarker.core.Environment.visit(Environment.java:210) at freemarker.core.IteratorBlock$Context.runLoop(IteratorBlock.java:167) at freemarker.core.Environment.visit(Environment.java:417) at freemarke  {code}"
1,Mismatch between 'merged' status of XML and BuildDefinition,Causes 'build definition snapshot' problem - the build definition is no longer taken from the master chain.
0,"Combine ""Automatic Detection"" and ""Automatic removal"" into a single ""Automatically manage branches"" feature","* Implement mockup (see attached) ** For Subversion, the branches root section should still be at the top of the screen! * heading ""Automatic Branch Detection"" change to ""Automatically Manage branches"" ** Description ""Bamboo will automatically manage the creation of new branches when Bamboo detects a new branch in your version control system and the removal of the branch from Bambooo after a specified period of inactivity. Detection of new branches runs every 5 minutes."" ** ""Enabled checkbox"" becomes ""Automatically manage branches""  * Remove after ""Enabled"" checkbox is removed and remove after becomes part of the same section as ""Automatically manage branches""  * Remove after description ""Branches will be automatically removed after a specified period if no commits to the VCS branch have been made. You can exclude individual Branches on their Branch Details pages. Branches will not be removed from your Version Control System and cleanup runs daily at 3am.""  * Create an upgrade task to disable automatic removal if the automatically manage branches checkbox is disabled. ** Since enabling automatic creation and automatic removal will be combined, it wont be possible to enable, disable or change the automatic removal value  * Move ""Notifications"" section to the very bottom of the page  * If the expiry is zero, do not expire branches  "
0,Create Branch error handling looks broken,"\The spinner will continue to spin, and the error message sits weird."
0,Get more tasks link on task browser is broken,* File a bug with AMKT (ecosystem.atlassian.net) for the broken link  * Directly to category https://marketplace.atlassian.com/plugins/app/bamboo/popular?category=Bamboo+Tasks
0,A user can manually link any branch to any issue.,* Handle situations where the manual linking was wrong or didn't work * Link the branch to an issue if there is not one already set * Remove the issue that is linked to the branch * Should be only be able to link to ONE issue * Field should go on Branch details 
0,Improve the UX of the custom build dialog,"!https://extranet.atlassian.com/download/attachments/2013305598/run-with-revision.png?modificationDate=1343797439081&amp;effects=border-simple,blur-border!    * An empty text input should mean that we will use the latest revision.  * In this dialog we should re-use the same Variables widget that we use to override variables in the Branch config and NOT the one from JBAM.  * the inline overriding of Stages: We already have this in the current dialog (hidden in a tab) - all we are doing is moving all of the operations onto a single screen.  * What do we want to do if there's a long list of stages? Anything? Just scroll? Maybe switch the order of the fields around? -> For the moment, we should just scroll.  * You can only specify revisions for the default repository (for the moment)  "
0,Pass the revision to build via REST,"Ask James what he had exactly on mind. Was it about ""queue"" resource to accept additional parameter like ""customRevision""?  I envisioned that this (or similar) issue will be done as a part of BDEV-1082 anyway...  "
0,Ensure that once off builds are clearly differentiated from normal builds,"  !https://extranet.atlassian.com/download/attachments/2013305598/buildtypes-result-summary.png?version=1&modificationDate=1346118879586&effects=border-simple,blur-border!    !https://extranet.atlassian.com/download/attachments/2013305598/buildtypes-result-list.png?version=1&modificationDate=1346119274450&effects=border-simple,blur-border!    !https://extranet.atlassian.com/download/attachments/2013305598/buildtypes-summary.png?version=1&modificationDate=1346118918062&effects=border-simple,blur-border!"
0,Bundle Heroku plugin into Bamboo,* We need a plan to test the plugin on our Bamboo server (it has tests)  * Bundle into download and OnDemand
0,"Allow ""re-running this build"" without triggering dependant child-plans.","!https://extranet.atlassian.com/download/attachments/2013305598/rerun-this-build-menu.png?version=2&amp;modificationDate=1343795054858&amp;effects=border-simple,blur-border!    * User may only want to re-run the single plan without triggering dependant builds.  * When you click re-run this build a dialog should popup  * Checkbox to disable the dependencies  ** ""Disable building dependencies"""
0,Doc pages that need updating,https://developer.atlassian.com/display/REST/_Version+Compatibility+for+REST+Plugin https://developer.atlassian.com/display/SAL/_Version+Compatibility+for+Shared+Access+Layer
1,Add the MSTest Agent to stock Windows Elastic Image,"The MS Test command line utility is very useful for running Microsoft Framework Unit Test. By utilizing the MS Test Agent installer, you do not have to fully install Visual Studio on the elastic image.    This can allow folks to utilize MS Test on the Elastic Images without having to customize and AMI.    [MS Test Agent 2010|http://www.microsoft.com/en-us/download/details.aspx?id=1334]  [MS Test Agent 2012|http://www.microsoft.com/en-us/download/details.aspx?id=38186]    Notes and comments from the support team are located here: [https://support.atlassian.com/browse/JST-70692]"
0,Line coverage data is inconsistent,"I'm running 2.4.1 on IDEA 7 and get inconsistent line and branch coverage in the editor: every line that is hit by the a test always gets ""1"" as the hitcount.  "
0,Surefire classpath is incorrect when depending on a jar and a test-jar from the same maven module,"We have two different applications that have the same symptom. Their test cases work properly when executed without clover, but when we run them with clover, they fail because classes or resource files are missing from the classpath.    When running the builds with clover, the test cases are executed multiple times. Only the last time they run they fail. When checking the maven debug output (using the -X option), I found that the last time the test cases are run, a JAR file is missing from the classpath. With both applications it is a jar file from a maven module for which we both depend on the jar and test-jar types of the module.     For example we have the following in the pom.xml file:            <dependency>              <groupId>${parent.groupId}</groupId>              <artifactId>server-data-access</artifactId>              <version>${version}</version>              <type>test-jar</type>              <scope>test</scope>          </dependency>          <dependency>              <groupId>${parent.groupId}</groupId>              <artifactId>server-data-access</artifactId>              <version>${version}</version>          </dependency>    And for some reason, when the test cases are executed for the last time, the following second dependency is not included in the list of jars in the classpath.    I have attached the pom.xml files from the module for which the test cases fail (server-web.pom), the pom.xml files from its parents, the pom.xml file for the module that generates both a jar and a test-jar (server-data-access.pom) and the log file that I get when running maven with the -X option.    This is the classpath when the test cases fail (notice that  /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-data-access/target/server-data-access-1.0.2-SNAPSHOT.jar is missing):  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-web/target/test-classes  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-web/target/classes  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-model/target/server-model-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/commons-lang/commons-lang/2.1/commons-lang-2.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-actions/target/server-actions-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/commons-logging/commons-logging/1.0.4/commons-logging-1.0.4.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/log4j/log4j/1.2.13/log4j-1.2.13.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/util/8.19.0/util-8.19.0.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/tk-util/8.19.0/tk-util-8.19.0.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/jboss/netty/netty/3.0.0.GA/netty-3.0.0.GA.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/springframework/spring/2.5.6/spring-2.5.6.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-core/target/server-core-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-data-access/target/server-data-access-1.0.2-SNAPSHOT-tests.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate/3.2.6.ga/hibernate-3.2.6.ga.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/net/sf/ehcache/ehcache/1.2.3/ehcache-1.2.3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/commons-collections/commons-collections/2.1.1/commons-collections-2.1.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/transaction/jta/1.0.1B/jta-1.0.1B.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/asm/asm-attrs/1.5.3/asm-attrs-1.5.3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/dom4j/dom4j/1.6.1/dom4j-1.6.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/antlr/antlr/2.7.6/antlr-2.7.6.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/cglib/cglib/2.1_3/cglib-2.1_3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/asm/asm/1.5.3/asm-1.5.3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-annotations/3.3.0.ga/hibernate-annotations-3.3.0.ga.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/persistence/persistence-api/1.0/persistence-api-1.0.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-commons-annotations/3.3.0.ga/hibernate-commons-annotations-3.3.0.ga.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-validator/3.0.0.ga/hibernate-validator-3.0.0.ga.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-search/3.0.0.GA/hibernate-search-3.0.0.GA.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/apache/lucene/lucene-core/2.2.0/lucene-core-2.2.0.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/servlet/servlet-api/2.3/servlet-api-2.3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/drools/drools-core/4.0.7/drools-core-4.0.7.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/mvel/mvel/1.3.1-java1.4/mvel-1.3.1-java1.4.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/commons-io/commons-io/1.4/commons-io-1.4.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/j2ee/1.3/j2ee-1.3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/sun/xml/bind/jaxb-impl/2.1.3/jaxb-impl-2.1.3.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/xml/bind/jaxb-api/2.1/jaxb-api-2.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/common/common-configuration/1.0.25/common-configuration-1.0.25.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/opensymphony/oscache/2.1.1/oscache-2.1.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/bml-core/8.20.0-SNAPSHOT/bml-core-8.20.0-SNAPSHOT.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/cache/8.20.0-SNAPSHOT/cache-8.20.0-SNAPSHOT.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/common/common-spring-util/1.0.25/common-spring-util-1.0.25.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/esper/esper/2.2.0/esper-2.2.0.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/antlr/antlr-runtime/3.0.1/antlr-runtime-3.0.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/org/antlr/stringtemplate/3.1-b1/stringtemplate-3.1-b1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/quartz/quartz/1.5.2/quartz-1.5.2.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/mockrunner/mockrunner/0.3.1/mockrunner-0.3.1.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/common/common-abstract-unit-tests/1.0.25/common-abstract-unit-tests-1.0.25-tests.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/unittest/8.20.0-SNAPSHOT/unittest-8.20.0-SNAPSHOT.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-data-access/target/server-data-access-1.0.2-SNAPSHOT-tests.jar  build	20-Nov-2008 08:47:57	[DEBUG]   /opt/bamboo/.m2/repository/com/oracle/oracle_jdbc/10.2.0.2.0/oracle_jdbc-10.2.0.2.0.jar    This is the classpath for when it succeeds:  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-web/target/test-classes  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-web/target/classes  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-model/target/server-model-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/commons-lang/commons-lang/2.1/commons-lang-2.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-actions/target/server-actions-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/commons-logging/commons-logging/1.0.4/commons-logging-1.0.4.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/log4j/log4j/1.2.13/log4j-1.2.13.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/util/8.19.0/util-8.19.0.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/tk-util/8.19.0/tk-util-8.19.0.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/jboss/netty/netty/3.0.0.GA/netty-3.0.0.GA.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/springframework/spring/2.5.6/spring-2.5.6.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-core/target/server-core-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-data-access/target/server-data-access-1.0.2-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate/3.2.6.ga/hibernate-3.2.6.ga.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/net/sf/ehcache/ehcache/1.2.3/ehcache-1.2.3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/commons-collections/commons-collections/2.1.1/commons-collections-2.1.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/transaction/jta/1.0.1B/jta-1.0.1B.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/asm/asm-attrs/1.5.3/asm-attrs-1.5.3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/dom4j/dom4j/1.6.1/dom4j-1.6.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/antlr/antlr/2.7.6/antlr-2.7.6.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/cglib/cglib/2.1_3/cglib-2.1_3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/asm/asm/1.5.3/asm-1.5.3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-annotations/3.3.0.ga/hibernate-annotations-3.3.0.ga.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/persistence/persistence-api/1.0/persistence-api-1.0.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-commons-annotations/3.3.0.ga/hibernate-commons-annotations-3.3.0.ga.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-validator/3.0.0.ga/hibernate-validator-3.0.0.ga.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/hibernate/hibernate-search/3.0.0.GA/hibernate-search-3.0.0.GA.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/apache/lucene/lucene-core/2.2.0/lucene-core-2.2.0.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/servlet/servlet-api/2.3/servlet-api-2.3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/drools/drools-core/4.0.7/drools-core-4.0.7.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/mvel/mvel/1.3.1-java1.4/mvel-1.3.1-java1.4.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/commons-io/commons-io/1.4/commons-io-1.4.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/j2ee/1.3/j2ee-1.3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/sun/xml/bind/jaxb-impl/2.1.3/jaxb-impl-2.1.3.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/xml/bind/jaxb-api/2.1/jaxb-api-2.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/common/common-configuration/1.0.25/common-configuration-1.0.25.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/opensymphony/oscache/2.1.1/oscache-2.1.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/bml-core/8.20.0-SNAPSHOT/bml-core-8.20.0-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/cache/8.20.0-SNAPSHOT/cache-8.20.0-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/common/common-spring-util/1.0.25/common-spring-util-1.0.25.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/esper/esper/2.2.0/esper-2.2.0.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/antlr/antlr-runtime/3.0.1/antlr-runtime-3.0.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/org/antlr/stringtemplate/3.1-b1/stringtemplate-3.1-b1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/quartz/quartz/1.5.2/quartz-1.5.2.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/mockrunner/mockrunner/0.3.1/mockrunner-0.3.1.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/common/common-abstract-unit-tests/1.0.25/common-abstract-unit-tests-1.0.25-tests.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/i4commerce/bml/unittest/8.20.0-SNAPSHOT/unittest-8.20.0-SNAPSHOT.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/Bamboo/xml-data/build-dir/MONITORING-NIGHTLY/monitoring-server/server-data-access/target/server-data-access-1.0.2-SNAPSHOT-tests.jar  build	20-Nov-2008 08:45:07	[DEBUG]   /opt/bamboo/.m2/repository/com/oracle/oracle_jdbc/10.2.0.2.0/oracle_jdbc-10.2.0.2.0.jar"
0,Setting special instrumentation output dir to a source dir deletes all source!,NULL
0,Add method/statement level instrumentation setting in project properties page ,NULL
1,Make Test Run Explorer perform better and stop locking the UI thread,NULL
0,Message in Balloon after clean snapshot is bogus,# Clean shapshot  # First Optimised test run the balloon message is ok (see CIJ-272 re my thoughts on it)  # Second optmised test run has some odd figures in it relating to expected savings and total test run time.    
0,"No instrumentation Done, Always get two Tests run.","Although 0 instr. was done, I still see two tests run.    {code}  2009-05-08 14:19:48,851 [8933804]   INFO - a.clover.core_v2.5.0-build-dev - Updating existing database at '/Users/niick/work/CLOV-trunk/.clover/coverage.db'.   2009-05-08 14:19:49,638 [8934591]   INFO - a.clover.core_v2.5.0-build-dev - Processing files at 1.5 source level.   2009-05-08 14:19:52,664 [8937617]   INFO - a.clover.core_v2.5.0-build-dev - Clover all over. Instrumented 0 files (0 packages).   2009-05-08 14:19:52,664 [8937617]   INFO - a.clover.core_v2.5.0-build-dev - Elapsed time = 3.026 secs. (0 files/sec, 0 srclines/sec)   2009-05-08 14:19:52,664 [8937617]   INFO - over.idea.build.CloverCompiler - completed Instrumentation.   2009-05-08 14:19:53,738 [8938691]   INFO - a.clover.core_v2.5.0-build-dev - Clover estimates having saved around 41 seconds on this optimized test run. The full test run takes approx. 41 seconds   2009-05-08 14:19:53,738 [8938691]   INFO - a.clover.core_v2.5.0-build-dev - Clover included 2 test classes in this run (total # test classes : 84)   2009-05-08 14:19:54,689 [8939642]   INFO -        CloverProgressIndicator - Creating database   2009-05-08 14:19:54,690 [8939643]   INFO -        CloverProgressIndicator - Loading registry file   2009-05-08 14:19:56,686 [8941639]   INFO -        CloverProgressIndicator - Building Index   2009-05-08 14:19:56,686 [8941639]   INFO -        CloverProgressIndicator - Loading coverage data   2009-05-08 14:19:56,758 [8941711]   INFO -        CloverProgressIndicator - Reading per-test data   2009-05-08 14:19:56,995 [8941948]   INFO -        CloverProgressIndicator - Resolving coverage data   2009-05-08 14:19:57,345 [8942298]   INFO -        CloverProgressIndicator - Creating tree   2009-05-08 14:19:57,480 [8942433]   INFO -        CloverProgressIndicator - Tree model ready     {code}"
0,Clover should prompt to rebuild a project if its flush or initstring settings change,NULL
0,Test Columns are empty (or -1) in project.js when generated via a JSON report  ,"When a JSON report is generated, the test fields:  ""ErroneousTests"":0,""FailingTests"":0,""FilteredElements"":0.42738318,""PassingTests"":0,""PcErroneousTests"":-1,""PcFailingTests"":-1,""PcPassingTests"":-1,""TestExecutionTime"":0,""Tests"":0,    are all -1 or 0 in the project.js.  e.g.  https://bamboo.extranet.atlassian.com/browse/CCD-WCLV-488/artifact/Clover-HTML/project.js    The test data is definitely available in the HTML report."
0,Clover-for-Grails generates an exception when using the -clover.view option on the Grails command line.,"Clover-for-Grails generates an exception when using the -clover.view option on the Grails command line. According to Nick's assessment so far, this is a Windows-specific problem.    See output below:    {noformat}  C:\grails-1.2.1\samples\petclinic>grails test-app -clover.on -clover.view  Welcome to Grails 1.2.1 - http://grails.org/  Licensed under Apache Standard License 2.0  Grails home is set to: C:\grails-1.2.1    Base Directory: C:\grails-1.2.1\samples\petclinic  Resolving dependencies...  Dependencies resolved in 2641ms.  Running script C:\grails-1.2.1\scripts\TestApp.groovy  Environment set to test  Clover: Using config: [on:true, view:true]  Using clover license path: C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\plugins\clover-0.2\grails-app\conf\clover\clover-evaluation.  license  [clover-env] Loading clover.xml: jar:file:/C:/Documents%20and%20Settings/Giles%20Gaskell/.ivy2/cache/com.cenqua.clover/clover/jars/clover-3.0.0-m3.jar!/clover.x  ml  Using default clover-setup configuration.  Clover:                 directories: [src/java, src/groovy, test, grails-app]                 includes:    [**/*.groovy, **/*.java]                 excludes     [**/conf/**, **/plugins/**]  [clover-setup] Clover Version 3.0.0-m3, built on February 26 2010 (build-780)  [clover-setup] Loaded from: C:\Documents and Settings\Giles Gaskell\.ivy2\cache\com.cenqua.clover\clover\jars\clover-3.0.0-m3.jar  [clover-setup] Clover Evaluation License registered to Clover Grails Plugin.  [clover-setup] You have 23 day(s) before your license expires.  [clover-setup] Clover is enabled with initstring 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'  Clover: Forcing a clean to ensure Clover instrumentation occurs. Disable by setting: clover.forceClean=false     [delete] Deleting directory C:\grails-1.2.1\samples\petclinic\target\classes     [delete] Deleting directory C:\grails-1.2.1\samples\petclinic\target\test-classes     [delete] Deleting directory C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover  Clover: Compile start.      [mkdir] Created dir: C:\grails-1.2.1\samples\petclinic\target\classes    [groovyc] Compiling 2 source files to C:\grails-1.2.1\samples\petclinic\target\classes  [clover-setup] Creating new database at 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'.    [groovyc] Compiling 17 source files to C:\grails-1.2.1\samples\petclinic\target\classes  [clover-setup] Creating new database at 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'.      [mkdir] Created dir: C:\grails-1.2.1\samples\petclinic\target\test-reports\html      [mkdir] Created dir: C:\grails-1.2.1\samples\petclinic\target\test-reports\plain    Starting unit test phase ...      [mkdir] Created dir: C:\grails-1.2.1\samples\petclinic\target\test-classes\unit    [groovyc] Compiling 1 source file to C:\grails-1.2.1\samples\petclinic\target\test-classes\unit  [clover-setup] Updating existing database at 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'.    -------------------------------------------------------  Running 1 unit test...  Running test org.grails.samples.OwnerControllerTests...PASSED  Tests Completed in 2015ms ...  -------------------------------------------------------  Tests passed: 6  Tests failed: 0  -------------------------------------------------------    Starting integration test phase ...       [copy] Copying 1 file to C:\grails-1.2.1\samples\petclinic\target\test-classes\integration       [copy] Copying 1 file to C:\grails-1.2.1\samples\petclinic\target\test-classes       [copy] Copied 3 empty directories to 2 empty directories under C:\grails-1.2.1\samples\petclinic\target\test-classes  Clover: Compile start.    [groovyc] Compiling 1 source file to C:\grails-1.2.1\samples\petclinic\target\classes  [clover-setup] Updating existing database at 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'.       [copy] Copying 1 file to C:\grails-1.2.1\samples\petclinic\target\classes  Clover: Compile start.    [groovyc] Compiling 1 source file to C:\grails-1.2.1\samples\petclinic\target\classes  [clover-setup] Updating existing database at 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'.  [junitreport] Processing C:\grails-1.2.1\samples\petclinic\target\test-reports\TESTS-TestSuites.xml to C:\DOCUME~1\GILESG~1\LOCALS~1\Temp\null1750708552  [junitreport] Loading stylesheet jar:file:/C:/Documents%20and%20Settings/Giles%20Gaskell/.ivy2/cache/org.apache.ant/ant-junit/jars/ant-junit-1.7.1.jar!/org/apac  he/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl  [junitreport] Transform time: 735ms  [junitreport] Deleting: C:\DOCUME~1\GILESG~1\LOCALS~1\Temp\null1750708552    Tests PASSED - view reports in target\test-reports  Clover: Tests ended  [clover-historypoint] Clover Version 3.0.0-m3, built on February 26 2010 (build-780)  [clover-historypoint] Loaded from: C:\Documents and Settings\Giles Gaskell\.ivy2\cache\com.cenqua.clover\clover\jars\clover-3.0.0-m3.jar  [clover-historypoint] Clover Evaluation License registered to Clover Grails Plugin.  [clover-historypoint] You have 23 day(s) before your license expires.  [clover-historypoint] Writing report to 'C:\grails-1.2.1\samples\petclinic\.cloverhistory\clover-20100305112533.xml.gz'  [clover-html-report] Clover Version 3.0.0-m3, built on February 26 2010 (build-780)  [clover-html-report] Loaded from: C:\Documents and Settings\Giles Gaskell\.ivy2\cache\com.cenqua.clover\clover\jars\clover-3.0.0-m3.jar  [clover-html-report] Clover Evaluation License registered to Clover Grails Plugin.  [clover-html-report] You have 23 day(s) before your license expires.  [clover-html-report] Not overwriting existing history point 'C:\grails-1.2.1\samples\petclinic\.cloverhistory\clover-20100305112533.xml.gz'. To force, set overw  rite=""true"".  [clover-html-report] Loading coverage database from: 'C:\Documents and Settings\Giles Gaskell\.grails\1.2.1\projects\petclinic\clover\db\clover.db'  [clover-html-report] Loading historical coverage data from: 'C:\grails-1.2.1\samples\petclinic\.cloverhistory'  [clover-html-report] Writing HTML report to 'C:\grails-1.2.1\samples\petclinic\build\clover\report'  [clover-html-report] Done. Processed 1 packages in 3047ms (3047ms per package).  [clover-html-report] Writing historical report to 'C:\grails-1.2.1\samples\petclinic\build\clover\report'  [clover-html-report] Read 3 history points.  [clover-html-report] using movers interval of 18 hours  [clover-html-report] using movers interval of 18 hours  [clover-html-report] Writing JSON historical-data to: C:\grails-1.2.1\samples\petclinic\build\clover\report\historical-json.js  [clover-html-report] Done.  Error executing script TestApp: java.lang.StringIndexOutOfBoundsException: String index out of range: 1  gant.TargetExecutionException: java.lang.StringIndexOutOfBoundsException: String index out of range: 1          at gant.Gant$_dispatch_closure4.doCall(Gant.groovy:331)          at gant.Gant$_dispatch_closure6.doCall(Gant.groovy:334)          at gant.Gant$_dispatch_closure6.doCall(Gant.groovy)          at gant.Gant.withBuildListeners(Gant.groovy:344)          at gant.Gant.this$2$withBuildListeners(Gant.groovy)          at gant.Gant$this$2$withBuildListeners.callCurrent(Unknown Source)          at gant.Gant.dispatch(Gant.groovy:334)          at gant.Gant.this$2$dispatch(Gant.groovy)          at gant.Gant.invokeMethod(Gant.groovy)          at gant.Gant.processTargets(Gant.groovy:495)          at gant.Gant.processTargets(Gant.groovy:480)  Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 1          at java.lang.String.charAt(String.java:687)          at java.util.regex.Matcher.appendReplacement(Matcher.java:703)          at java.util.regex.Matcher.replaceAll(Matcher.java:813)          at java.lang.String.replaceAll(String.java:2190)          at _Events.launchReport(_Events.groovy:138)          at _Events$_run_closure6.doCall(_Events.groovy:108)          at _GrailsEvents_groovy$_run_closure5.doCall(_GrailsEvents_groovy:58)          at _GrailsEvents_groovy$_run_closure5.call(_GrailsEvents_groovy)          at _GrailsTest_groovy$_run_closure1.doCall(_GrailsTest_groovy:214)          at TestApp$_run_closure1.doCall(TestApp.groovy:102)          at gant.Gant$_dispatch_closure4.doCall(Gant.groovy:324)          ... 10 more  Error executing script TestApp: java.lang.StringIndexOutOfBoundsException: String index out of range: 1  Application context shutting down...  Application context shutdown.  {noformat}"
0,Implement Test Optimization for Grails: -clover.optimize,"Running -clover.optimize will:  * save a clover.snapshot on the first build  * use this, to discern the optimal set of tests and their order, to run for subsequent builds based on:  ** which source files have been modified  ** length of test  ** test result - failed tests will always be re-run"
1,clover + ant + GWT does not compile and test with clover.,"https://support.atlassian.com/browse/BSP-2952      It looks like the GWT.clover.xxx are not executed.    build	06-May-2010 17:24:30	Build sequence for target(s) `clover.fullclean' is [clover.clean, clover.fullclean]  build	06-May-2010 17:24:30	Complete build sequence is [clover.clean, clover.fullclean, clover.save-history, GWT.clover.report, GWT.clover.json, checkstyle, common.checkstyle, buildonly, dist-dev, tools, apicheck-nobuild, apicheck, test, verify, common.all, build, doc, dist, -compile.emma.if.enabled, dev, GWT.with.clover, compile.emma.if.enabled, common.clean, GWT.clover.fullclean, common.build, GWT.clover.current, presubmit, all, GWT.clover.clean, GWT.clover.save-history, clover.snapshot, clover.json, clover.report, user, clover.log, common.compile.emma.if.enabled, ci, common.-compile.emma.if.enabled, GWT.clover.log, bikeshed, emma.merge, GWT.test, with.clover, jni, clean, clover.all, common.-create.emma.coverage.if.enabled, clover.xml, common.verify, -create.emma.coverage.if.enabled, buildtools, clover.current, samples, dist-bikeshed, soyc, GWT.clover.all, GWT.clover.xml, common.test, servlet, GWT.clover.snapshot, ]    However targets without GWT are executed."
0,Assertion failing (call not made from EDT),"{code}  Access is allowed from event dispatch thread only.  Details: Current thread: Thread[ApplicationImpl pooled thread,5,Idea Thread Group] 1631384517  Our dispatch thread:Thread[AWT-EventQueue-1 9.0.2#IU-95.66, eap:false,6,Idea Thread Group] 1360788520  SystemEventQueueThread: Thread[AWT-EventQueue-1 9.0.2#IU-95.66, eap:false,6,Idea Thread Group] 1360788520    java.lang.Throwable    com.intellij.openapi.diagnostic.Logger.error(Logger.java:55)    com.intellij.openapi.application.impl.ApplicationImpl.a(ApplicationImpl.java:860)    com.intellij.openapi.application.impl.ApplicationImpl.assertIsDispatchThread(ApplicationImpl.java:847)    com.cenqua.clover.idea.coverage.DefaultCoverageManager.setCoverageTree(DefaultCoverageManager.java:176)    com.cenqua.clover.idea.coverage.DefaultCoverageManager.clearCache(DefaultCoverageManager.java:160)    com.cenqua.clover.idea.coverage.DefaultCoverageManager.setContextFilter(DefaultCoverageManager.java:151)    com.cenqua.clover.idea.ProjectPlugin.getCoverageManager(ProjectPlugin.java:219)    com.cenqua.clover.idea.config.ContextFilterPane.<init>(ContextFilterPane.java:16)    com.cenqua.clover.idea.config.ProjectConfigPanel.getFilterPane(ProjectConfigPanel.java:200)    com.cenqua.clover.idea.config.ProjectConfigPanel.getTabbedContent(ProjectConfigPanel.java:120)    com.cenqua.clover.idea.config.ProjectConfigPanel.getLicensedPanel(ProjectConfigPanel.java:82)    com.cenqua.clover.idea.config.ProjectConfigPanel.initComponents(ProjectConfigPanel.java:63)    com.cenqua.clover.idea.config.ProjectConfigPanel.<init>(ProjectConfigPanel.java:54)    com.cenqua.clover.idea.ProjectPlugin.createComponent(ProjectPlugin.java:328)    com.intellij.openapi.options.newEditor.OptionsEditor$Simple.<init>(OptionsEditor.java:1113)    com.intellij.openapi.options.newEditor.OptionsEditor.b(OptionsEditor.java:350)    com.intellij.openapi.options.newEditor.OptionsEditor.access$2100(OptionsEditor.java:63)    com.intellij.openapi.options.newEditor.OptionsEditor$6$1.run(OptionsEditor.java:328)    com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:695)    com.intellij.openapi.options.newEditor.OptionsEditor$6.run(OptionsEditor.java:326)    com.intellij.openapi.application.impl.ApplicationImpl$5.run(ApplicationImpl.java:329)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)    java.util.concurrent.FutureTask.run(FutureTask.java:138)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)    java.lang.Thread.run(Thread.java:637)    com.intellij.openapi.application.impl.ApplicationImpl$1$1.run(ApplicationImpl.java:125)    Access is allowed from event dispatch thread only.  Details: Current thread: Thread[ApplicationImpl pooled thread,5,Idea Thread Group] 1631384517  Our dispatch thread:Thread[AWT-EventQueue-1 9.0.2#IU-95.66, eap:false,6,Idea Thread Group] 1360788520  SystemEventQueueThread: Thread[AWT-EventQueue-1 9.0.2#IU-95.66, eap:false,6,Idea Thread Group] 1360788520    java.lang.Throwable    com.intellij.openapi.diagnostic.Logger.error(Logger.java:55)  {code}"
1,Occasionally clover is getting broken on the Hudson ,"We are using Grails 1.2.2 , Hudson, clover  From time to time Hudson fails with the following console output. Without any changes the next build can be fine.    Licensed under Apache Standard License 2.0  Grails home is set to: D:\Apps\grails-1.2.2    Base Directory: D:\Apps-Pub\HUDSON_HOME\jobs\LifePoints\workspace\Lifepoints  WARNING: Dependencies cannot be resolved for plugin [clover-3.0.2] due to error: null  Resolving dependencies...  Dependencies resolved in 1422ms.  Running script D:\Apps\grails-1.2.2\scripts\TestApp.groovy  Environment set to test  Clover: Using config: [on:true]  Using clover license path: D:\Apps-Pub\HUDSON_HOME\jobs\LifePoints\workspace\Lifepoints/clover.license    [taskdef] Could not load definitions from resource cloverlib.xml. It could not be found.  Error executing script TestApp: Problem: failed to create task or type clover-env  Cause: The name is undefined.  Action: Check the spelling.  Action: Check that any custom tasks/types have been declared.  Action: Check that any <presetdef>/<macrodef> declarations have taken place.    : Problem: failed to create task or type clover-env  Cause: The name is undefined.  Action: Check the spelling.  Action: Check that any custom tasks/types have been declared.  Action: Check that any <presetdef>/<macrodef> declarations have taken place.      org.apache.tools.ant.UnknownElement.getNotFoundException(UnknownElement.java:484)    org.apache.tools.ant.UnknownElement.makeObject(UnknownElement.java:416)    org.apache.tools.ant.UnknownElement.maybeConfigure(UnknownElement.java:160)    _Events.toggleCloverOn(_Events.groovy:302)    _Events$_run_closure3.doCall(_Events.groovy:56)    _GrailsEvents_groovy$_run_closure5.doCall(_GrailsEvents_groovy:58)    _GrailsEvents_groovy$_run_closure5.call(_GrailsEvents_groovy)    _GrailsEvents_groovy.run(_GrailsEvents_groovy:62)    _GrailsEvents_groovy$run.call(Unknown Source)    _GrailsClean_groovy$run.call(Unknown Source)    _GrailsClean_groovy.run(_GrailsClean_groovy:29)    _GrailsClean_groovy$run.call(Unknown Source)    TestApp.run(TestApp.groovy:44)    TestApp$run.call(Unknown Source)    gant.Gant.processTargets(Gant.groovy:494)    gant.Gant.processTargets(Gant.groovy:480)  Error executing script TestApp: Problem: failed to create task or type clover-env  Cause: The name is undefined.  Action: Check the spelling.  Action: Check that any custom tasks/types have been declared.  Action: Check that any <presetdef>/<macrodef> declarations have taken place."
0,<testsources> does not work with Ant groovyc task,Since TestSourceSet is not Serializable it doesn't work with Ant/groovyc.    {noformat}Clover failed to integrate with <groovyc/>java.io.NotSerializableException: com.cenqua.clover.tasks.TestSourceSet          at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)          at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:302)          at java.util.ArrayList.writeObject(ArrayList.java:569)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:917)    ...             at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:302)          at com.atlassian.clover.instr.java.InstrumentationConfig.saveToFile(InstrumentationConfig.java:427)          at com.atlassian.clover.ant.groovy.GroovycSupport.newConfigDir(GroovycSupport.java:360)          at com.atlassian.clover.ant.groovy.GroovycSupport.augmentCompilationClasspath(GroovycSupport.java:350)          at com.atlassian.clover.ant.groovy.GroovycSupport.taskStarted(GroovycSupport.java:160)          at org.apache.tools.ant.Project.fireTaskStarted(Project.java:2182){noformat}       
0,"Clover's embedding of GSON brings in ""assembly-descriptor.xml"" into clover.jar which causes problems for ejbdoclet, others",see support case https://support.atlassian.com/browse/CLV-5576 and forum post http://forums.atlassian.com/thread.jspa?messageID=257357810    the solution is to simply exclude this file when building the clover.jar  
0,Cannot select contexts in IDEA 10.0.2,I select either the filter button in the coverage panel or go via the project settings (same either way) to get to the clover setting dialog. I select the 3rd tab (Contexts) and check the check-boxes 'catch bosy' and 'property methods'. The boxes become checked but the Apply button remains disabled (this is the main issue).    I add a new custom context - and the Apply button is enabled.    I click apply and the check boxes all reset back to their un-set state. The custom context remains.    
0,Ternary clauses never executed still get coverage,"I don't know if this is disoverable at all, but the following example will work with any ternary statement:    @Test  public void testTernaryStatement() {  	ternaryStatement(true);  }    public static void ternaryStatement(boolean bool) {  	String value = bool ? ""true"" : ""false"";  	System.out.println(value);  }    This will mark the entire ternary statement as covered, while the false branch was in fact never triggered.      Since Clover is primarily a coverage tool, I marked this issue Major."
0,clover2:check fails with negative coverage if no class to instrument,"This happens either if no java class was found for instrumentation or if they were all excluded.  Clover then reports a negative coverage of -100%.    To my mind, this is not correct : strictly speaking, if no java class has to be tested for coverage, there is no relevant coverage value (I guess it is what the -100% means) but check should be OK.  "
0,Coverage data not showing up in Intellij,"I've got the dev build from CLOV-1004, which worked for Intellij 10.3 but since I upgraded to 10.5.1, neither the latest public rev (3.1.0.v20110608114626-dev) nor the dev version of the plugin from CLOV-1004 works.    In the projects view, it looks like the code is instrumented.  But after I run a test, no coverage information is displayed.  When I explicitly update the clover database, nothing is updated.  The ""Test Runs"" view behaves as if I never ran a test."
1,Test Optimization says 0 tests should run but all tests are run anyway,"Recently upgraded my Maven builds to use the Maven Surefire plugin version 2.9. Now my builds that run Clover2:optimized and say 0 tests should run, run all tests anyway. The upshot of this is that our optimized test runs are now taking over an hour versus the 15 to 20 minutes they were when the tests were correctly optimized away.      Here is a section of the log file that shows the problem:    {noformat}      [shell] cd dba; mvn -PPERSONAL,UnitTests,Clover.optimize -Djava.awt.headless=true --offline -Dforkmode=once -Dmaven.download.meter=silent -DskipAssembly -Dmaven.test.failure.ignore=true -Dmaven.test.redirectTestOutputToFile=false -Dmaven.clover.licenseLocation=/prodinfolocal/BambooHome/clover.license -Dclover.plugin.version=3.1.0 -Dclover.version=3.1.0 install  clover2:aggregate clover2:clover  [INFO]   NOTE: Maven is executing in offline mode. Any artifacts not already in your local  repository will be inaccessible.    [INFO] Scanning for projects...  [WARNING]   	Profile with id: 'PERSONAL' has not been activated.    [WARNING]   	Profile with id: 'UnitTests' has not been activated.    [INFO] ------------------------------------------------------------------------  [INFO] Building dba  [INFO]    task-segment: [install]  [INFO] ------------------------------------------------------------------------  [INFO] [clover2:setup {execution: clover}]  [INFO] No Clover instrumentation done on source files in: [/Users/mhusby/workarea/dba/src/main/java] as no matching sources files found  [INFO] No Clover instrumentation done on source files in: [/Users/mhusby/workarea/dba/src/test/java] as no matching sources files found  [WARNING] Using [Squid:configuration:jar:1.0-SNAPSHOT], built on Fri Jul 22 15:44:02 EDT 2011 even though a Clovered version exists but it's older (lastModified: Fri Jul 22 15:43:02 EDT 2011 ) and could fail the build. Please consider running Clover again on that dependency's project.  [INFO] [resources:resources {execution: default-resources}]  [WARNING] Using platform encoding (MacRoman actually) to copy filtered resources, i.e. build is platform dependent!  [INFO] Copying 2 resources  [INFO] Copying 1 resource  [INFO] [compiler:compile {execution: default-compile}]  [INFO] Nothing to compile - all classes are up to date  [INFO] [resources:testResources {execution: default-testResources}]  [WARNING] Using platform encoding (MacRoman actually) to copy filtered resources, i.e. build is platform dependent!  [INFO] Copying 6 resources  [INFO] [compiler:testCompile {execution: default-testCompile}]  [INFO] Nothing to compile - all classes are up to date  [INFO] [clover2:optimize {execution: clover}]  [INFO] Adding fileset: directory=/Users/mhusby/workarea/dba/target/clover/src-test-instrumented, includes=[**/*Tests.*, **/*Test.*, **/Test*.*, qxjShouldNeverMatchAClass], excludes=[**/*TestCase.*, **/Test*Servlet.*, **/Abstract*.*, **/*FormModuleTests.*, **/*.xml, **/*$*.*]  [INFO] Clover estimates having saved around 44 seconds on this optimized test run. The full test run takes approx. 44 seconds  [INFO] Clover included 0 test classes in this run (total # test classes : 22)  [INFO] [surefire:test {execution: default-test}]  [INFO] Surefire report directory: /Users/mhusby/workarea/dba/target/surefire-reports    -------------------------------------------------------   T E S T S  -------------------------------------------------------  Running edu.mit.broad.prodinfo.dba.BlobDefTest  Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.327 sec  Running edu.mit.broad.prodinfo.dba.LongDefTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec  Running edu.mit.broad.prodinfo.dba.ExceptionWrapperTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec  Running edu.mit.broad.prodinfo.dba.DoubleDefTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec  Running edu.mit.broad.prodinfo.dba.CacheMapTest  Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.108 sec  Running edu.mit.broad.prodinfo.dba.OurDBsTest  Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.596 sec  Running edu.mit.broad.prodinfo.dba.VArrayDefTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec  Running edu.mit.broad.prodinfo.dba.VArrayFormatterTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec  Running edu.mit.broad.prodinfo.dba.RowDefTest  Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.311 sec  Running edu.mit.broad.prodinfo.dba.BigDecimalDefTest  Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec  Running edu.mit.broad.prodinfo.dba.DoubleFormatterTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec  Running edu.mit.broad.prodinfo.dba.DateFormatterTest  Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec  Running edu.mit.broad.prodinfo.dba.LongFormatterTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec  Running edu.mit.broad.prodinfo.dba.ClobDefTest  Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec  Running edu.mit.broad.prodinfo.dba.DBTest  Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.778 sec  Running edu.mit.broad.prodinfo.dba.DataIteratorTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec  Running edu.mit.broad.prodinfo.dba.ProcedureDefTest  Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.052 sec  Running edu.mit.broad.prodinfo.dba.VArrayListTest  Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec  Running edu.mit.broad.prodinfo.dba.DBPoolTest  Tests run: 8, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 39.08 sec  Running edu.mit.broad.prodinfo.dba.StringDefTest  Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec  Running edu.mit.broad.prodinfo.dba.DataDefIteratorTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec  Running edu.mit.broad.prodinfo.dba.RowTest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.54 sec    Results :    Tests run: 49, Failures: 0, Errors: 0, Skipped: 1    [INFO] [clover2:snapshot {execution: clover}]  [INFO] No span specified, using span of: 48s  [INFO] Saving snapshot to: /Users/mhusby/.clover/Squid-dba/clover.snapshot  [INFO] Clover Version 3.1.0, built on May 31 2011 (build-821)  [INFO] Loaded from: /Users/mhusby/.m2/repository/com/cenqua/clover/clover/3.1.0/clover-3.1.0.jar  [INFO] Clover: Academic License registered to Broad/MIT Institute.  [INFO] Updating snapshot '/Users/mhusby/.clover/Squid-dba/clover.snapshot' against Clover database at '/Users/mhusby/workarea/dba/target/clover/clover.db'  {noformat}"
0,"Can't Use ""Google App Engine"" Run Configuration in IDEA 10.5 when Clover Plug-in is Installed","I am trying to start a GAE run configuration in IDEA 10.5 - after compilation and the web artifact being built, the process simply fails to start, with no errors.  I uninstalled the plug-in and the process once again starts correctly.    I also attempted to create a plain Java Application run configuration to start the process instead and this also fails with the same issue.  The GAE libraries are pulled in as module libraries, via a Maven POM.  I'm not sure if this is linked, but there definitely seems to be a problem with Run configurations when the main Java class being run is located in an external library, rather than being part of the project code-base.  I've checked the IDEA logs and no errors are thrown.  The plain Java Application run configuration had the following settings:    Main Class: com.google.appengine.tools.development.DevAppServerMain  VM Parameters: -Dappengine.sdk.root=<sdk_root> -javaagent:<sdk_root>/lib/agent/appengine-agent.jar  Program Parameters: --disable_update_check <war_file_location>  Working Directory: <sdk_root>"
0,"Log the actual percentage coverage instead of ""Coverage check PASSED"".","Clover logs the minimum percentage:  Checking for coverage of [90.0%] for database    But not the actual percentage.  Coverage check PASSED    Please add this logging, in order to help us know when it's safe to bump the threshold higher."
0,IDEA 11 error turning OFF Clover coverage,"Error during dispatching of java.awt.event.MouseEvent[MOUSE_CLICKED,(691,256),absolute(1963,178),button=1,modifiers=Button1,clickCount=1] on frame0  java.lang.NullPointerException    clover.net.sf.jtreemap.swing.JTreeMap$Zoom.setNewDimension(JTreeMap.java:694)    clover.net.sf.jtreemap.swing.JTreeMap$Zoom.execute(JTreeMap.java:664)    clover.net.sf.jtreemap.swing.JTreeMap.zoom(JTreeMap.java:570)    clover.net.sf.jtreemap.swing.JTreeMap$HandleMouseClick.mouseClicked(JTreeMap.java:616)    java.awt.AWTEventMulticaster.mouseClicked(AWTEventMulticaster.java:270)    java.awt.AWTEventMulticaster.mouseClicked(AWTEventMulticaster.java:269)    java.awt.Component.processMouseEvent(Component.java:6507)    javax.swing.JComponent.processMouseEvent(JComponent.java:3321)    java.awt.Component.processEvent(Component.java:6269)    java.awt.Container.processEvent(Container.java:2229)    java.awt.Component.dispatchEventImpl(Component.java:4860)    java.awt.Container.dispatchEventImpl(Container.java:2287)    java.awt.Component.dispatchEvent(Component.java:4686)    java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4832)    java.awt.LightweightDispatcher.processMouseEvent(Container.java:4501)    java.awt.LightweightDispatcher.dispatchEvent(Container.java:4422)    java.awt.Container.dispatchEventImpl(Container.java:2273)    java.awt.Window.dispatchEventImpl(Window.java:2713)    java.awt.Component.dispatchEvent(Component.java:4686)    java.awt.EventQueue.dispatchEventImpl(EventQueue.java:707)    java.awt.EventQueue.access$000(EventQueue.java:101)    java.awt.EventQueue$3.run(EventQueue.java:666)    java.awt.EventQueue$3.run(EventQueue.java:664)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:87)    java.awt.EventQueue$4.run(EventQueue.java:680)    java.awt.EventQueue$4.run(EventQueue.java:678)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.awt.EventQueue.dispatchEvent(EventQueue.java:677)    com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:694)    com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:530)    com.intellij.ide.IdeEventQueue.b(IdeEventQueue.java:416)    com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:374)    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:211)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:128)    java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:117)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:113)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:105)    java.awt.EventDispatchThread.run(EventDispatchThread.java:90)    "
2,Clover-for-Groovy failed to instrument Groovy sources having methods overridden by metaClass ,"We were able to get clover report from maven-clover2-plugin even for groovy classes successfully. However when we try to override a method via Groovy metaclass in some tests, clover plugin fails on these tests.    We simply added the following lines to our tests.    MyClass.metaClass.myMethod = {     return ""called""  }     Clover plugin gave the following error.    Exception in thread ""main"" BUG! exception in phase 'instruction selection' in source unit Test.groovy Clover-for-Groovy failed to instrument Groovy source: Test.groovy    I attached the full stacktrace for further details."
0,Missing icons on TreeView and Cloud Report tabs,"On IDEA 11 the ""Tree View"" and ""Cloud Report"" tabs have missing icons. See attached screenshot. "
0,Write up on CAC how to get Clover working with Maven and integration tests,In particular:  * numClients=1 should be set in the config  * that the test runner JVM should not have -Dclover.distributed.coverage=ON set as this will override the numClients setting  * that a context listener that starts on webapp start needs to be registered in web.xml so that the webapp Clover runtime connects back to the test runner JVM so that tests proceed 
0,Prepare tutorial how to configure distributed coverage for web applications,"Extend a following page:  http://confluence.atlassian.com/display/CLOVER/Working+with+Distributed+Applications  or add a subpage.     Prepare a step-by-step tutorial how to set compile options, deploy artifacts and set container options in order to test web applications. Things to be considered (not a complete list):     * fact that -Dclover.distributed.coverage=ON it will use the default settings including the default numClients=0     * numClients must be set to 1 in order to have blocking unit tests     * we need a servlet context listener to avoid circular dependency: unit tests are waiting for business code to connect, but business code will not connect until first request (which will be triggered by unit tests)"
1,Consistent statistics reporting in IDE and HTML reports for subpackages,"See https://answers.atlassian.com/questions/11664/package-summary-including-sub-packages    A problem is as follows.     In IDE (IntelliJ/Eclipse) in the Coverage view when we see statistics for given package it is a aggregated summary of the package and all its subpackages. By clicking ""Flatten packages"" we can see statistics for a package without subpackages. See idea1.png.     In HTML report we can see aggregated coverage only for the whole project. When we click on a certain package, we see statistics gathered only from classes from given package and not its subpackages. See html1.png and html2.png.    The expected behaviour is as follows:    1) When we click on the ""Coverage"" link in top-left frame in HTML report, the functionality shall be as it is now. I.e. it should open in the right (main) frame a list of packages, a statistics panel in the top-right corner will show stats for whole project, clicking on a package on the list will open a package details frame with stats in top-right corner for the package only.    2) When we click on the ""Coverage (Aggregate)"" link in top-left frame, it will open in right frame list of package and a statistics panel for whole project (as it is now). But when user clicks on a package from list it will open a different package details frame with aggregated stats in top-right corner for the package and all subpackages.    3) A stats panel     for option 1 shall have a label like ""Statistics for package (without subpackages) <package name>"" and     for option 2 like ""Statistics for package (with subpackages) <package name>""    4) A stats panel should have a link ""show with subpackages"" / ""show without subpackages"" for easy switch between reports.    See html3.png      *Update:*    In new HTML report introduced in Clover 4, a package-level page could look this way:    [^clover4-adg-package.png]    so that toggle buttons would be similar to those a project-level page:    [^clover4-adg-project.png]    "
0,Finish About+Clover-for-Grails page,Complete content on http://confluence.atlassian.com/display/CLOVER/About+Clover-for-Grails and make it visible.    Expand http://confluence.atlassian.com/display/CLOVER/Coverage+Legend  Retake screengrab - with more context - i.e. bigger image and provide an explanation of the columns.    Expand http://confluence.atlassian.com/display/CLOVER/Source+Cross-Referencing+in+Reports  Add image with mouse pointer - i.e. hyperlinks within the code. Try to avoid getting the tooltip in the image as it is unrelated to the (point of the) hyperlink itself.    Add more examples for advanced setuptask / reporttask configuration. 
0,Guide users through Eclipse setup,"Our current Clover-for-Eclipse documentation is hard to follow.     First it tells you how to enable Clover for your projects, mentions briefly that the views will pop up but won't have anything interesting on them yet, and then goes into great detail about each view.    It would be helpful to instead walk the user through how to enable Clover for a project, and then go straight to configuration. There's no need to explain the views until there's something to show on them!    It would also be helpful to describe how a project should be laid out. Do you need your test classes in the same Eclipse project as your source code, for example?  "
0,"Drop support for Eclipse 3.4, 3.5 and RAD 7.5","Drop support for Eclipse 3.4.x and RAD 7.5. These tools are practically unused.     Background:    According to the latest 2012 [Eclipse community survey|http://ianskerrett.wordpress.com/2012/06/08/eclipse-community-survey-result-for-2012/] the 3.4 version is being used by 0.7% of users; 3.5 is used by 2.3% of users.    Rational Application Developer 8.0 which is based on Eclipse 3.6 is available since September 2010, so since nearly two years - i.e. we can expect that IBM customers had a lot of time to upgrade to the latest RAD. The previous version of RAD 7.5 (based on Eclipse 3.4) was released in 2008.    To do:   * update http://confluence.atlassian.com/display/CLOVER/Supported+Platforms page   * update clover-for-eclipse build scripts and manifest files  "
0,license information inconsistent in IDEA,see screenshot
0,"bad alignment of ""About"" box in IDEA",see screenshot
0,Eclipse does not exclude filtered-out methods from statistics,"When we define certain methods to be filtered-out in Eclipse, they are being marked as not executed (hit count = 0) instead of not being taken into consideration at all (no hit count).     What's interesting, an HTML report generated from Eclipse use filters correctly. Metrics in Eclipse says: 4 methods, 4 statements; Metrics in HTML says: 3 methods, 3 statements.    See attached screenshots."
0,Unit test failure during Clover2 custom lifecycle do not fail builds,The Maven Clover2 plugin creates a custom lifecycle. When the unit tests are run in that lifecycle the build will not fail if there are test failures. This is because the lifecycle.xml file forces the Surefire plugin's testFailureIgnore configuration property to false. This is especially problematic in Jenkins builds where unit tests should fail the build (rather than allowing them to be unstable). Please remove this configuration or replace it with a mechanism that can be set in the POM.    The file in question is src/main/resources/META-INF/maven/lifecycle.xml. See the [related Answers discussion|https://answers.atlassian.com/questions/61973/clover-ignore-dmaven-test-failure-ignore-false].
0,Prepare tutorial how to instrument RCP application under Eclipse,"Prepare manual on confluence.atlassian.com describing how to instrument and run RCP application under Eclipse IDE. Topics to be covered:   * ""Run with Clover as ..."" vs ""Run as ...""   * various options in Run Configuration dialog   * adding Clover dependency via Xbootclasspath or manually created plugin   * exporting instrumented code (plugins, fragments, features, product)   * config.ini for product and Xbootclasspath - running exported product (separately) and running it in IDE    Prepare also code example showing how we can use Clover with RCP application.     Open issue: how to distribute it with Clover-for-Eclipse. Options:   * prepare sample aside and attach as zip to manual on confluence   * implement a wizard so that using ""File > New > Example > Clover RCP Example"" will generate sample projects in workspace (preferred solution, but more work effort)  "
0,Reading clover database across platforms does not resolve path correctly for FileInfo,"Clover database contains FileInfo objects for every source file. One of FileInfo fields is an absolute path to given file. When database is written, it uses current platform file separator for file name ('\' for windows, '/' for Linux/MacOS). However, when database is read it interprets file name using again current platform file separator.     As a consequence, when database is generated on Windows and report generated on Linux, it cannot resolve file name properly - it takes whole path as a single path segment, resulting in paths like:    /path/from/report/generation/sourcepath/D:/path/from/instrumented/build/file.java    As a consequence report generation does create html file containing source code with highlighted coverage."
0,Prepare manual how to set-up cross-platform testing,"Write how to set up Clover in case when someone is using a test framework not written in Java - for example shell scripts, C/C++ code. How to wrap this so that Clover can not only execute, but also optimize such tests. "
0,Unicode CR/LF characters in javadoc causes Eclipse/IntelliJ editors get out of sync,"Try the following example:    {code:java}  class Unicode {         void foo() {      	System.out.println(""1"");      	/** {@code \u000a} LF */      }                        void goo() {      	System.out.println(""2"");    	      	/** {@code \u000d} CR */      }                        void hoo() {      	System.out.println(""3"");      }                    } // end  {code}    In Eclipse editor, every CR/LF written in unicode causes that it interprets it as newline character. As a result left ruler ""sees"" more lines and all subsequent coverage blocks are shifted down.       In IntelliJ it's even worse, as we get exception. Stack trace:    {noformat}  Error during dispatching of java.awt.event.InvocationEvent[INVOCATION_DEFAULT,runnable=com.cenqua.clover.idea.content.ContentPlugin$1@789e7511,notifier=null,catchExceptions=false,when=1343648365649] on sun.awt.windows.WToolkit@2018aa27: Wrong end: 270; document length=249; start=246  java.lang.IllegalArgumentException: Wrong end: 270; document length=249; start=246    com.intellij.openapi.editor.impl.RangeMarkerImpl.<init>(RangeMarkerImpl.java:47)    com.intellij.openapi.editor.impl.DocumentImpl.createRangeMarker(DocumentImpl.java:303)    com.intellij.openapi.editor.impl.RangeHighlighterImpl.<init>(RangeHighlighterImpl.java:58)    com.intellij.openapi.editor.impl.MarkupModelImpl.a(MarkupModelImpl.java:119)    com.intellij.openapi.editor.impl.MarkupModelImpl.addRangeHighlighter(MarkupModelImpl.java:135)    com.cenqua.clover.idea.content.DocMarkupPlugin$HighlightMarkupBuilder.highlightStatement(DocMarkupPlugin.java:286)    com.cenqua.clover.idea.content.DocMarkupPlugin$HighlightMarkupBuilder.process(DocMarkupPlugin.java:220)    com.cenqua.clover.idea.content.DocMarkupPlugin.updateMarkups(DocMarkupPlugin.java:136)    com.cenqua.clover.idea.content.DocMarkupPlugin.refresh(DocMarkupPlugin.java:424)    com.cenqua.clover.idea.content.ContentPlugin$1.run(ContentPlugin.java:96)    java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:209)    java.awt.EventQueue.dispatchEventImpl(EventQueue.java:646)    java.awt.EventQueue.access$000(EventQueue.java:84)    java.awt.EventQueue$1.run(EventQueue.java:607)    java.awt.EventQueue$1.run(EventQueue.java:605)    java.security.AccessController.doPrivileged(Native Method)    java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)    java.awt.EventQueue.dispatchEvent(EventQueue.java:616)    com.intellij.ide.IdeEventQueue.f(IdeEventQueue.java:627)    com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:508)    com.intellij.ide.IdeEventQueue.b(IdeEventQueue.java:405)    com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:369)    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:269)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:184)    java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:174)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:169)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:161)    java.awt.EventDispatchThread.run(EventDispatchThread.java:122)    {noformat}  "
1,Fix Clover-for-IDEA compatibility with IntelliJ IDEA 12,IDEA12 is going to be released quite soon. Make sure that Clover will work on it properly:     http://blogs.jetbrains.com/idea/2012/08/meet-august-with-new-intellij-idea-12-eap-build-120305/  http://confluence.jetbrains.net/display/IDEADEV/Leda+120.305+Release+Notes  http://confluence.jetbrains.net/display/IDEADEV/IDEA+12+EAP
0,Coverage ruler does not read settings from Preferences,"A left ruler showing code coverage markers is always reading default color values (defined in plugin.xml), instead of current settings from IDE (Window > Preferences > ... Text editors > Annotations). Note that color in text editor is updated correctly. See attached screenshot. "
0,Coverage ruler does not refresh correctly when all source code annotations are disabled,"A left Clover coverage ruler does not refresh correctly after change in coverage in case when we don't have Clover-related source code annotations enabled. Steps to reproduce:   - install Clover, enable it for a Moneybags project, perform full rebuild   - open MoneyBagTest -> coverage is red   - open Window > Preferences > Editors > Text editors > Annotations   - for all ""Clover: xxx"" annotation types disable ""Vertical ruler"", ""Overview ruler"" and ""Text as"" checkboxes   - run opened test case (right click, run as, junit test)    Result:   - JUnit view shows that tests were executed, Coverage Explorer view is also updated correctly, but the ruler is still highlighted in red   - now scroll the text editor up and down - ruler refreshes during repaint; (the same happens when editor is reopened or resized)    See attached screenshot.    Expected result:  1) ruler should refresh automatically when coverage data is changed, no matter whether source annotation is enabled or not OR  2) ruler should not draw any color if corresponding source code annotation is disabled  "
0,Remove grover*.jar after build,"A maven-clover2-plugin creates a groover*.jar file in $java.io.tmpdir directory. This JAR is next being dynamically added as a build dependency artifact. It must be available till end of the build.    Unfortunately Maven has nothing like post-build-cleanup phase (which would be always executed at end of build). We need a workaround for this, for example:    1) Create a new Clover goal, like clover2:post-build-cleanup, which will remove this file.     2) Alternatively, find a way to add Clover groovy runtime classes to classpath, so that creation of JAR file would not be needed anymore.     As soon as this bug is fixed, a corresponding update of Bamboo Clover Plugin is necessary. See linked issue.    *Solution implemented:*    Instead of playing with post-build cleanup, there are two new options available in <clover-setup/> taks (Ant) and clover2:setup / clover2:instrument goals (Maven):   * skipGroverJar - if set to true it will not extract artifact at all (can be used only if there's no Groovy code in a project)   * groverJar - target location of grover.jar file (fixed name and location instead of generating temporary file name in java.io.tmpdir)  "
0,Create utility for upgrading third party libraries bundled in Clover,"In Clover-for-Ant we have an upgrade.xml Ant script, which can repackage clover.jar and install new version of jfreechart library into it. We shall extend this script to support replacement of all third party libraries used in Clover. Such script should be also available for Clover-for-Eclipse and Clover-for-IDEA.     Keep in mind that we have to retrotranslate third party classes during repackaging to have them compatible with Java 1.4.    Keep in mind that we have to prefix java packages with ""clover."" in order to avoid name space conflicts.    Packages:   * com.google.gson   * com.atlassian.* (Atlassian API)   * com.keypoint.*   * com.lowagie.*   * gnu.cajo.*   * it.unimi.dsi.fastutil.*   * org.apache.commons.*   * org.apache.log4j.*   * org.apache.oro.*   * org.apache.velocity.*   * org.codehaus.groovy.antlr (?)   * org.jfree   * retrotranslator   * org.apache.tools.ant.* (?)  "
1,Make Clover-for-Android prototype publicly available,"Take a ShipIt prototype prepared by Michael, merge with the latest Clover version, make it working on Eclipse ADT, publish manual how to use it on CAC/Clover-for-Android page. "
2,Refresh Clover-for-Scala prototype,"Tasks:   * take Scala prototype https://confluence.atlassian.com/display/DEV/ShipIt+12+Delivery+-+Clover+for+Scala,    * change the code so that it will use new Clover Service Provider Interface - CLOV-1142   * make it working with the latest Clover code base (version 4.0.x),   "
1,Expose a Service Provider Interface for Clover for handling new languages,"Create a Service Provider Interface for Clover, which will allow to attach new source code and/or byte code instrumenters. It will allow us to create a pluggable architecture so that new programming languages (e.g. Scala, JRuby, Jython, Closure) could be supported.     *Rough idea:*   * new language instrumenters can be attached by META-INF/services   ** they can provide some callback like canInstrument(File) or canInstrument(InstrumentationSource)   ** clover shall pass file/instrumentation source to such instrumenter together with Clover's configuration and an open instrumentation session   ** extension instruments on its own calling session.enterXyz()/exitXyz() methods   ** extension writes output file on its own   ** language instrumenter could work also with an Abstract Syntax Tree (e.g. Groovy or Scala - we may need a different way of attaching here)     * new language renderers can be attached by META-INF/services - move to another issue?      "
0,Remove obsolete pages from Clover 3.1 space,"Hi Melanie,    May I ask your team to delete following pages from CLOVER space? All of them are obsolete or available in spaces for older Clover releases. They are unnecessarily cluttering current documentation.     https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.3.2+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.3+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.0+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.1+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.0+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.3+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.2+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.1+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.0+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2-beta5+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2-beta4+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2-beta3+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2-beta2+for+IDEA  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.1-beta1+for+IDEA    https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.3+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.2+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.0+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.1+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.0+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.3+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.2+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.0+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.1+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.2.1+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.1.0+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.3+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.2+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.1+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.0+for+Eclipse  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.0.b2_1+for+Eclipse    https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.3+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.2+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.0+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.1+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.0+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.3+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.2+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.0+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.1+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.0+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.2.1+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.2.0+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.1.0+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0b2+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0b1+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0a5+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0a4+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0a3+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0a2+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0a1+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.3+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.2+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.1+for+Ant  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.0.0+for+Ant    https://confluence.atlassian.com/display/CLOVER/Changes+in+2.6.0+for+Maven+2  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.5.0+for+Maven+2  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.3+for+Maven+2  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.4.2+for+Maven+2  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.2+for+Maven+2  https://confluence.atlassian.com/display/CLOVER/Changes+in+2.3.0+for+Maven+2  https://confluence.atlassian.com/pages/viewpage.action?pageId=110035179  https://confluence.atlassian.com/pages/viewpage.action?pageId=134873325  https://confluence.atlassian.com/pages/viewpage.action?pageId=103710921    https://confluence.atlassian.com/display/CLOVER/%5BTRASH%5D+Using+Clover+for+Maven+2+with+the+gwt-maven-plugin  https://confluence.atlassian.com/display/CLOVER/%5BTRASH%5D+Using+Clover+with+the+GWT-maven+plugin    https://confluence.atlassian.com/pages/viewpage.action?pageId=93683890  https://confluence.atlassian.com/pages/viewpage.action?pageId=71599635  https://confluence.atlassian.com/display/CLOVER/%5BTRASH%5D+Clover-for-Eclipse+User%27s+Guide  https://confluence.atlassian.com/display/CLOVER/%28hidden+draft%29+Clover-for+Eclipse+Upgrade+Guide  "
1,Support *.groovy files in src/main/java folder for groovy-eclipse-plugin,"*Problem:*    Clover fails during instrumentation of *.groovy files in case when sources are put in src/main/java directory, instead of src/main/groovy and when the groovy-eclipse-plugin is used.     *Steps to reproduce:*     * create a project with groovy-eclipse-plugin and maven-clover2-plugin   * put a *.groovy file inside src/main/java   * run build    *Expected error:*    {noformat}  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 1.201s  [INFO] Finished at: Mon Sep 10 10:16:48 CDT 2012  [INFO] Final Memory: 7M/18M  [INFO] ------------------------------------------------------------------------  [ERROR] Failed to execute goal com.atlassian.maven.plugins:maven-clover2-plugin:3.1.7:setup (default-cli) on project test.project: Clover has failed to instrument the source files in the [C:\workspaces\workspace\TestProject\target\clover\src-instrumented] directory -> [Help 1]  org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.atlassian.maven.plugins:maven-clover2-plugin:3.1.7:setup (default-cli) on project test.project: Clover has failed to instrument the source files in the [C:\workspaces\workspace\TestProject\target\clover\src-instrumented] directory    org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)    org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)    org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)    org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)    org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)    org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)    org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)    org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:319)    org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)    org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)    org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)    org.apache.maven.cli.MavenCli.main(MavenCli.java:141)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)    org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)    org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)    org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)  Caused by: org.apache.maven.plugin.MojoExecutionException: Clover has failed to instrument the source files in the [C:\workspaces\workspace\TestProject\target\clover\src-instrumented] directory    com.atlassian.maven.plugin.clover.internal.instrumentation.AbstractInstrumenter.instrumentSources(AbstractInstrumenter.java:197)    com.atlassian.maven.plugin.clover.internal.instrumentation.AbstractInstrumenter.instrument(AbstractInstrumenter.java:72)    com.atlassian.maven.plugin.clover.CloverInstrumentInternalMojo.execute(CloverInstrumentInternalMojo.java:309)    com.atlassian.maven.plugin.clover.CloverSetupMojo.execute(CloverSetupMojo.java:31)    org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)    org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)  	... 19 more  [ERROR]   [ERROR]   [ERROR] For more information about the errors and possible solutions, please read the following articles:  [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException    *** ERROR: No source files specified      USAGE: com.cenqua.clover.CloverInstr [OPTIONS] PARAMS [FILES...]  {noformat}    A reason is that Java compiler is involved here, Clover is called, it filters out all files which do not have *.java extension and as a consequence it passes wrong argument to CloverInstr.    *Solution:*    1) Add check to AbstractInstrumenter (maven-clover2-plugin) so it does not call CloverInstr in case no matching files are found    2) Add support for src/main/java, src/test/java directories for Groovy - probably in GroovySourceScanner + CloverInstrumentInternalMojo      *Workaround:*    Put Groovy files in correct location i.e. src/main/groovy - they'll be handled correctly by Clover.    *Background:*    According to groovy-eclipse-plugin documentation, they recommend putting groovy files inside java folder - it's not nice, but it's the fastest set-up.     http://groovy.codehaus.org/Groovy-Eclipse+compiler+plugin+for+Maven"
1,Implement integration with Maven Tycho Plugin,Currently Clover-for-Maven cannot handle maven build with the maven-tycho-plugin. More details here: https://answers.atlassian.com/questions/1919/maven-3-tycho-and-clover    To be done: hack the maven-tycho-plugin and intercept its internal call of javac compiler and instrument sources on the fly.
0,Make possible to see instrumented sources in Clover-for-Eclipse,"Clover-for-Eclipse: Clover instrumentation in performed in memory during compilation. As a consequence when there is any build error, we can see original source only and cannot check whether the problem lies in the Clover instrumenter itself.     For example, in Clover-for-IDEA we can see instrumented sources because they're stored in temporary directory.    To be implemented: add an option to get instrumented sources in Eclipse too."
1,"Clover does not instrument groovy source files, (groovy-eclipse compiler) when located in src/main/groovy","When building a groovy project with maven, I am trying to get clover code coverage reports. With all source files in .java and in src/main/java, clover works fine. However, with all source files in .groovy and in src/main/groovy, clover can not instrument the files (clover2:instrument, or clover2:setup).     Everything that I can find is saying that clover versions after 3.1.3 support the maven groovy eclipse compiler (so long as the groovy files are located in src/main/groovy). See this ticket: https://jira.atlassian.com/browse/CLOV-1021?page=com.atlassian.jira.plugin.system.issuetabpanels:changehistory-tabpanel      There is another support ticket that I found that seems to cover the same issue, however I cannot access it. Here is the link  https://support.atlassian.com/browse/CLV-5878    A partial output log is included here. I will try to come up with a small test project that I can upload here. Any help would be appreciated.    "
0,Clover fails to find coverage snapshot files when IBM JRE is used,"*Problem:* it seems that IBM JRE implementation differs from Oracle JRE in such way that Object.hashCode() can return a negative value. As a consequence, file name suffix generated for coverage snapshot and test slice snapshot may be composed not only of '0-9a-z_' characters, but also have minus ""-"" sign in it.     As a result such file name does not match regular expression used for searching of coverage files and coverage is not read.     *Workaround:* rename coverage snapshot file(s) and remove ""-"" character.  "
0,Inconsistent popup menu in different IDEA versions,"IDEA 10 and newer is not showing the ""Exclude from compile"" option when sources are instrumented. In IDEA9 it was working fine. See 'popup_menu_in_ideas.png' screenshot. My suspicion is that IDEA somehow does not treat instrumented file as a source code, thus hiding this option from popup menu.    What's more, when you click on the ""Jump to source"" it opens the Explorer window, instead of opening the source file in IDEA.    See also:    META-INF/plugin.xml  com.cenqua.clover.idea.CloverPlugin.initComponent()    com.intellij.openapi.actionSystem.IdeActions;      @NonNls String GROUP_COMPILER_ERROR_VIEW_POPUP = ""CompilerErrorViewPopupMenu"";    com.intellij.compiler.imp.CompilerErrorTreeView.addExtraPopupMenuActions();  "
0,Simple installer for Clover-for-IDEA,"When someone downloads the Clover-for-IDEA from www.atlassian.com he gets clover-idea-x.x.x.jar file. What to do next? Of course you have to open IDEA, select Settings > Plugins > Install from disk, but user might not know. Especially when taking into account that ""Install from disk"" is not available in older IDEA versions.     New feature: provide simple installer for Clover-for-IDEA. When user double-clicks on a JAR, a GUI installer starts (class defined in MANIFEST.MF) which asks for location of IDEA installation. Simple check whether selected folder contains config/plugins directory and copy jar into given location. Show also message how it can be installed from IDEA GUI.    Business value: makes Clover evaluation easier.  "
0,Refresh clover.atlassian.com,The http://clover.atlassian.com/ page was not updated for last 2 years. Re-activate Bamboo builds related with this site to have live data. Add a link to this page on main Clover product page:    http://www.atlassian.com/software/clover/overview  
0,Run with Clover as on a project with no Clover,"Clover-for-Eclipse. When you have a project with Clover disabled, the ""Run with Clover as"" and ""Run optimized"" buttons are still active. It makes no sense, buttons should be grayed-out."
0,Test Run Explorer does not show test contribution with optimized run,"When you execute optimized test runs, the Test Run Explorer does snow show test contribution data."
0,Create small legend for Top Risks and Quick Wins in Cloud report,Currently user does not know what the font size and colour means until he reads the documentation. Add small legend box or at least a single-line description for these two reports.     Affects HTML report as well as Eclipse/IDEA views. 
0,Change default sort order in coverage report for packages/classes/methods,"Coverage report - show the least covered packages/classes/methods first. Table should not start with the highest coverage (starting from 100%) at the top, because developer is not interested in code which is already covered and tested, but in white spots. Add the possibility to sort by any column.    Affected: HTML/IDE"
0,Add drop-down with two options for coverage,"Add the ""drop-down"" listbox which will have ""show cumulative coverage"" and ""show coverage from last run"" options. It should be added in Eclipse and IDEA.    Note that the ""span"" option is already available (Window > Preferences > Clover) but we can have either:   * have time-based span or    * set it to 0seconds which means load everything.      Background reason: coverage is cumulative. Some customer have pointed out that coverage data for successive incremental change/save/compile/test cycles can lie and show coverage that's no longer there. That's because we aggregate all coverage data since the last full rebuild. We will hopefully have a better way of showing this in the future. In the meantime, scrubbing the coverage for a project and re-running the tests will show the coverage of tests just run.    "
1,Generate cumulative statement / complexity metric for need of methodContext filtering,"A method context has maxComplexity / maxStatements attributes which allow to reduce the scope of context filter for methods where these metrics are lower than defined level (by default maxComplexity/maxStatements is Int.MAX_VALUE, i.e. all methods are taken into account). Example:    {code:xml}  <methodContext name=""trivial"" regexp="".*"" maxStatements=""1""/>  {code}    However, customer reported a problem that it excludes methods containing inline classes, for instance:    {code:java}  // the getListener() method has only one statement: ""return <object>;""  // if you use <methodContext name=""trivial"" regexp="".*"" maxStatements=""1"">  // then the whole code block will be filtered-out, including the actionPerformed() method  ActionListener getListener() {     return new ActionListener() {         public void actionPerformed(ActionEvent e) {             System.out.println(""statement #1"");             System.out.println(""statement #2"");         }     };   }  {code}    In the case above ActionListener() is an inner class so it has their own metrics. And because of fact that context filters are ""cumulative"" (so that whenever any code pattern is matched, then the whole block is excluded), the whole getListener() method is filtered-out, including inner class.    *Solution:*    We cannot change existing metrics (without breaking backward compatibility) and we cannot change the way how blocks are excluded (it would make no sense). But we can create new metrics called: cumulativeComplexity and cumulativeStatements. It would work as follows:     * for every interface, class, enum or method find all inner classes, interfaces, enums or methods (*)   ** this can be checked by matching occupied code regions (on a database level during metric calculation - preferred) or during instrumentation (code parser)   * calculate cumulativeComplexity and cumulativeStatements by adding cumulative value from all inner objects to current entity complexity/statements value   * add maxCumulativeComplexity / maxCumulativeStatments to Ant clover-setup/clover-instr and Maven clover2:setup / clover2:instr goals   ** html reports would probably remain unchanged (just grey-out)    (*) yes we should make code more flexible in order to make it ready to support code constructs like default methods in Java8 interfaces or Scala's functions inside functions etc   "
0,Fix AbstractAntLogger,"The AbstractAntLogger class logs the message only if Ant Task is not null:    {noformat}  protected void antLog(Project proj, Task task, String aMsg, int antLogLevel) {          if (task != null) {              proj.log(task, aMsg, antLogLevel);          }      }  {noformat}    As a consequence, no log is availalbe in case when code it's called outside Ant - problem was found in open-source Gradle Clover Plugin."
0,Clover does not work with Groovy 2.x,"*Problem:*    Clover instrumentation fails for Groovy 2.x with a stack trace like below:    {noformat}    BUILD FAILED  c:\Work\support\clv-5892-groovyjarjarasm\tutorial\build.xml:32: java.lang.IncompatibleClassChangeError          at com.atlassian.clover.instr.groovy.bytecode.RecorderGetterBytecodeInstruction.visit(RecorderGetterBytecodeInstruction.java:109)          at org.codehaus.groovy.classgen.AsmClassGenerator.visitBytecodeSequence(AsmClassGenerator.java:1771)          at org.codehaus.groovy.classgen.BytecodeSequence.visit(BytecodeSequence.java:64)          at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:81)          at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:155)          at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:455)          at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)          at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)          at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)          at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:319)          at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:276)          at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)          at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:396)          at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)          at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)          at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:180)          at org.codehaus.groovy.control.CompilationUnit$14.call(CompilationUnit.java:786)          at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1027)          at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:564)          at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:542)          at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:519)          at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:498)          at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)          at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:213)          at org.codehaus.groovy.ant.Groovyc.runCompiler(Groovyc.java:947)          at org.codehaus.groovy.ant.Groovyc.compile(Groovyc.java:994)          at org.codehaus.groovy.ant.Groovyc.execute(Groovyc.java:630)  {noformat}    *Reason:*    RecorderGetterBytecodeInstruction extends org.codehaus.groovy.classgen.BytecodeInstruction  and implements a following method:  public void visit(groovyjarjarasm.asm.MethodVisitor methodVisitor)    In Groovy 1.x MethodVisitor is an interface. In Groovy 2.x MethodVisitor is a class. Although RecorderGetterBytecodeInstruction compiles correctly with both Groovy versions, it fails at runtime during bytecode verification, becase visit() method gets different bytecode signature.    *Solution:*    Compile RecorderGetterBytecodeInstruction against two different Groovy versions. Load proper version using reflections at runtime.   "
0,Grails Clover Plugin does not intercept 'testc' compiler,"Grails 2.x has introduced a new compiler for building unit tests (from test/unit directory). The 'testc' task is the *org.codehaus.groovy.grails.test.compiler.GrailsTestCompiler* Build log looks like this:    {noformat}  |Compiling 1 source files      ...      [testc] ... \test\unit\clover\test\UnitTests.groovy    |Compiling 1 source files     ...    [groovyc] ... \test\integration\IntegrationServiceTests.groovy  {noformat}    Clover does not intercept this compiler call (it supports Groovyc, Grailsc, GrailsCompiler). As a consequence unit tests are not instrumented and not included in coverage."
0,CloverOptimizerMojo does not support full include/exclude syntax,"*Problem:*    CloverOptimizerMojo does not support full include/exclude syntax, which can be handled by maven-surefire-plugin. Two cases are not handled right now:    (1) multiple paths separated by comma   {noformat}  <include>**/Test1.java, **/Test2.java</include>  {noformat}    (2) regular expression instead of Ant-style pattern  {noformat}  <include>%regex[.*Test.class]</include>  {noformat}    See: http://maven.apache.org/plugins/maven-surefire-plugin/examples/inclusion-exclusion.html    *Workaround:*    (1) use separate entries, like:  {noformat}  <include>**/Test1.java</include>  <include>**/Test2.java</include>  {noformat}    (2) not available; user can try to replace regexp by set of Ant-style patterns      *Implementation:*    CloverOptimizerMojo relies an Ant-style patterns underneath. The CloverOptimizerMojo.createFileSet() method has    {noformat}  private FileSet createFileSet(Project antProject, final File directory, List includes, List excludes) {    FileSet testFileSet = new FileSet();    ...    testFileSet.appendIncludes((String[]) includes.toArray(new String[includes.size()]));  }  {noformat}      fix for (1) - split every element in includes/excludes arrays using the comma delimiter and pass expanded arrays to appendIncludes()    fix for (2) - find all elements from includes/excludes arrays having %regexp keyword, run file search on _directory_ using a filename matcher with regexp, create include/exclude for every file matched  "
0,Add maxComplexity / maxStatements for Clover-for-Maven,"Ant has extra maxComplexity and maxStatements attributes available which are not supported in Maven plugin, for example:    {code:xml}  <methodContext name=""simple_method"" regexp=""(.* )?public .*(get|set|is)[A-Z0-9].*"" maxStatements=""1""/>  {code}    in Maven we have _name_ and _regexp_ attributes only:    {code:xml}  <configuration>    <methodContexts>      <!-- vvvvv name    vvvvvv regexp -->      <simple_method>((.* )?public .*(get|set|is)[A-Z0-9].*</simple_method>    </methodContexts>  </configuration>  {code}      *Possible solution*    Create <methodContextsExt> which would accept entries like:    {code:xml}  <methodContextsExt>    <methodContext>      <name>simple_method</name>      <regexp>(.* )?public .*(get|set|is)[A-Z0-9].*</regexp>      <maxComplexity>10</maxComplexity>      <maxStatements>10</maxStatements>      <!-- for CLOV-1162: -->      <maxCumulativeComplexity>20</maxCumulativeComplexity>      <maxCumulativeStatments>20</maxCumulativeStatments>    </methodContext>  <methodContextsExt>   ... more ...  {code}    Alternatively, extend parsing of existing <methodContexts> tag so whenever it finds the <methodContext> entry it will not treat it as name of new context but as a structure from sample above.    See also CLOV-1162."
0,Make HTML report more consistent about number of classes,See attached screenshot. HTML report should list interfaces too.
1,The clover-log should use default test detection if testSources are not defined,"*Steps to reproduce:*    1) Add following target to clover-ant-X.Y.Z/tutorial/build_completed.xml:    {code:xml}  <target name=""clover.log"">    <clover-log codeType=""APPLICATION""/>    <clover-log codeType=""TEST""/>    <clover-log codeType=""ALL""/>  </target>  {code}    2) Run     {noformat}  ant -f build_completed.xml -Dtest.target=test.run clover.all  {noformat}    It will produce output like:    {noformat}  clover.log:  [clover-log] Report for code   : APPLICATION  [clover-log] Coverage:-  [clover-log]       Methods: 61/62 (98,4%)  [clover-log]    Statements: 171/178 (96,1%)  [clover-log]      Branches: 40/48 (83,3%)  [clover-log]         Total: 94,4%  ...  [clover-log] Report for code   : TEST  [clover-log] Coverage:-  [clover-log]       Methods: 0/0 ( - )  [clover-log]    Statements: 0/0 ( - )  [clover-log]      Branches: 0/0 ( - )  [clover-log]         Total:  -  ...  [clover-log] Report for code   : ALL  [clover-log] Coverage:-  [clover-log]       Methods: 61/62 (98,4%)  [clover-log]    Statements: 171/178 (96,1%)  [clover-log]      Branches: 40/48 (83,3%)  [clover-log]         Total: 94,4%  {noformat}    *Result:*    All classes were qualified to APPLICATION; if we compare it with XML or HTML reports, test and application classes are distinguished there. This is normal behaviour because <clover-log> requires the <testSources> tag to be defined, for instance:    {code:xml}  <clover-log codeType=""TEST"">    <testSources dir=""src"" includes=""**/*Test*""/>		  </clover-log>		  {code}    works fine.    *Improvement:*    Try to use the [default test detection algorithm|https://confluence.atlassian.com/display/CLOVER/Clover+test+detection] in order to find test classes. It would work in the same way as [<clover-setup>|https://confluence.atlassian.com/display/CLOVER/clover-setup] task when the <testsources> tag is not defined.   "
0,Errors in ErrorLog when toggling between projects with Dashboard view active,"1) Toggle between cloverized projects. Dashboard visible.    Message: Unexpected URI in the Dashboard view: C:\Users\Marek\.workspace-eclipse-3.4.2\Moneybags\.clover\report\dashboard-eclipse.html    {noformat}  java.net.URISyntaxException: Illegal character in opaque part at index 2: C:\Users\Marek\.workspace-eclipse-3.4.2\Moneybags\.clover\report\dashboard-eclipse.html    java.net.URI$Parser.fail(Unknown Source)    java.net.URI$Parser.checkChars(Unknown Source)    java.net.URI$Parser.parse(Unknown Source)    java.net.URI.<init>(Unknown Source)    com.cenqua.clover.eclipse.views.dashboard.DashboardLocationListener.changing(DashboardLocationListener.java:62)  {noformat}      2) Right click on a closed project, which  was cloverized before. Dashboard view visible.     Message: Error creating the dashboard  {noformat}  java.lang.NullPointerException    com.cenqua.clover.eclipse.views.dashboard.DashboardGenerator.execute(DashboardGenerator.java:41)    com.cenqua.clover.eclipse.views.dashboard.DashboardView.generateReport(DashboardView.java:100)    com.cenqua.clover.eclipse.views.dashboard.DashboardView.projectSelected(DashboardView.java:91)    com.cenqua.clover.eclipse.views.dashboard.DashboardView.selectionChanged(DashboardView.java:130)    org.eclipse.ui.internal.AbstractSelectionService.firePostSelection(AbstractSelectionService.java:179)    org.eclipse.ui.internal.AbstractSelectionService$2.selectionChanged(AbstractSelectionService.java:71)  {noformat}      3) Cloverized project selected in Package Explorer, Dashboard view opened. Close the selected project.    Message: Problem generating Dashboard report    {noformat}  java.lang.NullPointerException    com.cenqua.clover.eclipse.views.dashboard.DashboardView.ensureReportFolderCreated(DashboardView.java:107)    com.cenqua.clover.eclipse.views.dashboard.DashboardView.generateReport(DashboardView.java:97)    com.cenqua.clover.eclipse.views.dashboard.DashboardView$1.run(DashboardView.java:153)    org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)  {noformat}  "
0,Fix ruler coloring when include coverage from passed only is enabled,"Eclipse. Java Editor. Left ruler showing coverage colours (green/yellow/blue). Rules shows ""yellow coverage"" when the ""include from passed only"" option is enabled. There should be no yellow colour in such case.     Note: source code is highlighted correctly, metrics are fine too.    See screenshot."
1,Highlight whole method body if method is not covered,"If given method was not executed at all, the HTML report shows red highlighting only for method header. It's not a bug in code - our Velocity templates are just written this way. We should highlight the whole method body in red - in the same way as Eclipse/IntelliJ does. See screenshot. "
0,Multiple test cases in Test Run Explorer in IDEA,"Quickly click 'run' button in order to execute multiple sets of unit tests in parallel. IDEA runs them in parallel, as a consequence we've got duplicated test cases on a list. See attached screenshot. Affects all supported IDEA versions.     We should have test cases from last test run only."
1,Multiple test cases in Test Runs view in Eclipse,"If unit tests are executed several times since the last build, the Test Run view shows multiple test results. Not a bug actually, but it could be improved by showing last results only. See screenshot. Affects all supported eclipse versions. "
0,Cloud Report in IDEA shows interfaces,"Cloud Report in IDEA is inconsistent with a Cloud Report in Eclipse - it shows interfaces, which does not actually have any executable code. It shows also annotations.    Fix: don't list objects (interfaces, classes) which do not have any statements in the Cloud Report. Note that simple check if !isInterface() is not sufficient as interfaces in Java8 will have default methods with code."
0,Check Now button does not work if project is not opened in IDEA,"Install the latest Clover plugin version in IDEA. Click Settings > IDE Settings > Clover > Auto Update > Check now button.     If project is opened then pop-up is shown that the latest version is already installed. If no project is opened (i.e. you're on a IDEA's welcome page), pop-up is not shown. See attached images.    Problem occurs on IDEA 11.x and 12.x. "
0,Test optimization is broken in IDEA11 and above,"JetBrains have changed sth in IDEA 11 regarding unit test run configuration, as a consequence Clover does not ""see"" and thus does not optimize all unit tests. Problem does not occur on IDEA 9.x, 10.x, 10.5.x. It is present on IDEA 11.x and probably also on 12.x.    See attached images."
1,IDEA12 JavaSourceTransformingCompiler functionality has been removed,"Since build 122.694 Clover 3.1.8 cannot work with IDEA12 EAP, because the ""external"" build functionality was introduced and the JavaSourceTransformingCompiler is no longer supported. See issue    http://youtrack.jetbrains.com/issue/IDEA-94612"
0,Don't spam the console with println anytime clover grails plugin is installed,"From looking at the _Events.groovy hooks in the plugin, there are several println statements which will execute in all contexts, not just testing and not just when clover.on is specified. Specifically, the set classpath and compile start/end hooks will happen everytime any Grails command is run.    This is bad practice for a plugin. The println statements should be changed to use the grailsConsole which is available automatically    println ""foo"" => grailsConsole.log ""foo""    or grailsConsole.updateStatus ""foo""    Secondly, this should probably be wrapped in a helper which only prints if the the clover is enabled, ie:    def logStuff(msg) {      if( config.on ) grailsConsole.log msg  }"
2,Aggregate coverage by current author and commit regex,"I love CIJ.  When I use git, I find myself making a lot of commits, and I want to see my test coverage aggregated by the jira ticket I've put in my commit log.  For example, let's say I'm working on a project for 3 days.  I might make 50 commits.  All throughout my development, I'd like to keep a running track of how the test coverage is *for my code, bounded by a commit regex*.  In other words, I'd like to use something like git blame, married with coverage, to give me feedback like ""Hey Zim, the feature you're working on only has 52% coverage"".  Awesome-er still would be something like the class coverage pane, but using only lines I've changed.  So I could have a window that shows the classes I've changed, and a summary of the coverage for the lines that I've changed, again bounded by a the jira ticket in my commit logs.    This would be crazy to do in something other than a lightning fast DVCS like git or mercurial...but I have totally drunk the git cool aid."
0,The <testsources> tag from <clover-setup> is not passed to groovyc,"As in subject. As a consequence, the DefaultTestDetector is always used for Groovy code.     *Fix:*    Build proper TestDetector instance in GroovycSupport, which will be next serialized into InstrumentationConfig (instrumentation.ser file), so that next the Grover can deserialize it and pass to InstrumentingCodeVisitor.  "
1,Create ClassInfo objects for anonymous inline classes,"During implementation of CLOV-1162 I've found that anonymous inline classes does not have their own ClassInfo object crated. Instead of this, methods of inline class are added to the enclosing class.    For instance, a following code:    {code:java}  public class AggregatedMetrics {      /**       * Inner class, case with a method having an inline anonymous class       * statements = 3 (sum of direct methods' statements)       * aggregatedStatements = 5 (sum of direct methods' aggregated statements)       */      class C {          /**           * Method: statements = 2; aggregatedStatements = 4 (method's statements + inline class aggregated statements)           */          Iterator methodThree() {              int d = 4;              /**               * Inline class: statements = aggregatedStatements = 2 (sum of its methods)               */              return new InstrumentationSessionImplTestSample2() {                  /** statements = aggregatedStatements = 1 */                  public boolean hasNext() {                      return false;                  }                    /** statements = aggregatedStatements = 1 */                  public Object next() {                      return null;                  }                    /** statements = aggregatedStatements = 0 */                  public void remove() {                    }              };          }            /** statements = aggregatedStatements = 1 */          void methodFour() {              int e = 5;          }      }  }  {code}    is stored in clover database in this way:    {code:xml}  <class name=""AggregatedMetrics.C"" qualifiedName=""AggregatedMetrics.C"">  	<metrics  statements=""5"" aggregatedStatements=""5""/>  	<method name=""hasNext() : boolean"">  		<metrics  statements=""1"" aggregatedStatements=""1""/>  	</method>  	<method name=""next() : Object"">  		<metrics  statements=""1"" aggregatedStatements=""1""/>  	</method>  	<method name=""remove() : void"">  		<metrics  statements=""0"" aggregatedStatements=""0""/>  	</method>  	<method name=""methodThree() : Iterator"">  		<metrics  statements=""2"" aggregatedStatements=""2""/>  	</method>  	<method name=""methodFour() : void"">  		<metrics  statements=""1"" aggregatedStatements=""1""/>  	</method>  </class>  {code}    It means that on reports the AggregatedMetrics.C class is presented as having 5 methods, while it has 2 actually.     It's not a bug. There was a design decision to not present anonymous classes in a report.     Possible improvement:   * create ClassInfo objects for anonymous classes   * add option for reporting whether to show anonymous classes (like InstrumentationSessionImplTestSample2$1) or not  "
0,as a developer I don't need to run Clover on JDK1.4 anymore,"*Idea:*    Sun Java 1.4 has:   * end of public updates - Oct 2008   * end of Premier Support - Feb 2010   * end of Extended Support - Feb 2013   * see http://www.oracle.com/technetwork/java/eol-135779.html    IBM Java 1.4:   * is no longer Generally Available (GA) since Dec 2004;    * its End Of Servcie (EOS) will expire on September 2013:   * see https://www.ibm.com/developerworks/java/jdk/lifecycle/      According to various sources, most popular are Java6 and Java7; Java1.4 has a fraction of market right now:   * http://www.statowl.com/java.php  although the report above shows Java version in web browser. Production environment may be different (especially regarding IBM Java).      *Benefits:*    Dropping support for Java 1.4 would ease Clover development a lot, as we could:   * directly use Java5 generics in code   * replace raw collections (List, Map, Set etc) by typed collections (List<Foo> etc) in code    * get rid of Retrotranslator (2x faster build )   * it opens a door to introduce generic-based libraries in Clover code (for example replace Apache Commons by Guava)    *Tasks:*     * update Supported-Platforms page   * remove retrotranslator from build scripts   * update bamboo / maven configurations   * affects Clover, Clover-for-Maven2&3, Clover-for-Eclipse, Clover-for-IDEA  (note that Clover-for-Grails already need JDK1.5 or above)      *Out of scope:*     * replacing existing raw collections by typed ones (like List -> List<Foo> ), introducing foreach loops etc    ** this will be done gradually during daily development    "
1,Groovy 2.x @CompileStatic annotation causes static methods to be ignored by Clover,Adding @CompileStatic to a tested Groovy class results in 0% coverage for static methods.    e.g.  {code}  //All ok here  class MyClass{    String sayHello(){      'hello' //Full coverage shown by Clover    }       static String sayHelloAgain(){      'hello again' //Full coverage shown by Clover    }  }    //Same class with the annotation  @CompileStatic  class MyClass{    String sayHello(){      'hello' //Full coverage shown by Clover    }       static String sayHelloAgain(){      'hello again' //***No coverage shown by Clover***    }  }  {code}  Can anyone confirm this? Thanks.  
0,Add testSources parameter for clover2:clover mojo,"The Ant <clover-report/> task has an optional nested <sourcepath> parameter where user can specify an Ant path that Clover should use when looking for source files.     Thanks to this, it is possible to generate report using sources from a different location (on a different machine, for instance).     Such option is missing for Maven clover2:clover MOJO. The workaround which can be used is to create a custom report descriptor, as described on https://confluence.atlassian.com/display/CLOVER/Creating+custom+reports, which is not convenient actually.    Fix: add <sourcepath> parameter for clover2:clover MOJO."
1,Perform migration of CLMVN and CLMVNONE from studio.plugins.atlassian.com,"The site studio.plugins.atlassian.com is deprecated. All new projects shall be migrated to bitbucket.org or eventually to ecosystem.atlassian.net.     *Deadline:*      * final deadline 5.03.2013    * it's recommended to finish it before 5.02.2013    *Scope:*    -1) Migrate SVN repositories-     * https://studio.plugins.atlassian.com/svn/CLMVN   * https://studio.plugins.atlassian.com/svn/CLMVNONE    -to Mercurial on Bitbucket-      -2) Migrate Bamboo builds (if necessary) from-      * https://studio.plugins.atlassian.com/builds/browse/CLMVN-TRUNK    -Note: CLMVNONE has no builds configured-    -to new Bamboo location.-      -3) There is no need to migrate Wiki pages and Code reviews (practically empty).-      -4) Migrate JIRA issues from-     * https://studio.plugins.atlassian.com/browse/CLMVNONE   * https://studio.plugins.atlassian.com/browse/CLMVN    -to-      * https://jira.atlassian.com/browse/CLOV     -if this is feasible and set component ""Maven 1 Plugin"" and ""Maven Plugin"", respectively.-    -Alternatively, create new issue trackers on Bitbucket or Ecosystem, copy all issues ""as is"" and set projects read-only with a message that issue tracking shall be done in JAC CLOV project.-      -5) Update build scripts (Maven / Ant) and handle:-   * -new repository type (tagging)-   * -new way of deployment of binaries (optional)-    -Update also BEAC builds.-    "
0,Compile maven-clover2-plugin using JDK1.5,Currently we have two executions of maven-compiler-plugin:   * one with source/target=1.3 for src/main   * one with source/target=1.5 for src/test    Use source/target=1.5 for the whole module. 
0,Update Supported-Platforms page,On http://confluence.atlassian.com/display/CLOVER/Supported+Platforms:   * set JRE/JDK = 1.5 or later   * add footnote that support for JDK 1.5 has ended with 3.1.x  
0,Remove retrotranslator,Remove all retrotranslator stuff:   * jar files (several extlib locations)   * build.xml and upgrade.xml files   * about boxes in Eclipse and IDEA   * etc/rights license files  
0,CLMVNONE new property: maven.clover.instrumentationLevel,"Moved from https://studio.plugins.atlassian.com/browse/CLMVNONE-14    This should only be set to 'method' if used in conjunction with Test Optimization if the default level (statement) is not performant enough.  Valid values are ""statement"" and ""method""."
0,"CLMVNONE Use includes, instead of nested include to allow more than one pattern when merging","Moved from https://studio.plugins.atlassian.com/browse/CLMVNONE-9    For Maven1 plugin: It would be better to change the following code in the clover:merge goal from:    {noformat}      <ant:cloverDbSet dir=""${_multiproject_basedir}"">      <ant:include name=""${maven.clover.merge.databases}""/>      </ant:cloverDbSet>  {noformat}        to:    {noformat}      <ant:cloverDbSet dir=""${_multiproject_basedir}"" includes=""${maven.clover.merge.databases}""/>  {noformat}    "
1,CLMVNONE Integrate Distributed coverage,Moved from https://studio.plugins.atlassian.com/browse/CLMVNONE-13    Integrate Distributed coverage with Clover-Maven1-Plugin  
0,Show warning if new update is out of scope of the current license,"Case:   * user has a valid license key with a certain end-of-maintenance date   * new Clover version is released but the release date is later than maintenance end   * IDEA / Eclipse gives user an ability to upgrade but   * after upgrading user cannot use Clover anymore (!)    Improvement:   * check end-of-maintenance date from current license key, if it is < than release date of the upgrade we want to download, put additional warning about it the the upgrade dialog box"
1,Auto update version check reports new version when version strings are identical,"Under File > Settings > IDE Settings: Clover > Auto Update, when you press ""Check now"" a window pops up saying:    ""New version available: 3.1.10.v20130108000000, you have 3.1.10.v20130108000000  Would you like to upgrade?""    "
0,Add <profiles> element for <clover-setup> and <clover-instr>,"Enhance Ant tasks by adding new sub-types and attributes:    {code:xml}  <clover-setup> <!-- the same for <clover-instr> -->    <profiles>   <!-- AntCloverProfiles with List<AntCloverProfile> -->      <profile name=""default"" coverageRecorder=""..."">  <!-- copied as CloverProfile to InstrumentationProfileConfig -->              </profile>      <!-- more profiles possible, but we will handle ""default"" only right now -->    </profiles>  </clover-setup>  {code}    coverageRecorder   * fixed - default value as now, fixed-size coverage recorder requiring presence of clover.db to read the size   * growable - like for Android, no need to have clover.db   * shared - for Grails app, not only with dynamic size, but also reusing the same instance of recorder if initstring is the same    Parse it on Ant level, pass it to InstrumentationConfig and Instrumenter.   "
0,Document <profiles> element on CAC,Write documentation for:   * http://confluence.atlassian.com/display/CLOVER/clover-setup   * http://confluence.atlassian.com/display/CLOVER/clover-instr   * https://confluence.atlassian.com/display/CLOVER/Advanced+setup+configuration  
0,Add profiles for InstrumentationConfig,NULL
1,Embed clover profile array in instrumented classes,"Enhance existing instrumentation and embed an array of CloverProfile in instrumented sources/classes, pass this array to Clover.getRecorder().    Java: source based instrumentation  RecorderInstrEmitter + Clover.getRecorder()     - add static field for _CLR class of *CloverProfile[]* type    Groovy: byte code instrumentation     - instantiate new CloverProfile[] { new CloverProfile(), ... } array at getRecorder() call    "
1,Use GrowableBitSet for PerTestRecorder.One,"In PerTestRecorder.One - wrap 'coverage' and coverageShortcut into a helper class (x)    set() method - if index is out of bounds then it shall resize 'coverage' BitSet and 'coverageShortcut' array (/)    refactor PerTestRecorder.Any, None, One, Many to top-level classes (/)    refactor ThreadVisibilityStrategy to top-level class (/)    write unit tests for (/)  ThreadVisibilityStrategy.SingleThreaded  ThreadVisibilityStrategy.Synchronized  ThreadVisibilityStrategy.Volatile  "
0,Create GrowableBitSet class,"extend CloverBitSet,     add() method shall check index vs size() and call growToInclude();     make it thread-safe - check how antlr.collections.impl.BitSet.growToInclude() is implemented  "
0,Dynamically select coverage recorder based on clover.profile,"Clover.getRecorder():    Read -clover.profile system property and if securityexcetion or null then fallback to ""default"".    If list of profiles is empty or given profile name was not found, fallback to fixed coverage recorder.    Should read coverageRecorder value and instantiate proper recorder type    Should fetch from cache using one of:    recorderKey = initString + """" + dbVersion + """" + cfgbits;  recorderKey = initString;    Add debug log messages for actions taken.    Prepare necessary tests for java and groovy (assert on log messages).  "
0,Add 'clover.enabled' system property,If set to 'false' it should disable recording of the coverage data at runtime.
3,as a developer I'd like not to deploy clover.jar to app server,"*Embed clover-runtime.jar into created JAR/WAR/EAR*    *Problem:*    Instrumenting code by Clover is not enough, user has to manually add clover.jar to the runtime classpath. Otherwise it gets ""ClassNotFoundException com_cenqua_clover/CoverageRecorder"" error. There are over 6'000 views on answers.atlassian.com about this problem (it's #1 on the list)    *Solution:*    Create a possiblity to automatically or manually embed clover-runtime.jar into produced JAR/WAR/EAR artifacts.     *Tasks:*    1) (/) Restructure Clover modules and find exact set of runtime classes, create clover-runtime.jar artifact out of it (will be published on Maven Central). Such subset was already found for Clover-for-Android, but it does not have classes for Distributed Coverage feature. See linked issue.     2) Create Ant task like <clover-embed-runtime-jar targetJar=""my.jar""/> which will allow to manually put runtime classes into it.    3) Check if it would be possible to intercept all <jar/> calls as well and do it automatically. Add option to disable this automatic integration and/or to specify regular expression pattern which jars shall be enhanced.    4) Create Maven goal like clover2:embed-runtime-jar with <targetJar> property.    5) Check how to automatically embed Clover runtime classes. Possibilities:   * extract clover-runtime.jar into target/classes or target/classes-clover   * scan build reactor, intercept packaging tasks like maven-ear-plugin, maven-jar-plugin, maven-rar-plugin, maven-war-plugin, maven-assembly-plugin   * modify maven project at runtime and add clover:clover-runtime dependency    6) Eclipse integration - we already have -Xbootclasspath for ""Run with Clover as..."" but it's not enough if user has web application for example. Extract clover-runtime.jar into project's build folder so that other packaging tools could automatically pick these classes. Add pre-build or post-clean event handler to make sure that these classes will be still present after rebuild. Add some option in Window > Preferences or Project > Properties for this.    7) IDEA integration - put these classes into out/production and out/test. Add option in File > Settings > Project Settings or File > Settings > IDE Settings for this.    8) Grails integration - t.b.d."
1,Split clover-core into clover-runtime + clover-core,"1) Use class dependency analyzer. Check for classes loaded via reflections too!!! (/) _class dependency analyzer was also used to strip third party libraries from unused classes_    2) Move runtime classes to clover-runtime module. (/)    3) -Rename packages to com.atlassian.clover.runtime.* but keep few core classes in com_atlassian_clover. Reason: Clover-for-Ant will still have single clover.jar probably and it will use FileInputStream(clover.jar) to dynamically extract classes form archive so we must be able to distinguish them.- (/) _cancelled; too many changes in referenced classes and runtime still ""borrows"" few classes from core_    4) Enhance build script to publish this as com.atlassian.clover:clover-runtime artefact on Atlassian Central and Maven Central. (/)    Note:   - for backward compatibility clover-core shall still contain runtime classes and it's name will not change (com.atlassian.clover:clover) (/)   - -we can add dependency to clover-runtime in pom.xml, however - just to keep programmers informed about this change- (/) _dependency not added as clover-core already bundles the clover-runtime_    5) Update documentation on CAC and inform that new, small artifact is available since version 4.0.x. (/) _done_    6) Update UML in clover-maven-module-structure.graphml (/) _done_    7) Test it with Clover-for-Android (original clover-core exceeds method limit in Dalvik image)  (/) _couldn't test as Android Studio uses Gradle now and is based on IDEA not Eclipse_"
1,Create <clover-embed-runtime> task,"Create Ant task:  {noformat}  <clover-embed-runtime targetFile=""jar/war/ear file""/>  {noformat}  which will put all Clover's runtime classes into targetFile.     Unzip all com_cenqua_clover.* and com.atlassian.clover.runtime.* classes from clover.jar archive which is currently on a classpath (we've got a trick how to find it - see searching for clover.license) into a temporary directory and next put them into jar (use Ant <jar/> task for this or handle zip directly)."
1,Create clover2:embed-runtime-jar goal,Create MOJO for:    {noformat}  clover2:embed-runtime-jar  {noformat}    which will read the <targetJar> property and do the same what Ant task does.    Difference: we might use maven-assembly-plugin and download clover-runtime artifact for this.
1,Maven extracts clover-runtime into target/classes or target/classes-clover,"Automatic integration for unknown plug-ins: extract clover-runtime artifact into current build folder (target/classes for clover2:setup and target/classes-clover for clover2:instrument). Thanks to this these runtime classes might be automatically picked up and packaged. Check if there is any  Maven's build property holding list of classes which have to be updated.    Add configuration option for this, like:  {noformat}  extractCloverRuntimeIntoTargetClasses=true  {noformat}  "
1,Maven: intercept maven-jar-plugin and add clover-runtime,"Check if it would be possible to define some Maven property for maven-jar-plugin which would allow to define directory or list of files to be added to the archive. If yes, then extract clover-runtime into e.g. target/clover/runtime (note: it shall not interfere with CLOV-1241 somehow) and pass it to maven-jar-plugin. "
1,Maven: intercept maven-ear-plugin and add clover-runtime,The same as CLOV-1242 but for maven-ear-plugin
1,Maven: intercept maven-rar-plugin and add clover-runtime,The same as CLOV-1242 but for maven-rar-plugin
1,Maven: intercept maven-war-plugin and add clover-runtime,The same as CLOV-1242 but for maven-war-plugin
1,Maven: intercept maven-cargo-plugin and add clover-runtime dependency,The maven-cargo-plugin is the most popular plugin for running in-container tests. Add <dependency> for clover-runtime there. Add configuration option to have a possibility to disable this.
1,Eclipse: extract clover-runtime classes into build directory,Extract clover-runtime.jar into project's build folder so that other packaging tools could automatically pick these classes. Add pre-build or post-clean event handler to make sure that these classes will be still present after rebuild. Add some option in Window > Preferences or Project > Properties for this.
1,IDEA: extract clover-runtime classes into build directories,IDEA integration - put these classes into:   * out/production and    * out/test.     Add option in File > Settings > Project Settings or File > Settings > IDE Settings for this.
1,Grails: extract clover-runtime into build directory,Usually it's here:    <home>\.grails\X.Y.Z\projects\<project_name>\classes
1,Prepare integration tests for GrowableCoverageRecorder,Integration tests like:     * GrowableCoverageRecorder on a large code base     * GrowableCoverageRecorder with different strategies     ** PerTestRecorder.One / Many / None     ** ThreadVisibilityStrategy.SingleThreaded / Volatile / Synchronized    => especially check if/how per-test code coverage is affected by multithreaded tests of multithreaded application     * GrowableCoverageRecorder with DistributedCoverage  
1,Prepare integration tests for SharedCoverageRecorder,"Integration tests like (all with partial global and per-test coverage):     * SharedCoverageRecorder with single InstrumentationSession and many classes     * SharedCoverageRecorder with multiple InstrumentationSessions, classes from sessions running together     * SharedCoverageRecorder with multiple clover databases (different initstrings)      * SharedCoverageRecorder with the same class compiled multiple times, different versions are running with coverage measurement,  "
1,Some of per-test recording strategies do not work for multi-threaded applications,"*Introduction*    We've got five per-test recording strategies:    * Diffing - clover.pertest.coverage=diff  * Null - clover.pertest.coverage=off  * SingleThreaded  - clover.pertest.coverage=<empty> and clover.pertestcoverage.threading=<empty> - DEFAULT POLICY  * Synchronized clover.pertest.coverage=<empty> and clover.pertestcoverage.threading=synchronized  * Volatile - clover.pertest.coverage=<empty> and clover.pertestcoverage.threading=volatile    *Current status*    1) A SingleThreaded does not work with multi-threaded apps. Of course it's designed for single thread only. But at least it should have a correct value of coverage for a test case from a current thread.     Expected number of hit counts is between:    {noformat}   hits(current test) <= ... <= hits(curent test + all other concurrent tests)  {noformat}    but currently actualHits can be < than hits(current test) due to fact that CloverBitSet.set\(n\) is not thread-safe.    2) A Volatile strategy has a similar problem. CloverBitSet.set\(n\) is not thread safe. Starting/ending of test slices works fine.    3) A Synchronized is correct because we've got 'CloverBitSet.set\(n\)' wrapped in a synchronized block. Starting/ending of test slices works fine.    4) A Diffing needs more tests.     *Possible fix*    CloverBitSet could have two variants:   * compact - store hits as bit mask in long[] array - set\(n\) is not thread-safe, can be used for report generation as it consumes less memory   * exploded - store hits as boolean[] array - set\(n\) \{ arr[n] = true \} would be thread-safe, used for runtime recording; it consumes 8x more memory but on the other hand we usually don't have 10'000 tests running in parallel; number of parallel tests is ~ number of processor cores available  "
1,Add Ant dependendency to com.cenqua.clover:clover maven artifact,"Problem occurs in Gradle framework. When developer declares dependency to Clover artefact and tries to instrument project with Clover, it fails with a not-very-informative error:    {noformat}  [clover-setup] Detected groovyc compiler org.codehaus.groovy.ant.Groovyc    [groovyc] Compiling 1 source file to .......    [groovyc] ERROR: Clover-for-Groovy encountered an error while loading config:  {noformat}    Debug logging reveals more information:    {noformat}    [groovyc] ERROR: Clover-for-Groovy encountered an error while loading config: Clover-for-Groovy encountered an error while loading config:  : org/apache/tools/ant/BuildException    [groovyc] java.lang.NoClassDefFoundError: org/apache/tools/ant/BuildException    [groovyc]   java.lang.Class.getDeclaredFields0(Native Method)    [groovyc]   java.lang.Class.privateGetDeclaredFields(Unknown Source)    ...    [groovyc]   com.atlassian.clover.instr.java.InstrumentationConfig.loadFromStream(InstrumentationConfig.java:439)    [groovyc]   com.atlassian.clover.instr.groovy.Grover.newConfigFromResource(Grover.java:241)    [groovyc]   com.atlassian.clover.instr.groovy.Grover.<init>(Grover.java:210)  {noformat}    A reason is that normally this code is being called from Ant so the ant.jar is provided. In Gradle it's not available by default.    Workaround: add ant.jar dependency in the project build file.  "
0,Prepare code example for Grails,"Prepare sample Grails project with domain classes, controllers and unit tests which will use the 'shared' coverage recorder."
1,Add clover.coverageRecorder field for Clover-for-Grails plugin,"*A follow-up of CLOV-1189:*    Support the coverageRecorder natively in the plugin config. It would be nice to go back to using the simpler configuration instead of the setuptask/reporttask closures. Like:    {noformat}  clover {    coverageRecorder: 'SHARED'  }  {noformat}    *Implementation:*     If coverageRecorder field is defined, then create a single profile named ""default"" with coverageRecorder as specified and undefined distributedCoverage.    In case clover.setuptask is specified, then settings from this closure will override clover.coverageRecorder value.  "
1,Upgrade third party libraries used by Clover,"Upgrade third party libraries to their latest versions:   * jebrains annotations 13.0 (/)   * ASM 5.0  (/) we're using ASM 3.0, 4.1 and 5.0 actually (for different purposes)   * commons-codec 1.9 (/)   * commons-collections 3.2.1 (/)   * commons-lang 2.6 (/)   * guava 18.0 (/)   * JCommon 1.0.23 (/)   * JFreeChart 1.0.19 (/)   * Log4J 1.2.17 (/)    * Velocity 1.7 (/)   "
1,Reduce friction in tutorial,"Make Clover evaluation easier by simplifying the tutorial. The build.xml shall contain full Clover set-up so that just typing ""ant"" shall produce all reports. So swap build.xml <-> build_completed.xml, fix ""test"" target name, add default=""clover.all"" etc.     Futhermore, Ant and Maven configuration in tutorial shall be unified - use 'target' directory for both of them. Get rid of clutter like 'clover', '.clover', 'build' directories.     Update tutorial page on CAC.    Consider adding groovy-all-1.7.0.jar into /lib directory so that user won't need to define GROOVY_HOME path."
0,Add url for 'utils.js' home page,"In Clover-for-Eclipse and Clover-for-IDEA ""About"" dialogs the Utils.js link does not open the library home page. "
0,"PDF report with ""include failed test coverage"" option fails","Eclipse    Install Clover-for-Eclipse, open tutorial project, select Run new report > PDF report > select ""Include failed test coverage"" > click Finish.    PDF report generation fails with error log message:    ""The JVM report process failed with error code 1 - see log for details""    !pdf_fail.png!    Increasing heap size from 512M to 768M solved the problem. To do: check why such small project requires so much memory.    Check also PDFReporter, Clover-for-IDEA."
1,IDEA Darcula theme ignores <table border=0>,"Clover's ""About"" dialog looks ugly when the ""Darcula"" theme is selected in IDEA 12. Despite having <table border=""0""> in JEditorPane(""text/html""), it shows table with a border. Example:    !clover_about_idea12_themes.png!"
1,Race condition in IDEA during full rebuild and test execution,"Clover-for-IDEA. Race condition during database cleanup and running unit tests at the same time.     Steps to reproduce;    1. Open ""Moneybags"" tutorial project  2. Prepare 'run all unit tests' configuration  3. Click ""Delete Coverage Database"" button in the ""Cloverage"" view  4. Click ""Rebuild now"" in the prompt dialog  5. Quickly click ""Run (Shift+F9)""    Problem:    Project is being rebuilt and clover.db is recreated, at the same time unit tests are compiled and also clover.db is being modified. There's a warning in console log like this:    {noformat}  WARN: CLOVER: Clover database: 'C:\Work\release\testing-sandbox\clover-ant-3.1.11\tutorial\.clover\coverage.db' is no longer valid. Min required size for currently loading class: 172, actual size: 96  WARN: CLOVER: Coverage data for some classes will not be gathered.  Tests taking too long? Try Clover's test optimization.  2 test classes found in package ''  {noformat}  "
1,Performance problem with SHARED coverage recorder in Grails app,"Follow up of CLOV-1189.      {noformat}      setuptask = { ant, binding, plugin ->          ant.'clover-setup'(initstring: "".clover/common.db"") {              ant.fileset(dir: ""grails-app"", includes: ""**/domain/**, **/controllers/**, **/jobs/**, **/services/**, **/taglib/**, **/utils/**"") { }              ant.fileset(dir: ""src"", includes: ""**/*.groovy, **/*.java"", excludes: ""**/script@*.groovy, **/system/generator/**, **/system/database/generators/**, **/castor/**"") { }              ant.fileset(dir: ""test"", includes: ""**/*.groovy"") { }              // ant.testsources(dir: ""test"", enabled: false) { }              ant.profiles {                  ant.profile(name: ""default"", coverageRecorder: ""SHARED"")              }          }      }  {noformat}    If I uncomment the testsources enabled:false line, it runs about as quickly as before. Normally our coverage run takes about 42 minutes. With the above configuration it's been running for about 3 hours and is about half finished."
1,as a developer I'd like to track build events in IDEA12 external build,"*Prepare empty plugin for external build.*    analyze external build API, prepare a simple plugin which will log compilation progress (module name, file name being compiled) and send notifications about it to IDE; it should also notify about ""compilation finished"" event."
2,Check how clover.db can be accessed from build server and IDE processes at once,"probably there are two possible ways to interact with build server process and the IDEA IDE process:    1) build server performs (parallel) instrumentation in memory and at the end of the whole build writes or updates clover.db (it has write access); next it sends notification to IDE which refreshes views and editors; potential problems: deletion of database during build, locking files    2) build server performs (parallel) instrumentation sending information about code structure back to IDEA IDE, which manages the database and sends back indexes of elements; potential problems: high messaging overhead -> need to send at most one message per file -> need to introduce local offset constant per each file;"
1,Check how parallel instrumentation can be handled,"External build supports parallel compilation. Modules are built in parallel, while files in a single module are compiled sequentially.     Clover expects that at least a single file is instrumented sequentially (the FileInto getCoverageMask which returns bitmask based on index range), so that the clover database contains continuous range of indexes for code elements from a single file. It means that we need to synchronize per file. It should not be a problem that subsequent files in database are from different modules.     possible solutions:    1) simple lock per whole file in some synchronized block; drawback: turns parallel build into single-threaded one.    2) batch operations, like first instrument whole file using relative indexes, next lock database and store information about whole file, database would return base offset value; still in sychronized block but locked for shorter amount of time;    3) change clover.db structure so that elements could be registered in any order; change FileInfo getCoverageMask function (and related); check for any data structures which needs to be made thread-safe;  "
2,Implement clover.db sharing between build server and IDE processes,Based on analysis
2,Implement parallel compilation handling,Based on analysis of CLOV-1267
2,Write sequential integration tests for build server,Prepare integration tests for external build feature. Test the scenario when 'parallel compilation' for 'external build' is disabled.    Check if any helper test classes for it are available in IDEA code based. 
1,Test IDEA IDE - build server interaction,Exploratory tests in user interface.
1,Reduce coverage requirements for groovy safe operator,"Currently any usage of the groovy safe operator requires that that reference be tested with a null and non-null value.  This adds significant overhead in the case where multiple calls are chained together with the safe operator.  It would be more useful as a coverage measurement if the chain was treated as one call.      Look at the following line as an example.    def value = a.getB()?.getC()?.getD()?.getE()    This line of code should require a test that sets value to null and a test that sets value to something non-null.  It should not however require a test where B is null a test where c is null a test where d is null and a test where e is null to reach 100% conditional coverage.    While I hope that lines that look like the above are not written very often, it still could happen, and having to write a bunch of tests to cover one line seems like a heavy burden.    "
0,Document how to configure Clover + Bamboo with 'mvn deploy' goal on CAC,"The problem is that if 'mvn deploy' is used with Clover, it will deploy instrumented JARs into repository. There's no documentation on confluence.atlassian.com in the BAMBOO space how such separation can be achieved.       Create a page with a description like:      If you use ""Automatically integrate Clover into this build"" option, then Bamboo will add clover2:setup + clover2:clover goals into every Maven task found in the Job (more details can be found here). It means that you shall not use ""mvn deploy"" together with automatic Clover integration.    There are at least three ways to solve this problem:    1) Create a separate Plan (or Job) in which automatic Clover integration is enabled and the Maven 2.x task does not deploy artifacts (""mvn verify"" is used for instance). So this plan would be used only for Clover reporting.    Benefits: full Clover separation from the Plan (or Job) performing deployment    Drawbacks: doubled Plan (or Job)    or    2) Use manual Clover integration (""Clover is already integrated into this build"") but run Clover in a parallel build lifecycle. It means that you'd have to configure Clover goals manually in the pom.xml and use the ""clover2:instrument"" instead of ""clover2:setup"". You could have then just one Maven task with ""mvn clean deploy"". Instrumented jars would have -clover.jar suffix.    Benefits: build is executed only once    Drawbacks: sometimes there might be problems with resolving correct (""cloverized"") dependent artifacts for multi-module projects or deploying the instrumented war to test server    or    3) Use manual Clover integration, but run Clover in a default build lifecycle. It means that you'd have to configure Maven task manually and use ""clover2:setup"" goal. Then you have to use two Maven tasks - one with ""... clover2:setup test ..."" and one with ""deploy"". This is an approach you have followed (according to screenshots I see in attached pdf).    Benefits: no problems with artifact dependencies    Drawbacks: two Maven tasks     Recommendation: approach #3.     Steps for approach #3:     clean clover2:setup test clover2:aggregate clover2:clover    b) define ""Clover Report"" artifact    On the 'Artifacts' tab, click Create Definition and complete the form as follows:  Name 	This should begin with with ""Clover Report"".  Location 	This should point to the HTML report directory (e.g. target/site/clover)  Copy Pattern 	Use **/*.*    More details on https://confluence.atlassian.com/display/BAMBOO/Enabling+the+Clover+add-on#EnablingtheCloveradd-on-ManualCloverintegration    "
1,Error when trying to Check for an Update,I just recently upgraded from IntelliJ IDEA 10.5.4 to 12.1.1 and now 12.1.2.  I think my last update to Clover was BEFORE I updated IDEA versions.
0,Improve message for NoSuchRegistryException,"See https://confluence.atlassian.com/x/AIP3F    A message ""Clover registry file: <path/to/clover.db> does not exist."" is too general and does not help developer to understand what the exact problem is. Find a more meaningful message. "
2,Write parallel integration tests for build server,Create IT for external build process with a parallel compilation enabled.
2,as a developer I'd like to build projects using 'classic' build in IDEA12,"Make sure that new 'external build' feature support does not break the old build functionality when the compilation is performed in IDEA IDE process.     It shall still use JavaSourceTransformingCompiler class, old Project structure, event handling etc. "
3,Code instrumentation using jps-builders,scope:  * java code instrumentation  * statement/method contexts  * test detection 
2,Implement communication between IDEA IDE and JPS builder," * notifications about build events, progress, error handling   * model refreshes"
1,Support custom metrics in PDF report,Currently the PDF report does not handle <columns> element so it's not possible to have a custom set of metrics in the summary. 
1,PDF report with class-level and method-level summary,The PDF report contains package-level coverage table. Add an option for:   * <clover-report> Ant task   * clover2:clover mojo   * 'generate report' button in Eclipse    * 'generate report' button in IDEA    which would allow user to select report detail level:   * package (default)   * class   * method    for PDF file format. 
2,Check how parallel instrumentation sessions can be handled,"External build will start/close single instrumentation session in which modules will be compiled in parallel (see CLOV-1267).    However, a parallel build in ant/maven might involve starting multiple instrumentation sessions at once (just think about <parallel> + <javac> in Ant for instance).     Check how we could deal with this. Possible problems:   * clover.db file locking (appending instrumentation session)    Areas for prototyping:   * clover db shared in memory   * clover db structure change - adding top-level index for sessions/files/classes etc, possibility to interleave data from several instrumentation sessions   * etc ..."
3,Implement Clover data serialization from IDE to JPS,"The following data must be serialized and passed to JPS builder:   * global configuration - other.xml (license key, sid, install date)   * project settings - <project>.ipr (flush policy, initstring etc ...)   * module settings - <module>.iml (exluding entire module from instrumentation)   * idea test detector (default test detector + all test source folders)   * state of some buttons in UI (FeatureManager) (e.g. 'toggle build with clover')"
1,Prepare documentation on CAC and AAC,"On Clover-for-IDEA pages:     * screenshot with ""Compiler"" settings page from IDEA   ** external build can be enabled   ** incremental build shall be disabled (recommended)   ** parallel build must be disabled (refer to future story)    On supported platforms page:   * info that IDEA12.0-12.1 + Clover 3.1.8-3.1.11 must have 'external build' disabled    Release notes about this new nice feature    Update quiestion on AAC"
1,HTML report shows only one test result per file if external XML JUnit files are used,"In the <clover-report> we can define <testresults> element which causes that test results are being read from JUnit-compatible XML files, instead of from Clover's coverage recording files.     This works as long as the test name can be mapped to the method name (which is usually the case). However, in case of the Spock framework, test cases are named using a free text like:    {noformat}  def ""check if this test does xyz""  {noformat}    which is later translated to a method named like:    {noformat}  $spock_feature_0_0  {noformat}    As a consequence Clover is not able to match test name to the method name (in order to link to sources). It uses an empty test id to store the test result (SLICE_ID = -1). If there are more than one non-matched method in the same XML file, all get the same ID=-1, thus overwriting each other in hashmap. As a consequence only one is listed.    *Fix:*    Generate a unique test id using the test name - a String.hashCode() for instance.  "
1,Contributed and unique coverage are equal,Possible bug. Investigate why these values are identical for MoneyBags tutorial.     !unique_and_contributed_coverage.png!
1,Find workaround or fix for IDEA-108852,See http://youtrack.jetbrains.com/issue/IDEA-108852
0,Add more values for a 'threads' drop-down in report dialog,"The 'Generate Report' dialog has max 4 threads:  !report_generation_threads.png!    In age of multi-core multi-threaded processors, we shall have it up to 16 at least. "
0,The 'include line info' toggle works in the opposite way,"This toggle    !include_line_info.png!    works exactly opposite: when selected, the XML report does NOT contain <line> tags; and vice versa."
1,Exclusion icon decoration is not visible in IDEA,Affects IDEA versions 10.5.x-12.1.x (works under IDEA 9.0.x). The toggle 'Annotate icons on included/excluded files ...' does not work:    !icon_decoration_do_not_work.png!
1,Class loader fails on clover.jar package-info,"Occurs in Bamboo, for instance.    {noformat}  2013-07-15 12:16:10,250 ERROR [http-9087-Processor12] [[default]] Servlet.service() for servlet default threw exception  java.lang.ClassFormatError: Illegal class name ""com/google/common/collect/package-info"" in class file com/google/common/collect/package-info          at java.lang.ClassLoader.defineClass1(Native Method)          at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)          at java.lang.ClassLoader.defineClass(ClassLoader.java:615)          at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)          at org.apache.catalina.loader.WebappClassLoader.findClassInternal(WebappClassLoader.java:1876)          at org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:889)          at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1353)          at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1232)          at java.lang.Class.forName0(Native Method)          at java.lang.Class.forName(Class.java:249)          at java.lang.Package.getPackageInfo(Package.java:352)          at java.lang.Package.getAnnotation(Package.java:367)          at org.codehaus.jackson.xc.JaxbAnnotationIntrospector.findAnnotation(JaxbAnnotationIntrospector.java:885)          at org.codehaus.jackson.xc.JaxbAnnotationIntrospector.findAccessType(JaxbAnnotationIntrospector.java:265)          at org.codehaus.jackson.xc.JaxbAnnotationIntrospector.findAutoDetectVisibility(JaxbAnnotationIntrospector.java:216)          at org.codehaus.jackson.map.AnnotationIntrospector$Pair.findAutoDetectVisibility(AnnotationIntrospector.java:1018)  {noformat}  "
2,as a Clover developer I'd like to use the latest version of ANTLR,"Clover uses ANTLR 2.x currently which is few light-years from the latest version, which is ANTLR 4.x. Upgrade to the latest ANTLR library version.     * check ANTLR 4.x license (it's different from 2.x) whether we can actually use it; if not - try to use 3.x, if possible   * check if there's any Java grammar (the best for Java 8) available for Antlr 4   * estimate whether it's better to use completely new grammar file and embed clover stuff there or to upgrade our existing grammar file from 2.x to 4.x   ** watch out for our custom extensions like unicode escape sequence handling, annotations etc"
3,as a developer I'd like to instrument Java8 new language features,"This is an enhancement of CLOV-1139. Clover should be able not only to parse Java8 syntax without error, but also be able to add it's own code statements in order to measure code coverage for:  * lambda expressions  * ???  Scope:  * instrumentation (java.g file)  Out of scope:  * database format  * reporting"
2,as a developer I'd like to generate reports with Java8 sources,"New language features might require enhancements of existing reports.     Evaluate how to represent lambdas - for instance:   - as a separate entity in the report, like a class or a method   - as an integral part of enclosing scope - similarly as the inline class is being handled by Clover    Scope:   * HTML, XML, JSON, PDF reports  "
3,as a developer I'd like to have information about lambdas in the Clover database,"1) Evaluate how to represent Java8 lambdas - for instance:   - as a separate entity in the model, like a class or a method   - as an integral part of enclosing scope - similarly as the inline class is being handled by Clover    2) Design the DB format having in mind a future extensibility (like functions, functions-in-functions, global variables etc) for other JVM-based languages (like Scala or Closure)    3) Implement it.    Scope:   * database format    Out of scope:   * source code instrumentation   "
1,as a developer I want be able to select 1.8 language level,"The ""1.8"" language level shall be available for:   * <clover-setup> <clover-instr> Ant tasks   * clover2:setup clover2:instrument goal   * project settings in Clover-for-Eclipse, Clover-for-IDEA     Add such option, if necessary, write unit tests. "
1,Warn if directory selected on Test Classes tab is disabled on Source Files tab,"On the ""Project Properties > Clover > Source Files"" tab user can select which source folders shall be instrumented by Clover. By de-selecting the checkbox, the whole source root becomes excluded from instrumentation:    !source_files.png!    On the ""Test Classes"" files user can select which source roots contain test sources:    !test_classes.png!    It it possible to have such combination that:   * source root is excluded   * source root is marked as a test root    This can be misleading. Add appropriate warning for it."
3,"The <testmethod name="".*> does not match constructors","*Problem:*    If you declare custom test detector, for instance:    {code:xml}  <clover-setup>    <testsources dir=""src/test/java"">      <testclass name="".*"">        <testmethod name="".*""/>      </testclass>    </testsources>  </clover-setup>  {code}    Then the test detector does not match constructors. As a consequence, a class like this:    {code:java}  public class SomeException extends Exception {    public SomeException(String s) {       super(s);     }  }   {code}    will be treated as APPLICATION not as TEST code.    *Background reason:*    {code:java}  public boolean isMethodMatch(SourceContext sourceContext, MethodContext methodContext) {          final MethodSignature signature = methodContext.getSignature();          return methodMatches(signature.getName()) &&                  methodAnnotationMatches(signature.getModifiers()) &&                  methodReturnPatternMatches(signature.getReturnType()) &&                  methodTagMatches(signature.getTags());      }    public boolean methodReturnPatternMatches(String methodReturnType) {          return methodReturnType != null &&                  (methodReturnTypePattern == null || methodReturnTypePattern.matcher(methodReturnType).matches());      }  {code}    The methodReturnType==null for constructors, thus constructor do not match the pattern.     *Solution:*    We could rewrite check like this:    {code:java}  return methodReturnTypePattern == null ||      (methodReturnType != null && methodReturnTypePattern.matcher(methodReturnType).matches());  {code}    However, this would require entirely new handling of code instrumentation for constructors. The reason is that code would be currently rewritten like:    {code:java}  public SomeException(String s) {          __CLR3_1_12_100hjv4vvu6.R.globalSliceStart(getClass().getName(), 0);          int $CLV_p$ = 0;          java.lang.Throwable $CLV_t$ = null;          try {              __CLR3_1_12_162c4pt0(s); // <<< ORIGINAL CONSTRUCTOR              $CLV_p$ = 1;          } catch (java.lang.Throwable $CLV_t2$) {              if ($CLV_p$ == 0 && $CLV_t$ == null) {                  $CLV_t$ = $CLV_t2$;              }              __CLR3_1_12_100hjv4vvu6.R.rethrow($CLV_t2$);          } finally {              __CLR3_1_12_100hjv4vvu6.R.globalSliceEnd(getClass().getName(), ""SomeException.SomeException"", 0, $CLV_p$, $CLV_t$);          }      }        private __CLR3_1_12_162c4pt0(String s) { // << COMPILATION FAILURE, MISSING RETURN TYPE          super(s);          __CLR3_1_12_100hjv4vvu6.R.inc(1);          __CLR3_1_12_100hjv4vvu6.R.inc(0);      }  {code}    Problems:   * call to super() must be first   * we should not move constructor body into another method  "
1,Implicit return in Groovy switch statement is instrumented incorrectly,"*Problem:*    Consider the following code:    {code:java}  def create(boolean b) {          switch (b) {              case true:                  new Integer(10)                  break              case false:                  new String(""abc"")                  break          }  }  {code}     * switch statement is the last statement in function, so it's value is being returned from a function call   * new Integer() / new String() are last statements before 'break' which is a function's exit point   * thus 10 or ""abc"" are returned    Clover instruments code as follows:    {code:java}  def create(boolean b) { recorder.inc(0);          switch (b) {              case true:                  recorder.inc(1); new Integer(10)                  recorder.inc(2); break              case false:                  recorder.inc(3); new String(""abc"")                  recorder.inc(4); break          }  }  {code}     * as a consequence, the last statement before 'break' becomes recorder.inc(), which returns void; as a result the create() function returns null    *Workaround:*    Use return instead of break in a switch statement, e.g.:    {code:java}              case true:                  return new Integer(10)                  // no break   {code}      *Fix:*     *  do not add recorder.inc() before 'break'; drawback of the solution: we loose tracking of empty breaks, see below.    {code:java}  def foo(int i) {    switch (i) {      case 1:        recorder.inc(1); println(""one"")        /*recorder.inc(2); not added*/ break;      case 2:      case 3:         /*recorder.inc(3); not added*/ break;    }  }    foo(1)  foo(2)  {code}    in the code above it would be nice to have ""case2: case 3: recorder.inc(3); break;"" colored in green, despite that code makes nothing ...  "
1,Added LICENSES.html file to Clover artfiacts,"Add the LICENSES.html file containing a table with:   * name of third party library   * link to home page (download sources / binaries)   * link to the license file    into the following artifacts:   * Clover-for-Ant ZIP   * Clover-for-Eclipse update site ZIP   * Clover-for-IDEA plug-in JAR    Note:   * there's no need to add such file in the Clover-for-Grails and Clover-for-Maven2&3, because these plug-ins do not contain any third party libraries embedded in it (just references in pom.xml / BuildConfig.groovy)    Reason for change:    * ensuring that we obey requirements imposed by various open source licences    "
0,review navigation links on a page summary,"frames/no frames - deleted show help - exists on some of pages overview/package/file - moved to breadcrumbs on a page header  todo: discuss with UX designer how breadcrumbs shall show the structure, e.g.:  - shall we display report name and a project name only?  - or some main section?  - or the entire path - if so, shall we have:  Report > Project > Test/App/Clouds > Package > FIle/Class > (Test)  or something else?"
0,Add ADG navigation bar in the HTML report," * probably should be optional, because we embed reports in Bamboo -> how to handle this? all links from navigation bar shall be available somewhere else?     * what to put in the nav bar? we don't have 'create issue' button for sure ;-)"
2,Create code samples with java8 syntax,annontations on java types  repeating annotations  lambdas  virtual extension methods
2,Prepare unit tests compiling and instrumenting code samples,"test:  compile code samples from CLOV-1352 using JDK1.8 javac  instrument these code samples using Clover    expected:  no instrumentation of lambdas yet, just test if files are parsed correctly"
1,Set up a Bamboo build for JDK1.8,"run all clover tests, including tests of java8 syntax on JDK1.8 on Bamboo"
1,Add new grammar rules for java8 syntax,"just to correctly parse new syntax, do not add clover instrumentation to these new code constructs    something like:    lambda :      ( lambdaNoArgs | lambdaImplicitTypesArgs | lambdaExplicitTypesArgs )     '->'     ( lambdaExpressionLikeStatement | lambdaCodeBlock )    plus extend the method definition and add 'default' keyword as optional; probably won't need to distinguish between method declaration in interface and in a class"
2,as an administrator I need to monitor Clover usage on machines,"As a system administrator I need to have a way to track down usage of Clover licenses within my organization in order to make sure that Clover is not installed or used on more machines than the license allows to. Currently there is no tool for this purpose and the only indirect way is to scan for clover license files.     One of possible solutions:   * add the environment variable which will enable tracking of clover license usage, such variable could be set on all machines in users' profiles by system administrator     * the variable would point to the URL which will collect data about license usage, e.g.:   {noformat}  -Dclover.license.usage.url=http://path.to.web.server/clover.php?someoptions  {noformat}     * whenever code instrumentation or report generation is performed and the environment variable is defined, clover will connect to the specified URL using simple HTTP GET request     * it would be up to the system administrator to collect data from such HTTP GET requests (e.g. parsing apache logs, using some php script to store data in database ...)     * options of HTTP GET    {noformat}  ?LicenseID=ABCDEF&LicenseExpiryDate=YYYY-MM-DD&MaintenanceExpiryDate=YYYY-MM-DD&Organisation=CompanyName&NumberOfUsers=NN&LicenseTypeName=DESKTOP_OR_SERVER  {noformat}      Scope: clover core (would work in ant, maven, grails, eclipse, idea ...)"
3,Enhance Clover database to keep full instrumentation history,"*Problem:*    Currently Clover database keeps list of instrumentation sessions plus information about classes which is a sum of all previous instrumentation sessions in such way that for given class only the latest class version is stored.     There are two limitations of current design:    1) Test optimization might not work correctly for some test frameworks for Groovy language. This is due to nature of the Groovy language itself, where compiler allows to compile code which does not have all symbols resolved (they're being resolved at runtime). As a consequence if a dependent application class is changed, this does not to have trigger recompilation of the test class. See comment for more details.     2) Syntax highlighting in HTML report is always based on the latest class version, so if someone has older source file, it can be rendered incorrectly. There's a warning for this in HTML report, but we could fetch the actual line/char indexes from the older class version.    *Solution:*    1) Enhance UpdateableRegFile and keep all versions of classes. This should be optional and disabled by default.     2) Enhance RegFile API to allow querying for older versions of given file (by default it should always return the latest version).     3) Add new option for <clover-setup> and <clover-instr> to allow to keep full instrumentation history. Either as ""true/false"" or as a number (maximum number of versions of the single class to store).    4) Add API to calculate index bitset as a logical OR of bitsets from all versions of the given file or class.  "
2,Fix test optimization and check all class versions,Depends on CLOV-1358    Test optimization should scan for hit count indexes of all versions of given class. This is necessary in order to workaround the problem of the groovyc compiler which resolves some of symbols at runtime and thus it does not always recompile test class if application class is not changed. 
2,Analyse how tests are converted by Spock ,"Analyse how a test named like:    {noformat}  def ""my test name""  {noformat}    is converted to a method like:    {noformat}  $spock_feature_1_0  {noformat}    It's being done during AST transformation I guess.     Check how it's possible to either:   * reverse engineer this behaviour i.e. get a back-ward mapping $spock_feature_1_0 -> def ""my test name"" or   * reproduce this behaviour, i.e. instrument methods in such order/way that Clover could 'calculate' the same method name   "
2,Implement test name to method name mapping in Clover,"Based on the investigation results from CLOV-1360 write proper mapping in Clover so that we could keep original names of test cases in Clover database (like ""my test name"" instead of $spock_feature_1).     And these tests should properly ""link"" to appropriate sections in source code - for instance in order to allow navigation in HTML report from test result to the test class source code."
1,Add Spock test name patterns in default test detector ,"Clover's default test detector should properly recognize Spock-framework test classes and which methods are test ones and which are helper ones. We shall not see stuff like this in the test summary:    {noformat}  Tests 	Started 	Status 	Time (secs) 	Message  OpsWiseJobQueryTestSpec. 	12 mar 10:32:50 	PASS 	0 	     // BAD  OpsWiseJobQueryTestSpec.$spock_feature_1_0 	12 mar 10:32:49 PASS 	0,001 	// OK, but use original name  OpsWiseJobQueryTestSpec. 	12 mar 10:32:49 	PASS 	0 	     // BAD  OpsWiseJobQueryTestSpec.setup 	12 mar 10:32:49 	PASS 	0,009        // BAD  {noformat}  "
1,Ensure that reports show the test name and not the method name,"Make sure that HTML, XML, PDF, JSON reports will display the test name (like ""my test name"" taken from def) rather than the actual name of test method (like $spock_feature). "
1,as a developer I don't need to run Clover on JDK1.5 anymore,"*Idea:*    Sun Java 1.5 has:   * end of public updates - Oct 2009   * end of Premier Support - May 2011   * end of Extended Support - May 2015   * see http://www.oracle.com/technetwork/java/eol-135779.html    IBM Java 1.5:   * its End Of Servcie (EOS) will expire on September 2015   * see https://www.ibm.com/developerworks/java/jdk/lifecycle/    According to various sources, most popular are Java6 and Java7 with Java8 growing up; Java1.5 has a fraction of market right now:   * http://www.statowl.com/java.php   although the report above shows Java version in web browser. Production environment may be different (especially regarding IBM Java).    *Benefits:*    Dropping support for Java 1.5 would improve Clover development, as we could:   * use @Override for interfaces   * use newer version of Guava (we're on Guava 9 (!) due to required JDK1.5 binary compatibility)   * have all Clover modules and plugins on the same JDK level   ** Groovy/Grails require JDK6+ to compile and execute   ** Clover-for-IDEA (IDEA12 JPS) requires JDK6+ to compile and execute   ** rest is on JDK5    *Tasks:*     * update Supported-Platforms page   * update guava library (and all others which are compiled against jdk6)   * update build scripts   * affects Clover Core, Clover-for-Ant, Clover-for-Maven2&3, Clover-for-Eclipse, Clover-for-IDEA, Clover-for-Grails       *Out of scope:*     * adding Override annotations - this will be done gradually during daily development    "
1,Extend API for loading and manipulate coverage files ,"Clover 3.2.0 published new interfaces describing a database model (com.atlassian.clover.api.registry). We need to reveal more Clover stuff via API in order to make it more usable by other developers.    For example:   * loading database and coverage files, handling instrumentation sessions   * manipulating coverage files"
1,Change signature of getXyz() methods in HasXyz interfaces,"Current signatures have <? extends X>, for example:    {code:java}  List<? extends MethodInfo> getMethods()  {code}    This works fine for Clover's internal data structures (as there are subclasses), but it's not usable by developers using this API. It should be possible to have an assignment like:    {code:java}  List<HasMetrics> methodMetrics = classInfo.getMethods()  {code}    Fix: change to <? super X>.     Note: We may need to expose additional methods in interface. They could throw Exception(""not implemented"") or return nulls."
1,Create interfaces and factory methods for loading Clover database,"*Problem:*    Database entities are already in c.a.c.api.registry package.     However, in order to load the database you have to call an inner class, for example:    {code:java}  import com.atlassian.clover.CloverDatabase;      // XXX internal class  import com.atlassian.clover.CoverageDataSpec; // XXX internal class  import com.atlassian.clover.api.registry.ProjectInfo;  // ...  CloverDatabase db = CloverDatabase.loadWithCoverage(args[0], new CoverageDataSpec());  ProjectInfo projectInfo = db.getRegistry().getProject();  {code}    *Fix:*    Create proper interfaces and a factory hiding the CoverageDataSpec and CloverDatabase. "
1,Create interfaces and some template class for handling instrumentation sessions,"*Problem:*    In order to update content of a database and store new instrumentation session a number of internal classes must be used, e.g.:    {code:java}  import com.atlassian.clover.context.ContextSet;  import com.atlassian.clover.registry.Clover2Registry;  import com.atlassian.clover.registry.FixedSourceRegion;  import com.atlassian.clover.registry.entities.MethodSignature;  import com.atlassian.clover.registry.entities.Modifier;  import com.atlassian.clover.registry.entities.Modifiers;  import com.atlassian.clover.registry.entities.Parameter;  // ...  registry = Clover2Registry.createOrLoad(dbFile, projectName);  session = registry.startInstr(encoding)  // ...  session.enterMethod(new ContextSet(), new FixedSourceRegion(12, 1),     methodSignature, false, false, 5, LanguageConstruct.Builtin.METHOD);  {code}    *Fix:*   - move MethodSignature, Modifier, Modifiers, Parameter to API   - create factory for FixedSourceRegion, ContextSet    - enterMethod (and others) shall use ContextSet from API  "
1,Create API for handling coverage recording files,"*Problem:*    Currently there's no API which would allow to create a global recording or a per-test recording file. Such API would be useful, e.g. for conversion of coverage data from other tools such as Cobertura or Emma.     *Fix:*   - publish global and per-test recording file format on CAC (including naming convention for files)   - expose class for global coverage file (equivalent of FileBasedGlobalCoverageRecording)    - expose class for per-test coverage file (equivalent of FileBasedPerTestRecording)  "
1,Create API for handling optimization snapshot,*See:*   * [Hacking Clover / Updating optimization snapshot file|https://confluence.atlassian.com/display/CLOVER/Updating+optimization+snapshot+file]   * [src/it/optmized/snapshot-hacking|https://bitbucket.org/atlassian/maven-clover2-plugin]    *Problem:*    Internal classes are used to manipulate snapshot.    {code:java}  import com.atlassian.clover.registry.entities.TestCaseInfo;  import com.atlassian.clover.optimization.Snapshot;  // ...  Snapshot snapshot = Snapshot.loadFrom(snapshotLocation);  final Set<TestCaseInfo> allTestCaseInfos = db.getCoverageData().getTests();  for (TestCaseInfo tci : allTestCaseInfos) { ... }  snapshot.store();  {code}    *Fix:*    Create proper interfaces/classes in c.a.c.api.optimization  
0,Add showInnerFunctions and showLambdaFunctions to clover2:setup,Command line tools and a <clover-report> Ant task have these options.     Add:    showInnerFunctions=false   showLambdaFunctions=false    to CloverReportMojo. Pass these values to a report descriptor. Define such values also in the default descriptor (clover-report.xml).
0,Add lambda toggle to report wizards in Eclipse and IDEA,"Add toggles for showInnerFunctions and showLambdaFunctions in report wizards in Eclipse and IDEA plugins. Toggles shall be visible for HTML and XML reports.     Consider using a drop down ""show lambda functions"":   - NONE (inner=false, lambda=false)   - FIELDS ONLY (inner=false, lambda=true)   - FIELDS AND METHODS (inner=true, lambda=true)  "
0,Fully qualified java.lang is not used for system properties in Clover.getRecorder ,"Generated code:    {code:java}  R=com_cenqua_clover.Clover.getNullRecorder();_R=com_cenqua_clover.Clover.getNullRecorder();_R=com_cenqua_clover.Clover.getRecorder(     ""\u0063....\u0064\u0062"",     1382598086449L,8589935092L,49,profiles,      new String[]{""clover.distributed.coverage"",null});             ^^^^^ java.lang. is missing  {code}  "
1,Perform Scala spike,"This task is time-boxed to 5 days. Take a look at:  * our Scala prototype  * SCCT  * Scoverage  to learn how instrumentation could be implemented. Expected outcome:  * knowledge how scalac works (compilation phases, AST structure, plugging into it)  * detailed list of tasks required  * task estimation  * prioritisation"
3,Clover integration with Bamboo - enhancements,*THIS EPIC IS OPTIONAL*    There is a number of open issues in Bamboo JIRA project related with a Clover plugin. Sorted by importance (in my opinion):     * BAM-13564 (Remove the $\{buildKey\} from Clover plugin URLs)   * BAM-12852 (Add Clover tab for Build Summary page)   * BAM-13404 (Use BuildConfig's DSL to install Clover plugin)   * BAM-13208 (Improve Automatic Clover integration for multi-module maven builds)   * BAM-13610 (Facilitate Clover configuration for plans using automatic Clover integration)   * BAM-13786 (Test Coverage History drops to zero on days with no tests)   * BAM-12118 (Perform cleanup of clover_fs_rez* and grover*jar files)   * BAM-13787 (Allow Clover test coverage history duration be customized for the graph)   * BAM-11590 (Add job failure based on coverage threshold for Clover plugin)      TODO: check which ones could be implemented for this release.     
3,as a developer I'd like to have Spock @Unroll annotation being recognized by Clover,"The {noformat}@Unroll{noformat} annotation causes that a test name will be unrolled for every combination of test data. It means that we can have multiple tests having different name, which in fact are related with exactly the same test method, but ran with different input arguments.     Examples (from http://docs.spockframework.org/en/latest/data_driven_testing.html):    {code:java}  // sequence index at the end  @Unroll  def ""maximum of two numbers""() { ... }  maximum of two numbers[0]   PASSED  maximum of two numbers[1]   FAILED    // variable substitution   @Unroll  def ""maximum of #a and #b is #c""() { ... }  maximum of 3 and 5 is 5   PASSED  maximum of 7 and 0 is 7   FAILED  {code}      Problem for Clover: we don't know neither a number of tests nor their names during compilation. it would have to be deferred to runtime.     => add some inner class for a test class which would monitoring the test name and somehow pass to the per-test coverage recorder?  "
2,Instrumentation of a branch condition with a generic type leads to javac compilation error,"*For a declaration like this:*  {code:java} public <T> T getFeatureValue(); ... if (client.getFeatureValue()) { {code}  the instrumented line gets instrumented like:  {code:java} _CLR3_1_113f23f2hnz3nmqe.R.inc(5146);if ((((client.getFeatureValue())&&(CLR3_1_113f23f2hnz3nmqe.R.iget(5147)!=0|true))||(_CLR3_1_113f23f2hnz3nmqe.R.iget(5148)==0&false))) {{ {code}  which leads to compilation error:  {noformat} error: bad operand types for binary operator '&&' {noformat}  *Reason:*  Javac performs autoboxing of the if condition. However it cannot deal properly if the same generic value is used with && or || operators. It looks like if javac wraps the entire if condition, not a single element.   Look at the example:  {code:java} public class BranchCoverageWithAutoboxing {     interface Data {         public <T> T getValue();     }      public boolean testGetValue(Data source) {         if (source.getValue()) {  // Implicit conversion to Boolean via autoboxing             return true;         }         return false;     }      public boolean testGetValueWithBoolean(Data source) {         if (source.getValue() && true) {  // Error: Operator && cannot be applied to java.lang.Object, boolean             return true;         }         return false;     }      public boolean testGetValueWithWrappedBoolean(Data source) {         if (Boolean.valueOf(source.getValue().toString()) && true) { // Explicit conversion, compilation is successful             return true;         }         return false;     } } {code}  *Workaround:*  Don't rely on boolean autoboxing and change a generic type to Boolean.   or  Extract expression being autoboxed to a local variable and evaluate before ""if"". Use the variable in ""if"".  {code:java} // original: if (source.getValue)  // fixed: Boolean b = source.getValue(); if (b)  {code}  or  surround problematic code block with ""///CLOVER:OFF"" and ""///CLOVER:ON"" inline comments (note that three slashes are used)"
0,clover intrumented testcode with PersitenceConstructor and parameters fails ,"If the integration tests running without clover instrumentation the test works fine, but with clover instrumentation the one test will fail.    {code}  org.springframework.data.mapping.model.MappingException: No property null found on entity class com.test.framework.Model.DataModel to bind constructor parameter to!    org.springframework.data.mapping.model.PersistentEntityParameterValueProvider.getParameterValue(PersistentEntityParameterValueProvider.java:74)    org.springframework.data.mapping.model.SpELExpressionParameterValueProvider.getParameterValue(SpELExpressionParameterValueProvider.java:63)    org.springframework.data.convert.ReflectionEntityInstantiator.createInstance(ReflectionEntityInstantiator.java:71)    org.springframework.data.mongodb.core.convert.MappingMongoConverter.read(MappingMongoConverter.java:232)    org.springframework.data.mongodb.core.convert.MappingMongoConverter.read(MappingMongoConverter.java:212)    org.springframework.data.mongodb.core.convert.MappingMongoConverter.read(MappingMongoConverter.java:176)    org.springframework.data.mongodb.core.convert.MappingMongoConverter.read(MappingMongoConverter.java:172)    org.springframework.data.mongodb.core.convert.MappingMongoConverter.read(MappingMongoConverter.java:75)    org.springframework.data.mongodb.core.MongoTemplate$ReadDbObjectCallback.doWith(MongoTemplate.java:1841)    org.springframework.data.mongodb.core.MongoTemplate.executeFindOneInternal(MongoTemplate.java:1492)    org.springframework.data.mongodb.core.MongoTemplate.doFindOne(MongoTemplate.java:1303)    org.springframework.data.mongodb.core.MongoTemplate.findOne(MongoTemplate.java:475)    org.springframework.data.mongodb.core.MongoTemplate.findOne(MongoTemplate.java:470)    com.test.framework.DatabaseSpec.search User Query(DatabaseSpec.groovy:52)  {code}      its seem to be a problem with follow declaration  {code}      @PersistenceConstructor      public DataModel(String username, String password)  {code}  "
3,as a code reviewer I'd like to see ADG fonts and colours in the HTML report,NULL
0,review ADG page layout in the HTML report,review layouting of various components on the page and an interaction between them
2,IDEA13 compatiblity issues,"Compatiblity issues of Clover-for-IDEA with the latest IDEA13:    1) Attempt to load a license key from a file (About Clover box > License button > Load) fails with an error:    {noformat}  com.intellij.openapi.fileChooser.FileChooser.chooseFiles(Ljava/awt/Component;Lcom/intellij/openapi/fileChooser/FileChooserDescriptor;)[Lcom/intellij/openapi/vfs/VirtualFile;: com.intellij.openapi.fileChooser.FileChooser.chooseFiles(Ljava/awt/Component;Lcom/intellij/openapi/fileChooser/FileChooserDescriptor;)[Lcom/intellij/openapi/vfs/VirtualFile;  java.lang.NoSuchMethodError: com.intellij.openapi.fileChooser.FileChooser.chooseFiles(Ljava/awt/Component;Lcom/intellij/openapi/fileChooser/FileChooserDescriptor;)[Lcom/intellij/openapi/vfs/VirtualFile;    com.cenqua.clover.idea.config.LicenseConfigPanel.loadLicenseFile(LicenseConfigPanel.java:321)    com.cenqua.clover.idea.config.LicenseConfigPanel.actionPerformed(LicenseConfigPanel.java:216)    javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2018)    javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2341)    javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:402)    javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:259)    javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:252)    java.awt.Component.processMouseEvent(Component.java:6505)    javax.swing.JComponent.processMouseEvent(JComponent.java:3321)    java.awt.Component.processEvent(Component.java:6270)    java.awt.Container.processEvent(Container.java:2229)    java.awt.Component.dispatchEventImpl(Component.java:4861)    java.awt.Container.dispatchEventImpl(Container.java:2287)    java.awt.Component.dispatchEvent(Component.java:4687)    java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4832)    java.awt.LightweightDispatcher.processMouseEvent(Container.java:4492)    java.awt.LightweightDispatcher.dispatchEvent(Container.java:4422)    java.awt.Container.dispatchEventImpl(Container.java:2273)    java.awt.Window.dispatchEventImpl(Window.java:2719)    java.awt.Component.dispatchEvent(Component.java:4687)    java.awt.EventQueue.dispatchEventImpl(EventQueue.java:729)    java.awt.EventQueue.access$200(EventQueue.java:103)    java.awt.EventQueue$3.run(EventQueue.java:688)    java.awt.EventQueue$3.run(EventQueue.java:686)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:87)    java.awt.EventQueue$4.run(EventQueue.java:702)    java.awt.EventQueue$4.run(EventQueue.java:700)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.awt.EventQueue.dispatchEvent(EventQueue.java:699)    com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:696)    com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:520)    com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:335)    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:242)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:161)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:154)    java.awt.WaitDispatchSupport$2.run(WaitDispatchSupport.java:182)    java.awt.WaitDispatchSupport$4.run(WaitDispatchSupport.java:221)    java.security.AccessController.doPrivileged(Native Method)    java.awt.WaitDispatchSupport.enter(WaitDispatchSupport.java:219)    java.awt.Dialog.show(Dialog.java:1082)    com.intellij.openapi.ui.impl.DialogWrapperPeerImpl$MyDialog.show(DialogWrapperPeerImpl.java:786)    com.intellij.openapi.ui.impl.DialogWrapperPeerImpl.show(DialogWrapperPeerImpl.java:462)    com.intellij.openapi.ui.DialogWrapper.showAndGetOk(DialogWrapper.java:1543)    com.intellij.openapi.ui.DialogWrapper.show(DialogWrapper.java:1511)    com.intellij.ide.actions.ShowSettingsUtilImpl.a(ShowSettingsUtilImpl.java:263)    com.intellij.ide.actions.ShowSettingsUtilImpl.editConfigurable(ShowSettingsUtilImpl.java:228)    com.intellij.ide.actions.ShowSettingsUtilImpl.editConfigurable(ShowSettingsUtilImpl.java:213)    com.cenqua.clover.idea.AboutDialog$1.actionPerformed(AboutDialog.java:216)    javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2018)    javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2341)    javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:402)    javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:259)    javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:252)    java.awt.Component.processMouseEvent(Component.java:6505)    javax.swing.JComponent.processMouseEvent(JComponent.java:3321)    java.awt.Component.processEvent(Component.java:6270)    java.awt.Container.processEvent(Container.java:2229)    java.awt.Component.dispatchEventImpl(Component.java:4861)    java.awt.Container.dispatchEventImpl(Container.java:2287)    java.awt.Component.dispatchEvent(Component.java:4687)    java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4832)    java.awt.LightweightDispatcher.processMouseEvent(Container.java:4492)    java.awt.LightweightDispatcher.dispatchEvent(Container.java:4422)    java.awt.Container.dispatchEventImpl(Container.java:2273)    java.awt.Window.dispatchEventImpl(Window.java:2719)    java.awt.Component.dispatchEvent(Component.java:4687)    java.awt.EventQueue.dispatchEventImpl(EventQueue.java:729)    java.awt.EventQueue.access$200(EventQueue.java:103)    java.awt.EventQueue$3.run(EventQueue.java:688)    java.awt.EventQueue$3.run(EventQueue.java:686)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:87)    java.awt.EventQueue$4.run(EventQueue.java:702)    java.awt.EventQueue$4.run(EventQueue.java:700)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.awt.EventQueue.dispatchEvent(EventQueue.java:699)    com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:696)    com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:520)    com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:335)    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:242)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:161)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:154)    java.awt.WaitDispatchSupport$2.run(WaitDispatchSupport.java:182)    java.awt.WaitDispatchSupport$4.run(WaitDispatchSupport.java:221)    java.security.AccessController.doPrivileged(Native Method)    java.awt.WaitDispatchSupport.enter(WaitDispatchSupport.java:219)    java.awt.Dialog.show(Dialog.java:1082)    com.intellij.openapi.ui.impl.DialogWrapperPeerImpl$MyDialog.show(DialogWrapperPeerImpl.java:786)    com.intellij.openapi.ui.impl.DialogWrapperPeerImpl.show(DialogWrapperPeerImpl.java:462)    com.intellij.openapi.ui.DialogWrapper.showAndGetOk(DialogWrapper.java:1543)    com.intellij.openapi.ui.DialogWrapper.show(DialogWrapper.java:1511)    com.cenqua.clover.idea.actions.AboutAction.actionPerformed(AboutAction.java:19)    com.intellij.openapi.actionSystem.ex.ActionUtil.performActionDumbAware(ActionUtil.java:162)    com.intellij.openapi.actionSystem.impl.ActionButton.a(ActionButton.java:170)    com.intellij.openapi.actionSystem.impl.ActionButton.a(ActionButton.java:133)    com.intellij.openapi.actionSystem.impl.ActionButton.processMouseEvent(ActionButton.java:311)    java.awt.Component.processEvent(Component.java:6270)    java.awt.Container.processEvent(Container.java:2229)    java.awt.Component.dispatchEventImpl(Component.java:4861)    java.awt.Container.dispatchEventImpl(Container.java:2287)    java.awt.Component.dispatchEvent(Component.java:4687)    java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4832)    java.awt.LightweightDispatcher.processMouseEvent(Container.java:4492)    java.awt.LightweightDispatcher.dispatchEvent(Container.java:4422)    java.awt.Container.dispatchEventImpl(Container.java:2273)    java.awt.Window.dispatchEventImpl(Window.java:2719)    java.awt.Component.dispatchEvent(Component.java:4687)    java.awt.EventQueue.dispatchEventImpl(EventQueue.java:729)    java.awt.EventQueue.access$200(EventQueue.java:103)    java.awt.EventQueue$3.run(EventQueue.java:688)    java.awt.EventQueue$3.run(EventQueue.java:686)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:87)    java.awt.EventQueue$4.run(EventQueue.java:702)    java.awt.EventQueue$4.run(EventQueue.java:700)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.awt.EventQueue.dispatchEvent(EventQueue.java:699)    com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:696)    com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:520)    com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:335)    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:242)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:161)    java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:150)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:146)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:138)    java.awt.EventDispatchThread.run(EventDispatchThread.java:91)  {noformat}    2) Compilation of the project with Clover enabled fails with compilation error ""package com_cenqua_clover"" does not exist. The most probably Clover fails to add clover.jar to project's class path.    {noformat}  java: package com_cenqua_clover does not exist  {noformat}      3) There is no option to enable/disable external build feature. It seems that the ""classic"" build functionality has been removed in IDEA13.    "
0,"""No such property: testTargetPatterns"" exception with Grails 2.3",Unable to use Clover-for-Grails with Grails 2.3. Build fails with an exception:  {noformat} |Environment set to test ...................................    [mkdir] Created dir: C:\Work\grails-clover-plugin-hg\testcases\petclinic233\target\test-reports\html .    [mkdir] Created dir: C:\Work\grails-clover-plugin-hg\testcases\petclinic233\target\test-reports\plain .Error | Error executing script TestApp: No such property: testTargetPatterns for class: _Events groovy.lang.MissingPropertyException: No such property: testTargetPatterns for class: _Events         at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:50)         at org.codehaus.groovy.runtime.callsite.PogoGetPropertySite.getProperty(PogoGetPropertySite.java:49)         at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGroovyObjectGetProperty(AbstractCallSite.java:231)         at _Events$_run_closure3.doCall(_Events.groovy:75)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:601)         at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)         at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)         at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1086)         at groovy.lang.ExpandoMetaClass.invokeMethod(ExpandoMetaClass.java:1110)         at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:910) {noformat}  *Reason:*  In the Grails 2.3 the _GrailsTest.groovy has been refactored into GrailsProjectTestRunner and GrailsProjectTestCompiler. The testTargetPatterns property has been removed.    *Probable fix:*  Replace testTargetPattern by iterator over testNames. 
2,Java+Groovy compilation fails with Clover enabled in IntelliJ 12.1.x,"When building my project in IntelliJ the build fails with the following exception. I saw a couple other similar comments about how using an external build can cause problems but I thought this was fixed with Clover 3.2? If I turn off the external build then the compilation does succeed. The project is a combined Java/Groovy project if that matters. Let me know if you need any more details.    {noformat}  Error: java.lang.IllegalArgumentException: org.jetbrains.jps.javac.TransformableJavaFileObject  java.lang.RuntimeException: java.lang.IllegalArgumentException: org.jetbrains.jps.javac.TransformableJavaFileObject    com.sun.tools.javac.main.Main.compile(Main.java:475)    com.sun.tools.javac.api.JavacTaskImpl.call(JavacTaskImpl.java:132)    org.jetbrains.jps.javac.JavacMain.compile(JavacMain.java:167)    org.jetbrains.jps.incremental.java.JavaBuilder.compileJava(JavaBuilder.java:364)    org.jetbrains.jps.incremental.java.JavaBuilder.compile(JavaBuilder.java:276)    org.jetbrains.jps.incremental.java.JavaBuilder.doBuild(JavaBuilder.java:190)    org.jetbrains.jps.incremental.java.JavaBuilder.build(JavaBuilder.java:162)    org.jetbrains.jps.incremental.IncProjectBuilder.runModuleLevelBuilders(IncProjectBuilder.java:1018)    org.jetbrains.jps.incremental.IncProjectBuilder.runBuildersForChunk(IncProjectBuilder.java:742)    org.jetbrains.jps.incremental.IncProjectBuilder.buildTargetsChunk(IncProjectBuilder.java:790)    org.jetbrains.jps.incremental.IncProjectBuilder.buildChunkIfAffected(IncProjectBuilder.java:705)    org.jetbrains.jps.incremental.IncProjectBuilder.buildChunks(IncProjectBuilder.java:526)    org.jetbrains.jps.incremental.IncProjectBuilder.runBuild(IncProjectBuilder.java:314)    org.jetbrains.jps.incremental.IncProjectBuilder.build(IncProjectBuilder.java:179)    org.jetbrains.jps.cmdline.BuildRunner.runBuild(BuildRunner.java:129)    org.jetbrains.jps.cmdline.BuildSession.runBuild(BuildSession.java:220)    org.jetbrains.jps.cmdline.BuildSession.run(BuildSession.java:112)    org.jetbrains.jps.cmdline.BuildMain$MyMessageHandler$1.run(BuildMain.java:132)    org.jetbrains.jps.service.impl.SharedThreadPoolImpl$1.run(SharedThreadPoolImpl.java:41)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: java.lang.IllegalArgumentException: org.jetbrains.jps.javac.TransformableJavaFileObject    com.sun.tools.javac.file.JavacFileManager.inferBinaryName(JavacFileManager.java:660)    javax.tools.ForwardingJavaFileManager.inferBinaryName(ForwardingJavaFileManager.java:84)    com.sun.tools.javac.api.ClientCodeWrapper$WrappedJavaFileManager.inferBinaryName(ClientCodeWrapper.java:225)    com.sun.tools.javac.jvm.ClassReader.fillIn(ClassReader.java:2524)    com.sun.tools.javac.jvm.ClassReader.fillIn(ClassReader.java:2505)    com.sun.tools.javac.jvm.ClassReader.complete(ClassReader.java:2143)    com.sun.tools.javac.code.Symbol.complete(Symbol.java:421)    com.sun.tools.javac.comp.Enter.visitTopLevel(Enter.java:298)    com.sun.tools.javac.tree.JCTree$JCCompilationUnit.accept(JCTree.java:459)    com.sun.tools.javac.comp.Enter.classEnter(Enter.java:258)    com.sun.tools.javac.comp.Enter.classEnter(Enter.java:272)    com.sun.tools.javac.comp.Enter.complete(Enter.java:484)    com.sun.tools.javac.comp.Enter.main(Enter.java:469)    com.sun.tools.javac.main.JavaCompiler.enterTrees(JavaCompiler.java:929)    com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:824)    com.sun.tools.javac.main.Main.compile(Main.java:439)  	... 23 more  {noformat}      {panel:title=Problem summary|bgColor=yellow}  This bug occurs when all of the following conditions are met:   * IDEA 12.1.1 or later is used   * Clover 3.1.12 or later is used    * an ""external build"" feature is enabled (Settings > Compiler > Use external build)   * project has a mixed Java + Groovy code    It's an IDEA bug - http://youtrack.jetbrains.com/issue/IDEA-110835 - and has been fixed in IDEA13.     Therefore:   * in IDEA 12.x - disable the ""external build"" feature   * in IDEA 13.x - problem does not occur  {panel}  "
1,Java 1.8 Exceptions not being propagated correctly,"I have the following test stub (src/main/java)    {code:java}  public final class TestStub {      TestStub() {          throw new IllegalArgumentException();      }        public static void getHandler() {          new HashMap<>().computeIfAbsent(String.class, t -> new TestStub());      }  }  {code}    And the following test (src/test/java)    {code:java}  public class FooTest {        @Test(expected = IllegalArgumentException.class)      public void test0() {          TestStub.getHandler();      }  }  {code}    Using maven It runs fine using ""clean install"" however using clover I get the following exception    {noformat}  java.lang.reflect.UndeclaredThrowableException    com.sun.proxy.$Proxy12.apply(Unknown Source)    java.util.HashMap.computeIfAbsent(HashMap.java:1118)    org.cakeframework.internal.container.TestStub.getHandler(TestStub.java:26)    org.cakeframework.internal.container.FooTest.test0(FooTest.java:29)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:483)    org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)    org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)    org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)    org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)    org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)    org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)    org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)    org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)    org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)    org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)    org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)    org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)    org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)    org.junit.runners.ParentRunner.run(ParentRunner.java:309)    org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)    org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)    org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)    org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)    org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)    org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)  Caused by: java.lang.reflect.InvocationTargetException    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:483)    org.cakeframework.internal.container.TestStub$__CLR3_2_0wqwqhpd53be5$1.invoke(TestStub.java:20)  	... 28 more  Caused by: java.lang.IllegalArgumentException    org.cakeframework.internal.container.TestStub.<init>(TestStub.java:22)    org.cakeframework.internal.container.TestStub.lambda$getHandler$6(TestStub.java:26)    org.cakeframework.internal.container.TestStub$$Lambda$13/1227074340.apply(Unknown Source)  	... 33 more  {noformat}    If I replace with a standard anonymous class implementing java.util.function.Function it runs fine"
1,Java 1.8 compilation fails when lambda is passed to a generic argument,"The following piece of code fails to be instrumented  {code:java}  public class Fails<N> {        public final <S extends N> Iterable<S> depthFirstTraversal(Class<S> type) {          return (Iterable<S>) depthFirstTraversal(e -> type.isInstance(e));      }        public final Iterable<N> depthFirstTraversal(Predicate<? super N> predicate) {          return null;      }  }  {code}  Javac gives the following error message:    [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project cake-util: Compilation failure: Compilation failure:  [ERROR] /Users/kasperni/workspace/CAKE/cake-util/cake-util/target/clover/src-instrumented/org/cakeframework/internal/util/tree/Fails.java:[27,67] reference to depthFirstTraversal is ambiguous  [ERROR] both method <S>depthFirstTraversal(java.lang.Class<S>) in org.cakeframework.internal.util.tree.Fails and method depthFirstTraversal(java.util.function.Predicate<? super N>) in org.cakeframework.internal.util.tree.Fails match  [ERROR] /Users/kasperni/workspace/CAKE/cake-util/cake-util/target/clover/src-instrumented/org/cakeframework/internal/util/tree/Fails.java:[27,86] incompatible types: cannot infer type-variable(s) I,T  [ERROR] (argument mismatch; java.lang.Class is not a functional interface)  [ERROR] -> [Help 1]"
1,Clover fails to instrument generic type with a constructor reference,"This works fine:    {code:java}  myLambda = ArrayList::new;   // javac error since JDK8 b114  myLambda = ArrayList::<String>new;  {code}    But the following:    {code:java}  myLambda = ArrayList<String>::new;  myLambda = ArrayList<String>::<String>new;  {code}    fails with a parser error:    {noformat}  ERROR: Instrumentation errorInstrumentation error : c:\Work\clover-hg\clover-core\src\test\resources\javasyntax1.8\LambdaAndMethodReferences.java:78:28:unexpected token: ArrayList  com.atlassian.clover.api.CloverException: c:\Work\clover-hg\clover-core\src\test\resources\javasyntax1.8\LambdaAndMethodReferences.java:78:28:unexpected token: ArrayList    com.atlassian.clover.instr.java.Instrumenter.instrument(Instrumenter.java:158)    com.atlassian.clover.CloverInstr.execute(CloverInstr.java:73)    com.atlassian.clover.CloverInstr.mainImpl(CloverInstr.java:49)    com.atlassian.clover.JavaSyntaxCompilationTestBase.instrumentSourceFile(JavaSyntaxCompilationTestBase.java:145)    com.atlassian.clover.JavaSyntaxCompilationTestBase.instrumentAndCompileSourceFile(JavaSyntaxCompilationTestBase.java:111)    com.atlassian.clover.JavaSyntax18CompilationTest.testLambdaAndMethodReferences(JavaSyntax18CompilationTest.java:84)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:483)    junit.framework.TestCase.runTest(TestCase.java:168)    junit.framework.TestCase.runBare(TestCase.java:134)    junit.framework.TestResult$1.protect(TestResult.java:110)    junit.framework.TestResult.runProtected(TestResult.java:128)    junit.framework.TestResult.run(TestResult.java:113)    junit.framework.TestCase.run(TestCase.java:124)    junit.framework.TestSuite.runTest(TestSuite.java:243)    junit.framework.TestSuite.run(TestSuite.java:238)    org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)    org.junit.runner.JUnitCore.run(JUnitCore.java:157)    com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77)    com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)    com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:483)    com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)  Caused by: line 78:28: unexpected token: ArrayList    com.atlassian.clover.instr.java.JavaRecognizer.conditionalExpression(JavaRecognizer.java:4407)    com.atlassian.clover.instr.java.JavaRecognizer.assignmentExpression(JavaRecognizer.java:4274)    com.atlassian.clover.instr.java.JavaRecognizer.assignmentExpression(JavaRecognizer.java:4319)    com.atlassian.clover.instr.java.JavaRecognizer.expression(JavaRecognizer.java:3366)    com.atlassian.clover.instr.java.JavaRecognizer.statement(JavaRecognizer.java:2909)    com.atlassian.clover.instr.java.JavaRecognizer.outerCompoundStmt(JavaRecognizer.java:2696)    com.atlassian.clover.instr.java.JavaRecognizer.field(JavaRecognizer.java:2265)    com.atlassian.clover.instr.java.JavaRecognizer.classBlock(JavaRecognizer.java:1728)    com.atlassian.clover.instr.java.JavaRecognizer.classDefinition(JavaRecognizer.java:736)    com.atlassian.clover.instr.java.JavaRecognizer.typeDefinition2(JavaRecognizer.java:666)    com.atlassian.clover.instr.java.JavaRecognizer.typeDefinition(JavaRecognizer.java:524)    com.atlassian.clover.instr.java.JavaRecognizer.compilationUnit(JavaRecognizer.java:445)    com.atlassian.clover.instr.java.Instrumenter.instrument(Instrumenter.java:212)    com.atlassian.clover.instr.java.Instrumenter.instrument(Instrumenter.java:121)  	... 27 more  {noformat}    "
0,Cloverage view layout is crappy with the Coverage Summary panel,1. Not all buttons are visible by default.     2. Width/height proportion which triggers a change between horizontal/vertical layout is not set correctly. As a result we've got a crappy behavior as on the following screen shot:    !cloverage_view_layout.png!
0,Source highlighting label is cropped in the settings dialog,"Project Settings > Clover > View    ""Out of date"" labels are cropped    [^project_settings_clover.png]"
0,URISyntaxException from Clover IDEA plugin under IDEA13 on Mac OS X,"{noformat}  java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar: java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar  java.lang.RuntimeException: java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar    com.cenqua.clover.idea.LibrarySupport.getCloverClassBase(LibrarySupport.java:150)    com.cenqua.clover.idea.build.CloverCompiler$RefreshCloverGlobalLibraryTask.execute(CloverCompiler.java:282)    com.intellij.compiler.impl.CompileDriver.a(CompileDriver.java:2375)    com.intellij.compiler.impl.CompileDriver.access$900(CompileDriver.java:122)    com.intellij.compiler.impl.CompileDriver$8.run(CompileDriver.java:706)    com.intellij.compiler.progress.CompilerTask.run(CompilerTask.java:167)    com.intellij.openapi.progress.impl.ProgressManagerImpl$TaskRunnable.run(ProgressManagerImpl.java:464)    com.intellij.openapi.progress.impl.ProgressManagerImpl$2.run(ProgressManagerImpl.java:178)    com.intellij.openapi.progress.ProgressManager.executeProcessUnderProgress(ProgressManager.java:209)    com.intellij.openapi.progress.impl.ProgressManagerImpl.executeProcessUnderProgress(ProgressManagerImpl.java:212)    com.intellij.openapi.progress.impl.ProgressManagerImpl.runProcess(ProgressManagerImpl.java:171)    com.intellij.openapi.progress.impl.ProgressManagerImpl$8.run(ProgressManagerImpl.java:373)    com.intellij.openapi.application.impl.ApplicationImpl$8.run(ApplicationImpl.java:436)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)    java.util.concurrent.FutureTask.run(FutureTask.java:138)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:695)    com.intellij.openapi.application.impl.ApplicationImpl$1$1.run(ApplicationImpl.java:153)  Caused by: java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar    java.net.URI$Parser.fail(URI.java:2810)    java.net.URI$Parser.checkChars(URI.java:2983)    java.net.URI$Parser.parseHierarchical(URI.java:3067)    java.net.URI$Parser.parse(URI.java:3015)    java.net.URI.<init>(URI.java:577)    com.cenqua.clover.idea.LibrarySupport.getCloverClassBase(LibrarySupport.java:121)  	... 19 more    java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar: java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar  java.lang.RuntimeException: java.net.URISyntaxException: Illegal character in path at index 37: file:/Users/grant/Library/Application Support/IntelliJIdea13/clover-idea-3.2.1.jar    com.cenqua.clover.idea.LibrarySupport.getCloverClassBase(LibrarySupport.java:150)  {noformat}"
1,As a developer I'd like to see JUnit Parameterized tests in the report,"Similarly to the Spock framework (CLOV-1256) I'd like to see runtime test names from the JUnit4 parameterized tests. Example:    {code:java}  import org.junit.*;  import org.junit.runner.*;  import org.junit.runners.*;  import java.util.*;    import static org.junit.Assert.*;  import static org.junit.runners.Parameterized.*;    @RunWith(Parameterized.class)  public class FibonacciTest {      @Parameters      public static Collection<Object[]> data() {          return Arrays.asList(new Object[][] {                                     { 0, 0 }, { 1, 1 }, { 2, 1 }, { 3, 2 }, { 4, 3 }, { 5, 5 },{ 6, 8 }               });      }        private int fInput;        private int fExpected;        public FibonacciTest(int input, int expected) {          fInput= input;          fExpected= expected;      }        @Test      public void test() {          assertEquals(fExpected, Fibonacci.compute(fInput));      }  }    class Fibonacci {     static int compute(int a) {  	   return a;     }    }   {code}    "
0,Split velocity templates to 2 sets: adg and classic,"make a copy of original templates    extend HtmlReporter and add option to choose one of them, e.g.:    HtmlReporter -style classic/adg    ADG is the default one"
0,"Add report style=adg/classic options for Ant, Maven, Grails","scope: ant, maven, grails plugins  new toggle, default adg, pass to HtmlReporter"
0,Add radio button style=adg/classic in eclipse and idea IDEs,optional: add radio button in reports wizards    alternatively: use the default one (ADG) 
0,Use ADG-compliant colour set for widgets,"Replace existing colours by those recommended by ADG:   * https://developer.atlassian.com/design/latest/colors.html    Scope:   (/) tree map    (/) cloud view   (/) coverage bars (dashboard, package/class/file summary)   (/) test results bars (dashboard, test results pages)   (/) coverage markers on the page margin   (/) class coverage distribution / class complexity diagrams (use ""Blue"" from ADG)    (/) colours of hyperlinks  "
0,as a developer I expect that Clover will automatically configure IntelliJ IDEA compiler settings,"Since IDEA12 a new external build feature has been introduced. It has also an option to perform parallel build (Settings > Compile > Build independent modules in parallel toggle), however Clover does not support parallel instrumentation (see CLOV-1284).     Clover should automatically disable the parallel build toggle whenever compilation starts (it can notify about this fact by a baloon), otherwise it will throw various runtime exceptions during compilation."
0,The clover2:clover goal does not pass the sourcepath property to underlying <clover-report> task,"*Original problem from the cloned issue:*    {quote}  Clover database contains FileInfo objects for every source file. One of FileInfo fields is an absolute path to given file. When database is written, it uses current platform file separator for file name ('\' for windows, '/' for Linux/MacOS). However, when database is read it interprets file name using again current platform file separator.     As a consequence, when database is generated on Windows and report generated on Linux, it cannot resolve file name properly - it takes whole path as a single path segment, resulting in paths like:    /path/from/report/generation/sourcepath/D:/path/from/instrumented/build/file.java    As a consequence report generation does create html file containing source code with highlighted coverage.  {quote}    *Problem with the clover2:clover goal:*    The clover2:clover call <clover-report> using a default report descriptor (which is located in the maven-clover2-plugin-X.X.X.jar/default-clover-report.xml). This report descriptor has a reporting defined like:    {code:xml}  <current outfile=""${output}"" summary=""${summary}""                       charset=""${charset}"" title=""${title}"" titleAnchor=""${titleAnchor}""                       span=""${span}"" alwaysReport=""${alwaysReport}""                       showInnerFunctions=""${showInnerFunctions}"" showLambdaFunctions=""${showLambdaFunctions}"">     <format refid=""clover.format""/>     <testsources refid=""test.sources""/>     <columns refid=""clover.columns""/>  </current>  {code}    i.e. there is no <sourcepath> property.     Possible fix #1:     * add the <sourcepath> property   * set it's value to a list of standard source roots, similarly as it's done for <testsources> tag    _excerpt from the CloverReportMojo:_  {code:java}  antProject.setProperty(""testPattern"", ""**/src/test/**"");  {code}    _excerpt from the default-clover-report.xml:_    {code:xml}  <fileset id=""test.sources"" dir=""${projectDir}"">    <include name=""${testPattern}""/>  </fileset>  ...  <testsources refid=""test.sources""/>  {code}    so it could be like this:  {code:java}  antProject.setProperty(""sourceRootsPattern"", ""**/src/test/*;**/src/main/*"");  ...  {code}    (!) potential problem: build may have source roots different than src/test/xxx and src/main/xxx; in such case sources would not be found; it means that settings this <sourcepath> property should be optional; it could be done via boolean flag    Possible fix #2:   * add the <sourcepath> property to the clover2:clover goal   * if it's not null then pass it's value to the <sourcepath> property in the <clover-report> goal in the report descriptor    (!) potential problem: how developers could easily find all source roots in all modules in their maven project in order to pass it as the property?   "
0,change project/package statistics to the boxed component,Change this:    !package_summary.png!    Move it under a horizontal navigation (on overview tab for instance). Keep it as a boxed component (div grid). 
0,change package and class list tables into TablesSortable,"Change this (project summary, package summary):    !package_and_class_list.png!    into a sortable table like this:    https://docs.atlassian.com/aui/latest/sandbox/#"
1,change contributing tests table to TablesSortable etc,"Change a table with contributing tests:  !contributing_tests.png!    To do:   * table into TableSortable   * ""Show/hide tests"" label into expander   * ""Select all/deselect all"" labels into single master toggle   * remember about highlighting of lines of code and class' methods in a table"
0,change small class histogram into container,similarly as for coverage statistics    put into a box:  https://developer.atlassian.com/design/1.2/containers.html    this:  !small_histogram.png!
0,change dashboard boxes look,This: [^dashboard_boxes.png] should look like this:    https://developer.atlassian.com/design/1.2/containers.html
0,change page footer look,This: [^page_footer.png]  should look like this:    https://developer.atlassian.com/design/1.2/page-footer.html  
0,change cloud tabs,"The ""Package risks"" and ""Quick Wins"" tabs:    !cloud_tabs.png!    should have an ADG tabs look:    https://developer.atlassian.com/design/1.2/tabs.html    "
0,change classes/tests/results bottom left frame,Change this:    !classes_tests_results.png!    Scope:   * add a title: https://developer.atlassian.com/design/1.2/typography.html   * use tabs: https://developer.atlassian.com/design/1.2/tabs.html   * use sortable table: https://developer.atlassian.com/design/1.2/tables.html 
0,chage test result labels to lozenges,"""pass"" / ""fail"" / ""error"" test statuses shall be changed to Subtle lozenges:    https://developer.atlassian.com/design/1.2/lozenges.html  "
1,JDK8 sources fail to compile when instrumented by Clover,"See the attached comparison of original and instrumented code:    [^unicode_decoding_error.png]    A problem is as follows:   * Clover has a UnicodeDecodingReader class which translates 6-letter character sequences ""\uNNNN"" into a single UTF16 character   * this is used to correctly tokenize code identifiers (IDENT) written in non-ascii characters (a variable with German umlaut for instance)    In this specific example, some ""\uNNNN"" sequences were translated inside comments, and some characters were ""swallowed"". So for instance a fragment:    {noformat}  {@code \u}  {noformat}    ends with ""java: illegal unicode escape"".    Related issues:   * CLOV-1305 - syntax highlighting problem   * CLOV-1131 - Eclipse/IDEA editors get out of sync  "
1,Integration tests are not being instrumented in Grails 2.3,"Clover does not instrument integration tests in Grails 2.3.     Caused by:   * new GrailsIntegrationTestCompiler wrapper introduced in Grails 2.3.0 is not being recognized by Clover as a Groovy compiler   * GrailsTestRunner calls Groovy compiler internally and this compiler does not have grover.jar on a classpath, thus Clover AST transformer is not called    Todo:   * check how to recognize/pass inclusion/exclusion patterns to the internal compiler of GrailsTestRunner    Workaround:   * not available"
0,Transitive dependency resolution fails in Grails 2.3 during installation of the Clover plugin,"Since Grails 2.3 sth has changed in the dependency resolution mechanism. As a consequence, when the Clover plugin is being installed for a first time, the transitive dependency to com.cenqua.clover:clover is not resolved and this jar is not available on class path.     As a consequence build fails when it tries to import ""cloverlib.xml"".    Workaround: define a dependency to com.cenqua.clover:clover in BuildConfig.groovy (or in pom.xml)  - in the very same way as it was made in Clover-for-Grails 3.1.12 or older under the Grails 2.2.    Fix:    1) reconfigure dependencies.groovy  / pom.xml ?     2) roll back to a previous approach - i.e. merge clover-grails1 branch into default, produce one plugin version, rollback documentation"
1,Create an API for plugging custom I/O handler,"Currently Clover uses a standard Java IO/NIO libraries to read from and write to: a coverage database, coverage recording files (global and per-test), test optimization snapshots.     This is not sufficient when user want to run an application on a custom file system, such as Apache HDFS.    Provide an API which would allow to provide a custom IO handler.       Scope:   * writing coverage files (global and per-test)   * reading/writing database (optional - we deal a lot with file channels which might be a problem; a growable coverage recorder can be used at runtime instead of this)   * writing optimization snapshot (optional - not everyone uses test optimization)    References:   * https://answers.atlassian.com/questions/255286/does-clover-support-to-read-and-write-clover-db-and-coverage-recording-files-from-hdfs   * https://answers.atlassian.com/questions/48360/how-can-i-get-functional-test-code-coverage-reports-for-mapreduce-program"
0,Clover plugin doesn't load on IDEA 13 Startup,{noformat}  null  java.lang.NullPointerException    com.cenqua.clover.idea.LibrarySupport.getValidatedCloverLibrary(LibrarySupport.java:81)    com.cenqua.clover.idea.build.CloverLibraryInjector$1.run(CloverLibraryInjector.java:44)    com.intellij.openapi.application.impl.ApplicationImpl.runWriteAction(ApplicationImpl.java:997)    com.cenqua.clover.idea.build.CloverLibraryInjector.addCloverLibrary(CloverLibraryInjector.java:61)    com.cenqua.clover.idea.build.CloverLibraryInjector.updateModulesDependencies(CloverLibraryInjector.java:35)    com.cenqua.clover.idea.ProjectPlugin.projectPostStartup(ProjectPlugin.java:116)    com.cenqua.clover.idea.ProjectPlugin$1.run(ProjectPlugin.java:86)    com.intellij.ide.startup.impl.StartupManagerImpl$7.run(StartupManagerImpl.java:286)    com.intellij.ide.startup.impl.StartupManagerImpl.a(StartupManagerImpl.java:259)    com.intellij.ide.startup.impl.StartupManagerImpl.access$100(StartupManagerImpl.java:52)    com.intellij.ide.startup.impl.StartupManagerImpl$4.run(StartupManagerImpl.java:184)    com.intellij.openapi.project.DumbServiceImpl.a(DumbServiceImpl.java:238)    com.intellij.openapi.project.DumbServiceImpl.access$500(DumbServiceImpl.java:53)    com.intellij.openapi.project.DumbServiceImpl$IndexUpdateRunnable$1$3.run(DumbServiceImpl.java:420)    java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:251)    java.awt.EventQueue.dispatchEventImpl(EventQueue.java:733)    java.awt.EventQueue.access$200(EventQueue.java:103)    java.awt.EventQueue$3.run(EventQueue.java:694)    java.awt.EventQueue$3.run(EventQueue.java:692)    java.security.AccessController.doPrivileged(Native Method)    java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)    java.awt.EventQueue.dispatchEvent(EventQueue.java:703)    com.intellij.ide.IdeEventQueue.e(IdeEventQueue.java:696)    com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.java:524)    com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.java:335)    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:242)    java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:161)    java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:150)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:146)    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:138)    java.awt.EventDispatchThread.run(EventDispatchThread.java:91)  {noformat}  
0,Use more unique property names in clover.xml,"The clover.xml contains the following properties:    {code:xml}  <property name=""project.name"" value=""${ant.project.name}""/> <!-- By default, use the ant project name. -->  <property name=""project.title"" value=""${project.name}""/> <!-- If no title is set, use the project.name. -->  <property name=""test.target"" value=""test""/> <!-- The name of the target which runs the tests. -->  {code}    Their names are not very unique so there's some chance that developer could actually use similar ones in his project. This may lead to clash an Ant properties are not read-only once set.     Use more distinct names, e.g. ""clover.project.name"".     Update documentation on CAC accordingly. "
0,Clover Grails Plugin fails when tests are launched from IDEA,"Steps to reproduce:   * import Grails-based project into IntelliJ IDEA    * add Clover Grails plugin to the project   * run *forked* tests from IDEA    Build fails with an error like:    {noformat}  | Error Error running forked test-app: No such property: testNames for class: _Events (Use --stacktrace to see the full trace)  | Error Forked Grails VM exited with error  {noformat}    Workaround:   * disable forked test execution in IDEA    Probable cause:    IDEA does not call ""grails test-app"" but executes the  GrailsScriptRunner, e.g.:    {noformat}  java -cp ... org.codehaus.groovy.grails.cli.support.GrailsStarter --main org.codehaus.groovy.grails.cli.GrailsScriptRunner  {noformat}    The most probably this runner does not import the _GrailsTest.groovy, which contains a definition of the testNames property."
0,Tree map report has an empty line in context menu,"Open TreeMap report in IDEA, right click on a class, a context menu is opened which has an empty first line:    !jump_to_source.png!  "
1,Add possibility to configure <profiles> in Eclipse and IDEA IDEs,"The <clover-setup> and <clover-instr> tasks allow to define <profiles> section, where we can select coverage recorder type etc. There's no possibility to configure it in Clover-for-Eclipse and Clover-for-IDEA plugins."
1,Test spock under gradle,"We don't support Gradle. However, it's worth checking how our Spock works (or not :-) ) with open-source gradle clover plugins available.    To do:    * prepare a sample app with a build.gradle script   * integrate Clover into it using the https://github.com/bmuschko/gradle-clover-plugin    * check if spock instrumentation works here   * publish results on CAC as a developer guide  "
0,Test spock with XML importing,Test how the <clover-report> can be used together with:   * Spock Spec classes with @Unroll    * test results imported from an external XML file    Purpose of test:    * check if runtime test names recorded by Clover matches test names reported by Spock     Outcome:   * document whether it works (and how) or not
0,Fix uncheched warnings for code instrumented by Clover,"Follow up of CLOV-1399:    {quote}  I tried it with 3.2.2 same issue for every instrumented class.  [WARNING] /Users/kasperni/workspace/cake-container/cake-container/cake-container-api/target/clover/src-instrumented/org/cakeframework/container/RuntimeContainerException.java:[24,758] unchecked cast  required: I  found: java.lang.Object    It is this generated line that generates the warning:    return (I) java.lang.reflect.Proxy.newProxyInstance(l.getClass().getClassLoader(), l.getClass()  .getInterfaces(), h);    It can be solved by adding a @SuppressWarnings(""unchecked"") to the instrumented file:    @SuppressWarnings(""unchecked"")  public static <I, T extends I> I lambdaInc(final int i, final T l, final int si)  {........}   {quote}"
0,///Clover:OFF does not work with lambdas,"The following piece of code fails to be instrumented  {code:java}  public class Foo {      // /CLOVER:OFF      public static void main(String[] args) {          new ConcurrentHashMap<>().computeIfAbsent(""foo"", e -> null);      }      // /CLOVER:ON  }  {code}  Results in    [ERROR] /Users/kasperni/workspace/cake-container/cake-container/cake-container-impl/target/clover/src-instrumented/org/cakeframework/internal/container/handler/Foo.java:[28,58] cannot find symbol    symbol:   variable __CLR3_2_2133133hrrnhz2t    location: class org.cakeframework.internal.container.handler.Foo"
0,clover-for-grails plugin is not working with grails.project.fork,"Hello,  When I run the grails test-app command in grails 2.3.5. I get the following error:    {noformat}  Error running forked test-app: No such property: testNames for class: _Events  groovy.lang.MissingPropertyException: No such property: testNames for class: _Ev  ents          at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptByteco  deAdapter.java:50)          at org.codehaus.groovy.runtime.callsite.PogoGetPropertySite.getProperty(  PogoGetPropertySite.java:49)          at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGroovyObjec  tGetProperty(AbstractCallSite.java:231)          at _Events$_run_closure3.doCall(_Events.groovy:75)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.  java:57)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces  sorImpl.java:43)          at java.lang.reflect.Method.invoke(Method.java:606)          at org.springsource.loaded.ri.ReflectiveInterceptor.jlrMethodInvoke(Refl  ectiveInterceptor.java:1254)          at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:  90)          at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)          at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1086)          at groovy.lang.ExpandoMetaClass.invokeMethod(ExpandoMetaClass.java:1110)  {noformat}  "
0,native2ascii error converting i18n bundles for plugin,"When running with -clover.on in our CI environment (TeamCity), we are getting a weird error:    Error native2ascii error converting i18n bundles for plugin [xxxx] java.lang.reflect.InvocationTargetException    It's not constrained to a single plugin, typically it lists different plugins. The full stack trace is:    {code}  [15:28:49][Step 4/8] | Error native2ascii error converting i18n bundles for plugin [spring-security-core-1.2.7.3] java.lang.reflect.InvocationTargetException  [15:28:49][Step 4/8] | Error Error executing script TestApp: : java.lang.reflect.InvocationTargetException (NOTE: Stack trace has been filtered. Use --verbose to see entire trace.)  [15:28:49][Step 4/8] : java.lang.reflect.InvocationTargetException  [15:28:49][Step 4/8]   org.codehaus.gant.GantBuilder.invokeMethod(GantBuilder.java:99)  [15:28:49][Step 4/8]   _GrailsTest_groovy$_run_closure1.doCall(_GrailsTest_groovy:102)  [15:28:49][Step 4/8]   org.codehaus.gant.GantMetaClass.invokeMethod(GantMetaClass.java:133)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16_closure18.doCall(GantBinding.groovy:185)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16_closure18.doCall(GantBinding.groovy)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding.withTargetEvent(GantBinding.groovy:90)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding.this$4$withTargetEvent(GantBinding.groovy)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16.doCall(GantBinding.groovy:185)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16.doCall(GantBinding.groovy)  [15:28:49][Step 4/8]   org.codehaus.gant.GantMetaClass.invokeMethod(GantMetaClass.java:133)  [15:28:49][Step 4/8]   TestApp$_run_closure1.doCall(TestApp:32)  [15:28:49][Step 4/8]   org.codehaus.gant.GantMetaClass.invokeMethod(GantMetaClass.java:133)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16_closure18.doCall(GantBinding.groovy:185)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16_closure18.doCall(GantBinding.groovy)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding.withTargetEvent(GantBinding.groovy:90)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding.this$4$withTargetEvent(GantBinding.groovy)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16.doCall(GantBinding.groovy:185)  [15:28:49][Step 4/8]   org.codehaus.gant.GantBinding$_initializeGantBinding_closure5_closure16.doCall(GantBinding.groovy)  [15:28:49][Step 4/8]   gant.Gant$_dispatch_closure5.doCall(Gant.groovy:381)  [15:28:49][Step 4/8]   gant.Gant$_dispatch_closure7.doCall(Gant.groovy:415)  [15:28:49][Step 4/8]   gant.Gant$_dispatch_closure7.doCall(Gant.groovy)  [15:28:49][Step 4/8]   gant.Gant.withBuildListeners(Gant.groovy:427)  [15:28:49][Step 4/8]   gant.Gant.this$2$withBuildListeners(Gant.groovy)  [15:28:49][Step 4/8]   gant.Gant$this$2$withBuildListeners$0.callCurrent(Unknown Source)  [15:28:49][Step 4/8]   gant.Gant.dispatch(Gant.groovy:415)  [15:28:49][Step 4/8]   gant.Gant.this$2$dispatch(Gant.groovy)  [15:28:49][Step 4/8]   gant.Gant.invokeMethod(Gant.groovy)  [15:28:49][Step 4/8]   gant.Gant.executeTargets(Gant.groovy:591)  [15:28:49][Step 4/8]   gant.Gant.executeTargets(Gant.groovy:590)  [15:28:49][Step 4/8]   org.grails.wrapper.GrailsWrapper.main(GrailsWrapper.java:93)  [15:28:49][Step 4/8] Caused by: java.lang.reflect.InvocationTargetException  [15:28:49][Step 4/8] 	... 30 more  [15:28:49][Step 4/8] Caused by: java.util.ConcurrentModificationException  [15:28:49][Step 4/8]   com.atlassian.clover.ant.groovy.GroovycSupport.logGroovycTasks(GroovycSupport.java:102)  [15:28:49][Step 4/8]   com.atlassian.clover.ant.groovy.GroovycSupport.taskStarted(GroovycSupport.java:123)  [15:28:49][Step 4/8]   org.apache.tools.ant.Project.fireTaskStarted(Project.java:2184)  [15:28:49][Step 4/8] 	... 30 more  [15:28:49][Step 4/8] | Error Error executing script TestApp: : java.lang.reflect.InvocationTargetException  [15:28:50][Step 4/8] Process exited with code 1  {code}    Running without ""-clover.on"" removes this problem. "
0,ClassNotFoundException when running tests in IDEA 13.1 RC with Clover enabled,"Install IDEA 13.1 RC and Clover 3.2.2. Load Java project. Create a run configuration for JUnit, select ""Test kind: All in package"", select ""Before launch: Make"".    Run tests with Clover disabled - all tests are executed.     Run tests with Clover enabled - all tests are executed, but after second and next runs - ClassNotFoundException on a test class occurs. "
1,Several tests fail with Cannot cast object ... to class 'com_cenqua_clover.CoverageRecorder',See here for a travis build exhibiting this problem:    https://travis-ci.org/thehyve/transmart-core-db/jobs/21220075    19 tests there fail because of this problem. The culprit seems to be this line: https://github.com/thehyve/transmart-core-db/blob/5e8f0400b9ddce75e1d3d2adccf747e42bff3ee6/src/groovy/org/transmartproject/db/dataquery/clinical/TerminalConceptVariablesTabularResult.groovy#L39
1,Rename com.cenqua to com.atlassian,"Rename all references to ""Cenqua"" to ""Atlassian"". It affects:   * java packages - com.cenqua.* and com_cenqua_clover (/)   * Clover-for-Eclipse plugins and features (/)   * com.cenqua.clover:clover artifact (clover core) (/)    Optional:    Rename also Clover-for-Maven plugin from 'maven-clover2-plugin' to 'clover-maven-plugin'.     (x) The maven plugin will not be renamed. Reason? Typing 'mvn clover:setup' in a project where clover-maven-plugin is not explicitly defined in pom.xml causes that Maven resolves the 'clover:setup' as 'com.atlassian.maven.plugins:maven-clover-plugin:maven-plugin:3.7' which is wrong. So it will be very confusing for customers.    "
0,Maven plugin sholud handle target/generated-test-sources,"The AbstractInstrumenter recognizes only the target/generated-sources path:    {code:java}  private boolean isGeneratedSourcesDirectory(final String sourceRoot) {          String generatedSrcDirDefaultLifecycle = File.separator + ""target"" + File.separator + ""generated-sources"";          String generatedSrcDirCloverLifecycle = File.separator + ""target"" + File.separator + ""clover"" + File.separator + ""generated-sources"";          return sourceRoot.indexOf(generatedSrcDirDefaultLifecycle) != -1                  || sourceRoot.indexOf(generatedSrcDirCloverLifecycle) != -1;      }  {code}    It shall also detect the target/generated-test-sources."
0,Maven clover2:setup ignores generated sources,We have several maven projects that contain only generated sources (target/generated-sources/...). The default source directory does not exist for those projects. clover2:setup does not work properly for those projects: the generated sources do not get compiled. We currently work around the issue by creating a marker file in the default source directory.
1,Maven clover2:setup triggers duplicate class exception,"We have several maven projects that have more than one source directory. The non-default directories are added using the build-helper plugin. The clover2:setup goal instruments all source folders, but then sets all non-generated directories as source folders on the maven project. That results in compile errors, since source files are both present in the clover instrumented sources and original location.    Example:  java/src << 1. default source folder  java/src-build << 2. additional source folder  target/generate-sources/jaxb << 3. generated source folder    Case (2) is causing problems."
0,Double instrumentation error when running 'mvn site' on JDK8,Run 'mvn clean site' on the src/it/setTestFailureIgnore project using the JDK8 and Maven 2.2.1. It fails with an error:    {noformat}  [INFO] Processing files at 1.8 source level.  [INFO] Instrumentation error  com.atlassian.clover.api.CloverException: Double instrumentation detected: f:\Work\release\release-maven-clover2-plugin-hg\src\it\setTestFailureIgnore\target\clover\src-instrumented\com\atlassian\maven\plugin\clover\samples\setTestFailureIgnore\Simple.java appears to have already been instrumented by Clover.    com.atlassian.clover.instr.java.CloverTokenStreamFilter.guardAgainstDoubleInstrumentation(CloverTokenStreamFilter.java:66)  ...  {noformat}    The same site goal works correctly under JDK7.  
0,Invalid instrumentation code for test methods inside anonymous inline classes,"Clover generates incorrect reference to the static field  __CLR3_2_2_TEST_NAME_SNIFFER in case when a test method is declared inside   an inner (static or non-static) or inside an anonymous inline class, which are located inside a non-test top-level class.     In such case, the top-level class doesn't have the __CLR3_2_2_TEST_NAME_SNIFFER field (as it's not a test class), but inner/inline classes (which are test ones) are trying to access this field.     {noformat}  [clover] 2 test methods detected.  [javac] ----------  [javac] 1. ERROR in ...  [javac] public boolean test() {__CLR3_2_262c962c9htee2hlb.R.globalSliceStart(getClass().getName(),284284);int $CLV_p$=0;java.lang.Throwable $CLV_t$=null;try  {boolean $CLV_r$=__CLR3_2_2qua64563cs();$CLV_p$=1;return $CLV_r$;}    catch(java.lang.Throwable $CLV_t2$){if($CLV_p$==0&&$CLV_t$==null){$CLV_t$=$CLV_t2$;}_CLR3_2_262c962c9htee2hlb.R.rethrow($CLV_t2$);return false;}finally{CLR3_2_262c962c9htee2hlb.R.globalSliceEnd(getClass().getName(),""com.xyz.RuntimeDebugger.SaveTestCaseAction.getDefaultAvailablePredicate.test"",CLR3_2_2_TEST_NAME_SNIFFER.getTestName(),284284,$CLV_p$,$CLV_t$);}}private boolean __CLR3_2_2qua64563cs(){_CLR3_2_262c962c9htee2hlb.R.inc(284284);  [javac] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  [javac] __CLR3_2_2_TEST_NAME_SNIFFER cannot be resolved  [javac] ----------  [javac] 1 problem (1 error)  {noformat}    Code example:    {code:java}  public class SnifferInInnerAndInline {      interface TestAction {          boolean test();      }        static class InnerStaticTest {          public void testInnerStatic() {  // FAILS HERE            }      }        class InnerTest {          public void testInnerNonStatic() {  // FAILS HERE            }      }        TestAction testWithInline() {          return new TestAction() {              public boolean test() {  // FAILS HERE  (assuming that we have a custom test pattern which matches it)                  return true;              }          };      }        public static void main(String[] args) {          new SnifferInInnerAndInline.InnerStaticTest().testInnerStatic();          new SnifferInInnerAndInline().new InnerTest().testInnerNonStatic();          new SnifferInInnerAndInline().testWithInline().test();      }    }  {code}    "
0,change historical report,"scope: header, footer, tables with ""added"" and ""movers"",     optionally, put charts in some boxes"
1,Clover safeEval method is imcompatible with Groovy's @CompileStatic annotation,"When trying to use Clover on a Groovy class that is marked @CompileStatic and it contains a null-safe operator (e.g. Bar bar = foo?.bar), then the groovyc compilation fails with the error:     No such property: bar for class: java.lang.Object    The error is that the generated {code}$CLV_safeEval${code} method returns an Object instead of the type of the property. This then causes Groovy compiler to fail later.    I've created a sample project that exhibits the problem at https://github.com/johnrengelman/clover-grooyv-compilestatic    Simply run {code}./gradlew test{code}    "
1,Grails' controller methods are not shown as entered,"Hello,  For some reason the method in a controller is marked as not entered even though the content inside is shown to be executed. Please can you help me rationalize/ understand this?    Please refer to me screenshots:  index_method_not_executed.png: to see that the index() method is not executed even though its contents are.  test_for_index_method.png: the test that is run to get the method to run.  "
1,reduce html report size,"As a developer I'd like to have small HTML report size in order to reduce disk space consumption, especially in case of builds on CI server.     Possible ways to reduce report size:    1) extract list of packages to separate JSON file and load it dynamically using JS function (instead of embedding it on virtually every page)  (/) DONE    2) use some html compression utility (which would strip all whitespace etc)       https://code.google.com/p/htmlcompressor/#Compressing_selective_content_in_Velocity_templates    (x) performance of the HtmlCompressor is unacceptable (+340% time)    3) remove whitespace from most frequent loops from velocity templates (/) DONE"
1,improve report accessiblity,http://www.w3.org/TR/wai-aria-practices/#TreeView
1,improve navigation between report entities,"We can navigate vertically and horizontally, but on a file level we've got a break:    {noformat}   project   - dashboard | app | test | results | clouds | treemap     v   package - app | test | results | clouds    |    +--------------------------------------------------+    v                                                  v   file - classes | contr tests | methods        test class                                                         v                                                       test  {noformat}    fix it so that we can jump from test file to test results for a file    fix it so that we have one ""source"" tab for a file   -> put 'classes and methods' and 'contributing tests' into expandable tables, dialogs etc  "
1,implement global search in the html report - backend,"Implement search box in a blue app header.  Generate JSON data:  * packages, files, classes, methods, tests  Split into several indexes (for smaller file size) as theoretical size is up to 40MB (200'000 objects per 200 chars).  Add ""automatic"" option (up to 10'000 elements).   (/) Add options for:   <clover report>/<current>/<format>/<globalSearch scope=""package,file,class,method,test|auto"" limit=""10000""/>  (/) Add option for HtmlReporter:  --globalSearchScope=package,file,class,method,test|auto  --globalSearchLimit=10000  to choose which elements shall be searchable."
0,tree view is not compacted properly when only 1 package is present,"!tree_view_bug.png!    We have com.cenqua + samples.money instead of exactly one (root) node named ""com.cenqua.samples.money"".   "
0,provide explanation when number of files is greater than number of classes for a package,"There might be a case that number of files > number of classes listed:    !more_files_than_classes.png!    This is normal behaviour, because by default Clover hides entities having no executable code, such as: empty classes, enums (with no methods), interfaces (with no virtual extension methods).     However, this may be confusing for end user.     How we could improve this:    1) Set the showEmpty to true by default. Benefit: 'classes >= files' in all cases. Drawback: report will contain unnecessary noise.     2) Add [(?) Help] button above the table. Example:    !package_help.png!    3) Add tooltip for 'Files'  (TotalElements.getHelp())    4) Add small text below the class table. For example:    ""3 classes are not listed because they contain no executable code.""  "
1,replace a vertical navigation component by a sidebar,Instead of the aui-vertical-nav use the aui-sidebar component. 
0,Upgrade HTML report to ADG 2.0 / AUI 5.7,"Upgrade AUI library to 5.7.x. We may need to make some corrections of CSS to be compatible (probably: colors, shadows, notification messages)."
1,as a developer I'd like to have nanosecond precision for test duration,In reports lots of tests have duration = 0. This is because unit tests are fast and the System.currentTimeMillis() is used to record start and end time of a test. Which is not precise by design.    We could increase precision to nanoseconds using System.nanoTime().    Note: we'd still need to call System.currentTimeMillis() to record datetime when test was run.
0,"Remove ""See more"" links from the report dashboard",for:  * Most complex packages  * Most complex classes  * Least tested methods  * Class Coverage Distribution  * Class Complexity          
0,"""Show contributing test"" change to ...","""<a>Select tests</a> to highlight the code coverage"" or something similar"
0,"The ""Show methods"" link does not open the modal dialog","This happens for inner classes (i.e. when a class name contains ""."")    May be caused by a way how Modal Dialog handles element id."
0,tool tips in dashboard widgets shall not be displayed on the right,"Tool tips are by default placed on the right, so they don't fit in the 'Clover Dashboard' view:    !screenshot-1.png!"
0,use ADG fonts and colours in Cloud View in IDEA,The ADG HTML report in Clover 4 uses new colour set (shades of red instead of blue-red) for tag clouds. The Clover-for-Eclipse has this theme:    [^cloud_view_eclipse.png]    but the Clover-for-IDEA still uses the old colour set:    [^cloud_view_idea.png]
0,AntIntegrator adds unnecessary double quotes for -Dparam=value JVM options,"Class com.atlassian.clover.ci.AntIntegrator puts values of the following properties:    -Dclover.historydir and -Dclover.license.cert, -Dclover.license.path    in double quotes. On Windows platform it causes problems when a value contains space character - parameters are being split. See BAM-10740.    Add a check if OS is Windows and don't add double quotes in such case. "
1,implement global search in the html report - frontend,"Implement search box in blue app header.    Generate JSON data:   * packages, files, classes, methods, tests    Split into several indexes (for smaller file size) as theoretical size is up to 40MB (200'000 objects per 200 chars).    Add ""automatic"" option (up to 10'000 elements).     Add options for <clover report>     <index scope=""package, file, ..., method | auto"">    to choose which elements shall be searchable."
1,View latest Clover information (BAM-14428),See BAM-14428.
0,Clover tab with an HTML report generated by Clover 4 shall have a vertical scroll bar (BAM-14777),See BAM-14777.
0,Configure Fisheye/Crucible build to use Clover 4,NULL
0,Move CloverCompilerAdapter out from org.apache.tools.ant.taskdefs package,The org.apache.tools.ant.taskdefs.CloverCompilerAdapter may cause conflicts with Apache Ant packaged as OSGI bundle which exports the same package.     Possible fix:  (x) The Javac.getFileList() is a getter for compileList. Try using it - we must ensure that compileList and replacementCompileSet have the same size or  (/) Use reflections to access the package-protected field.    Thanks to this we could move CloverCompilerAdapter  to com.atlassian.clover package. 
0,Application Packages list is empty in Classic report,NULL
0,Instrumenting assignment of method reference to field results in reference to undefined symbol.,"Instrumenting the following snippet results in the compilation error  [ERROR] clover/src-instrumented/Test.java:[2,18] cannot find symbol  [ERROR] symbol:   variable __CLR3_3_000hysmxbbd  [ERROR] location: class Test    Source:  public class Test {      Runnable r = System::currentTimeMillis;  }    Instrumented code:  /* $$ This file has been instrumented by Clover 3.3.0#2014033113020816 $$ */public class Test {      Runnable r = __CLR3_3_000hysmxbbd.lambdaInc(0,System::currentTimeMillis,1);  }  "
1,BAM-13404 BAM-15085 Use BuildConfig to install Clover,NULL
1,BAM-13208 Automatic integration in multi-module builds,Follow the approach #1
1,BAM-7272 BAM-10516 BAM-11151 Working subdirectory,NULL
1,BAM-14734 BAM-14907 Clover charts,https://jira.atlassian.com/browse/BAM-14734  https://jira.atlassian.com/browse/BAM-14907
0,Release Clover 4.0.2,"ant, eclipse, idea, maven, grails"
0,Resolve path to clover.jar when packed as OSGI bundle,"ClassPathUtil.getCloverJarPath() searches for a JAR in case when URL returned by class loader is like:     {noformat}  jar:file:!/path/to/clover.jar/com/atlassian/clover/util/ClassPathUtil.class  {noformat}    or    {noformat}  file:/path/to/classes/directory/com/atlassian/clover/util/ClassPathUtil.class  {noformat}    However, it does not handle a case for OSGI bundles, which have an URI like:    {noformat}  bundle://28.0:2/com/atlassian/clover/util/ClassPathUtil.class  {noformat}    Fix: use getClass().getProtectionDomain().getCodeSource().getLocation().toURI()  "
1,The Grails' @Transactional annotation conflicts with Clover's AST transformations,"The @Transactional annotation is the new Grails-y way of controlling transactional behavior in a service. It uses an AST transformation to essentially wrap public methods (if added to the class) in a GrailsTransactionTemplate. So you get code that looks like this:    {code:java}  public void reject(Invoice invoice)    {      Reference invoice = new Reference(invoice);      CallSite[] arrayOfCallSite = $getCallSiteArray();      GrailsTransactionAttribute $transactionAttribute = (GrailsTransactionAttribute)ScriptBytecodeAdapter.castToType(arrayOfCallSite[9].callConstructor(GrailsTransactionAttribute.class), GrailsTransactionAttribute.class);      GrailsTransactionTemplate $transactionTemplate = (GrailsTransactionTemplate)ScriptBytecodeAdapter.castToType(arrayOfCallSite[10].callConstructor(GrailsTransactionTemplate.class, this.transactionManager, $transactionAttribute), GrailsTransactionTemplate.class);      $transactionTemplate.execute(new _reject_closure5(this, invoice));    }  {code}    The real body of the method is in the reject_closure5, which ends up calling a method named ""$tt_reject(...)"".    Clover doesn't realize that the code originally under the reject method has essentially been relocated to the $tt__reject method.    Given that @Transactional is a core part of Grails (starting in 2.3, I believe) and the recommended way to control transactions going forward, this is probably something that should be addressed in Clover core."
1,Upgrade FastUtil library to version 6.5.2 or later,"API was changed since version 4.4.3, some methods used by Clover are missing and there's no direct replacement for them. Migrate Clover's code. "
1,Upgrade Atlassian Extras to 3.2.x,"API has changed in Atlassian Extras 3.x vs 2.5. There's no   LicenseManagerFactory.getLicenseManager()    We can compose our own license manager and decoder using:  Version1LicenseDecoder + Version2LicenseDecoder + DelegatingLicenseDecoder + CloverProductLicenseFactory + DefaultLicenseManager    However, there are problems with decoding the ""Clover Desktop"" license = the CloverLicenseTest.testDesktopLicense fails.     Investigate:   * maybe Clover Desktop license format has changed?   * or maybe overlooked by atlassian-extras developers?   * or ... ?"
1,Replace iText library,"Replace current iText 2.0.1 library by:   * a commercial version of iText 5.x or    * a PDFBox (http://pdfbox.apache.org) or other tool based on a good open-source license (Apache 2.0, MIT, BSD, PD)."
0,Improve HTML ADG side bar view and a package tree performance,NULL
3,Create Bitbucket Connect Clover Plugin,Bitbucket - Dan Bennett (Team Lead) - Investigate the creation of a Connect plugin to expose Clover functionality in Bitbucket diff & file viewer (same as Stash functionality)
1,S3 Artifact handler corrupts Clover tab in Bamboo,"Bamboo 5.7 introduces new artifact handler to store files in Amazon S3. Due to a fact that it works on a file-by-file basis, we cannot expose the entire HTML report as a 'site' embedded in the <iframe>. As a consequence it looks like this:    !clover-tab-with-s3-handler.png!          *Workaround:*    Do not use S3 artifact handler for builds with Clover. Default handler settings can be overridden in Plan configuration > Miscellaneous:    !artifact_handlers.png!          *Possible fixes:*    1) Implement a wrapper on S3 handler, which would download all files from an artifact definition to some temporary directory and expose it in <iframe>     * problem: Clover's reports may have up to several GB for large projects, downloading all files every time someone opens the 'Clover' tab will be time consuming, we'd need to keep such artifacts for a longer period of time    2) Add warnings for Clover integration in the UI in case when S3 handler is found. Write information about this limitation on confluence.atlassian.com.    3) Create a ZIP archive out of Clover Report artifact and give user an ability to download this file.    4) Add extra option in artifact definition so that user could select which handler should keep given artifact. Thanks to this, S3 could be excluded.     5) Due to a fact that new Clover HTML ADG reports do not contain frames, we could think about a thin wrapper, which would download only one page and replace all original links in the HTML file by proper links to S3; if user would click on a link, he would get another page processed this way...   * we'd have to handle *.js, *.css, images etc too.    5b) Create something like a ""ZIP File Handler"", which could serve files from an archive (then we could pack Clover's HTML report and store on S3 or any other medium). It would also require rewriting links.    6) Look for some solution provided by Amazon, such as Static Website Hosting on S3    http://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html    "
0,Distributed Coverage does not work due to java.rmi.UnmarshalException,"{noformat}  Received result: Remote_Stub[UnicastRef2 [liveRef: [endpoint:[192.168.157.181:11  98,clover.gnu.cajo.invoke.Remote$RCSF@732e103d](remote),objID:[-26c3f6bf:148ea10  98a9:-7ffb, -3801964032288496245]]]]  Started proxy: Thread-9  cutOff(java.rmi.UnmarshalException: error unmarshalling return; nested exception   is:          java.io.WriteAbortedException: writing aborted; java.io.NotSerializableE  xception: com.atlassian.clover.ErrorInfo); from: localhost:1198. Attempting to r  econnect.  {noformat}"
0,Test optimization in IDEA 14 shows wrong number of classes,"There are 2 test classes in the 'Moneybags' tutorial project. Running test optimization in IDEA 14 (EAP build 138.2458.8) shows ""3 test classes"".    !idea14_clover_test_optimization.png!"
0,An orphaned lozenge when there are no search results ,!html_adg_orphaned_lozenge.png!
0,A wrong caption in a search box in an HTML ADG report,!search_box_wrong_caption.png!
0,A historical report contains link to an XML report,!wrong_links_in_a_historical_report.png!
1,Upgrade third party libraries used by Clover - 2nd phase,"Upgrade third party libraries to their latest versions:   * Antlr 4.0, including java.g syntax file (x) tracked in CLOV-1329, CLOV-1331 and CLOV-1332   * atlassian-extras 3.2 (x) tracked in CLOV-1553   * cajo 1.175   * commons-lang3 3.3.2 (x)   * fastutil 6.5.2 (x) tracked in CLOV-1552   * GSON 2.2.2    * JDOM 2.0.4   * JSON - replace by com.google.code.gson:gson    Upgrade feasibility to be verified:   * theJIT   * KTreeMap/JTreeMap    Get rid of:   * Overlibvms (x) not until we drop Classic reports (Clover 4.1)   * Utils.js (x) not until we drop Classic reports (Clover 4.1)   * iText (x) tracked in CLOV-1554    To be considered:   * upgrade Log4j to or 2.0.x"
1,BAM-699 BAM-2883 BAM-13786 Chart points with failed builds or no daily data,NULL
1,BAM-9048 More configuration options for automatic Clover integration,NULL
0,Test Clover-for-IDEA compatibility with IDEA 14,NULL
0,Add additional logging for <clover-setup>/<files> element,"Currently, when <clover-setup> contains a <fileset> element, Clover prints information about included/excluded files (on a verbose logging level).    In <clover-setup> we can also use a similar <files> element - but there's no logging for it. Add logging on a verbose level."
1,Insert Clover goals between original ones,"    add ""clover2:setup"" after the ""clean"" (if present) and before other goals          but ""jaxb2:generate clover2:setup""          but ""wsdl2java clover2:setup""  "
1,Add warnings in UI about install/deploy phases,"    what to do in case when ""install"" or ""deploy"" phase is declared? deploying instrumented JARs shall be avoided in general          write help for ""automatically integrate clover"" radio button?          write warning in UI if such goals are found?          write warning in a build log if such goals are found?  "
0,Prepare Bamboo documentation about the change,"* Write big warning in the Bamboo 5.8 Release Notes what do to with Clover-enabled plans before upgrading  * Update ""Enabling the Clover add-on"" page (how it works for maven in bamboo 5.8 and 5.7)  * Update answers.a.c questions about this problem and CLOVERKB too  "
1,Integrate Clover with JUnitParams,The JUnitParams (https://github.com/Pragmatists/junitparams) is much more convenient way to parameterize tests instead of using JUnit's 4.10 @Parameterized annotation. 
1,Clover does not work with Grails 2.4.4,"When developers use ""./grailsw run-app"" it gives the following error.     {noformat}  Running Grails application  Error Error loading event script from file [/opt/stanson/target/work/plugins/clover-4.0.2/scripts/_Events.groovy] startup failed:  /opt/stanson/target/work/plugins/clover-4.0.2/scripts/_Events.groovy: 4: unable to resolve class org.codehaus.groovy.grails.test.GrailsTestTargetPattern  @ line 4, column 1.  import org.codehaus.groovy.grails.test.GrailsTestTargetPattern  ^  1 error  (Use --stacktrace to see the full trace)  {noformat}"
2,Use heuristics to solve JDK8 Stream compilation errors,Spike in which a problem with Java8 streams will be attacked as it seems that using stream() is very popular and the most probably it causes the majority of compilation problems with Clover's lambdaInc().    Scope: focus on streams only.    Out of scope: attacking a generic problem of method signatures with a mixture of functional and non-functional arguments. 
0,Set instrumentLambda=none as a default value for instrumentation,"This is a temporary workaround thanks to which java8 code will always compile, at the cost that lambdas are not measured. User will be still able to enable lambda instrumentation, but this will be a conscious decision, so seeing compilation errors should be less surprising.     Scope: CloverInstr, ant, maven, idea, eclipse, grails plugins."
0,"CloverOptimizedTestNGSelector does not add ""clover-optimized"" group to TestNG @Before and @After annotations","In order for Clover to optimize a TestNG test suite, we have to use the {{CloverOptimizedTestNGSelector}} listener, and restrict TestNG to only run the ""clover-optimized"" group, as in:  {code:xml|title=build.xml snippet}  <testng groups=""clover-optimized"" listeners=""com.atlassian.clover.ant.tasks.testng.CloverOptimizedTestNGSelector"" >        <!-- ... -->    </testng>  {code}    The problem with this is that TestNG's {{@Before}} and {{@After}} annotations ({{@BeforeMethod}}, {{@AfterClass}}, etc) will not be run, because they are not part of the ""clover-optimized"" group.    I have a workaround, which is to qualify all such annotations with {{alwaysRun=true}}, but this is pretty ugly.    A possible solution is for {{CloverOptimizedTestNGSelector}} to implement {{IAnnotationTransformer2}} and override [transform( IConfigurationAnnotation, Class, Constructor, Method )|http://testng.org/javadoc/org/testng/IAnnotationTransformer2.html#transform%28org.testng.annotations.IConfigurationAnnotation,%20java.lang.Class,%20java.lang.reflect.Constructor,%20java.lang.reflect.Method%29], automatically adding the ""clover-optimized"" group to all non-tests."
0,Replace clover.licenseLocation by maven.clover.licenseLocation,"While it's perfectly fine to have a declaration like this:    {code:xml}  <properties>    <clover.licenseLocation>clover.license</clover.licenseLocation>  </properties>  {code}     and next refer it in maven-clover2-plugin's configuration like this:    {code:xml}  <configuration>     <licenseLocation>${clover.licenseLocation}</licenseLocation>  </configuration>  {code}    it's misleading for users, because in case they would like to pass it via -D parameter or declare in settings.xml without having the <licenseLocation> in maven-clover2-plugin section, it will not work, because the correct property name is 'maven.clover.licenseLocation'. See:    https://docs.atlassian.com/maven-clover2-plugin/latest/setup-mojo.html#licenseLocation      Correct all references in documentation on confluence.atlassian.com, answers.atlassian.com, maven-clover2-plugin source code, tutorial source code.   "
1,Java8: method reference to a non-static method fails to compile,"{quote}  *NOT REPRODUCIBLE*  {quote}    In this code sample:    {code:java}  return map.entrySet().stream()  			.collect(Collectors.toMap(  				Map.Entry::getKey,  				e -> e.getValue().getAverage()  			));  {code}    the Collectors.toMap() takes a ""Map.Entry::getKey"" method reference as input argument. However, the getKey method is not static.     Wrapping it by Clover's lambdaInc() method leads to a compilation error like this:    {noformat}      [javac]   symbol:   method lambdaInc(int,Map.Entry::getKey,int)      [javac]   location: class __CLR4_0_22ci2cii4lxhoaj      [javac]  Foo.java:33: error: invalid method reference      [javac] 					__CLR4_0_22ci2cii4lxhoaj.lambdaInc(3045,Map.Entry::getKey,3046),      [javac] 					                                           ^      [javac]   non-static method getKey() cannot be referenced from a static context      [javac]   where K is a type-variable:      [javac]     K extends Object declared in interface Entry  {noformat}                  *Possible workarounds ??*    1) Disable instrumentation of expression-like lambda functions (method references are treated as expression-like lambda) - use instrumentLambda=""block"" or ""none"".    2) Wrap such method reference with ///CLOVER:OFF and ///CLOVER:ON keywords, e.g.:    {code:java}  return map.entrySet().stream()  			.collect(Collectors.toMap(  ///CLOVER:OFF  				Map.Entry::getKey,  ///CLOVER:ON  				e -> e.getValue().getAverage()  			));  {code}    3) Use a variable with a non-static method reference or change to a lambda expression.    {code:java}  return map.entrySet().stream()  			.collect(Collectors.toMap(  				e -> e.getKey(), // unfortunately IDEA suggests to change it back  				e -> e.getValue().getAverage()  			));  {code}    "
0,Java8: don't instrument lambda functions when source < 1.8,"No matter what ""-source"" value is set, Clover instruments both expression-like and block-like lambdas.     It's a behaviour slightly different from the one we have for e.g.:   * -source 1.3 (it handles 'assert' as not a keyword) or    * -source 1.4 (no 'enums').     Ways to improve it:    1) change Java grammar so that handling lambda functions is optional - only for source >= 1.8; otherwise Clover should throw a parse error      2) or parse lambdas also for source < 1.8 (no parse errors would be thrown, only javac will raise a compilation error), but do not add Clover's instrumentation to it    {code:java}  private LambdaBlockEntryEmitter instrEnterLambdaBlock(MethodSignature lambdaSignature, CloverToken leftCurly) {          if (cfg.getInstrumentLambda() == LambdaInstrumentation.ALL || cfg.getInstrumentLambda() == LambdaInstrumentation.BLOCK  /* add source level check*/ ) {      ...  {code}    "
0,New colour scheme for a tag cloud and a coverage treemap in the HTML report,"Customers reported that using a single, red color gradient in a coverage tree map and in tag clouds has two drawbacks:   * it's harder to find which code areas require improvement   * seeing always a red color is discouraging  :-)    "
0,Support Groovy 2.4.0 in Clover,NULL
0,Clover doesn't support Spock 1.0 and 1.0-SNAPSHOT,"It works with Spock v0.7-groovy-2.0, but it doesn't work with Spock v1.0-groovy-2.0-SNAPSHOT.    The root cause is that the IGlobalExtension interface changed and it now has start() and stop() methods, which the v0.7 version did not have, and so an AbstractMethodError: null is thrown:    {code}  java.lang.AbstractMethodError: null          at org.spockframework.runtime.GlobalExtensionRegistry.startGlobalExtensions(GlobalExtensionRegistry.java:105)          at org.spockframework.runtime.RunContext.start(RunContext.java:63)          at org.spockframework.runtime.RunContext.get(RunContext.java:165)          at org.spockframework.runtime.Sputnik.runExtensionsIfNecessary(Sputnik.java:88)          at org.spockframework.runtime.Sputnik.run(Sputnik.java:61)          at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)          at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)          at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)  {code}    Because of this, we cannot use the most recent features in Spock, since v0.7 is more than 2 years old."
0,Clover fails to generate HTML report for more than one project in Eclipse,"    {noformat}  Clover Version 4.0.3, built on January 27 2015 (build-dev)  Loaded from: D:\Soft\Eclipse\eclipse-4.4-luna\plugins\com.atlassian.clover.eclipse.core_4.0.3.v20150127120908-dev.jar  Clover: Developer License registered to Atlassian.  [C:/Users/Marek/.workspace-eclipse-4.4/Moneybags/.clover/coverage.db, 0 seconds""C:/Users/Marek/.workspace-eclipse-4.4/javasyntax18/.clover/coverage.db, 0 seconds--report, --testsIncludePattern, C:/Users/Marek/.workspace-eclipse-4.4/Moneybags/**/*Test.java, C:/Users/Marek/.workspace-eclipse-4.4/Moneybags/**/Test*.java, C:/Users/Marek/.workspace-eclipse-4.4/javasyntax18/**/*Test.java, C:/Users/Marek/.workspace-eclipse-4.4/javasyntax18/**/Test*.java--testsExcludePattern, C:/Users/Marek/.workspace-eclipse-4.4/Moneybags/, C:/Users/Marek/.workspace-eclipse-4.4/javasyntax18/, -a, -i, C:\Users\Marek\AppData\Local\Temp\clover3525180172645953949merge, -o, C:\Users\Marek\Desktop, -t, Coverage Report, -tc, 0, -f, , -s, 0 days]  Error generating report: java.lang.NumberFormatException  java.lang.NumberFormatException    java.math.BigDecimal.<init>(BigDecimal.java:459)    java.math.BigDecimal.<init>(BigDecimal.java:728)    com.atlassian.clover.cfg.Interval.initialise(Interval.java:183)    com.atlassian.clover.cfg.Interval.<init>(Interval.java:121)    com.atlassian.clover.eclipse.core.reports.MergeReportJob$MergeAndReport.run(MergeReportJob.java:140)    com.atlassian.clover.eclipse.core.reports.ForkingReporter.main(ForkingReporter.java:25)  {noformat}  "
0,Unable to open generated reports in Eclipse 4.4,"Steps to reproduce:    Open ""Coverage Exploer"", click ""Run new report ..."" and create any report. Next click ""View report"" and select it on a list. Nothing happens in the UI and the ""Error log"" view shows ""Unhandled event exception"" entry with a stack trace as follows:    {noformat}  java.lang.NoClassDefFoundError: Could not initialize class com.atlassian.clover.eclipse.core.reports.OpenReportOperation    com.atlassian.clover.eclipse.core.views.actions.GenerateReportAction$2.widgetSelected(GenerateReportAction.java:151)    org.eclipse.swt.widgets.TypedListener.handleEvent(TypedListener.java:248)    org.eclipse.swt.widgets.EventTable.sendEvent(EventTable.java:84)    org.eclipse.swt.widgets.Display.sendEvent(Display.java:4353)    org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1061)    org.eclipse.swt.widgets.Display.runDeferredEvents(Display.java:4172)    org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3761)    org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1151)    org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)    org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:1032)    org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:148)    org.eclipse.ui.internal.Workbench$5.run(Workbench.java:636)    org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)    org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:579)    org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)    org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:135)    org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)    org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:134)    org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:104)    org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:382)    org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:236)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)    java.lang.reflect.Method.invoke(Unknown Source)    org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:648)    org.eclipse.equinox.launcher.Main.basicRun(Main.java:603)    org.eclipse.equinox.launcher.Main.run(Main.java:1465)  {noformat}    "
1,Error when closing Cloud Editor in Eclipse,"Closing the ""Cloud Editor"" in Eclipse causes that sometimes an exception is thrown and reported in the Error Log:    {noformat}  Error disposing widget for : org.eclipse.e4.ui.model.application.ui.basic.impl.PartImpl Clover Coverage Cloud Report      java.lang.NullPointerException    org.eclipse.swt.custom.CTabFolderLayout.computeSize(CTabFolderLayout.java:39)    org.eclipse.swt.widgets.Composite.computeSize(Composite.java:234)    org.eclipse.swt.layout.GridData.computeSize(GridData.java:491)    org.eclipse.swt.layout.GridLayout.layout(GridLayout.java:221)    org.eclipse.swt.layout.GridLayout.layout(GridLayout.java:197)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1290)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1276)    org.eclipse.swt.widgets.Composite.layout(Composite.java:665)    org.eclipse.swt.custom.CTabFolder.runUpdate(CTabFolder.java:3755)    org.eclipse.swt.custom.CTabItem.getBounds(CTabItem.java:149)    org.eclipse.swt.custom.CTabFolder.onPaint(CTabFolder.java:1993)    org.eclipse.swt.custom.CTabFolder$1.handleEvent(CTabFolder.java:289)    org.eclipse.swt.widgets.EventTable.sendEvent(EventTable.java:84)    org.eclipse.swt.widgets.Display.sendEvent(Display.java:4353)    org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1061)    org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1085)    org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1070)    org.eclipse.swt.widgets.Composite.WM_PAINT(Composite.java:1491)    org.eclipse.swt.widgets.Control.windowProc(Control.java:4667)    org.eclipse.swt.widgets.Display.windowProc(Display.java:5036)    org.eclipse.swt.internal.win32.OS.VtblCall(Native Method)    org.eclipse.swt.internal.ole.win32.IOleInPlaceObject.InPlaceDeactivate(IOleInPlaceObject.java:21)  {noformat}    and     {noformat}  Unhandled event loop exception    java.lang.NullPointerException    org.eclipse.swt.custom.CTabFolderLayout.computeSize(CTabFolderLayout.java:39)    org.eclipse.swt.widgets.Composite.computeSize(Composite.java:234)    org.eclipse.swt.layout.GridData.computeSize(GridData.java:491)    org.eclipse.swt.layout.GridLayout.layout(GridLayout.java:221)    org.eclipse.swt.layout.GridLayout.layout(GridLayout.java:197)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1290)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1297)    org.eclipse.swt.widgets.Composite.updateLayout(Composite.java:1276)    org.eclipse.swt.widgets.Composite.setLayoutDeferred(Composite.java:1087)    org.eclipse.swt.widgets.Display.runDeferredLayouts(Display.java:4196)    org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3754)    org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1151)    org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)    org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:1032)    org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:148)    org.eclipse.ui.internal.Workbench$5.run(Workbench.java:636)  {noformat}      Apparently, it fails in the CTabFolderLayout.computeSize() method:    {code:java}  protected Point computeSize(Composite composite, int wHint, int hHint, boolean flushCache) {      CTabFolder folder = (CTabFolder)composite;      CTabFolderRenderer renderer = folder.renderer;      ...      tabW = Math.max(tabW, renderer.computeSize(width, 2, gc, -1, -1).x);  {code}    the 'renderer' is null. This renderer is being set in CTabFolder.init() method and set to null in the CTabFolder.onDispose() method.     It *may* be related with the:     CloudEditor.dispose() which calls removeCoverageChangeListener after the super.dispose()  so a refresh event may come in the meantinme    or    CloudEditor.createPages() which ""hacks"" the container layout by changing it from FillLayout to GridLayout."
1,Automatic Clover integration does not work for <groovyc> task,"*Problem:*    Automatic Clover integration in Bamboo does now work for Ant Tasks containing the <groovyc> task.    *Root cause:*    When automatic Clover integration is enabled in Bamboo, it uses Clover's AntIntegrationListener:    {noformat}  ant ... -lib clover.jar -listener com.atlassian.clover.ci.AntIntegrationListener  {noformat}    This listener listens to Ant's build events and intercepts Ant tasks such as:    <javac>, <java>, <javadoc>, <junit>    However, it does not intercept the <groovyc> one.     Integration with <groovyc> task is performed by the <clover-setup> task, which calls:    {code:java}  GroovycSupport.ensureAddedTo(antProject);  {code}    Note that GroovycSupport implements Ant's BuildListener. However, it cannot be passed via command line, like this:    {noformat}  ant ... -lib clover.jar -listener com.atlassian.clover.ant.groovy.GroovycSupport  {noformat}    because the GroovycSupport does not have a default constructor.     *Workaround:*    Call the <clover-setup> from an Ant script. For example, add the following target:    {code:xml}  <target name=""instrument"" if=""with.clover"">          <!-- we assume that automatic integration is enabled, which adds the '-lib clover.jar' -->          <taskdef name=""clover-setup"" classname=""com.atlassian.clover.ant.tasks.CloverSetupTask""/> <!-- from the clover.jar -->            <!-- call clover-setup to intercept <groovyc> task -->          <clover-setup/>            <!-- This fake compilation is to cheat the automatic Clover integration, so that it won't display the               ""Clover reports not being generated. No Clover instrumentation was done."" error message.               We instrument java sources, but through the <groovyc> task; and we need to trick Clover so that it               generates an HTML report at the end. -->          <mkdir dir=""${target.classes}""/>          <javac failonerror=""false"" srcdir=""${src.java}"" excludes=""**/*"" destdir=""${target.classes}""/>  </target>    <target name=""compile"" depends=""instrument"">          <groovyc destdir=""${target.classes}"" classpathref=""build.classpath"" fork=""yes"">              <javac source=""1.7"" target=""1.7"" debug=""on"" encoding=""UTF8"">              </javac>              <src path=""${src.java}""/>              <src path=""${src.groovy}""/>          </groovyc>  </target>  {code}    Clover's initialization will occur only if '-Dwith.clover=true' is set. Therefore, modify the Ant Task in Bamboo, for instance:    {noformat}  clean test -Dwith.clover=true  {noformat}    *Possible fixes:*    1) Add default constructor for GroovycSupport. Pass it as a second '-listener' to ant command in Bamboo.    2) Change the AntIntegrationListener so that it will intercept all <groovyc> tasks as well.  "
1,Protect against deployment of instrumented code,"If user calls ""mvn clover2:setup install"" or ""mvn clover2:setup deploy"", code instrumented by Clover will be installed to ~/.m2 or deployed to a repository.     This may be not a desired behavior, especially if developer is not aware of it (for instance if someone switches on the automatic Clover integration in Bamboo).    Furthermore, installation may occur not only for ""install"" or ""deploy"" phases expressed explicitly in the command line, but it can also happen when a plug-in runs such life cycle (mvn release:perform for instance).     Implement a protection against it - in the clover2:setup check the reactor which phases are about to run and refuse code instrumentation if 'install' or 'deploy' will be called.    This protection shall not be enabled by default, because there may be actually a need to install instrumented code (for instance - multiple build plans using the same local m2 cache or repository to fetch compiled artifacts and we want to measure coverage for all modules).    Enable protection by a flag. e.g.    {code:xml}  <configuration>    <repositoryPollutionProtection>true</repositoryPollutionProtection>  </configuration>  {code}    mvn -Dmaven.clover.repositoryPollutionProtection=true    Expected behavior: fail a build if repositoryPollutionProtection=true and build lifecycle contains 'install' or 'deploy' phases.    *Extra:*    Fail also if artifact contains a custom classifier and clover2:instrument is used (as Maven cannot handle an artifact with two classifiers)."
0,Set up developmment environment for Stash,probably on Linux VM
1,as a plugin developer I'd like to have annotations for a package in a Clover database,"Currently, annotations are available for classes and methods only:   * ClassInfo.getModifiers().getAnnotations()   * MethodInfo.getSignature().getAnnotations()    There's nothing like PackageInfo.getAnnotations().     The reason to have this is that _package-info.java_ files may contain annotations (but no executable code). While having this adds no value for Clover instrumentation itself, it could be useful for plugin developers, who would like to read Clover database directly.     For instance, this information could be used to filter classes based on the information stored on a package level. See: https://answers.atlassian.com/questions/12781852/how-to-read-package-annotations-from-clover-db  "
0,Compiling Groovy code with Clover throws NPE,Clover fails to integrate with Groovyc task and throws NPE:    {noformat}  Clover failed to integrate with <groovyc/>  java.lang.NullPointerException  at com.atlassian.clover.ant.groovy.GroovycSupport.buildTestDetector(GroovycSupport.java:242)  at com.atlassian.clover.ant.groovy.GroovycSupport.taskStarted(GroovycSupport.java:141)  at org.apache.tools.ant.Project.fireTaskStarted(Project.java:2184)  at org.apache.tools.ant.Task.perform(Task.java:344)  at org.apache.tools.ant.Target.execute(Target.java:392)  at org.apache.tools.ant.Target.performTasks(Target.java:413)  at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)  ...  {noformat}    *Cause:*    NPE is thrown when Clover cannot find it's AntInstrumentationConfig in project references in an Ant-based build. This can happen when a build calls <ant> or <antcall> tasks without {{inheritrefs=true}} - lack of this property causes that AntInstrumentationConfig is not passed to these calls.    *Workaround:*    Ensure that  {{inheritrefs=true}} is used.   
1,Publish Clover-for-Scala prototype,"Tasks:   * request releasing it as open source   * publish plugin's source code on BitBucket,    * prepare and publish documentation on CAC/Clover-for-Scala.    * prepare plug-in developer documentation"
2,as a user I want to use Clover with maven-scala-plugin,"Integration of Clover-for-Scala with the maven-scala-plugin.     Tasks:   * clover2:setup shall ""connect"" to the maven-scala-plugin; t.b.d. how - META-INF/services? a plugin point? some properties?   * clover2:clover shall include also *.scala sources    Checks:   * is it possible to have joint Scala-Java compilation (like with joint Groovy-Java)?    Out of scope:   * clover2:instr is out of scope as integration is performed using AST, not source code   * clover2:optimize as test optimization is in another task  "
1,as a user I want to use Clover with scalac ,"Integration of Clover-for-Scala with scalac command line tool.    Tasks:   * shall be easy - plugging via META-INF/services    Checks:   * is it possible to have joint Scala-Java compilation? if yes, then how it works and how to intercept both compilers?  "
3,as a user I want to use Clover with SBT,"Integration of Clover-for-Scala with SBT.    Tasks:   * write build plugin for SBT - equivalent of clover2:setup, shall intercept any scalac calls; shall add clover.jar (clover-scala.jar?) to class paths for compilation and runtime   * write build plugin for SBT - equivalent of clover2:clover, shall generate reports; we may call Clover-for-Ant under the hood    "
1,as a user I want to see nested functions in reports,"Scala allows to nest functions inside other functions. Enhance existing HTML / XML / JSON reports in such way that we could see such nesting as well.     Some of the required functionality has already been done during implementation of lambda functions for Java 8 (Clover 3.2). Review if all possible nesting combinations are implemented in Velocity templates (method-in-method, class-in-method etc). Affects: table with list of methods for a class, calculation of metrics etc."
1,as a user I want to see for Scala exact code coverage highlighting in an HTML report,"As in Scala we may have a lot of statements in a single source line, it would be very valuable to have coverage highlighting not for a single line, but for a single character in HTML reports. Similarly as we have for Eclipse and IDEA.    See this issue: "
3,as a user I want to use Clover-for-Scala in IntelliJ IDEA,Tasks:   * integrate with Scala complier in IDEA (using external build process)   * show Scala classes in Cloverage view   * show coverage highlighting in Scala editors    (!) Challenge: simultaneous update of clover.db by Javac and Scalac (similar problem as for Groovy in IDE)    Out of scope:   * test optimization for Scala
3,as a user I want to use Clover-for-Scala in Eclipse,Integration of Clover-for-Scala with Eclipse.    Tasks:   * plug into *.scala text editors   * show scala classes in the 'Coverage Explorer'    * intercept compilation of Scala code    (!) Challenge: simultaneous updates of clover.db by Scalac and Javac (similarly as for Groovy in IDE)    Out of scope:   * test optimization for Scala
3,as a user I want to run test optimization for Scala tests,It should be possible to perform test optimization for Scala test classes.     Integrations:   * Ant (?)   * Maven   * SBT   * IDEA (?)   * Eclipse (?)  
3,as a user I want to see smart names for classes and functions in reports,"The point is that Scala code is being translated to a bytecode containing various ""cryptic"" names.     For instance, ""overloaded operators"" have ""$op"" in method's name.     Think how to reasonably represent methods, functions, inline classes, anonymous functions, case classes, traits etc etc in HTML report at least (XML / JSON are less important).   "
0,"Clover installs instrumented-jars in local Maven repo, in place of non-instrument ones","I'm not sure if this has been reintroduced or something else is wrong, since the CLOVKB claims it is not the case: https://confluence.atlassian.com/display/CLOVERKB/Maven+is+deploying+instrumented+jars  (I _am_ using the {{instrument}} goal - followed by {{log}} and {{check}})  In the below, you can see that, e.g., {{target/clover/magnolia-module-diff-1.7-SNAPSHOT-clover-tests.jar}} gets copied to {{.../magnolia-module-diff-1.7-SNAPSHOT-tests.jar}} (without {{-clover}} qualifier!)    This seems to happen for ~all artifacts except the ""main"" jar. (test jar, assemblies, etc..).    In this case, the situation is saved by the ""real"" install plugin that gets executed later and overwrites those wrong artifacts. However, if we use the (newish) {{installAtEnd}} feature of the {{maven-install-plugin}}, then this doesn't happen, and we end up with instrumented jars in place of the non-instrumented ones, which has tons of ugly consequences. See for yourself with the snippet:{code:xml}    <build>      <pluginManagement>        <plugins>          <plugin>            <groupId>org.apache.maven.plugins</groupId>            <artifactId>maven-install-plugin</artifactId>            <configuration>              <installAtEnd>false</installAtEnd>            </configuration>          </plugin>        </plugins>      </pluginManagement>    </build>  {code}    Why is Clover installing anything, anyway ? Shouldn't it just _attach_ the instrumented jars (and only those) to the build, and let Maven do the rest ?    {noformat}  [INFO] --- maven-clover2-plugin:4.0.2:check (instrument-and-check) @ magnolia-module-diff ---  [INFO]   [INFO] --- maven-install-plugin:2.5.2:install (default-install) @ magnolia-module-diff ---  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/clover/magnolia-module-diff-1.7-SNAPSHOT-clover.jar to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-clover.jar  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/pom.xml to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT.pom  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT-bundle.zip to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-bundle.zip  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT-bundle.tar.gz to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-bundle.tar.gz  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT-tests.jar to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-tests.jar  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/clover/magnolia-module-diff-1.7-SNAPSHOT-clover-bundle.zip to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-bundle.zip  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/clover/magnolia-module-diff-1.7-SNAPSHOT-clover-bundle.tar.gz to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-bundle.tar.gz  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/clover/magnolia-module-diff-1.7-SNAPSHOT-clover-tests.jar to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-tests.jar  [INFO]   [INFO] <<< maven-clover2-plugin:4.0.2:instrument (instrument-and-check) < [clover]install @ magnolia-module-diff <<<  [INFO]   [INFO] --- maven-clover2-plugin:4.0.2:instrument (instrument-and-check) @ magnolia-module-diff ---  [INFO]   [INFO] --- maven-clover2-plugin:4.0.2:log (instrument-and-check) @ magnolia-module-diff ---  [INFO] Clover Version 4.0.2, built on October 13 2014 (build-943)    [...]    [INFO] Coverage check PASSED  [INFO]   [INFO] --- maven-install-plugin:2.5.2:install (default-install) @ magnolia-module-diff ---  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT.jar to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT.jar  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/pom.xml to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT.pom  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT-bundle.zip to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-bundle.zip  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT-bundle.tar.gz to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-bundle.tar.gz  [INFO] Installing /Users/gjoseph/Dev/magnolia/git/enterprise/diff/target/magnolia-module-diff-1.7-SNAPSHOT-tests.jar to /Users/gjoseph/.m2/repository/info/magnolia/magnolia-module-diff/1.7-SNAPSHOT/magnolia-module-diff-1.7-SNAPSHOT-tests.jar  [INFO] ------------------------------------------------------------------------  [INFO] BUILD SUCCESS  [INFO] ------------------------------------------------------------------------    {noformat}"
1,Ensure that Clover JAR is readable by Ant 1.9.0-1.9.1,"Ant 1.9.0/1.9.1 have a bug related with zip64 file handling:    _central directory zip64 extended information extra field's length doesn't match central directory data. Expected length 0 but is 16_    While this bug has been fixed in Ant 1.9.2, it should be still possible to produce such Clover JAR file that it will be readable by buggy Ant versions as well.     Ensure that new Clover 4.0.5 (and further versions) will be compatible."
1,"Grails Clover Plugin compatibility with Grails 2.4.5, 2.5.0","New Grails versions have been released recently:   * 2.4.5 		24/Mar/15   * 2.5 		24/Mar/15    Make Grails Clover Plugin compatible with them."
0,MavenIntegrator shall not add arguments if they are already present in original command,"This applies to:   * build phases    * Clover's properties    Current problems:    1) The ""clean"" phase can be doubled    For example if original command has:  {noformat}  mvn clean test  {noformat}    it becomes:    {noformat}  mvn clean clean clover2:setup test clover2:aggregate clover2:clover ...  {noformat}    2) User cannot override default values of Clover properties    For example if a property is already defined:    {noformat}  mvn test -Dmaven.clover.licenseLocation=/my/clover.license  {noformat}    MavenIntegrator adds a second one with it's own value:    {noformat}  mvn test -Dmaven.clover.licenseLocation=/my/clover.license -Dmaven.clover.licenseLocation=/tmp/clover.license  {noformat}    "
1,Repository pollution protection reports a custom classifier under Maven 2.2.x,"*Problem:*    Probably bug in Maven, needs further investigation.     Running ""clean clover2:instrument"" on a simple project (no additional plugins) with repository pollution protection enabled causes a build failure.    {noformat}  maven-clover2-plugin-hg\src\it\pollutionProtection>mvn clean clover2:instrument  -Dmaven.clover.repositoryPollutionProtection=true  [INFO] Scanning for projects...  [INFO] snapshot com.atlassian.maven.plugins:maven-clover2-plugin:4.0.4-SNAPSHOT: checking for updates from atlassian-internal  [INFO] ------------------------------------------------------------------------  [INFO] Building Unnamed - com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  [INFO]    task-segment: [clean, clover2:instrument]  [INFO] ------------------------------------------------------------------------  [INFO] [clean:clean {execution: default-clean}]  [INFO] Deleting directory f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target  [INFO] Preparing clover2:instrument  [INFO] snapshot com.atlassian.clover:clover:4.0.4-SNAPSHOT: checking for updates from atlassian-internal  [INFO] [clover2:instrumentInternal {execution: default-instrumentInternal}]  [INFO] ------------------------------------------------------------------------  [ERROR] BUILD ERROR  [INFO] ------------------------------------------------------------------------  [INFO] Clover's repository pollution protection is enabled. Your build produces an artifact (com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:tests:1.0-SNAPSHOT) with a custom classi  fier. As Maven does not support multiple classifiers for an artifact, appending second 'clover' classifier may not be handled correctly. You can:   - remove a custom classifier or   - configure Clover to not append the '-clover' classifier  to fix it. You can also disable pollution protection (-Dmaven.clover.repositoryPollutionProtection=false) if you know that it doesn't affect your build.  {noformat}    Adding extra logging showed that it fails because a custom 'tests' classifier is used during a build - see XXXXXXXXXX below.     {noformat}  maven-clover2-plugin-hg\src\it\pollutionProtection>mvn clean clover2:instrument    [INFO] Scanning for projects...  [INFO] snapshot com.atlassian.maven.plugins:maven-clover2-plugin:4.0.4-SNAPSHOT: checking for updates from atlassian-internal  [INFO] ------------------------------------------------------------------------  [INFO] Building Unnamed - com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  [INFO]    task-segment: [clean, clover2:instrument]  [INFO] ------------------------------------------------------------------------  [INFO] [clean:clean {execution: default-clean}]  [INFO] Preparing clover2:instrument  [INFO] snapshot com.atlassian.clover:clover:4.0.4-SNAPSHOT: checking for updates from atlassian-internal  [INFO] [clover2:instrumentInternal {execution: default-instrumentInternal}]  [INFO] XXXXXXXXXX CLASSIFIER= com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:tests:1.0-SNAPSHOT  [INFO] Clover Version 4.0.4, built on April 15 2015 (build-dev)  [INFO] Loaded from: d:\Data\.m2\repository\com\atlassian\clover\clover\4.0.4-SNAPSHOT\clover-4.0.4-SNAPSHOT.jar  [INFO] Clover: Developer License registered to Atlassian.  [INFO] Creating new database at 'f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\clover.db'.  [INFO] Processing files at 1.8 source level.  [INFO] Clover all over. Instrumented 1 file (1 package).  [INFO] Elapsed time = 0,077 secs. (12,987 files/sec, 64,935 srclines/sec)  [INFO] Clover Version 4.0.4, built on April 15 2015 (build-dev)  [INFO] Loaded from: d:\Data\.m2\repository\com\atlassian\clover\clover\4.0.4-SNAPSHOT\clover-4.0.4-SNAPSHOT.jar  [INFO] Clover: Developer License registered to Atlassian.  [INFO] Updating existing database at 'f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\clover.db'.  [INFO] Processing files at 1.8 source level.  [INFO] Clover all over. Instrumented 1 file (1 package).  [INFO] 1 test method detected.  [INFO] Elapsed time = 0,016 secs. (62,5 files/sec, 562,5 srclines/sec)  [INFO] [resources:resources {execution: default-resources}]  [WARNING] Using platform encoding (Cp1250 actually) to copy filtered resources, i.e. build is platform dependent!  [INFO] skip non existing resourceDirectory f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\src\main\resources  [INFO] Copying 1 resource  [INFO] [compiler:compile {execution: default-compile}]  [INFO] Compiling 1 source file to f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\classes  [INFO] [resources:testResources {execution: default-testResources}]  [WARNING] Using platform encoding (Cp1250 actually) to copy filtered resources, i.e. build is platform dependent!  [INFO] skip non existing resourceDirectory f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\src\test\resources  [INFO] [compiler:testCompile {execution: default-testCompile}]  [INFO] Compiling 1 source file to f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\test-classes  [INFO] [surefire:test {execution: default-test}]  [INFO] Surefire report directory: f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\surefire-reports    -------------------------------------------------------   T E S T S  -------------------------------------------------------  Running ATest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.168 sec    Results :    Tests run: 1, Failures: 0, Errors: 0, Skipped: 0    [INFO] [jar:jar {execution: default-jar}]  [INFO] Building jar: f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\clover-pollution-protection-1.0-SNAPSHOT-clover.jar  [INFO] [install:install {execution: default-install}]  [INFO] Installing f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\clover-pollution-protection-1.0-SNAPSHOT-clover.jar to d:\Data\.m2\repository\com\atlassian\maven\plugins\sam  ple\clover-pollution-protection\1.0-SNAPSHOT\clover-pollution-protection-1.0-SNAPSHOT-clover.jar  [INFO] [clover2:instrument {execution: default-cli}]  [INFO] XXXXXXXXXX CLASSIFIER= com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO] BUILD SUCCESSFUL  [INFO] ------------------------------------------------------------------------  {noformat}    What's interesting, running the same test but with clover2:setup not only does not fails a build (because repository pollution protection does not fail due to custom classifier if no 'clover' is used), but the artifact produced does NOT have the 'tests' classifier:    {noformat}  maven-clover2-plugin-hg\src\it\pollutionProtection>mvn clean clover2:setup verify   [INFO] Scanning for projects...  [INFO] snapshot com.atlassian.maven.plugins:maven-clover2-plugin:4.0.4-SNAPSHOT: checking for updates from atlassian-internal  [INFO] ------------------------------------------------------------------------  [INFO] Building Unnamed - com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  [INFO]    task-segment: [clean, clover2:setup, verify]  [INFO] ------------------------------------------------------------------------  [INFO] [clean:clean {execution: default-clean}]  [INFO] Deleting directory f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target  [INFO] snapshot com.atlassian.clover:clover:4.0.4-SNAPSHOT: checking for updates from atlassian-internal  [INFO] [clover2:setup {execution: default-cli}]  [INFO] XXXXXXXXXX CLASSIFIER= com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  [INFO] Clover Version 4.0.4, built on April 15 2015 (build-dev)  [INFO] Loaded from: d:\Data\.m2\repository\com\atlassian\clover\clover\4.0.4-SNAPSHOT\clover-4.0.4-SNAPSHOT.jar  [INFO] Clover: Developer License registered to Atlassian.  [INFO] Creating new database at 'f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\clover.db'.  [INFO] Processing files at 1.8 source level.  [INFO] Clover all over. Instrumented 1 file (1 package).  [INFO] Elapsed time = 0,057 secs. (17,544 files/sec, 87,719 srclines/sec)  [INFO] Clover Version 4.0.4, built on April 15 2015 (build-dev)  [INFO] Loaded from: d:\Data\.m2\repository\com\atlassian\clover\clover\4.0.4-SNAPSHOT\clover-4.0.4-SNAPSHOT.jar  [INFO] Clover: Developer License registered to Atlassian.  [INFO] Updating existing database at 'f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover\clover.db'.  [INFO] Processing files at 1.8 source level.  [INFO] Clover all over. Instrumented 1 file (1 package).  [INFO] 1 test method detected.  [INFO] Elapsed time = 0,011 secs. (90,909 files/sec, 818,182 srclines/sec)  [INFO] [resources:resources {execution: default-resources}]  [WARNING] Using platform encoding (Cp1250 actually) to copy filtered resources, i.e. build is platform dependent!  [INFO] skip non existing resourceDirectory f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\src\main\resources  [INFO] Copying 1 resource  [INFO] [compiler:compile {execution: default-compile}]  [INFO] Compiling 1 source file to f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\classes  [INFO] [resources:testResources {execution: default-testResources}]  [WARNING] Using platform encoding (Cp1250 actually) to copy filtered resources, i.e. build is platform dependent!  [INFO] skip non existing resourceDirectory f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\src\test\resources  [INFO] [compiler:testCompile {execution: default-testCompile}]  [INFO] Compiling 1 source file to f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\test-classes  [INFO] [surefire:test {execution: default-test}]  [INFO] Surefire report directory: f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\surefire-reports    -------------------------------------------------------   T E S T S  -------------------------------------------------------  Running ATest  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.155 sec    Results :    Tests run: 1, Failures: 0, Errors: 0, Skipped: 0    [INFO] [jar:jar {execution: default-jar}]  [INFO] Building jar: f:\Work\maven-clover2-plugin-hg\src\it\pollutionProtection\target\clover-pollution-protection-1.0-SNAPSHOT.jar  [INFO] ------------------------------------------------------------------------  [INFO] BUILD SUCCESSFUL  [INFO] ------------------------------------------------------------------------  {noformat}      It affects Maven 2.2.x. Does not occur in Maven 2.1.x or Maven 3.x.     Summarizing, it seems that forking a parallel build life cycle causes that an artefact's classifier changes from empty to 'tests':    * with clover2:instrument:    {noformat}  [INFO] [clover2:instrumentInternal {execution: default-instrumentInternal}]  [INFO] XXXXXXXXXX CLASSIFIER= com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:tests:1.0-SNAPSHOT  ...  [INFO] [clover2:instrument {execution: default-cli}]  [INFO] XXXXXXXXXX CLASSIFIER= com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  {noformat}    * with clover2:setup:    {noformat}  [INFO] [clover2:setup {execution: default-cli}]  [INFO] XXXXXXXXXX CLASSIFIER= com.atlassian.maven.plugins.sample:clover-pollution-protection:jar:1.0-SNAPSHOT  {noformat}      *Workaround:*   * use clover2:setup instead of clover2:instrument to avoid forking life cycle or   * use different Maven version or   * disable the protection  "
1,Grails Clover Plugin compatibility with Grails 3.x,"Grails 3 has been released (3.0 GA on 31/Mar/15). It uses the Gradle build tool instead of Gant, thus we may need to have Gradle support as well. It has also various changes in API. Implement support for Grails 3 in the Grails Clover Plugin. "
0,OutOfMemoryError while flushing coverage data,"  {noformat}  29-May-2015 12:53:08 	ERROR: java.lang.OutOfMemoryError flushing coverage for recorder /media/ephemeral0/bamboo-working-dir/BAM-CLOVER344-UT/components/bamboo-core/target/clover/clover.dbbjv36u_ia9lwdsf: Java heap space  29-May-2015 12:53:08 	Exception in thread ""CloverFlushThread"" java.lang.OutOfMemoryError: Java heap space  29-May-2015 12:53:08 	        at java.util.zip.DeflaterOutputStream.<init>(DeflaterOutputStream.java:56)  29-May-2015 12:53:08 	        at com.cenqua.clover.util.FileUtils.createDeflateOutputStream(FileUtils.java:468)  29-May-2015 12:53:08 	        at com.cenqua.clover.FileBasedGlobalCoverageRecording.write(FileBasedGlobalCoverageRecording.java:39)  29-May-2015 12:53:08 	        at com.cenqua.clover.GlobalRecordingWriteStrategy$1.write(GlobalRecordingWriteStrategy.java:9)  29-May-2015 12:53:08 	        at com.cenqua.clover.BaseCoverageRecorder.write(BaseCoverageRecorder.java:108)  29-May-2015 12:53:08 	        at com.cenqua.clover.FixedSizeCoverageRecorder.write(FixedSizeCoverageRecorder.java:56)  29-May-2015 12:53:08 	        at com.cenqua.clover.BaseCoverageRecorder.flush(BaseCoverageRecorder.java:302)  29-May-2015 12:53:08 	        at com.cenqua.clover.BaseCoverageRecorder.flush(BaseCoverageRecorder.java:271)  29-May-2015 12:53:08 	        at com.cenqua.clover.BaseCoverageRecorder$CloverFlushThread.run(BaseCoverageRecorder.java:155)  29-May-2015 12:55:48 	ERROR: java.lang.OutOfMemoryError flushing coverage for recorder /media/ephemeral0/bamboo-working-dir/BAM-CLOVER344-UT/components/bamboo-core/target/clover/clover.dbdozy2b_ia9lwcia: Java heap space  29-May-2015 12:55:48 	Exception in thread ""CloverFlushThread"" java.lang.OutOfMemoryError: Java heap space  29-May-2015 12:56:03 	ERROR: java.lang.OutOfMemoryError flushing coverage for recorder /media/ephemeral0/bamboo-working-dir/BAM-CLOVER344-UT/components/bamboo-api/target/clover/clover.dbjcsfjm_ia9lwcij: Java heap space  29-May-2015 12:56:03 	Exception in thread ""CloverFlushThread"" java.lang.OutOfMemoryError: Java heap space  {noformat}  "
0,Drop support for IDEA versions older than 11.0.0,NULL
1,Make Clover compatible with IntelliJ 15,NULL
3,Support JDK9 by Clover,"* As per https://adtmag.com/articles/2015/05/08/oracle-java.aspx     we can expect to have the ""feature complete"" version before December, 2015.    Clover should be ready till then."
0,Add support for Eclipse Mars 4.5 version,Add support for new Eclipse Mars (June 2015)     https://adtmag.com/articles/2015/06/30/eclipse-release-train.aspx
0,Eclipse Mars 4.5 does not load platform:/ resources properly,"Clover uses ""platform:/"" URIs to load standard Eclipse icons from eclipse-core such as a package icon, a folder icon etc.     Eclipse Mars is buggy and does not always fetch these resources, as a consequence we don't see proper icons on a toolbar.    Fix: bundle new icons in Clover-for-Eclipse plugin and replace ""platform:/"" references by new icons using local URIs.    Note: try aligning look of new icons with ADG and Clover's HTML reports."
1,AspectJ spike,NULL
1,Rename maven-clover2-plugin artifactId to clover-maven-plugin,"{panel:title=Decision|titleBGColor=#205081|bgColor=#59afe1}  We are currently working on renaming Maven Clover2 Plugin according to the naming convention recommended by the Apache Maven project.    Due to a fact that this is a major breaking change, forcing customers to modify their POMs, CI build plans etc, it will be done in the nearest major Clover release (currently planned for end of 2015) and appropriate migration guides will be prepared.    New plugin name will be: *clover-maven-plugin*  {panel}    *Problem reported:*    According the Maven's Plugin Naming Convention, Clover's plugin artifactID violates the Apache Maven Trademark. The artifactId should be clover2-maven-plugin. I would even suggest removing the 2 from the artifactId.    [Plugin Naming Convention and Apache Maven Trademark|https://maven.apache.org/guides/plugin/guide-java-plugin-development.html]      *Scope of the change:*    _Before release_     (/) investigate if we can use clover-maven-plugin or must use cloverX-maven-plugin - _will be renamed to clover-maven-plugin_    (/) decide if major Clover release will be numbered 4.1.0 -or 5.0.0- (as per [Semantic Versioning|http://semver.org])   (/) prepare migration message in maven-clover2-plugin pom.xml for versions 4.0.x, similarly as it was done for Clover Core (com.cenqua.clover:clover:3.3.0) - _message added in pom in 4.0.6_   (/) rename maven-clover2-plugin artifactId for 4.1.0 / -5.0.0- , update integration tests   (/) prepare migration guide for Clover 4.1 / -5.0- Release Notes   (/) update Clover's Tutorial (Moneybags bundled in clover-ant.zip) -> see also CLOV-1827    _After release_    See CLOV-1828."
0,Instrumentation error on certain projects since 4.0.5 build of Clover. ,"Instrumentation error on certain projects since 4.0.5 build of Clover.     Making it a blocker as we can't run Clover on certain projects any longer.     {code}  build	27-Jul-2015 13:46:14	[INFO] Processing files at 1.8 source level.  build	27-Jul-2015 13:46:17	[INFO] Instrumentation error  build	27-Jul-2015 13:46:17	java.util.NoSuchElementException  build	27-Jul-2015 13:46:17	  java.util.ArrayDeque.getFirst(ArrayDeque.java:324)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.headIdentifiersStack(JavaRecognizer.java:113)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.pushIdentifierToHeadStack(JavaRecognizer.java:109)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.primaryExpressionPart(JavaRecognizer.java:3368)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.postfixExpression(JavaRecognizer.java:5047)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.unaryExpressionNotPlusMinus(JavaRecognizer.java:5007)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.unaryExpression(JavaRecognizer.java:4867)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.multiplicativeExpression(JavaRecognizer.java:4817)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.additiveExpression(JavaRecognizer.java:4787)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.shiftExpression(JavaRecognizer.java:4754)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.relationalExpression(JavaRecognizer.java:4706)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.equalityExpression(JavaRecognizer.java:4673)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.andExpression(JavaRecognizer.java:4654)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.exclusiveOrExpression(JavaRecognizer.java:4635)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.inclusiveOrExpression(JavaRecognizer.java:4616)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.logicalAndExpression(JavaRecognizer.java:4597)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.logicalOrExpression(JavaRecognizer.java:4578)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.conditionalExpression(JavaRecognizer.java:4549)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.conditionalExpression2(JavaRecognizer.java:5558)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.annMemberValue2(JavaRecognizer.java:5528)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.annMemberValuePair(JavaRecognizer.java:5465)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.annotation2(JavaRecognizer.java:1509)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.annotation(JavaRecognizer.java:667)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.packageDefinition(JavaRecognizer.java:593)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.JavaRecognizer.compilationUnit(JavaRecognizer.java:540)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.Instrumenter.instrument(Instrumenter.java:212)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.instr.java.Instrumenter.instrument(Instrumenter.java:125)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.CloverInstr.execute(CloverInstr.java:76)  build	27-Jul-2015 13:46:17	  com.atlassian.clover.CloverInstr.mainImpl(CloverInstr.java:54)  build	27-Jul-2015 13:46:17	  com.atlassian.maven.plugin.clover.internal.instrumentation.AbstractInstrumenter.instrumentSources(AbstractInstrumenter.java:197)  build	27-Jul-2015 13:46:17	  com.atlassian.maven.plugin.clover.internal.instrumentation.AbstractInstrumenter.instrument(AbstractInstrumenter.java:72)  build	27-Jul-2015 13:46:17	  com.atlassian.maven.plugin.clover.CloverInstrumentInternalMojo.execute(CloverInstrumentInternalMojo.java:289)  build	27-Jul-2015 13:46:17	  com.atlassian.maven.plugin.clover.CloverSetupMojo.execute(CloverSetupMojo.java:31)  build	27-Jul-2015 13:46:17	  org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)  build	27-Jul-2015 13:46:17	  org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)  build	27-Jul-2015 13:46:17	  org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:347)  build	27-Jul-2015 13:46:17	  org.apache.maven.DefaultMaven.execute(DefaultMaven.java:154)  build	27-Jul-2015 13:46:17	  org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)  build	27-Jul-2015 13:46:17	  org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)  build	27-Jul-2015 13:46:17	  org.apache.maven.cli.MavenCli.main(MavenCli.java:157)  build	27-Jul-2015 13:46:17	  sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  build	27-Jul-2015 13:46:17	  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  build	27-Jul-2015 13:46:17	  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  build	27-Jul-2015 13:46:17	  java.lang.reflect.Method.invoke(Method.java:483)  build	27-Jul-2015 13:46:17	  org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)  build	27-Jul-2015 13:46:17	  org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)  build	27-Jul-2015 13:46:17	  org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)  build	27-Jul-2015 13:46:17	  org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)  build	27-Jul-2015 13:46:17	[INFO]                                                                           {code}"
1,Groovy code with Generics throw NPEs,Starting with Clover version 4.0.1 when running JUnit tests against clover instrumented code I am getting a null pointer exception when attempting to call this method:    {code:java}  // in DateUtil.groovy    static <T extends Date> T copy(T date) {      (T) date?.clone()    }  {code}    It seems like something has changed with the instrumentation. Either of these two other variations of this same code *doesn't* have this problem:    {code:java}    static Date copy(Date date) {      date?.clone()    }      static def copy(def date) {      date?.clone()    }  {code}      Here's the top of the stacktrace:  {noformat}  java.lang.NullPointerException    com.sun.beans.TypeResolver.resolve(TypeResolver.java:203)    com.sun.beans.TypeResolver.resolve(TypeResolver.java:218)    com.sun.beans.TypeResolver.resolveInClass(TypeResolver.java:96)    java.beans.FeatureDescriptor.getParameterTypes(FeatureDescriptor.java:387)    java.beans.MethodDescriptor.setMethod(MethodDescriptor.java:116)    java.beans.MethodDescriptor.<init>(MethodDescriptor.java:72)    java.beans.MethodDescriptor.<init>(MethodDescriptor.java:56)    java.beans.Introspector.getTargetMethodInfo(Introspector.java:1205)    java.beans.Introspector.getBeanInfo(Introspector.java:426)    java.beans.Introspector.getBeanInfo(Introspector.java:173)    groovy.lang.MetaClassImpl$15.run(MetaClassImpl.java:3244)    java.security.AccessController.doPrivileged(Native Method)    groovy.lang.MetaClassImpl.addProperties(MetaClassImpl.java:3242)    groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:3225)    org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:222)    org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:253)    org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:255)    org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:859)    org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallStaticSite(CallSiteArray.java:72)    org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallSite(CallSiteArray.java:159)    org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)    org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)    org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)    com.surescripts.common.DateUtilTest.clone_NonNullValue(DateUtilTest.groovy:16)  {noformat}
0,Add migration message in maven-clover2-plugin POM,"(/) investigate if we can use clover-maven-plugin or must use cloverX-maven-plugin -> it would be great to have just clover- instead of cloverX- to avoid changing a number in future  (?) decide if major Clover release will be numbered 4.1.0 or 5.0.0 (as per Semantic Versioning) -> it may affect whether it will be named clover4- or clover-5  (/) prepare migration message in maven-clover2-plugin pom.xml for versions 4.0.x, similarly as it was done for Clover Core (com.cenqua.clover:clover:3.3.0) -> deliver in Clover 4.0.6, 4.0.7, ...    to be considered:  (x) prepare a fork of the plugin and release under two names since 4.0.6?  (x) add an extra migration warning printed by the clover2:setup / clover2:instr goal?"
0,Extend interfaces to manipulate test optimization,Interface to manipulate test optimization:  * to reorganize optimized test sets  * to manipulate content the test optimization snapshot file
1,Extend database format to handle statements and methods in a file,"Allow better modelling of file-level statements and methods so that we can better support languages like Ruby which support top-level scripting constructs.     Example:   {code:java}  session.enterFile(..);  session.enterMethod(...); // top-level-method, not inside a class  ...  // some statements  ...  session.exitMethod();  ...  session.addStatement(...); // top-level-statement, not inside a class or a method  ...  session.exitFile();  {code}    Some preparation have been already done for Java 8 (Clover 3.2).    Things to be checked:   * how our code calculates the start/end line/col range for a statement/method outside a class   * how it calculates code metrics for a file having top-level statements/methods   * how it handles exclusion filters (statement/method regexp)   * how it handles nested entities (method-in-method etc)    Things to be implemented:   * take into account these top-level statements/methods in reporting   ** show in XML / HTML / PDF / JSON   ** render lines   ** show in tables etc    Out of scope:   * test optimization    Note:  Looking at how Clojure is written, for any Clojure file there would only ever be one FileInfo containing lots of StatementInfos and MethodInfos but no ClassInfos."
0,Rename ClassInfo to TypeInfo,"ClassInfo is just OK for Java and Groovy (we have classes). But we also have enums, interfaces and annotations! And in other languages we also have closures (Groovy), traits (Groovy, Scala) etc. Thus the name ClassInfo does not suit well for other languages.       Rename ClassInfo to something better. TypeInfo might be better."
1,Make session.enterClass() more language-independent,"Clean up InstrumentationSession.enterClass(..., boolean isInterface, boolean isEnum, boolean isAnnotation) so that it instead takes a LanguageConstruct to indicated what it is which is then embedded in ClassInfo. This divorces this part of the API from Java allowing us to model languages like Scala who have things like traits, singleton objects, etc.    This may require a separate LanguageConstruct-like type (TypeConstruct ?) as it will likely need to be smarter than LanguageConstruct and be able to answer some basic questions Clover may need to ask e.g.    {code:java}  if (!typeInfo.getConstruct().isAbstract(typeInfo)) { //Java interfaces and annotations are always abstract, Scala trait is sometimes abstract, ...  //include in the report  }  {code}    The example above is a bit fake because surely we could determine if a TypeInfo is abstract by traversing it but there may be situations where we need to ask similar questions.  "
0,Add possibility to add custom properties for code elements,"Introduce a base interface to ClassInfo, MethodInfo, PackageInfo, FileInfo, StatementInfo, BranchInfo which allows API clients to add arbitrary properties to the model. They values should be raw types, possible Serializable objects too. This should allow instrumentation providers better answer questions Clover may have about the model they've produced.    Something like:    interface CustomInfo   - getProperties(): Map<String, Object>   - setProperty(String, Object)   - getProperty(String)    interface FileInfo extends ..., CustomInfo  interface TypeInfo extends ..., CustomInfo  interface MethodInfo extends ..., CustomInfo  interface BranchInfo extends ..., CustomInfo  interface StatementInfo extends ..., CustomInfo  "
0,Rename StatementInfo to ExpressionInfo,"Rename StatementInfo to ExpressionInfo in the API. Java is the odd one out here. Scala, Clojure etc all have expressions as there basic unit of execution. A statement is an expression with no result.   "
0,Drop classic HTML report,remove classic reports in Clover   * report type='classic' print error? or warning and ignore...   * remove old velocity templates   * drop libraries   ** Overlibvms  not until we drop Classic reports (Clover 4.1)   ** Utils.js (rewrite query parsing into clover.js)   ** sorttable.js
0,Drop Grails 1.3, * remove legacy code (if any)
0,Drop Eclipse 3.6 and 3.7,* compile against eclipse 4.2  * fix deprecation warnings etc  * remove legacy code (if any)  * bump version numbers in feature name
0,Remove --style from Command line processors,{{--style}} argument has been deprecated in 4.1.0 it should be removed in next major version 4.2.0.  Scope: * remove HtmlReporter#Style * update docs i necessary * cleanup all TODO CLOV-1795
0,Support for IDEA 14.1.5,It should work but let's be sure.   http://blog.jetbrains.com/idea/2015/09/intellij-idea-14-1-5-update-is-out/
1,Walk AST and store file and type info in database,Walk the AST tree and handle FileInfo and ClassInfo (TypeInfo) elements - register them in Clover database. Print them in reports.  We shall handle Scala's class / object / trait types.  (including anonymous?) 
1,Walk AST and store method info in a database,Walk AST and register methods and functions (including inline / anonymous ones?) in a database. Print them in reports.     To be considered: naming convention.
1,Walk AST and store statement info in a database,NULL
0,Add support for Eclipse Mars.1,Eclipse Mars.1 was released on 01.10. Make sure Clover is compatible by:    * smoke testing  * adding new version to Eclipse versions matrix. 
0,StringIndexOutOfBoundsException in CloverStartup.evaluateDaysLeft,Clover fails to decode the license key. Stacktrace:    {noformat}  Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 47   at java.lang.String.charAt(String.java:646)   at com.atlassian.clover.CloverStartup.evaluateDaysLeft(CloverStartup.java:334)   at com.atlassian.clover.CloverStartup.loadLicense(CloverStartup.java:122)   at com.atlassian.clover.CloverStartup.loadLicense(CloverStartup.java:26)   at com.atlassian.clover.CloverInstr.mainImpl(CloverInstr.java:45)   at com.atlassian.maven.plugin.clover.internal.instrumentation.AbstractInstrumenter.instrumentSources(AbstractInstrumenter.java:196)   at com.atlassian.maven.plugin.clover.internal.instrumentation.AbstractInstrumenter.instrument(AbstractInstrumenter.java:71)   at com.atlassian.maven.plugin.clover.CloverInstrumentInternalMojo.execute(CloverInstrumentInternalMojo.java:290)    at com.atlassian.maven.plugin.clover.CloverSetupMojo.execute(CloverSetupMojo.java:30)   at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager. java:134):  .. 11 more  {noformat}    
0,Fix colors of test passed / failed / error icons in Eclipse,"Currently we use dark blue color for a failed test. It's not contrast enough compared to the green one.     Furthermore, we use a red background for a ""FAIL"" status label, which is not consistent. Also, a green background used for a ""PASS"" label is different from the green dot in the icon.     Current colors used: green-pass, blue-fail, red-error are the same as in Eclipse's JUnit view.    Consider also changing a dot to three different figures to make it accessible for color-blinded people. For instance:   * light green circle - pass   * dark blue triangle - fail   * red square - error  "
0,Test optimization in Eclipse Mars 4.5.1 keeps running one test,"Steps to reproduce:   * rebuild a project, clean database and optimization snapshot   * two times on 'run optimized' button, for the first time it runs all test, for the second time it runs one test (and should none)"
0,Add IDEA 14.0.4 to a test matrix,"There are differences in app behaviour between IDEA 14.0.x and 14.1.x.     As we're testing Clover against 12.1.7 and 13.0.0, it'd be good to add also 12.0.4, 13.1.6, 14.0.4 to our test matrix.     This way we will cover IDEA 12.0, 12.1, 13.0, 13.1, 14.0 and 14.1.  "
0,Add more debug logging for clover2:aggregate,"Currently, it's not known why a project has no child databases and what user can do with it. "
0,Update Clover Tutorial, * use source=1.6 instead of 1.4   * download required JARs (build*.xml)   * do not require env.GROOVY_HOME set to 1.7   * rename 'test' to 'test.run'   * remove cloverjunitlib.xml   * convert unit tests to JUnit4   * simplify goals   * use clover-maven-plugin  
0,Rename maven-clover2-plugin to clover-maven-plugin (post-release tasks),"*Scope of the change:*  _Before release_  See CLOV-1775.  _After release_   (/) update documentation:   * (/) on confluence.atlassian.com,    * (/) docs.atlassian.com,   * (/) answers.atlassian.com (rename all references of maven-clover2-plugin),    * (?) add comments on stackoverflow -> not doing   * (!) update Bamboo Clover Plugin (automatic integration shall use new maven plugin) -> BAM-16590          tracked in CLOV-1859   * (/) check Hudson / Jenkins Clover Plugin if they have any references -> _no references_   * (/) update Clover Examples repository   optional (not doing):   * move repository to bitbucket.org/atlassian/clover-maven-plugin  ** note that bitbucket does not support redirects from old to new url - https://bitbucket.org/site/master/issues/6325/need-a-forwarder-option-for-renamed-repos   ** thus we'd have to a) clone the repo to new location, b) set old repo as read-only and add a notice in README, c) update URLs in pom.xml "
0,Instrumentation failure of an annotation inside the catch clause,"Clover is failing to instrument below section of java 7 code {code}         try         {             innerStream.close();         }         catch (@SuppressWarnings(""unused"") final IOException e)         {             // ignore          } {code} This is a perfectly valid java 7 syntax but clover instrumentation fails at @SuppressWarnings(""unused"") annotation. "
0,Unicode 0x200B (zero-width whitespace) causes instrumentation failure,"JDK 6 implements Unicode 4.0. In this version of Unicode, the zero-width whitespace character (0x200B) is being treated as a whitespace.    JDK 7 implements Unicode 6.0. In this version of Unicode, the zero-width whitespace has been reclassified to the 'format character' group (other characters in this group are, for example, left-/right- text direction markers).    Thus, Java compiler in JDK 6 allows to use 0x200B as a normal whitespace character, e.g. separating symbols.     The Java compiler since JDK 7 silently ignores the 0x200B, which means that it cannot be used to separate symbols anymore. However you can put this character virtually in any place, e.g.:    {code:java}  void this<200B>Is<200B>MyMethod();  {code}    Clover fails on parsing the 200B character:    {noformat}  Xyz.java:287:90:unexpected char: 0x200B  at com.atlassian.clover.instr.java.Instrumenter.instrument(Instrumenter.java:166)  at com.atlassian.clover.CloverInstr.execute(CloverInstr.java:76)  at com.atlassian.clover.CloverInstr.mainImpl(CloverInstr.java:54)  at ...  {noformat}    *Planned fix:*     * Ignore 200B characters in Java 7+. Treat 200B character as space in Java 6.   * Question: Shall it be based on source level setting or the JDK detected?   * Question: which other control characters needs to be ignored by Clover?   * Question: which other whitespace characters (other than space, \t, \n, \r) shall be recognized by Clover parser?    *Workaround:*    Remove all 200B character occurrences from the source code.   "
0,Prepare feature analysis of JCov tool,Analyse https://wiki.openjdk.java.net/display/CodeTools/jcov    and publish on https://confluence.atlassian.com/display/CLOVER/Comparison+of+code+coverage+tools    
0,IntelliJ Plugin should ask for rebuilding project after changing exclusions,Currently it doesn't always ask which sometime may lead to {{ClassNotFoudException}}    h2. Preconditions  IntelliJ IDEA with Clover installed     h2. Repro steps  # Clean project and coverage db  # Exclude some class with context menu  # Run tests  # Wait for exception    h2. Expected result  # Clean project and coverage db  # Exclude some class with context menu  # Clover asks user for rebuilding the project. 
0,IntelliJ Plugin should not use UI thread to generate Cloud report,"Currently Clover generates Cloud report in UI thread, it can take couple of seconds with bigger projects causing whole UI to freeze.     Let's try to move it back to background somehow. "
0,Add IntelliJ 15.0.2 to test matrix,Add IntelliJ 15.0.2 to test matrix after it being released. Currently there was RC released. 
0,Gradle Spec,Simple spec showing what do we want to achieve and what's the MVP for 4.2.0 
0,Drop support for IntelliJ IDEA 11.x,Scope:  * drop support for IntelliJ IDEA 11.x   * refactor deprecated code  * remove IntelliJ 11.x from test matrix  * update plugin.xml to tell correct minimal version of IntelliJ IDEA which is supported. 
0,IntelliJ IDEA plugin with version 4.1.1 shows wrong supported version,Site for Clover on IntelliJ plugin page indicates Clover 4.1.1 supports Idea 9x however it's not true.   Update plugin.xml to show correct version.     https://plugins.jetbrains.com/plugin/?id=132 
0,Update Bamboo Clover Plugin (automatic integration shall use new clover-maven-plugin),Place holder for BAM-16590
0,Create simple Gradle repository and run Bamboo plan to test it,"Scope: * Create simple Gradle plugin as a holder for Clover for Gradle * Create Bamboo plan, which will: ** fetch source code ** run tests"
0,CloverCompilerAdapter should look for Javac.compileList also in super classes,"It may happen that user will wrap or subclass the Apache's Javac field. In such case, Clover won't be able to find and set this field and break the build. The CloverCompilerAdapter should look it up in a class hierarchy.     We can use Apache's FieldUtils for this purpose."
1,Provide high level instrumentation task which will instrument sources to given directory,Consider only single module java projects for now
1,Provide high level task which will execute tests in simple module java project,NULL
0,Provide simple reporting task which generate Clover report,Let's stick to html report only for now.    Don't bother about multi project build for now 
1,Make Clover instrumentation work in multi java project builds,NULL
0,Clover shall be able to instrument Test sources,NULL
0,TestWithClover task must be aware of instrumented test sources and modify test classpath accordingly,NULL
1,Support Eclipse 4.6 Neon,"Make Clover-for-Eclipse compatible with Eclipse Neon. Problems found so far:    1. Running optimised tests throws an exception  {code:java}  eclipse.buildId=4.6.0.I20160525-2000  java.version=1.8.0_51  java.vendor=Oracle Corporation  BootLoader constants: OS=macosx, ARCH=x86_64, WS=cocoa, NL=en_US  Framework arguments: -product org.eclipse.epp.package.java.product -product org.eclipse.epp.package.java.product -product org.eclipse.epp.package.java.product  Command-line arguments: -os macosx -ws cocoa -arch x86_64 -product org.eclipse.epp.package.java.product -product org.eclipse.epp.package.java.product -data file:/Users/mparfianowicz/.eclipse-4.6-workspace/ -product org.eclipse.epp.package.java.product    org.eclipse.ui  Error  Tue Jun 14 15:51:17 CEST 2016  Unhandled event loop exception    org.eclipse.swt.SWTException: Failed to execute runnable (java.lang.NullPointerException)   at org.eclipse.swt.SWT.error(SWT.java:4533)   at org.eclipse.swt.SWT.error(SWT.java:4448)   at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:185)   at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:4024)   at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3700)   at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$4.run(PartRenderingEngine.java:1121)   at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:336)   at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:1022)   at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:150)   at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:687)   at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:336)   at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:604)   at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:148)   at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:138)   at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)   at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:134)   at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:104)   at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:388)   at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:243)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:497)   at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:673)   at org.eclipse.equinox.launcher.Main.basicRun(Main.java:610)   at org.eclipse.equinox.launcher.Main.run(Main.java:1519)  Caused by: java.lang.NullPointerException   at com.atlassian.clover.eclipse.testopt.views.actions.SavingsNotificationContributionItem$1$1.run(SavingsNotificationContributionItem.java:33)   at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)   at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:182)   ... 23 more      {code}      2. Deprecated API (/)    SubProgressMonitor in org.eclipse.core.runtime has been deprecated   ViewerSorter in org.eclipse.jface.viewers has been deprecated   setSorter(ViewerSorter) in StructuredViewer has been deprecated   Main(PrintWriter,PrintWriter,boolean) in Main has been deprecated    3. Code incompatibility (/)    CoverageAnnotationModel is not abstract and does not override abstract method getAnnotationIterator() in IAnnotationModel"
1,Clover does not instrument all nodes in Groovy AST,Problem was reproduced in Groovy 2.4.3 and 2.4.4:    https://answers.atlassian.com/questions/39141626/why-is-clover-showing-green-on-gray    https://answers.atlassian.com/questions/39141622/incomplete-clover-coverage-in-grails-2.5-integration-testing
0,Support InteliiJ IDEA 2016.2," * add 2016.2.0 to test matrix and verify all tests are green  * switch clover-idea to compile against 2016.2 API, check deprecation warnings, compilation problems"
1,Instrument Groovy traits and show code coverage for them,"*Current status*    Groovy 2.3 introduced traits - a code construct for composition of behaviours:   * http://docs.groovy-lang.org/latest/html/documentation/core-traits.html    Currently Clover handles them like an interface - it records the class name in a database but it does not instrument the class, so it shows no code coverage in reports for them.     Side note: Clover's reports by default do not show classes containing no executable code (empty classes, plain enums, interfaces with no default methods, traits) - you can show them by setting [showEmpty=true|https://confluence.atlassian.com/display/CLOVER/clover-report] parameter.    *Proposed improvement*    Instrument Groovy traits like normal classes or enums.     Side note: Groovy 2.5 will probably not introduce interfaces with default methods (like in Java 8).    * http://docs.groovy-lang.org/next/html/documentation/#_interface    *Technical obstacles*    * method '$CLV_R$' is private but should be public in a trait  * method 'com_atlassian_clover.CoverageRecorder $CLV_R$()' must not be static; only fields may be static in a trait  * field '$CLV_R$' is not 'public static final' in a trait  "
2,Transition git repositories to Stash,Transition gitolite-managed repositories to Atlassian Stash.
0,Finalize DM mission statement,"The proposed mission statement for the LSST Software Stack development: {quote} Enabling LSST science by creating a well documented, state-of-the-art, high-performance, scalable, multi-camera, open source, O/IR survey data processing and analysis system {quote} with the following rationale:  * well documented -- as otherwise it will be impossible to maintain or be usable for Level 3 * state-of-the-art -- because the quality of the instrument and needs of the science to be systematic limited require us to develop new algorithms an break new ground in a number of areas * high-performance -- because of the massive amount of data we need to process for a reasonable budget * scalable -- because we need it to run from between a single core (for developers, or Level 3 users) to tens of thousands of cores (for Data Release production) * multi-camera -- because we need to process precursor data for development purposes, and because our as-delivered camera won't be ideal. * open source -- because we want the community and future surveys to benefit from our efforts. "
0,Open up LSST software mailing lists,"We all benefit from making LSST software development as open as possible and conducive to outside volunteer contributions (*). One way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. For example, we could have:  * software-devel@lsstcorp.org: the development mailing list, equivalent to current lsst-data * software-users@lsstcorp.org: the users mailing list, equivalent to current lsst-dm-stack-users mailing list (but it could possibly be replaced by StackOverflow/Confluence Questions) * lsst-dm@lsstcorp.org: internal, DM-staff only mailing list, for the *rare* discussions/notices that should go out to staff only.  (*) Though we don't rely on them for meeting the project specs (legally required disclaimer :) )."
2,Transition to Confluence Questions,"Open the Confluence Questions site, for interaction with the community (and to enable community self-help).  "
2,Add derivatives-based optimizer to meas_multifit,"See https://dev.lsstcorp.org/trac/ticket/3146  Story points estimate is for remaining work only (just the code review, which is still substantial)."
0,Release EUPS 1.3.0,Release EUPS 1.3.0 in RHL's github repository.
0,Determine final URL/location for W'14 stack,Need to know where W'14 stack files are going to be housed.
0,Update newinstall.sh to check for existence of git and python on users' machines,NULL
0,Confirm stack builds on OS X 10.8,NULL
0,Build Winter'14 release,Run lsst-build scripts and 'eups distrib create' to build the Winter'14 release.
0,Update installation instructions,Update Confluence instructions on how to install the Winter'14 stack.  The instructions are at: https://confluence.lsstcorp.org/display/LSWUG/LSST+Software+User+Guide
0,Modify gitolite permissions to allow issue/DM-NNNN branches,"Use issue/DM-NNNN branches for issues tracked in JIRA, to differentiate them from tickets/NNNN branches that are still in Trac."
1,Qserv configuration - detailed design,Detailed design covering how all Qserv components will be configured for runtime. 
1,Node configuration and bootstrapping - detailed design,"Design covering how all Qserv components will be configured for runtime.  Design how new Qserv nodes will be bootstrapped when we add them to the cluster, and how already added nodes will get updated after they were offline (crashed, turned off for maintenance etc) "
2,Node bootstrapping - 1st prototype,NULL
3,Zookeeper-based CSS (v1),NULL
0,switch back to throwing exceptions in css/Facede.cc,NULL
3,Rewrite xrootd-facing code,"This task is related to DMTF-16570-09. It involves plugin in the new XRootD client (which has different interfaces, it is all async etc). We expect to also do a major cleanup on the code that is talking to XRootD while plugging in the new XRootD."
1,Data Distribution Design v1,"Need to come up with detailed design covering how we will deal with data distribution: managing multiple replicas, recovering from faults, adding new nodes to the cluster, registering new data from L2 ingest and user data (L3)."
0,Modify format of version numbers,"The versions auto-generated by the new EUPS+buildbot look like this:  {code} ========== $ eups list .... pipe_tasks            7.3.2.0_5_g455c355d0f+070b2c1b35  b61 pyfits                3.1.2+9ef17db9b7  b61 b60 python                0.0.1             b61 b60 scisql                0.3+2b5a2f1b52    b61 b60 scons                 2.1.0+2b5a2f1b52  b61 b60 sconsUtils            6.2.0.0_11_gf38997df3e+759c3944a1         b61 sconsUtils            6.2.0.0_19_g755151c0a5+759c3944a1         b60 shapelet              7.3.1.0_1_g9331ee763c+0c72f294dd  b61 shapelet              7.3.1.0_1_g9331ee763c+e56ee84a68  b60 skymap                7.3.1.0_1_g64b750c066+db36490146  b60 skymap                7.3.1.0_1_ga6cd540cd3+493a438aa2  b61 skypix                6.1.0.0_1_g1157bf09ae+65137c93cd  b61 skypix                6.1.0.0_1_gea33592463+6039c04989  b60 .... ========== {code}  where the part before the plus sign is the output of git describe (slightly mangled), and the part after is the SHA1 of the sorted names+sha1s of the dependencies.  While this has the benefit that any two causally disconnected buildbots with the same inputs will build the same versions, many people have complained that they're plain ugly.  So here's an alternative proposal:  * If a tag exist on a commit, use <tag> as the version. * If there's no tag, use branchname-gSHA1ABBREV, where any illegal characters in branchname get turned into dots * If the package has dependencies, and a build of this package with different dependencies already exists, append a +N to the end. Keep the mapping of +N -> (dependency name, sha1s) in a special git repository. Given the source code and this git repo, two causally disconnected buildbots will again generate the same set of versions.  Example versions: * 7.10.2.1 * 7.10.2.1+5 * master-gdeadbeef * feature.dm-1234-gdeadbeef * feature.dm-1234-gdeadbeef+3 "
0,Save a git-branch when a forced push is detected,"Create a gitolite hook that will save a branch when a forced push is detected.  E.g., if we have a ticket: 'tickets/DM-AAAA' and someone rebases it and pushes  with '--force' before applying the update --- then the hook will branch off the old state into (say):  backups/tickets/DM-AAAA/NNNN where NNNN is a monotonically increasing number (per branch)."
1,from __future__ import division breaks division of Extent*,"If one does: {{from __future__ import division}} the division operator on Extent types raises an exception.  How to repeat: I've tried this with v7_3 and master: {code:py} from __future__ import division import lsst.afw.geom as afwGeom npt = afwGeom.Extent2I(10,10)/2 {code} an exception is raised.  Removing the first line succeeds as expected."
3,Measurement - Aperture Corrections,"The current implementation of aperture corrections in meas_algorithms' CorrectFluxes class is broken, and should not be replicated in meas_base:  - it only works when the PSF model is correct  - it doesn't work when the aperture to correct to is larger than the PSF model image size  - it doesn't propagate the uncertainty in the aperture correction  We probably need to do this by estimating the aperture correction and its errors on single frames, using the PSF stars (not the PSF models), then attaching that information to the Psf object to be retrieved *and coadded* by CoaddPsf.  JK: In PMCS this would be 10% Bosch J 50% Krughoff S and 40% Owen R Breakdown: jbosch 10%; krughoff 50%; rowen 40%"
1,Publish Winter 2014 binaries,NULL
0,tests/testPsfDetermination.py has a broken test,"In meas_algorithms tests/testPsfDetermination.py has a test testRejectBlends which does not operate as expected. When it calls pcaPsfDeterminer it results in no usable psf candidates BEFORE blends are rejected. Formerly this resulted in a numpy array named ""sizes"" containing one uninitialized value, which might raise an unexpected exception or raise the desired exception, depending on whether that value was negative or positive.    On tickets/DM-3117 I pushed a fix for the bug that caused the invalid ""sizes"" array, but the unit test is now reliably broken because no viable psf candidates raises the wrong exception and does not test blend rejection in any case. So on this same ticket I have commented out the bad test for now."
0,Configure transition screens for DM agile workflow,"Whenever an issue is transitioned on JIRA Agile board to 'Ready for Review', a screen should pop up to ask for a reviewer.  Whenever it's moved out of that state, another screen should ask for a new assignee."
0,Make lsst-build reuse buildIDs if nothing's changed,"All built packages are EUPS-tagged with build IDs (the bNNN EUPS tags). Without this change, new EUPS tags are declared even when nothing changed since the previous build (and EUPS' tags code doesn't scale well at this time).  This will be implemented by comparing the newly built manifest against ones stored in versiondb, and reusing the build IDs if a matching one is found."
2,Write unit tests for lsst-build,"Unit tests need to be written for lsst-build; they should've been written together with the code, but due to Winter'14 release fire drill they had to be postponed."
0,clean up isr utility code,There is some commented code in isr.py.  This should be removed or updated so that it works.
0,Improve naming of getters in AmpInfoTable,"The names of the methods to get values from a record on AmpInfoCatalog are potentially confusing.    This is because the convention is to call the getters get[attributename].  We could change the method names in the AmpInfoCatalog, or add methods in the SWIG wrapper."
0,"Box2I(bbox.getMin(), bbox.getMax()) fails for an empty bbox","Empty Box2I cannot be round tripped: {code:py} from lsst.afw.geom import Box2I b1 = Box2I() b2 = Box2I(b1.getMin(), b1.getMax()) assert b2.isEmpty() {code}  It is confusing and surprising that this round tripping fails.  It is also a trap for the unwary because saving min and max is the logical way to store boxes in afw tables. Records can contain points but not extents and so it saves casting back and forth and simplifies and clarifies the code to save max instead of extent. Thus that is the path most users will take, and the problem can be a time bomb: it could be quite some time before somebody tries to store an empty box and finds that it does not get retrieved correctly."
2,log4cxx-based logging prototype - v2 ,"This is continuation of DMTF-16570-16. Initial work (v1) was done in branch u/bchick/protolog. V2 will include comments sent by K-T and issues discussed at the Qserv meeting March 13  + Free functions vs. a log object need to be discussed more.  In particular, when metadata key/value pairs need to be attached, an object might make more sense.  Avoiding the getLogger() call when no logging is needed (due to threshold) can be significant.   + It's a security breach to use vsprintf() with any user-provided arguments.  Use vsnprintf() instead so that you can check for overflow. (Or use stringstream or boost::format.)   + In my prototype, I used a combination of a set of static log4cxx::LevelPtr variables with isEnabledFor(level) and a set of cpp macros to avoid the switch.   + The return value from getLogger() shouldn't need to be cast.  ----   + play with hierarchical names  + don't execute code for formatting if debug level is off  + use shorter threadId  + experiment with defining special python handler, intercept and redirect     to our log4cxx-based logging  "
0,Replace PositionFunctor with some flavor of XYTransform,"afw has a special functor PositionFunctor that acts like an XYTransform. Unless PositionFunctor does not need to be invertible, it makes sense to merge these, likely by replacing PositionFunctor with the transform from afw::image::XYTransformFromWcsPair (as suggested by Jim Bosch on Trac ticket #2214)."
3,"Rework JOIN support, including Ref*Match tables",Add support to the Ref*Match tables. The relevant code in Qserv core (supporting joins) has already been written. This task is related to DMTF-1640-20
3,Develop new master-worker result system ,"Reimplement how results are returned from worker to the czar. Currently it relies on mysqldump, which is fairly inefficient. This is related to DMTF-1650-045"
2,Qserv: unit testing (controller module) ,"Design and build toy prototype of a test framework for testing controller module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the controller module at the same time. This is related to DMTF-16570-20."
1,Qserv: unit testing (query execution) ,"Design and build toy prototype of a test framework for testing query execution module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the query execution module at the same time. This is related to DMTF-16570-21."
1,Prepare for setting up new cluster at IN2P3 for continuous integration/testing,"Once the hardware is available, setup the environment where we could easily run integration testing of different Qserv releases, including testing/comparing performance.  Integrate changes implemented in DM-1078  Add install script that exposes individual steps and allows modifications to the config file: newinstall, qserv-configure --prepare, then edit config file (or copy from somewhere), qserv-configure"
2,Migrate Qserv czar code to the new logging system,This includes switching Qserv to the new logging. Fine-tuning (what messages are printed deciding on error level) is covered in DM-685.
0,catch exceptions from CSS,NULL
2,S15 Data ingest,"Rework existing scripts used to load data into plain mysql. This involves mostly simplifying it, and pushing some functionality like converting types outside of the loading scripts.   JK: Refer to loading spreadsheet for PMCS assignments"
2,Revise design for Qserv front-end rearchitect ,"Revisit the architecture. This includes proxy, down to XRootD client (mysqld, python, zookeeper). Capture all findings in new stories, add these stories to DM-1707"
2,Migrate away from using env variables in Qserv,Qserv is currently relying on many env variables. We should migrated away from that to the extend possible.
1,Setup multi-node testbed,"It'd be useful to test Qserv using Winter2014 or Summer2014 data set on a multi-node cluster, just to exercise all pieces of the software and double check we are not missing anything."
3,W15 C++ geometry,"Port the geometry related code used by Qserv (it is currently written in python) to C++, and switch Qserv to the C++ based version.  JK: Refer to loading spreadsheet for PMCS assignments"
2,Implement C++ geometry primitives for Qserv,NULL
2,Switch Qserv to C++ geometry primitives,NULL
2,Experiment with no-subchunking based approach,NULL
1,Qserv worker scheduler – code cleanup,"The qserv worker scheduler code is a bit ugly.  The actual composable scheduler classes (FifoScheduler, ScanScheduler, GroupScheduler, BlendScheduler) might be pretty clean, but the interactions with the rest (wdb, wcontrol) may be harder to understand.  There should be a small amount of low-hanging fruit of code to clean up, but to make things more sensible and understandable may require some new abstractions and shuffling of logic to new/different classes.  Since this issue was opened, some refactoring work has been done as part of the new xrdssi port, so the organization may be somewhat cleaner now. Still, it's worth it to take a fresh look to evaluate the design/interactions to see how much can/should be reorganized."
3,Implement new (async) XrootD client,"(oops, earlier description was meant for DM-878. Sorry for the confusion. -danielw) (I also made a mess with the assignees. I'm sorry. -danielw)"
1,Reference Test Server using new XRootD,NULL
1,Switch to MariaDB,We should switch Qserv to the MariaDB Foundation based MySQL.
1,Setup dev test environment,"Setup whole Qserv environment, including installing data set, and validate it by running some simple queries. Suggest changes/improvements as appropriate."
2,Refurbish existing configuration ,Refurbish existing configuration scripts to make them work with the new packaging/build system.
1,meas_base plugins for CModel magnitudes,"Create meas_base Plugins for single-frame and forced measurement that uses the model-fitting primitives in meas_multifit to implement SDSS-style CModel magnitudes, in which we fit an exp and dev model separately and then fit the linear combination with ellipse parameters held fixed.  An old-style plugin has already been implemented on the HSC fork, and should be used as a guide; this issue involves adapting that implementation to meas_base and potentially cleaning it up a bit.  Note that the HSC implementation cannot be transferred directly to the LSST side because the meas_algorithms APIs are slightly different on the two forks."
1,refactor forced tasks into two tasks,"After looking at it a bit more, I think we should refactor the current meas_base forced photometry task to separate the CmdLineTask from the Measurement task.  This will allow the forced measurement task to share a common base class with SingleFrameMeasurementTask (allowing us to move the callPlugin free functions into that base class), and give us better parallels with existing tasks:  - ProcesImageForcedTask (my proposed name for the base command-line task) will be more similar to ProcessImageTask.  We'll also have ProcessForcedCcdTask and ProcessForcedCoaddTask.  - ForcedMeasurementTask will be more similar to SingleFrameMeasurementTask.  In short, I think this will both clean up the ugliness in callPlugin and make the whole hierarchy easier for newcomers to understand."
1,switch from '.' to '_' in afw::table fields,"We've been mapping '.' to '_' in afw::table I/O, which unnecessarily complicates lots of things.  We'd like to switch to using '_' in the field names themselves, which requires ending this mapping in I/O, but we need to be backwards compatible.  So we'll add a version to the FITS headers, and continue the mapping if the version is not present or is less than some value.  Until we do this, the new field names being used in meas_base won't round-trip."
2,Implement HSC camera in new camera framework,"The HSC camera needs to be put in the new camera geometry.  I will implement is like we did lsstSim and sdss.  That is I will generate a repository with the camera config and ampInfo FITS files that can be unpersisted by the butler.  These changes should only require modifying obs_subaru.  My plan is to just use the policy file to populate the new geometry, but if there is more up to date information, I'll happily use that."
3,Investigate compensation for Dcr,"This is a continuation of the W14 image differencing work.  We have characterized the negative effects of Dcr on difference images, and now need to start working on compensation for these effects.  This work will also touch on the Wcs and Psf classes, which are probable consumers of this information.  Scope includes designing and implementing a class to describe the astrometric effects of Dcr, with consideration as to the other classes (Psf, Wcs) and tasks (ImageDifferencingTask) that may use it"
3,Design of Dcr Class,"Describes the design process for implementation of a class to model the effects of Dcr.  Includes design itself, and the design review process."
3,Implementation of Dcr Class,"Core implementation of the class that represents the effects of Dcr.  This only includes the initial implementation.  We should realistically expect that this class will evolve as it encounters more use cases, no matter how thorough the design process is.  "
0,Test and migrate to swig 3.0,"-------- Original Message -------- Subject: [LSST-data] Swig 3.0 is out (with C++11 support) Date: Mon, 17 Mar 2014 08:26:05 -0400 From: Robert Lupton the Good <rhl@astro.princeton.edu> To: LSST Data <lsst-data@lsstcorp.org>  I tried a pre-release on os/x 10.7.5 and it failed some tests, but I haven't tried this version.  I had some discussion about this with William, but haven't had time to follow through.  							R   > Date: Sun, 16 Mar 2014 22:44:42 +0000 > From: William S Fulton <wsf@fultondesigns.co.uk> >  > *** ANNOUNCE: SWIG 3.0.0 (16 Mar 2014) *** >  > http://www.swig.org >  > We're pleased to announce SWIG-3.0.0, the latest SWIG release. >  > What is SWIG? > ============= >  > SWIG is a software development tool that reads C/C++ header files and > generates the wrapper code needed to make C and C++ code accessible > from other programming languages including Perl, Python, Tcl, Ruby, > PHP, C#, Go, Java, Lua, Scheme (Guile, MzScheme, CHICKEN), D, Ocaml, > Pike, Modula-3, Octave, R, Common Lisp (CLISP, Allegro CL, CFFI, UFFI). > SWIG can also export its parse tree in the form of XML and Lisp > s-expressions.  Major applications of SWIG include generation of > scripting language extension modules, rapid prototyping, testing, > and user interface development for large C/C++ systems. >  > Availability > ============ > The release is available for download on Sourceforge at >  >      http://prdownloads.sourceforge.net/swig/swig-3.0.0.tar.gz >  > A Windows version is also available at >  >      http://prdownloads.sourceforge.net/swig/swigwin-3.0.0.zip >  > Please report problems with this release to the swig-devel mailing list, > details at http://www.swig.org/mail.html. >  > Release Notes > ============= > SWIG-3.0.0 summary: > - This is a major new release focusing primarily on C++ improvements. > - C++11 support added. Please see documentation for details of supported >   features: http://www.swig.org/Doc3.0/CPlusPlus11.html > - Nested class support added. This has been taken full advantage of in >   Java and C#. Other languages can use the nested classes, but require >   further work for a more natural integration into the target language. >   We urge folk knowledgeable in the other target languages to step >   forward and help with this effort. > - Lua: improved metatables and support for %nspace. > - Go 1.3 support added. > - Python import improvements including relative imports. > - Python 3.3 support completed. > - Perl director support added. > - C# .NET 2 support is now the minimum. Generated using statements are >   replaced by fully qualified names. > - Bug fixes and improvements to the following languages: >   C#, Go, Guile, Java, Lua, Perl, PHP, Python, Octave, R, Ruby, Tcl > - Various other bug fixes and improvements affecting all languages. > - Note that this release contains some backwards incompatible changes >   in some languages. > - Full detailed release notes are in the changes file. "
0,Board workflow modifications,"On DM Software Development board, I propose we:  * Change ""Ready to Merge"" to ""Review Complete"". Per K-T:  {quote} That's what I thought ""Review Complete"" would be -- most of the work is done, but some fixups are needed before merging, and a re-review is not necessary.  {quote}  * Remove ""Ready for Review"". Instead, the developer should just drag the issue to ""In Review"" and assign it to a reviewer. If/when we re-instate the ""review master"", we may re-introduce ""Ready for Review"""
3,Setup the new Buildbot CI system,"Setup the Buildbot 0.8.8 testbed for the DM environment.  This includes: (1) setting up slaves on the set of common OS on which the DM stack runs; (2) creating a new continuous integration slave using the new eupsPkg-based build and distribution support,  Definition of done: * Every git change of master should trigger a build of master * If a build failed, an e-mail will be sent to lsst-data (if the build succeeds, nothing happens) * Failures due to internal buildbot issues (e.g., config problems, transient system availability issues, timeouts, etc.) should go to the buildbot owner. * Allow user-triggered builds via web page (with specified refs to be built), with a common user (until we get LSSTC LDAP directory hooked up). It's understood that locking may not be fully implemented/fleshed out in this story. * It should be possible for a user on lsst-dev to easily setup the stack for either a failed or a successful build. "
0,Move TCT-relevant  twiki documentation to Confluence,"Congregate all the trac TCT-relevant documents (standards, policies, guidelines, meeting history) onto Confluence."
0,Develop and then create the organizational structure for DM Confluence space,"Before we start populating the DM Confluence space with active pages, we should define an overall organizational structure/taxonomy."
1,Adding support for numpy scalar array types as arguments to SWIG wrapped methods,"Building against the anaconda on lsst-dev causes construction of Point2I (and other point types) with numpy dtypes to fail with the standard exception: {code} NotImplementedError: Wrong number or type of arguments for overloaded function {code} I did not know that was not allowed.  If I am doing it others are as well.  I think this should be addressed in some way before W14 release.  How to repeat on lsst-dev: {code:sh} setenv EUPS_DIR ~lsstsw/stack/ source ~lsstsw/eups/bin/setups.csh setup afw setup anaconda echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}  Using the old 7_3 stack you can do a similar test: {code:sh} source /lsst/DC3/stacks/default/loadLSST.csh setup -t v7_3 afw echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}"
0,Improve handling errors occuring in AsyncQueryManager,"AsyncQueryManager is initialized based on configuration file, if the configuration is invalid, an exception should be thrown (eg in _readConfig()) and gracefully handled upstream."
1,clean up multiple aperture photometry code,"I've been doing some minor work on the HSC-side ApertureFlux algorithm, and I wanted to record some concerns here (from both me and RHL) that should be addressed in the new meas_base version:  - We should consider merging ApertureFlux and EllipticalApertureFlux into the same algorithm (with a config field to choose whether to use elliptical apertures).  We could still register it twice, with a different default config value, and this should eliminate a lot of code duplication.  We could also consider having them inherit from a common base class (instead of having EllipticalApertureFlux inherit from ApertureFlux, as is done now).  - We should test that the threshold at which we switch from Sinc to naive apertures is obeyed exactly.  - We should create a flag for the failure mode in which an aperture cannot be measured because we go off the edge of the image, and test that it appears at the right point.  If possible, we should set this flag and measure what area we can within that aperture, instead of just bailing out.  - (Somewhat off-topic) We should consider having utility functions on SourceSlotConfig to set all slots to None for use in unit tests."
0,More helpful location information for errors in duplicator/partitioner input,"Currently, if the Qserv duplicator or partitioner encounters erroneous input (such as a formatting error, or missing columns), the error message it outputs does not include a mapping back to the location of the error, making it very hard/annoying indeed to fix that erroneous input.  Fabrice would (quite reasonably) like to see a filename and line number in the error message.  Unfortunately, producing a line number is complicated by the following facts:  - multiple threads may be reading sub-sections of the same input file in parallel  - sub-sections are not guaranteed to be read in order  - lines can have varying length  In other words, the code reading/parsing the input has no idea which line it is working on, and making that information available would involve either deferring error reports or extra synchronization (performance loss).  What we can easily (and should) do is to arrange for processing code to know the file and byte offset of the input text being processed; error messages should include both pieces of information."
1,Eliminate dependence of query analysis on parser and antlr,"I would like to write and compile query analyzer code completely independently of the parser and ANTLR (transitively). This doesn't seem to work right now. This is not currently possible.  This might take any where from a day to a week. (I'm not sure if we can finish anything in half a day, if you include the testing, review, feedback, and revision process, but perhaps unit testing will make that faster).  Updates to follow after the scope is estimated.  Dependencies to be broken: query --> parser, antlr (due to predicate depending on antlr nodes) qana --> parser, antlr "
2,Investigate new XrdSsi interface,NULL
0,xrootd initialization should abort if mysql connection fails,"Currently xrootd will happily start even if it can't connect to mysql, it will only print a message:  {code} Configration invalid: Unable to connect to MySQL with config: {code}  This can be easily overlooked, plus, it is a fatal error and xrootd initialization should be aborted.  while working on this, I propose to also improve validateMysql(). At the moment it connects to mysql and database in one call. If connection is fine, but the database does not exist, it will fail without telling user why it failed. This can be confusing. It'd better to connect to mysql without connecting to database, then do ""select_db"", and if that fails, inform the user that the database does not exist.  (transferred from trac ticket 3165)"
1,fix namespaces in all Qserv core modules,"This was suggested by the code review for ticket 1945 (https://dev.lsstcorp.org/trac/wiki/SAT/CodeReviews/1945), pasted below:  common/src/*:  While it's not required by the coding standards, I'm a big proponent of using namespace scopes in .cc files, which usually save you from needing namespace aliases and will certainly save you from having prefix every declaration with qserv::.  At some point I'd recommend changing the header file extension from .hh to .h to match the rest of the LSST DM code, unless it's a big backwards compatibility issue.  (transferred from trac ticket 2528)"
0,Qserv should check for loaded spatial UDFs,"Qserv should have a way of checking for the existence of spatial UDFs loaded in the worker MySQL instances.  Obviously, the worker must perform the physical check. However, the worker has no knowledge that spatial UDFs even exist, since the master is responsible for translating spatial spec into UDF call.  At the moment, the preferred way of checking would be some sort of administrative command that runs on all nodes. An alternative would be some sanity check that is run on a worker before an admin starts a worker. Or the master could devise a MySQL query to check for things and dispatch to all chunks (although this would not get full coverage when replica exist, while being redundant while workers host more than 1 chunk).  (transferred from trac ticket 1959)"
0,restarting mysqld breaks qserv,"Restaring mysqld results in unusable qserv (even if the restart happens when qserv is completely idle). The error message is:  ERROR 2013 (HY000): Lost connection to MySQL server during query This happens most likely because qserv caches the connection, which becomes invalid when server is restarted. I am guessing the same will happen when there is a long period of inactivity (the connection times out).  (transferred from trac 2853)"
1,Centralize hardcoded constants,"Some values in qserv need to become constant(e.g. chunkId column names, dirs, filenames). Some of these are configurable, others are hardcoded in non-obvious places in the code. When multiple places need this value, they really need to agree, and unfortunately, qserv doesn't have a well-known place for these constants yet.  Any (constant) value that is needed by different parts of the code needs to be managed in a way that is reasonably obvious to unfamiliar programmers.  List of values: * chunkId, subChunkId column names (master...indexing.py, app.py) * environment variable names * + others. This should actually be fairly simple to implement, once the right (?) design is conceived and worked-out.  (transferred from trac ticket #2405)"
1,Jira for Qserv,"Jira setup for Qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more..."
1,Come up with a standard to handle C++ Exceptions in Qserv (and the rest of DM?),"Currently CSS is using one class and relies on different error codes to differentiate between different type of exceptions, while other parts of Qserv core define an exception for each different error. It'd be good to standardize and use the same approach. "
0,cleanup includes in Qserv core modules,"Includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. Also, unify forward declarations."
1,CSS - surviving mysql and zookeeper glitches,CSS should gracefully recover from failures such as lost connection to mysqld or zookeeper. It is not clear if it would survive such glitches right now -- this needs to be tested and the code improved as necessary.
0,Create a board (virtual or otherwise) with pictures and names of everyone in DM,I tried compiling a list of everyone who works or has worked on the DM code -- it was nearly impossible. We should have a (public) list both of current staff and our alumni (and where they are now).
0,Re-think thread.cc and dispatcher.cc python interface,The mess of thread.cc and dispatcher.cc need to be re-thought and re-designed so that the interface is smaller and more obvious.  
0,Trim python importing by czar in app.py,"Clean up the way modules are imported in qserv master, use relative import when appropriate instead of lsst.qserv.master.<package>   (migrated from Trac #2369)"
0,"Libraries being built in lib64 on OpenSUSE, when EUPS tables assume lib","A report from Darko Jevremovic <darko@aob.rs>: {quote} Hi Mario,  I managed to build stack v8 on OpenSuse13.1  There were standard problems with lib/lib64 - namely system builds libraries in $PREFIX/lib64 and some programs are hard wired for $PREFIX/lib  if you could  change the last line of  mysqlclient-5.1.65+3/ups/eupspkg.cfg.sh  from  (cd $PREFIX/lib && ln -s mysql/* . )  to  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi &&cd $PREFIX/lib && ln -s mysql/* . )  or something along that line (am not sure whether the syntax would  work).  Also if you could add  in the same manner to ups/eupspkg.cfg.sh  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi)  for the following packages:  minuit2 gsl cfitsio wcslib {quote} "
0,Take RAM into account when computing NCORES to use in installs,"Darko Jevremovic reported he's had to switch off hyperthreading and manually override NCORES, MAKEFLAGS and SCONFLAGS because his 8-core machine had too little RAM to build afw with -j 8.  To fix this, eupspkg default build routines should take RAM into account when computing the level of build parallelism.  In the meantime, we should document the workaround (contact darko@aob.rs)."
0,Local lsst-build invocations should use a different build number prefix,"Buildbot-invoked lsst-build installs packages in the stack with ""b#"" tags.  These are propagated to the distribution server by {{eups distrib create}}.  Local stacks maintained with lsst-build should use different tags so that they don't conflict with these distributed tags."
2,S14 Enhancements in Qserv installation procedure,NULL
0,Cut Qserv release,"It'd be very useful to have fully functioning Qserv release with the latest set of changes (build, packaging, CSS, Daniel's fixes etc) during the Hackathon week."
1,Migrate std::lists to std::vectors,"Suggested by Andy when reviewing DM-296, discussed at Qserv mtg 3/27.  std::list --> std::vector  * why? Default now is vector, iterating over vector is much more efficient than over list  * revisit on case by case bases, do not blindly replace  * preferred solution: typedef, and name it in a way that conveys the intent (e.g., might call it a ""container""), underneath use vector"
1,improve code that initializes shared_ptrs ,"Reported by Andy when reviewing DM-296. Discussed at Qserv mtg 3/27.   boost::shared_ptr =(new T())"" --> boost::make_shared()"
0,removed dead code in stringUtil.h,Remove obsolete strToDoubleFunc (and more) in util/stringUtil.h.
1,Make it easy to build and release point releases,NULL
1,Establish github mirror of LSST repositories,NULL
2,Transition to Stash,NULL
0,Enable gravatars,Could you enable gravatars for all our atlassian products (at least Jira + Agile; Confluence)  https://confluence.atlassian.com/display/AOD/Configuring+Gravatar+support
0,Add cameraGeom overview to Doxygen documentation,The CameraGeom package needs an overview page (part of afw's main.dox) as part of the Doxygen documentation. I think it's up to Simon or me to add this.
0,Install and tag multiple Qserv versions on the same distserver,Done in DM-366
1,Simplify Co-add example in Software User Guide,"The current example in the LSST Software User Guide for co-addition reflects the processes necessary to perform a DR production. While thorough, it only really works on the lsst cluster. The example should be simplified to work on a smaller subset of data, and on single-user machines.  Definition of Done: * Following the documentation, it will be possible for users to identify the SDSS data they need (the subset of files) * There will be instructions on how to download the necessary files to their local machine * There will be instructions on how to build the necessary repositories * There will be instructions on how to run the Co-Add+forced photometry tasks.  * Any issues requiring access rights to LSST machines or databases will be identified and issues created for later."
1,Integration tests dataset should be packaged in eupspkg,"A qserv-integration-tests package should be created : - it would allow to manage easily, in ups/qserv.table, tests version for a given Qserv version. - it would allow to install Qserv dependencies related to testing, like partition (and other data ingest code which may arrive."
1,Refactor install/distribution procedures using lsst-sw," Here's Andy Salnikov remark, during #3100 review : https://dev.lsstcorp.org/trac/ticket/3100#comment:18"
1,CSS performance optimizations (avoiding redundant checks),"Facade.cc: it seems worthwhile to think about how to tweak the implementation to reduce redundant calls. e.g., getChunkedTables() calls _cssI->exists() for the database for each contained table. It also calls exists() for each table, which it just retrieved. In reality, it only needs to call exists() once, for the db. So we are doing 1+3t reads rather than 1+t.  (This came up in the review of DM-56, the review comments are captured in DM-225) "
2,integrate qserv_admin backend into czar or separate admind,"client/qserv_admin_impl.py: I know the czar's Python code is ugly. But the concept of having more than one entry point for users is also ugly. I feel like the client should just wrap up stuff into JSON and make a REST call into the czar. Ideally the proxy would use REST, perhaps via: https://github.com/fperrad/lua-Spore/, which at least seems more thought-out than lua-xmlrpc.  (This came up in the review of DM-56, the review comments are captured in DM-225)"
1,Improve how CSS exceptions are handled,"CSS has one class for all exceptions. The model adopted by Daniel (each type as a separate exception) is better, as we can catch individual exceptions  instead of (a) having to catch all css exceptions, (b) checking type and (c) rethrowing if it is not the type we want. To be able to catch all CSS exceptions, we can just introduce a new base class (that is new comparing to Daniel's version)."
1,improved how default values for CSS are handled,"Need to improve how defaults are handled in qserv_admin. There seems to be some desire to warn when values are not set--how about setting defaults and just printing what configuration is being used? If this is something human-created, we should have reasonable defaults and not bother the user, unless no default is viable. I think we should only be strict on machine-generated input, where we would like to catch bugs as soon as possible.   (This came up in the review of DM-56, the review comments are captured in DM-225)"
0,fix testQueryAnalysis,5 tests fail in the testCppParser.
0,Catch AttributeError problems in czar,"I wonder if we could catch more exceptions in czar to simplify debugging. For example, if I change: {code} --- a/core/modules/czar/lsst/qserv/master/app.py +++ b/core/modules/czar/lsst/qserv/master/app.py @@ -418,7 +418,7 @@ class InbandQueryAction:                     self.constraints.size())          dominantDb = getDominantDb(self.sessionId)          dbStriping = getDbStriping(self.sessionId) -        if (dbStriping.stripes < 1) or (dbStriping.subStripes < 1): +        if (dbStriping.x < 1) or (dbStriping.y < 1): {code}  It will return to client a very cryptic error: {code} Qserv error: Unexpected error: (<type 'exceptions.AttributeError'>, AttributeError('x',), <traceback object at 0x969a02c>) {code}  with no other clues, traceback or information in the log. "
3,"S14 Improve error handling in all parts of Qserv, report sensible errors to users","Qserv needs to handle error gracefully. No matter what error occurs, it should try to automatically recover, and if it can't it should report a reasonable error to user. We should try to poke around and trigger various errors in random places in Qserv and watch what happens, how system fails and/or how it reports the error to user (then we should fix it...). I expect we will need to design a better error handling framework to achieve graceful error handling."
0,"loadLSST bug(s) for csh, ksh","A flaw in the v8.0 loadLSST scripts (and/or in eups/bin/setups) causes the following errors:     1) When using ksh:   {code}     $INSTALL_DIR/loadLSST.ksh     ksh: /path/to/INSTALL_DIR/eups/bin/setups.ksh: cannot open [No such file or directory]  {code}  And indeed, there is no eups/bin/setups.ksh file.     2) When attempting to run the installation demo (v7.2.0.0):  {code}     $> printenv SHELL     /bin/tcsh  {code}  [The same issue appears with csh, unsurprisingly.]  {code}     $> source /path/to/install_dir/loadLSST.csh     $> cd /path/to/demo     $> setup obs_sdss     $> ./bin/demo.sh     ./bin/demo.sh: line 7:  /volumes/d0/lsst/stack80/eups/*default*/bin/setups.sh: No such file or directory     ./bin/demo.sh: line 12: setup: command not found  {code}  After hand-editing the demo.sh script to omit the ""/default"" string from the offending line, the demo runs normally to completion.     Note that everything works fine for bash with v8.0, which is what I tested awhile back. "
0,Add Versioning to SourceTable in lsst::afw::table,"Add version to afw::table::SourceTable.  Persist that version number to fits file when the table is saved, and restore when the table is restored.  Tables created and saved to disk prior to this modification will have the version number 0, by default.  Tables created with the S14 version will have the version number 1.    This change is to enable a new version of slots and field naming conventions as needed by the Measurement Framework overhaul, at the same time allowing current clients of SourceTable to continue to function.  The work to define and persist the slots depending on the version will be on a separate issue.  Should not appear as an alterable member of the metadata, but should be saved with the metadata and reloaded when the file is reloaded.  getVersion and setVersion methods will be used to allow clients to alter this number."
1,Create Command/Event Sender,"Simulate the OCS and CCS (via the OCS) sending message to the Base DMCS.  Write a library to send commands via method calls, which will be used by commandable  entities and by the Base DMCS.  The method calls in this library will be used to simulate sending commands from the OCS.  This will initially be developed using DM messages, and later switched to use DDS.  Write a command line tool to send these messages.  Commands with no arguments: init, enable, disable, release, stop, abort, reset.  Commands with arguments: configure - arguments are: Set of computers, software and versions to be executed, parameters used to control that software.  Events with arguments: startIntegration, nextVisit.  Definition of done:  * A library that has method calls to each of these commands/events.  Each method call sends one message to the given Topic. * Command line tool that can send any of these commands/events to a commandable entity subscribed to a Topic. * Unit tests for each of the commands/events"
1,Build Base DMCS communications library,"Write a library to be used by each commandable entity and the Base DMCS.   Methods in this library receiving commands from the simulated OCS. Specific command actions will be handled by each entity, and events are handled by the Base DMCS.  This will initially be developed using DM messages, and later switched to use the DDS.  The library will include:  * An object with methods for blocking receives, blocking receives with timeouts, and non-blocking receives.  Any commands received by these methods are given to another class to call appropriate action methods. * An abstract class implementing each of the following methods for commands: init, configure, enable, disable, release, stop, abort, reset. * An abstract class with methods for implementing the following methods for events:  startIntegration and nextVisit"
2,Build replicator,"Replicator  On boot:  * Establishes connection with single, pre-configured distributor. * Checks connection with the network outage buffer, the Base raw image cache, and the tape archive.  On success, the replicator registers itself with the Orchestration Manager in the fully-operational pool.  From K-T: Note that what to check should eventually be dependent on what was specified in the configuration.   That way, if the tape library is down, we could still run using the network outage buffer or raw image cache as the redundancy source.  In operation:  * Receives a job with visit id, exposure sequence number within the visit and raft id. * Queries the Base EFD replica for information needed to process the image. * Subscribes to the Camera Data System (CDS) “startReadout” event and to the OCS telemetry topics. * Waits for a startReadout event * Request the cross-talk corrected exposure for the raft using the CDS client interface and block until it is available.  On receipt of crosstalk-corrected image:  * Verify integrity of image (hash or checksum) * Send image with telemetry to the distributor, compressing it if the configuration says to do so.  At the same time, the image is written to the network outage buffer and the raw image cache using the Data Access Client Framework, retrying until successful for a configured number of tries. All images that are written are tagged with the Archiver mode (by the replicator job).  * Request the raw image using the CDS client interface and block until it is available.  On receipt of the raw image:  * Verify integrity of image (hash or checksum) * Send image to the distributor and simultaneously write to the network outage buffer and the tape archive.  ALL image transmission statuses (successful and unsuccessful) are recorded to the Alert Production Database.  NOTE:  Image some calibration or engineering modes, there may only be raw-image data, not crosstalk-corrected image data.  The replicator job configuration will provide for this).  NOTE: Replicator jobs will need to detect they are not completing in the expected amount of time.  NOTE: Something (the Base DMCS?  The Event/Message Monitor?) needs to pay attention to the number of replicator re-registering themselves with the local-only pool and notify NetOps to investigate and resolve the issue.  On errors sending or keeping connected to the distributor, the replicator: * unregisters from the “fully-operational” pool * registers with the “local-only” pool  The replicator will continue to try and connect to the distributor, and if it is able to reestablish the connection: * unregisters from the “local-only” pool * registers with the “fully-operational” pool  Heartbeat messages between replicators and distributors will indicate when the replicators will be able to re-register with the fully-operational pool."
2,Build distributor,"Distributor  On boot:  * Checks the network, archive raw image cache and the tape archive.  After all have tested successfully, the distributor waits for a connection from its associated replicator.  In operation:  * On receipt of a visit id, exposure sequence number and raft id from its associated replicator, the distributor publishes them along with its network address to the Archive DMCS.  On receipt of crosstalk-corrected image and associated telemetry from its associated replicator:  * Verifies integrity using a hash or checksum * Writes to the raw image cache using the Data Access Client Framework * Decompresses (if necessary) * Separates into individual CCD-sized portions * Sends portions to the appropriate connected Workers  Note: Workers can connect to request a CCD-sized crosstalk-corrected image.  On receipt of raw image:  * Writes it to the tape system. * All images are tagged with the Archive mode.   Should include heartbeat monitoring of the replicator/distributor connection."
1,Write Linux Standard Base - compliant init.d scripts,"Qserv services init.d scripts have to rely on LSB, in order to work on multiple systems.  Remark : xrootd has to be launched as a background process (i.e. with a & at the end). But this always send of return code equal to 0, even if xrootd fails to start, a shell function wait_for_pid will be implemented in xrootd init.d scritps to solve that (inspired from mysqld init.d script)."
1,Simulate computation and production of VOEvents for worker batch job,Simulate jobs which do alert processing.  Produce VOEvents based on those results.
1,finish adding aliases to afw::table::Schema,"This issue picks up the partially-completed Trac Ticket #2351, which adds a string-substitution-based alias mechanism to afw::table::Schema. There are still some issues to sort out w.r.t. constness - in other respects, a Table or Catalog's schema cannot be changed once the Table has been constructed, but we do need to be able to change aliases after table construction."
0,Use aliases in slots,NULL
1,Remove measurement code from meas_algorithms,NULL
1,add basic FunctorKeys,"Add the basic FunctorKeys mechanism, and enough implementations to support Slots."
1,Add FunctorKeys to replace compound field functionality,Add more FunctorKeys to replace the functionality in all current compound field types and meas_base result objects.  May involve moving some meas_base definitions to afw.
1,Add FunctorKeys for common analysis tasks,"Add FunctorKeys for simple, common, calculated fields, including:  - Magnitudes from fluxes  - Coords from Points, Points from Coords  - Ellipse conversions and radius/ellipticity extraction"
2,Integrate FunctorKeys with SchemaMapper,NULL
0,Remove support for compound field types,NULL
1,add live DS9-based debugging to measurement framework,"The old measurement framework had a lot of live DS9-based display options.  We should ensure the new one has at least as many, and that it still works.    At some point, we should consider a different mechanism for enabling those displays and possibly other tools for displaying them, but that's out of scope for this issue."
1,Make NoiseReplacer outputs reproduceable,"We need a way to get back the noise-replaced Exposure as it was when a particular source was measurement, after the measurement has been run, without having to run noise-replacement on all the previous objects again.  There is already code in afw::math::Random to output its state as a string; I think we should probably just save this string in the output catalog.  This will require some API changes to allow the NoiseReplacer to modify the schema and set a field in the output records."
1,Control log levels on a per-plugin basis,We should be able to control log levels so that certain plugins are run at one level while the rest are run at another (to allow a particular plugin being debugged to be more verbose).
1,Add slot support for meas_base-style outputs,"The slot mechanism in afwTable currently uses compound fields to save the 3 slot types:  flux, centroid, and shape.  Since the new measurement framework uses a flattened representation in the SourceTable where these types are saved as multiple scalar fields, the slot mechanism need to be altered to handle this new table type.  1.  An alternative to KeyTuple for storing the keys required by the slot 2.  Fixup get(Centroid, Flux, Shape) in SourceRecord to use correct keys. 3.  Fixup the single value getters (getX, getY, etc) to use the correct keys. 4.  Persist slot info to fits correctly, based on table version."
1,add aperture-correction measurement code to the end of calibrate,"At the end of CalibrateTask, we'll want to compute the PSF and aperture fluxes of the PSF stars, and send those to the PSF model to be stored and interpolated (using the featured added via DM-434).  We'll also need to run any other flux measurement algorithms that need to be tied to the PSF fluxes on these same stars; because these can be somewhat slow, we probably want to limit these measurements to only the PSF stars, rather than requiring all these algorithms to be run as part of calibrate.measurement.  The relationships between these fluxes and the PSF fluxes will be additional fields to be added to and interpolated by the PSF.  The HSC implementation of this work (as well as that of DM-436) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191"
1,apply aperture corrections in measurement tasks,"We need to interpolate the aperture correction to the position of every source, and apply this correction to all appropriate fluxes.  The HSC implementation of this work (as well as that of DM-435) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191  Note that on the LSST side, we'll want to apply the aperture corrections either within a new plugin in meas_base or as a new part of BaseMeasurementTask, not as a change to the CorrectFluxes algorithm (which will be removed in the future along with the rest of the old measurement framework in meas_algorithms)."
1,Setup of four new measurement algorithms for processCcd testing,"The goal of this story is to take the following algorithms and make them fully operational with processCcd.  PsfFlux,SdssShape,SdssCentroid,SincFlux.  This code was moved to meas_base in w14, but has yet to be used in full operation.  This is the first step in that process.  Each algorithm will at least have one test to confirm that it works, but the unit tests will be very simple.  The actual confirmation of full operability will the to run tests against real data, and compare against the existing meas_algorithms.  The algorithm code will still require cleanup even after this story is completed.  That is because it has intentionally been left the same as what existed in meas_algorithms.    The goal of this ticket is to confirm that the each algorithm can (1) complete its measurement with some reasonable result, (2) responds to its major configuration options, and (3) handles at least some of its defined exceptions (that is, flags) correctly.  Confirmation that the results are identical with meas_algorithms is a subsequent ticket  Cleanup and documentation of this code will be done in a subsequent ticket."
0,Approve/Acquire HipChat licenses,"-------- Original Message -------- Subject: Getting HipChat licenses Date: Wed, 09 Apr 2014 08:51:46 -0700 From: Mario Juric <mjuric@lsst.org> Organization: Large Synoptic Survey Telescope Inc. To: Jeffrey Kantor <JKantor@lsst.org>, Iain Goodenow <IGoodenow@lsst.org>,  Stefan Dimmick <SDimmick@lsst.org>  Jeff et al., 	I'd like to start the process to acquire HipChat licenses (we have 10 days left on our evaluation one). Right now we have 26 users, so our first total would be $52/month. I'm expecting it will eventually grow to ~50 users or so.  	In terms of an account to charge, this should really be considered a project-wide tool, but if that's going to cause unnecessary delays I'd propose we charge it to DM as a sign of our infinite kindness and good will :).  	HipChat wants to simply bill a credit card -- once we get the necessary approvals, Iain, I can add you as an admin, and you can use our corporate one if that's OK. "
1,Write job ads for Tucson DM positions,NULL
1,Test four algorithms for compatibility with original meas_algorithms,"Do a trial run of a small area of sky (using a single exposure from measMosaicData).  First create a source catalog using the old measurement.py, then run the same test with the measurement task in sfm.py.  Compare the results.  If the code has been ported correctly, they should match."
0,Setup PeakLikelihoodFlux with new Algorithm Framework,Move PeakLikehoodFlux to meas_base framework 
0,Setup Flux algorithms for testing with processCcd,"Similar to DM-441 for flux algorithms GaussianFlux and NaiveFlux  Unit tests for major config options, just to be sure that they do something reasonable.  Test of at least one exception (flag)"
0,Test Centroid algorithms against meas_algorithms,NULL
0,Test Flux algorithms agains meas_algorithms,"Test for compatibility of NaiveFlux, GaussianFlux, and PsfFlux against meas_algorithms  "
1,reimplement shapelet PSF approximations,"The CModel code we want to transfer from HSC in DM-240 currently relies on the old ""multishapelet.psf"" algorithm in meas_extensions_multiShapelet.  That means we either need to convert that PSF-to-shapelets code in meas_extensions_multiShapelet to use the meas_base interface, or we need add new PSF-to-shapelets code in meas_multifit.  I think the latter is the better choice, even if we delay DM-240 as a result; the heavy algorithmic code is already available as primitives in meas_multifit, so it should just be a matter of packing those into a simple driver, creating a config class for it, and testing it on a few real and simulated PSFs to learn reasonable defaults for the configs."
1,Implement backup/restore for CSS,"It'd be very useful to have a way to ""dump"" and ""restore"" entire contents of the key/value store in zookeeper. The dump part is pretty much there (can dump to an ascii file) "
1,Add Classes of MeasurementError,"Some measurement failures are global for a whole exposure, such as a missing Psf or Wcs.  The framework currently does not distinguish this from a failure in a single measurement.  Should add a new subclass of MeasurementError which can be thrown in these cases.  Should also add configuration option to the measurement framework to determine what should be done with this type of error.  We should also add an exception class and associated how-to-handle config for problems that indicate that something has gone wrong in pre-measurement processing, such as NaNs in the image."
0,"PixelsFlags, SkyCoordAlgorithm, and Classification","SkyCoord was moved to DM-441 - done in Python  Classification is also simple if done in Python  Pixel Keys will be done in C++  Some work left from SdssShape will be done with this ticket  Also, since these are out only two Python algorithms in the base set, I will add the exception handling and base fail() methods at this time. "
1,"add and use ""suspect"" flag in slots and slot-like measurements","We need to have both ""fatal"" and ""suspect"" flags to handle different levels of warnings in measurement.  (other tasks previously included on this ticket have been split into other tickets, including DM-461 and DM-984)"
1,lsst-build updates based on feedback from 1st month of use,"change lsst-build interface to:   * rename 'prepare' to 'clone'   * use current directory to clone to   * remove the default REPOSITORY_PATTERN, as it isn't complete   * use the current username as default build tag prefix   * don't write the manifest by default; use --manifest=<filename> option instead   * refuse to change/overwrite repositories if they're dirty, use --force to override   * create a separate 'version' verb   * ProductFetcher.fetch doesn't need to return ref, sha1   * read config file within build directory, .btconfig   * rename the binary to bt   * have the versioner use git db by default, but fall back to generating versions from hash by default  See the linked web page for a mock of the command line interface."
0,Understand galfast bugs,NULL
0,Alias measurement.plugins to measurement.algorithms,"The config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base.  The creates a backward compatibility issue for code which refers to this class member.  Jim's suggested fix is to alias plugins with algorithms in the new measurement task."
0,Rework exceptions in css (python side),"Rework exceptions in css/KwInterface.py: split into key-value related exceptions, possibly moving the rest that deals with db/tables into client.  This came up in the CSS review, see DM-225: ""CssException feels a bit out of place...."""
2,Create LSSTsim processing example for SW User Guide,"Create a worked example in the Software User Guide of processing raw data with obs_lsstSim. The example should include:   - Pointers to documents for creating multi-band, raw datasets with PhoSim - Creating a data repository for the data - Creating an astrometry_net_data repository - Processing raw files through processCdc (including ISR, measurement, etc.) - Creating a catalog of measurements - Visualizing the output using existing tools  It is possible to work initially with a limited set of data (a raft, or CCD), and to start with eImages rather than raw; the example can be elaborated in due course."
1,Create an iPython Notebook visualization of the LSST Demo Data,"Develop an iPython Notebook to illustrate the processing and results of the LSST Demo. This can be used as both a basic tutorial for the Stack, and a how-to for creating additional visualizations for DM. The visualization should include the source catalog, and possibly also one or more of the processed images. "
0,Prioritize and define the backlog for Summer 2014,"At the DMLT meeting in Seattle, finish the backlog prioritization for Summer 2014 cycle.  Definition of Done: * All epics and major stories defined for Summer 2014. * Rank-ordered in terms of priority. * Leadership teem agreement on their prioritization and backlog. * Teams identified to execute the stories, and stories labeled accordingly. "
1,Document how to create an astrometry_net_data repository,"Document how to create astrometry_net_data index files for an arbitrary astronomical dataset, as well as how to create a repository of such data to support processing with the LSST Stack. It must be possible to limit the area of sky coverage in the index files to that appropriate for the images that the user wishes to process. "
0,Make JIRA notification e-mail more useful,"From HipChat/Data Management:  [12:09] Mario Juric: @jbosch @KTL @KSK Could you double-check if any of you got an e-mail from Jira on Saturday (Apr 12th) re issue DM-78 (I made you reviewers, but it looks like you weren't notified)? [12:10] K-T Lim: I don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] Simon Krughoff: I did get an email. [12:10] Jim Bosch: @mjuric, ah, it appears that I actually did.  The fact that I was a reviewer was just buried, and I didn't notice it. [12:10] Simon Krughoff: I must have missed that I was a reviewer. [12:10] Mario Juric: OK, thanks! 		That gives me not one, but two useful data points (#1 -- emails work, #2 -- they're useless :) ). [12:12] Simon Krughoff: I'm not sure why they are useless.  The emails from trac were a very important part of my workflow as far as being notified of review responsibility goes. 		Maybe it's just the volume from Jira. [12:14] Jim Bosch: Yeah, same here.  Though the volume from JIRA hasn't been so bad, so I don't think that's it.  Maybe my brain just has to get used to the new email format. [12:14] K-T Lim: (In my case, I'm mostly paying attention to the RSS feed although the mailbox serves as a backup.) [12:22] Robert Lupton: One of the things that made gnats a good bug tracker was that the emails contained the right amount of information (I did have source code...), and trac was pretty good too when we tuned it;  bugzilla always used to be awful.  I bet we can fiddle with Jira to make its mail more useful;  I don't just mean filtering what it sends, but making sure that each email is self contained, but not too long"
1,Build Base DMCS Archiver Command Receiver,"Base DMCS Archiver  The Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  prerequisite for configure command - sufficient replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - Subscribe to the startIntegration event.  * disable - Unsubscribe to the startIntegration event.  Note that this does NOT terminate any replicator jobs which are already executing.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
1,Build Base DMCS Catch-Up Archiver Command Receiver,"Base DMCS Catch-Up Archiver  The Catch-Up Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient catch-up-dedicated replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - 1) Allows Catch-Up Archiver to scan for unarchived images to be handled; 2) Enables the Orchestration Manager to schedule image archive jobs.  * disable - 1) Stop scanning for unarchived images; 2) Tell the Orchestration Manager to stop scheduling any new image archive jobs.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
1,Build Base DMCS EFD Replicator Command Receiver,"Base DMCS EFD Replicator  The EFD Replicator receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - communication with the US DAC EFD replica is possible.  Note that after configure, the EFD Replicator remains disabled.  * enable - Causes the Base DMCS to enable the US DAC EFD replica to be a slave to the Chilean DAC EFD replica.  * disable - Causes the Base DMCS to disable the slave operation of the US DAC EFD replica.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
1,Build Base DMCS Alert Production Cluster Command Receiver,"Base DMCS Alert Production Cluster  The Alert Production Cluster receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient workers are available.  Note that after configure, the Alter Production Cluster remains disabled.  * enable - Causes the Base DMCS to subscribe to the “nextVisit” topic in normal science mode; another event may be subscribed to in calibration or engineering mode.  * disable - Causes the Base DMCS to unsubscribe from the “nextVisit” topic.  It does NOT terminate any worker jobs already executing.  In particular, the processing for the current visit (not just exposure) will normally complete.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
0,improve initialization of kvMap in testQueryAnalysis,Build the kvMap at build-time and embed it into the executable. (this was brought up in DM-225)
0,improve generating kvMap in testFacade.cc,"Generating the kvmap file, and pasting it into a string inside the test program. (this was brought up in DM-225)"
0,shorten internal names in zookeeper,rename DATABASE_PARTITIONING to PARTITIONING  rename DATABASES to DBS
0,"rename ""dbGroup"" to ""storageClass"" in CSS metadata","It is meant to be used to indicate L1, L2, L3... At Qserv design week we decided to rename it (original plan was to remove it all together) "
0,Tweak metadata structure for driving table and secondary index,"There seem to be confusion about driving table and secondary index. At the moment in zookeeper structure we have {code} /DATABASES/<dbName>/objIdIndex /DATABASES/<dbName>/TABLES/<tableName>/partitioning/secIndexColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/drivingTable /DATABASES/<dbName>/TABLES/<tableName>/partitioning/latColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/lonColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/keyColName {code}  Issues to think about:  * we can't call it objIdIndex, it is too lsst-specific.  * drivingTable and keyColName - perhaps these should be at database level, which means we would only allow one drivingTable and one secondary index per database?  * or, maybe instead of database level, it is a partitioning parameter? Note that two databases might use different name for secondary index or driving table, yet they might be joinable. That argues for introducing a new group, something like /DATABASE/partitioning in addition to /DATABASE_PARTITIONING.  * consider renaming drivingTable to keyTable  * do we really need secIndexColName and keyColName? Can't we get rid of one, and rename to keyColName? "
0,rework ubuntu.patch,NULL
1,Generalizing data chunking (n-level chunking rather than stripes/subStripes),"It'd be cleaner to use numbering (e.g. 0 for no chunking, 1 for chunks, 2 for subchunks etc) instead of ""chunks"", ""subchunks"" throughout qserv code. This might also be true in partitioner, where flags like -S and -s etc are not entirely obvious.  This came up in the review of CSS, see DM-225."
1,fix threading issues in CSS watcher,"Fix problems with threads in watcher.py brought up in DM-225 by Serge:  * A thread per database doesn't scale  * There is a thread leak when a database is deleted  * There is another design problem, in that each database thread looks like it is holding on to the same lsst.db.Db instance under the hood. I don't remember any consideration for thread safety from the lsst.db code when I reviewed it. Note for one that it is not safe to use a MySQL connection simultaneously from multiple threads (and I seem to recall that you are caching a connection inside Db instances). In practice, even the Python GIL may not save you, since calls into C code (i.e. the mysql client library) may very well release it."
1,"Switch to the ""czar"" name consistently",1) Change lsst.qserv.master to  lsst.qserv.czar in the czar module.  2) Rename masterLib to czarLib  3) Rename startQserv to startCzar 
2,Fix race condition when creating db (and elsewhere?) in client/qserv_admin_impl.py,"This came up in the review of CSS, see DM-225:  QservAdminImpl: so, if there's a problem while inserting the various keys for a database, and another exception during cleanup, you can end up with a partially constructed database. How do you plan on handling this? This is perhaps another argument for trying to batch metadata more - if it's all in a single key, this worry goes away (at least in this particular case). The createDb implementations seem racy. Consider what happens if 2 admins try to create the same database D. Suppose admin A does the _dbExists(D) check first; it returns false. Then admin B comes along, and does the same check, which returns false again. Let's further suppose that admin B successfully issues all the key creates for D without being interrupted by A. Back to A: when A tries to create all the keys required for D, an exception will be raised immediately since D exists. At this point A will dutifully try to clean up (to avoid leaving around a partially constructed D), deleting D as created by B. End result: D was successfully created, no explicit drop was issued, but D does not exist. Database drops can also race with table creates: zookeeper doesn't have a native recursive delete. In other words, it is perfectly possible for an admin A to issue a database drop while someone else is issuing a table create. The recursive delete might descend into the table ""directory"" while the create is populating it with child znodes. The recursive delete will get an incomplete list of children, remove them, and finally fail to remove the table directory because the table create has since added more children, and it is illegal to remove a non-empty directory. At this stage, the drop will report success even though it has failed (that's the recursive delete behavior that kazoo gives you), and the table create will report success. But the database will still exist, and table keys may be partially present in zookeeper."
1,qserv_admin needs to deal with uncommon names/characters,"qserv/admin/bin/qserv_admin.py needs to deal with strange names (e.g. with embedded spaces or semi-colons), or at minimum, catch and forbid them."
0,Rework exceptions in qserv client,"There is a bunch of (I think) unnecessary translation from KvException to QservAdmException. Can't you just handle printing KvException in CommandParser.receiveCommands(), and get rid of the CSSERR error code? (This is in /admin/bin/qserv-admin.py)  (this came up in DM-225)"
1,rethink configuration for client,"_fetchOptionsFromConfigFile in client/qserv_admin.py:: If you are going to use the ConfigParser library, please use SafeConfigParser (or RawConfigParser if you don't need string interpolation). But anyway, I'm not sure the INI file format is the right one to use here, as it forces the use of sections in parameter files, which doesn't make much sense (I notice your code just throws away the section names when reading parameter files). I found myself wishing that the admin client just had syntax for the various options, rather than relying on more or less undocumented parameter files.  (this came up in DM-225)"
0,Remove old partitioner/ loader and duplicator,"Once Fabrice has migrated the integrated tests towards using the new partitioner and duplicator, we should delete the old partitioner/duplicator (in {{client/examples}})."
1,Confusing error message (non-existing column referenced),"A query that references non existing column for non-partitioned table results in a confusing message: ""read failed for chunk(s): 1234567890"".  To repeat, run something like {code} SELECT whatever FROM <existingTable>; {code}  Similar error occurs when we try to reference non-existing table, try something like:  {code} SELECT sce.filterName  FROM StrangeTable AS s,       Science_Ccd_Exposure AS sce  WHERE  (s.scienceCcdExposureId = sce.scienceCcdExposureId); {code} "
0,gracefully handle misconfigured scons,"When I try to run scons using the latest master (89aaa6), it fails with  {code} scons: Reading SConscript files ... AttributeError: 'NoneType' object has no attribute 'rfind':   File ""/home/becla/cssProto/qserv_css6/SConstruct"", line 17:     state.init(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 161:     _initEnvironment(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 128:     _initVariables(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 89:     (PathVariable('XROOTD_DIR', 'xrootd install dir', _findPrefix(""XROOTD"", ""xrootd""), PathVariable.PathIsDir)),   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 53:     (binpath, binname) = os.path.split(binFullPath)   File ""/usr/lib/python2.7/posixpath.py"", line 83:     i = p.rfind('/') + 1 {code}  The scripts should check that requires variables are not set, and print appropriate error (and ideally, suggest how to fix it) "
0,make Image construction robust against integer overflow,I just fixed a bug on the HSC side (DM-523) in which integer overflow in the multiplication of width and height in image construction caused problems.  We should backport this fix to LSST.
1,Table column names in new parser,"Running tests (qserv-testdata.sh) on pre-loaded data I have observed that many test fail for the only reason that the column names in the dumped query results are different between mysql and qserv. Here is an example of query reqult returned from mysql: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +------------+-------+--------+------+ | filterName | field | camcol | run  | +------------+-------+--------+------+ | g          |   670 |      2 | 7202 | +------------+-------+--------+------+ {code} and this is the same query processed by qserv: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +----------+----------+----------+----------+ | QS1_PASS | QS2_PASS | QS3_PASS | QS4_PASS | +----------+----------+----------+----------+ | g        |      670 |        2 |     7202 | +----------+----------+----------+----------+ {code}  We discussed this already with Daniel yesterday and at qserv meeting today, here I just want to collect what we know so far so that we can return to this again later.   As Daniel explained to me this is the result of the new parser assigning aliases to the columns which do not define aliases for themselves. This helps with tracking query proceeding through the processing pipeline. Daniel's observation is that different database engines may assign different names to result columns (or some may not even assign any names), there is no standard in that respect so there is no point in trying to follow what one particular implementation does. Additionally there are issues with conflicting column names and names which are complex expressions.  Difference in column names breaks our tests which dump complete results including table header. The tests could be fixed easily, we could just ignore table headers when dumping the data. More interesting issue is that there may be use cases for better compatibility between mysql and qserv including result column naming. In particular standard Python mysql interface allows one to use column names to retrieve values from queiry result. If qserv assigns arbitrary aliases to the columns it may confuse this kind of clients.  This issue depends very much on what kind of API qserv is going to provide to clients. If mysql (wire-level) protocol is going to be the main API (which would allow all kinds of mysql clients to talk to qserv directly) then we should probably think more about compatibility with mysql. OTOH if we decide to provide our own API then this may not be an issue at all (but we still need to fix current test setup which is based on mysql).  We probably should discuss API question at our dev meeting."
1,Qserv returns error table instead of error code,"Running the tests on pre-loaded data I noticed that for some queries qserv returns result which does not look like it is related in any way to the query - result column names are different from the columns in the query (different in a different way from DM-530). Here is an example of query and result produced: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM   Science_Ccd_Exposure AS sce WHERE  sce.filterName like '%'    AND sce.field = 535    AND sce.camcol like '%'    AND sce.run = 94; +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ | chunkId | code | message                                                                                                                                                                       | timeStamp   | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ |      -1 |    0 | NULL                                                                                                                                                                          | 1.39779e+09 | |      -1 |  100 | Dispatch Query.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1200 | Query Added: url=xroot://qsmaster@127.0.0.1:1094//q/LSST/1234567890, savePath=/dev/shm/qserv-salnikov-80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/7_1234567890_0 | 1.39779e+09 | |   32767 | 1300 | Query Written.                                                                                                                                                                | 1.39779e+09 | |   32767 | 1400 | Results Read.                                                                                                                                                                 | 1.39779e+09 | |   32767 | 1500 | Results Merged.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1600 | Query Resources Erased.                                                                                                                                                       | 1.39779e+09 | |   32767 | 2000 | Query Finalized.                                                                                                                                                              | 1.39779e+09 | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ {code}  Daniel and Bill explained to me what is happening there - when query results in an error instead of returning mysql error condition (which is an error code plus some text) proxy produces a diagnostic result which is a table (above) containing some info which could be useful to diagnose the issue.   This feature looks potentially useful but it may also be confusing for clients like me who are not aware of this feature. For regular mysql client it may be actually harder to intercept errors because one would need to analyze returned table to understand that error condition happened. It may also be ambiguous in a sense that the legitimate query could produce result with the same column names.  Like in DM-530 this issue is tied to a question what kind of API we want to provide to qserv clients. If mysql wire-level protocol is going to be our main API then we should probably try to be more mysql-compatible. OTOH if we are going to hide everything behind our own API then we may have more freedom in re-defining what kind of result error condition produces."
1,transfer multiband processing changes from HSC,"We have a new multi-band processing scheme for coadds on the HSC side, encompassing changes in afw, meas_deblender, and pipe_tasks.  These should be transferred to the LSST side, with RFCs for any backwards incompatible changes.  Changes to the Footprint classes may only be temporary, as we plan to refactor those classes soon anyway, but they're still worth doing now to support the higher-level changes."
2,Move HSC issues to hsc-jira.astro.princeton.edu,NULL
2,"ensure pipe_tasks, obs*, and other packages are compatible with meas_base","The switch to meas_base will involve changing the names of most measurement algorithms, as we're using a new naming convention that provides more traceability.  This will break downstream code that:  - uses field names instead of slots to access measurements  - reads or modifies the list of configured-to-run algorithms.  Whenever possible, we should fix the former by converting them to use slots, as this will automatically provide backwards compatibility.  I'm not yet sure how to handle the latter; the easiest solution would be to give up on full backwards-compatibility with meas_algorithms, but we might be able to find some way to make the old names aliases to the new ones during the deprecation period. "
1,scons rebuilds targets without changes,"I'm seeing something strange when I run scons from current master - running 'scons install' after 'scons build' re-compiles several C++ files even though nothing has changed between these two runs: {code:bash} $ scons build scons: Reading SConscript files ... ... scons: Building targets ... scons: `build' is up to date. scons: done building targets.  $ scons install scons: Reading SConscript files ... ... scons: Building targets ... swig -o build/czar/masterLib_wrap.cc -Ibuild -I/usr/include/python2.6 -python -c++ -Iinclude build/czar/masterLib.i g++ -o build/czar/masterLib_wrap.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/czar/masterLib_wrap.cc g++ -o build/control/AsyncQueryManager.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/AsyncQueryManager.cc g++ -o build/control/dispatcher.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/dispatcher.cc g++ -o build/control/thread.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/thread.cc g++ -o build/merger/TableMerger.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/merger/TableMerger.cc scons: `install' is up to date. scons: done building targets. {code}  This is kind of unexpected, or at least I can't understand now why it happens. Trying to run with --debug=explain shows that some dependencies have disappeared and in some dependencies order is different. No clue yet what that means and how it could happen. Need to study our scons scripts to understand what is going on."
2,rearchitect Qserv to fix dependencies between modules in qserv/core,"I looked at includes in ""core/modules"", here is a summary.  * "":"" indicates a dependency * global, log, util, wbase, wlog and xrootd are low level, it is an easy, I didn't bother showing them below  * ""!!!"" indicates circular dependency  * lower-level modules first  {core} css:      <nothing> global:   <nothing> mysql:    <nothing> obsolete: <nothing> proto:    <nothing> sql:      mysql wconfig:  sql, mysql wpublish: sql, wconfig  !!! wsched <--> wcontrol !!! wcontrol <--> wdb wsched:   proto, wcontrol wcontrol: mysql, sql, wsched, wdb, wcontrol, obsolete, proto wdb:      sql, mysql, proto, wconfig, wcontrol  xrdfs:    wpublish, wdb, wconfig, sql, obsolete, wcontrol merger:   sql, xrdfs xrdoss:   obsolete, wpublish, xrdfs  !!! control <--> qdisp !!! qana <--> qproc !!! qana <--> query !!! parser --> query -->qana --> parser !!! query --> qana --> qproc --> query control:  css, merger, obsolete, qdisp, qproc, query qdisp:    control qana:     css, parser, qproc, query qproc:    css, merger, parser, proto, qana, qdisp, query query:    css, qana, qdisp parser:   query {core} "
2,Base DMCS and Replicator Interaction for Simulator - v1,"Create the Base DMCS and Replicator processes that demonstrate  Send startIntegration Send startReadout  Assume:    Archiver and Alert Production Cluster already enabled, replicators already registered in fully-operational pool.  Demonstrate:  Sending startIntegration event to the Base DMCS. Replicator jobs submission Replicator jobs subscribe to startReadout Sending the startReadout event to the replicator jobs. Replicator jobs receive startReadout Replicator jobs pull data "
0,"clean up include <> --> """" for third party includes","According to our coding standard 4.15: https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Files#a4-15.OnlysystemincludefilepathsSHALLbedelimitedwith  we should be using """" for boost, but in quite a few places we do not:  {code} grep 'include <boost' */* |wc      146     314    7916 {code} "
0,cleanup includes - add module name,"Change places like {code}#include ""cssException.h""{code} to {code}#include ""css/cssException.h""{code}   "
1,Add distributor to simulator - v2,Add distributor to the simulator  Demonstrate:  Replicator node associated with to distributor node Replicator jobs send job info to distributor node Replicator jobs copy data to distributor node.
1,Add Archive DMCS to simulator - v3,"Add Archive DMCS  Demonstrate:  Receiving information from duplicator (visit id, exposure sequence number, raft id, and network address)."
1,Fix Doxygen Doc for new meas_base classes,The final stage of moving the old algorithms to meas_base will be to generate the doxygen documentation and be sure that it is useful.  We will do this after the  cleanup of the old meas_algorithms code.
3,Investigate Approaches to Dcr,"This story captures the work of investigating the implications of the different options to compensate for Dcr: ignore objects with extreme colors, compensate for Dcr in measurement, and compensate per-image at the per-object/pixel level.  We anticipate the latter option will be required going forward, but we need to do the background work to justify this.  We need to understand exactly which classes will need to make use of Dcr information, and what the limitations are of operating at the per-pixel level (e.g. blends)."
3,Validation of Dcr Approach at the Pixel Level,"Before we start the process of extending the Dcr functionality into the stack (e.g. Psf, Wcs, etc), we need to validate that the implementation works at the pixel level.  In this task we will work with simulated images to debug and optimize the implementation, to the degree possible."
0,Cleanup Source.h.m4,This include file was damaged somewhat by the addition of slot routines to work with the flattened field definitions.  It would be nice to put the Measurement abstraction back in place -- or get rid of it.  We need to decide whether the old slot and compound key mechanisms from SourceTable version 0 are going to be continued for doing this.   
0,The way the observation date is translated by obs_cfht breaks the defect registry.,The fields of the observation date and time as stored in the CFHT-LS headers are not padded with zeros.  This makes the sqlite DATETIME constructor return a NULL string when the observation time is < 10hrs.  The fix is to force the fields to be padded when the metadata from the input files are ingested.
2,Get one HTCondor ClassAds scenario running,"We have a worker script that takes three arguments: a CCD number, a cache limit, and a length of time.  The script starts out looking at a slot-specific filesystem for a calibration file /local/slot#/calib/CCD#.  If that file doesn't exist, it checks to see if there are more than the cache limit of files in /local/slot#/calib.  If there are, it removes one at random (later we could try other algorithms).  It then immediately transfers /gpfs/calib/CCD# (maybe a 1 GB file) to /local/slot#/calib/CCD#.  The script then waits for the given length of time and exits.  We make sure we can measure how many file copies from /gpfs are performed and when so that we can determine peak demand rates and average rates.  Meanwhile, the HTCondor hook checks for the same /local/slot#/calib/CCD# files and sets the ClassAds based on which CCD#s are present.  Scenario 1) 4 slots, 4 CCDs, cache limit 1, jobs take 1 min, total of 5 jobs per CCD, 1 job per CCD submitted every 2 min (1234 pause 1234 pause 1234 pause 1234 pause 1234)"
0,load non-LSST FITS tables as version 1,"In DM-384 and DM-242, we disabled the periods-to-underscores translations for new tables (""version 1"") but left it in place for old tables (""version 0"").  In addition, when reading a table without a version number, we assumed it was version 0, to maintain backwards compatibility.  I think we should modify this slightly: we should assume a table without a version number is version 0 if and only if it also has the AFW_TYPE key.  Otherwise we should assume it is version 1.  This will allow us to load externally-produced tables without turning any underscores they contain into periods, while still maintaining backwards compatibility with older tables written by afw."
0,Update all DM Software Copyright and License Agreement notices to reflect AURA/LSST,"The lsstcorp.org/LegalNotices/{LsstLicenseStatement.txt  LsstSourceCopyrightNotice.txt} need to be updated to reference AURA/LSST. The referenced list of LSST partner institutions needs to be either resurrected or the reference deleted.  The git repository for devenv/templates needs the Copyright templates to be  updated.  LsstLicenseStatement.txt needs to be updated to include recent additions of 3rd party tools' Licenses (~10 tools)  to the DM stack  and all the QSERV 3rd party tools' Licenses (~25 tools).  The Copyright banner in all software needs to be updated to reflect the new reality of AURA/LSST in place of LSST Corporation.  Files with no Copyright banner, need to add it.  Update may occur 'the next time' the code file is updated. THis needs to be broadcast to the developers once the Copyright templates and the website versions are updated."
1,running multiple Qserv installations on the same machine,"It would be very useful to be able to run multiple installations of Qserv on the same machine (say, 2 developers playing with Qserv on the same machine). I guess we are almost there, we just need to know how to configure all ports so that we are not colliding. Can you test it, tweak whatever is necessary and document what is involved in changing defaults to a unique set of port numbers? "
1,Setup multi-node Qserv ,"We are currently focusing on single-node Qserv. It'd be nice to try setting up multi-node Qserv (say 4 workers and a czar on lsst-dbdev*), and improve installation scripts to simplify the process."
0,Fix automated tests after css migration,"After yesterday's merge of DM-58 into master automated tests do not work any more. The part which is broken now is loading of metadata into qserv. We need to replace old script which created metadata with something different that creates metadata using new CSS.   The code which loads metadata in tests is in QservDataLoader class, createQmsDatabase() method (in tests/python/lsst/qserv/test/ directory)."
0,reorganize client module,"move everything in the client package (qserv_admin*, associated tests and examples) to admin/  move css/bin/watcher.py to admin/  "
2,Investigate off-the-shelf data distribution tools,"Research available off the shelf tools, try to find one that would best fit our needs. Produce a document (trac page)."
1,fix 12 issues in testCppParser (related to switching to CSS),NULL
2,Add support for installing qserv on machines without internet,It is common we install qserv on machines that are on internal network without external network connectivity. How do we do that?
1,Look into git-fat,"Look into the use of git-fat with the LSST DM workflow.  Specifically, how does this work with anonymous access."
1,Update parse/analysis tests to detect missing css-kvmap early,"Due to the CSS code merge, the testCppParser test depends on an external kvmap file. If this file doesn't exist, nearly every test will fail. There isn't a way to check whether the cssFacade (constructed from kvmap file) is valid.  This ticket includes, at minimum: * changes to css/ and qproc/ to make the unit tests fail early if the facade could not be constructed.  Optionally, this ticket could include: * renaming testCppParser to testQueryAnalyzer * compile-time linking the kvmap into testCppParser to eliminate the need for an external kvmap file. "
1,CSS throws exception if tableIsSubChunked is called for non partitioned table,NULL
1,referring to table without database context crashes czar,"Running a query like ""select * from Object"" if we do not have database context results in  {code} terminate called after throwing an instance of 'lsst::qserv::css::CssException_InternalRunTimeError'   what():  Internal run-time error. (*** css::KvInterfaceImplZoo::exists(). Zookeeper error #-8. (/DATABASES/)) {code}  Need to gracefully catch the zookeeper -8. Need to detect that empty database name is passed in Facade. Need to avoid it higher up. "
0,afw unit tests not built unless afwdata available,"If afw_data is not available then the afw unit tests are not built. I think it would make more sense to build all of them and run those we can (not all depend on afw_data). One advantage is that a version of afw installed using ""eups distrib install"" would include built tests, so the tests could be run. This is presently not practical because the SConstruct file is not installed (I intend to open a ticket about that, as well)."
0,Switch kazoo version to 2.0b1 or later,"While we aren't using any of the new features of Kazoo's 2.x series, the removal of zope.interface as a dependency is a worthwhile feature.  The 2.0b1 release seems at least as stable as our own code, so I don't think we'll see any negative effects.  This ticket covers: - Upgrade of the packaged kazoo from 1.3.1 to 2.0b1 (or later). - (optionally) patches for kazoo's setup.py so that it doesn't search for and try to download any dependencies. This can be done in a later ticket, though.  I note that kazoo can be run without installation: you can untar it, cd into the directory, and if you run python from there, you can immediately ""import kazoo"" and use it. Hence we could avoid setup.py completely and just copy the ""kazoo"" subdirectory into some directory in the PYTHONPATH."
0,remove obsolete QMS-related code,try running  {code} find core css admin client tests site_scons | xargs grep -i qms {code}  There is a lot of old unused qms related code.
1,Automated test differences after CSS migration,I'm running automated tests with recent master after merging DM-605 and observe some differences between mysql and qserv. Here I'm going to document all differences (and everything related).
0,rename qserv_admin.py to qserv-admin.py,"We have 6 scripts in qserv/admin/bin that start with ""qserv-"", and one that starts with ""qserv_"", we should rename qserv_admin to qserv-admin."
0,commons.config should always be managed as a global variable,"Qserv configuration object (i.e. commons.config) used in admin and tests should be managed the same way a logger object is (cf. http://hg.python.org/cpython/file/2.7/Lib/logging/__init__.py).  I.e. a function called config.getConfig(""config-type"") (returning an config occurence of a global dictionnary), should be used in order to ease retrieval of all configuration objects related to client, data, (and server?) anywhere in the admin/test python code."
1,User friendly single node loading script,"This entails the creation of an end-to-end loading script that, given database and table param files, a SQL table schema and one or more CSV data files, places appropriate metadata into zookeeper, runs the partitioner, and finally creates and loads chunk tables."
2,Qserv configuration tool refactoring,"* Instance configuration should not be in the same directory where qserv software is installed Qserv configuration, including things like mysql or zookeeper data directory should be separate from where Qserv software is installed. At the moment, when I want to work with branch ""a"" and ""b"", in order to switch from one to the other I have to run ""cd $QSERV_DIR/admin; scons"". This is ok the very first time after I created a branch, but it is way too heavy afterwards, because it wipes out all data from zookeeper and mysql."
1,Package antlr 2.7 in eups,NULL
1,Make sure we have queryId in all places where needed in logging,NULL
0,Too many connections from czar to zookeeper,"I have just managed to crash qserv czar by running repeated queries against it. What I see in the logs:  qserv-czar.log: {code} 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response terminate called after throwing an instance of 'lsst::qserv::css::CssException_ConnFailure'   what():  Failed to connect to persistent store. {code}  and zookeeper.log: {code} 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440015, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,585 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49913 which had 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440016, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,586 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49939 which had {code}"
0,ORDER BY and DISTINCT do not work reliably in qserv,"Queries with ORDER BY and DISTINCT are buggy. For example, results do not always come ordered and order changes from one run to another: {code} mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | +-----------------+ 9 rows in set (1.27 sec)  mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | +-----------------+ 9 rows in set (1.24 sec) {code}  This was done with testdata/case01 data, let me know if you need to load that data.  Also, using case03 data, e.g. {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} returns 6 rows in qserv (vs 1 in mysql).  The full list of DISTINCT failures is (all with testdata/case03): - 0002_fetchRunAndFieldById.txt - 0021_selectScienceCCDExposure.txt - 0030_selectScienceCCDExposureByRunField.txt "
1,"Switch to using new partitioner, loader",Integrated tests procedure has to rely on new loader
1,Non-partitioned table query returns duplicated rows,"Running automated test I noticed that a query on non-partitioned table returns multiple copies of the same row, one copy per chunk. Here is example: {code} mysql> SELECT offset, mjdRef, drift FROM LeapSeconds where offset = 10; +--------+--------+-------+ | offset | mjdRef | drift | +--------+--------+-------+ |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | +--------+--------+-------+ 13 rows in set (5.62 sec) {code}  Czar log file shows that it correctly finds that table is non-chunked but sends query to each chunk anyway: {code} 20140502 16:22:08.745081 0x3172430 INF *** KvInterfaceImplZoo::exist(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning 20140502 16:22:08.745735 0x3172430 INF *** LSST.LeapSeconds is NOT chunked. 20140502 16:22:08.745762 0x3172430 INF *** KvInterfaceImplZoo::get2(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning/subChunks 20140502 16:22:08.746393 0x3172430 INF *** LSST.LeapSeconds is NOT subchunked. 20140502 16:22:08.746409 0x3172430 INF getChunkLevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 INF <py> Using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 INF <py> Using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 INF <py> empty_LSST.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 ERR <py> Couldn't find empty_LSST.txt, using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891498 0x7fbb28003660 INF Msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 INF Added query id=6630 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 20140502 16:22:08.891705 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891882 0x7fbb28003660 INF Msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 INF Added query id=6631 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 20140502 16:22:08.892097 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.892275 0x7fbb28003660 INF Msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 INF Added query id=6800 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6800 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 ... {code}  Looking at the code together with Daniel we found that at the Python level (czar/app.py) the code that dispatches query does not check for chunkLevel, this is likely why this happens. The code to look at is in {{InbandQueryAction._applyConstraints()}} method."
2,Define C++ API for C++ Geometry,NULL
1,Query sessions are never destroyed,"Please see DM-625, when I run say 10 ""select count(*) from LSST.Object"" queries, for each query a new AsyncQueryManager is created in dispatcher, but the sessions are never destroyed."
0,admin/tests/test_qservAdminImpl.py has hardcoded connection info,NULL
0,complexity of eups dependencies relationships  for db package,"Hello,  I'm currently trying to use the very last version of db package (the one which relies on sconsUtils), but, in order to make it works with Qserv, I had to introduce next update : {code:bash} fjammes@clrlsstwn02-vm:~/src/qserv-packager/dist/dependencies/db (master) $ git diff HEAD~1 diff --git a/ups/db.cfg b/ups/db.cfg index e1ae31b..a469061 100644 --- a/ups/db.cfg +++ b/ups/db.cfg @@ -3,7 +3,7 @@  import lsst.sconsUtils    dependencies = { -    ""required"": [""mysqlclient"", ], +    ""required"": [""mysql"", ],  }    config = lsst.sconsUtils.Configuration( diff --git a/ups/db.table b/ups/db.table index 8c8d831..9e770a3 100644 --- a/ups/db.table +++ b/ups/db.table @@ -1,5 +1,5 @@  setupRequired(python) -setupRequired(mysqlclient) +setupRequired(mysql)  setupRequired(mysqlpython)  setupRequired(sconsUtils) {code}  Is there a solution to describe  in eups that mysqlclient is included in mysql ?  Thanks,  Fabrice"
0,update overview docs to clarify roles of meas_multifit and shapelet packages,"From the review of DM-17: {quote} It was not obvious how responsibility is split between {{meas_extensions_multishapelet}}, {{shapelet}}, and {{meas_multifit}}.  Shapelet could use an {{overview.dox}} file. {quote}  {{meas_extensions_multiShapelet}} is on its way out, so we'll wait until that's done and then document the relationship between meas_multifit and shapelet."
1,meas_base plugin for sampling-based galaxy fitter,"In addition to a CModel plugin for galaxy photometry (DM-240), we should create a plugin to do sampling-based galaxy fitting, using the optimizer as a starting point and the existing AdaptiveImportanceSampler class to do most of the work.  A major blocker for this is the fact that the {{SourceTable/SourceRecord}} don't currently provide any way to save multiple samples per object.  This issue may get worked on before it falls into an official sprint, as it's something I want to on my 20% time."
0,Implement DISTINCT aggregate in qserv,It looks like DISTINCT aggregate is not supported yet in qserv. Daniel told me that this should be relatively straightforward to add. Adding this ticket so that we do not forget it.
3,support samples and other many-to-one outputs in SourceRecord,"For DM-645, I need to be able to save multiple samples (each a parameter vector and possibly a weight) with each {{SourceRecord}}, with the number of samples possibly differing from {{SourceRecord}} to {{SourceRecord}}.  I'll also want to save Mixture distributions, which can also be easily represented as a sequence-of-tuples of values to be associated with a record.  Since we don't want to hard-code the outputs of a particular plugin into {{SourceRecord}} (which is what I've done with the {{ModelFitRecord}} class in meas_multifit, which I'd like to remove), I think this means we want a more general way of allowing algorithms to define many-to-one tables to be associated with each source.  While few algorithms will likely want to save samples, this plugin may not be the only one, and it's likely a general many-to-one feature could be used by other algorithms to save diagnostics when configured to do so.  I'm worried this is another case where afw::table may be intruding on a feature best left to a true RDBMS, but I don't like the idea of tying an RDBMS directly to the source measurement framework either.  I'm open to other ideas, if anyone has one.  It's also worth pointing out that because this is blocking DM-645, which is important for my 20%-time science work, I'd like to get moving on this even before it becomes a real priority for LSST.  But I'd still like to get some design feedback on this."
1,Add support for running unit tests in scons,Add code in scons that runs unit tests for Qserv.
1,"framework for documenting ""how to run qserv""","We need to have permanent location for how-to-run-qserv, currently we keep it in https://dev.lsstcorp.org/trac/wiki/db/Qserv/RedesignFY2014/Hackathon2/howToRunQserv. It should be in the code repo. Need to decide on how we format it, and need to expose it on our Qserv trac/confluence pages "
2,Migrate database trac pages to Confluence,Need to migrate Qserv trac pages to Confluence.
1,Run baseline HTCondor ClassAds Scenarios,"Run initial HTCondor ClassAds Scenarios to verify that the implementation for utilizing Rank to place Jobs near data is operating as anticipated.  The main test of the baseline scenarios is to verify that, e.g., a job for a CCD that has calibrations advertised for a particular slot/node  will consistency be executed within that location for appropriate Rank expression. This is to occur even when other open slots are always available (e.g., 4 CCDs, 5 slots). "
1,"Run ""single slow worker"" HTCondor ClassAds Scenario","We run and study a ""single slow worker"" HTCondor ClassAds Scenario. The scenario is a perturbation of the baseline HTCondor ClassAds Scenario. In the baseline, Jobs for a given CCD  are consistently pinned to a slot/node that advertises the presence of associated data files/calibration files for that CCD.  A baseline run may proceed, for example,  with jobs for 4 CCDs repeating executing within same HTCondor slot on a node (even when spare processing slots are readily available.). The baseline is observed to be quite stable, as the pool is empty each time a wave of jobs is submitted. In the ""single slow worker"" scenario, we cause one of the jobs for a chosen CCD to stall (mocking up a slow file transfer, lengthy computation in an algorithm, etc), such that the pool is not empty at the the submission time for a wave of jobs.  We seek to observe how Rank places jobs in this scenario, and work to assign Rank (especially for spare slots) in an optimal way so as to minimize file transfers. "
1,unknown column derails Qserv,"Running a query that references invalid column hangs Qserv, it looks like the query is never squashed.  For example, I run a query: {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} on the pt1.1 data set  Corresponding log from xrootd:  {code} Foreman:>>Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure \ AS QST_1_ WHERE run=94 AND field=535; <<---Error with piece 0 complete (size=1). Foreman:TIMING,q_f99cQueryExecFinish,1399667470 Foreman:Broken! ,q_f99cQueryExec---Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM L\ SST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  Foreman:Fail QueryExec phase for q_f99c: Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field \ FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  Foreman:(FinishFail:0x7f26700055c0) Db = q_f99cc9cd5519d465e673119a84b5570a, dump = /usr/local/home/becla/qserv/1/qserv/build/dist/xrootd-run/result/f99cc9cd5519d465e673119a84b\ 5570a hash=f99cc9cd5519d465e673119a84b5570a Foreman:Finished task Task: msg: session=7 chunk=3598 db=LSST entry time=Fri May  9 15:31:10 2014  frag: q=SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535, sc= rt=r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 ScanSched:ChunkDisk remove for 3598 : SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 BlendSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:_getNextTasks(32)>->-> ScanSched:ChunkDisk busyness: no ScanSched:_getNextTasks <<<<< BlendSched:Blend trying other sched. GroupSched:_getNextTasks(4)>->-> GroupSched:_getNextTasks <<<<< 140509 15:31:27 13960 cms_Finder: Waiting for cms path /usr/local/home/becla/qserv/1/qserv/build/dist/tmp/worker/.olb/olbd.admin {code}  and the log in czar has {code} 20140509 15:35:40.070654 0x7f4694003660 INF Still 1 in flight. {code}   "
2,"Database name used by  integration tests should use their own dedicated database, not ""LSST""","Currently automated tests use the database ""LSST"", and they will go ahead and delete that database without any warning. This is far from ideal, we should have a more unique database name. How about something like qservAutoTest_<uniqueId>."
0,"Parser has inverted order for ""limit"" and ""order by""","{code} SELECT run FROM LSST.Science_Ccd_Exposure order by field limit 2 {code}  Works in MySQL, fails in Qserv (ERROR 4120 (Proxy): Error executing query using qserv.)  {code} SELECT run FROM LSST.Science_Ccd_Exposure limit 2 order by field {code}  Works in Qserv, fails in MySQL (limit should be after order by) "
0,"""out of range value"" message when running qserv-testdata (loader.py)","Fabrice  I am getting ""out of range value"" when I run the qserv-testdata:  Are you seeing that too?   2014-05-09 18:11:55,975 {/usr/local/home/becla/qserv/1/qserv/build/dist/lib/python/lsst/qserv/admin/commons.py:134} INFO     stderr : /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_centroid_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweightedbad' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweighted' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_shift' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_sinc_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 2   self.cursor.execute(stmt) "
1,partition package has to detect eups-related boost,"partition package doesn't detect eups-related boost. This has to be fixed by using sconsUtils, or hand-made procedure.  {code:bash} [fjammes@lsst-dev lsstsw]$ export LD_LIBRARY_PATH=""$LSSTSW/anaconda/lib:$LD_LIBRARY_PATH"" [fjammes@lsst-dev lsstsw]$ setup boost 1.55.0.1+1 [fjammes@lsst-dev lsstsw]$ rebuild partition            partition:  ok (0.5 sec).                boost:  ok (0.3 sec).               python:  ok (0.3 sec).                scons:  ok (0.4 sec). # BUILD ID: b49               python: master-gcbf93ab65b (already installed).                scons: 2.1.0+8 (already installed).                boost: 1.55.0.1+1 (already installed).            partition: master-gf2ef2cf2dc ERROR (1 sec). *** error building product partition. *** exit code = 1 *** log is in /lsst/home/fjammes/src/lsstsw/build/partition/_build.log *** last few lines: :::::  scons: Reading SConscript files ... :::::  Checking for C++ library boost_system-mt... no :::::  Checking for C++ library boost_system... no :::::  Checking for C++ library boost_thread-mt... no :::::  Checking for C++ library boost_thread... no :::::  Checking for C++ library boost_filesystem-mt... no :::::  Checking for C++ library boost_filesystem... no :::::  Checking for C++ library boost_program_options-mt... no :::::  Checking for C++ library boost_program_options... no :::::  Missing required boost library! # BUILD b49 completed. {code}"
0,fix handling of nested control objects,"Work on the HSC side has revealed some problems with nested control objects being wrapped into config objects.  This is a pull request for those changes (along with writing a unit test for some of them).  Some (but not all of these changes) are part of Trac ticket #3163 (https://dev.lsstcorp.org/trac/ticket/3163), which I'll now close as a duplicate."
1,Citizen methods should be private and accessible only through a friend interface,"The Citizen interface is useful, but it pollutes its derived classes with methods and attributes that can cause confusion later on (I've got a concrete example of that confusion that Perry and I just spent a few days tracking down - Citizen's {{getId()}} was being mistaken for {{SourceRecord.getId()}}).  I think everything Citizen provides should be hidden and only accessible through a friend interface, e.g.: {code} afw::image::Image<float> image(4, 5); daf::base::CitizenAccess::getId(image); {code}  We should also make an effort to ensure that other aspects of Citizen's design don't affect derived classes, perhaps by prefixing an name that could be seen by derived classes with a ""Citizen"" prefix; see https://dev.lsstcorp.org/trac/ticket/2461."
0,Implement HTCondor dynamic classad solution for Slot based values,"The HTCondor team will be updating their HOWTO for managing Slot based classads/dynamic classads set by a cron startd process.  We currently have a technique for  dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the HTCondor team plans to provide."
1,Develop monitoring for identifying Data processed on a Node/in a Slot,"To understand the effectiveness with which we are mapping Jobs to Data, it is vital to monitor/record what data has been processed on a given Node, or within a given Slot on a Node.   Under this issue we examine HTCondor monitoring standards like STARTD_HISTORY,  as well as more custom implementation of blackboard type records via  Job Update hooks to be  executed on the execute node (along the lines of OWL.) "
1,Run HTCondor ClassAds Scenarios with heterogeneous data cache,"The initial series of tests with HTCondor ClassAds work with jobs for individual ccds with a single file representing the data dependency  for the job. In this issue we consider the management of multiple types of data dependencies that may have to be cached for jobs (calibrations, templates, catalogs of sources/objects, etc). "
1,Study ORDER BY support,"We don't have a proper implementation of ORDER BY. Actually, to support ORDER BY properly, we really have to manage all the column names, so we would need to have an in-memory list of columns for the table in question. This is because the general case requires us to ORDER BY a column that may not exist in the select list. In this case, we must add it to the select list if it is not there (or apply *). The easiest solution is to only allow ORDER BY if the sort key column exists explicitly in the select list."
1,Parser ignores syntax after LIMIT,"Parser stops after the LIMIT condition, believing that it has a complete select statement. It ignores whatever is afterwards.To fix this, we would need to alter the parser to make sure that there isn't garbage afterwards. So we have to make sure that the only thing acceptable afterwards is a semicolon or a comment. The right thing to do is probably to add a grammar rule for this. The easiest thing is to check to see if the entire string was consumed (give-or-take a semicolon), but this would disallow comments."
0,Estimate expected counts of unassociated sources ,"We need to have an idea how many sources/forcedSource/diaSources that are not associated with any object we will have to deal with in the database. Can we have a rough estimate? (e.g., per DR)."
1,Fine-tune logging messages,"Fine-tune log messages in Qserv (what messages are printed, what is the error level, etc)"
0,During scons configure : check if mysql isn't runing,Mysqld can't be configured is its running before configuration step.
1,Minor possible enhancements in install procedure,"Usefull enhancements :  - add swigged target to ""build"" alias (run scons install to see that swigged target are re-builded at install time)  Other possibles enhancements : - manage default (i.e. const.py) for server configuration file ? - state.py : where to save state ? print it to sdtout ? "
2,Create tutorial on the use of eups,"Create a beginner's guide to the use of EUPS for the DM Developer Guide. Describe the capabilities and basic usage for ""everyday"" user and developer activities. Provide pointers to the EUPS reference manual for advanced usage. "
0,rename git repository qservdata to qserv_testdata,"eups package have the same name as their related git repos. Renaming git repos would lead to a more understandable name.  Please note that the qserv-testdata may also be cloned from qservdata, and and qservdata be removed.  New repos will also have to be distributed with lsst-sw tool."
1,Buildbot CI needs to save manifest file of failed build for later user debug,"The manifest file created during a build instance is transient and removed as soon as the next build commences.  Due to that volatility, it's important to save the manifest to some well-known location so that the developer responsible for debug and repair can easily setup the failing environment. The location of the manifest file will be provided to the developer(s) in the failure notification."
0,Use of HipChat for Buildbot CI failure notifications should be explored,K-T recommended the use of HipChat rather than email when notifying users of a buildbot build failure.  The purpose was twofold: get immediate attention from the developers and help change the culture towards using HipChat more.  This Issue is to explore the feasibility of using HipChat for the notifications.
0,Better review notification e-mails,"Russell writes:  {quote} I think our system for getting code reviewed using JIRA needs some improvements. It seems that people don't always know that they have been assigned to review a ticket. Also, even if I know I have been assigned to review a ticket, I find it hard to find on JIRA.  More concretely, I would like to see these improvements: - Much clearer notification that one has been assigned as a reviewer. Presently the email is quite generic and easy to miss. In fact I find that most JIRA notifications are rather hard to read -- it's not always easy to see what has changed and thus why I should care. The signal to noise ratio is poor.  - By default a user should see which issues they have been assigned as reviewer when they log into JIRA. (If there is a way to reconfigure the dashboard for this, I'd like to know about it, but it really should be the default). One way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that.  -- Russell {quote}  and I added:  {quote} In fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment.  If the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review.  With Trac, the important information was at the top of the e-mail. {quote}"
2,"CSS keys are too fine-grain, consider merging them together (design)","I've seen a lot of sentiment (from Serge and Daniel) to try and combine keys in CSS. Current design has one key for each table-chunk (leading to tens of thousands of keys in production), and small-grain keys like ""nStripes"", ""nSubStripes"", and ""overlap"" in partitioning are each stored in separate key. This issue will capture discussion, decisions, and implementation of a more compact version. "
0,cleanup extra file names in docstring,"Reported by Serge in email:  When using doxygen to document C++ source, you can mark a comment block with just:  {code} /** @file   * Blah blah   */ {code}  in which case doxygen assumes you want the comment block tied to the file it appears in. We seem to have lots of ""@file <fileName>” statements all over the place, which is an extra thing we have to remember to change when renaming files. Is there some reason to do it that way that I’m missing?"
0,cleanup exception code in CSS,"Reported by Serge:  In CssException.h you’ve got:  {code} class CssRunTimeException: public std::runtime_error { … }; class CssException_XXXX : public CssRunTimeException { … }; {code}  This is inconsistent (shouldn’t it be CssRunTimeException_XXX, or maybe even CssRunTimeError?), lengthy, violates the LSST C++ naming conventions, and doesn’t match the KvInterface docs, which all still talk about a CssException class that does not exist. Can we consider changing this to something more like:  {code} class CssError : public std::runtime_error class KeyError : public CssError class NoSuchTable : public KeyError class NoSuchDb : public KeyError class AuthError : public CssError class ConnError : public CssError {code}  ? Then we can succinctly throw and catch css::NoSuchTable, css::AuthError etc…"
2,IN2P3: Cloud-computing Platform / OpenStack,"CC-IN2P3 offer computing resources via Openstack :  https://indico.in2p3.fr/getFile.py/access?sessionId=2&resId=0&materialId=0&confId=8236  This platform allow to boot virtual machines on multiples linux distribution. It could be a powerfull tool for continuous integration and testing.  Virtual machines are easilly available to all IN2P3 users, only a SSH account on ccage.in2p3.fr is required. "
1,Prepare a fedora64 openstack image which allow to easily build and test Qserv,Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests.  IN2P3 Openstack platform offer next virtual machines :  {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name              | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1  | m1.tiny           | 512       | 0    | 0         |      | 1     | 1.0         | True      | | 15 | cc.windows.small  | 4096      | 20   | 0         |      | 2     | 1.0         | True      | | 16 | cc.windows.xlarge | 8192      | 50   | 0         |      | 4     | 1.0         | True      | | 2  | m1.small          | 2048      | 10   | 20        |      | 1     | 1.0         | True      | | 3  | m1.medium         | 4096      | 10   | 40        |      | 2     | 1.0         | True      | | 4  | m1.large          | 8192      | 10   | 80        |      | 4     | 1.0         | True      | | 5  | m1.xlarge         | 16384     | 10   | 160       |      | 8     | 1.0         | True      | | 6  | cc.lsst.medium    | 4096      | 20   | 40        |      | 2     | 1.0         | False     | | 7  | cc.lsst.large     | 16384     | 20   | 160       |      | 8     | 1.0         | False     | | 9  | cc.lsst.xlarge    | 40000     | 20   | 160       |      | 20    | 1.0         | False     |  cc.lsst.xlarge would allow a quick build/test of new Qserv release.
0,Reduce and comment client configuration file,"Client configuration file '~/.lsst/qserv.conf) is used by integration test procedure.  Next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config   file"""
3,Upgrade various external packages,I note that the LSST and HSC versions of external packages are slightly out of sync.  I propose uprevving the LSST packages to match as HSC has tested these versions.  {quote} cfitsio               3.360             HSC cfitsio               3310+2            sims Winter2014 current b4 b5 b6 b3 doxygen               1.8.2+2           sims b4 Winter2014 current b5 b6 b3 doxygen               1.8.5             HSC eigen                 3.1.1+2           Winter2014 current b5 b6 b3 b4 eigen                 3.2               HSC fftw                  3.3.2+2           Winter2014 current b5 b6 b3 b4 fftw                  3.3.3             HSC gsl                   1.15+2            Winter2014 current b5 b6 b3 b4 gsl                   1.16              HSC minuit2               5.22.00+2         Winter2014 current b5 b6 b3 b4 minuit2               5.28.00           HSC mysqlclient           5.1.65+3          Winter2014 current b5 b6 b3 b4 mysqlclient           5.1.73            HSC pyfits                3.1.2+2           sims b4 Winter2014 current b5 b6 b3 pyfits                3.2               HSC scons                 2.1.0+7           sims b4 Winter2014 current b5 b6 b3 scons                 2.3.0             HSC sqlite                3.7.14+2          Winter2014 current b5 b6 b3 b4 sqlite                3.8.2             HSC wcslib                4.14        wcslib                4.14+3            b4 Winter2014 current b5 b6 b3 xpa                   2.1.14+2          Winter2014 current b5 b6 b3 b4 xpa                   2.1.15            HSC {quote} 
3,Rewrite secondary index system and merge empty chunks functionality,"We'd like to rewrite the indexing system so that it doesn't depend on a narrow innodb table. We'd also like to fold in the emptychunks functionality.  secondary index: dirColumn->(chunkId, subChunkId) emptychunks: hasChunk = chunkID -> bool  danielw has a dirt simple implementation of the ""create"" portion of the secondary index."
1,Rendering an IR node tree should produce properly parenthesized output,"It appears that rendering a tree of IR nodes doesn't always result in correct generation of parentheses. Consider the following tree: {panel} * OrTerm ** BoolFactor *** NullPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""refObjectId"") ** BoolFactor *** CompPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""flags"") **** Token(""<>"") **** ValueExpr ***** ValueFactor: Const(""2"") {panel}  which corresponds to the SQL for: ""refObjectId IS NULL OR flags<>2"". If one prepends this (via {{WhereClause.prependAndTerm()}}) to the {{WhereClause}} obtained by parsing ""... WHERE foo!=bar AND baz<3.14159;"" and renders the result using {{QueryTemplate}}, one obtains:      {{... WHERE refObjectId IS NULL OR flags<>2 AND foo!=bar AND baz<3.14159}}  This is equivalent to      {{... WHERE refObjectId IS NULL OR (flags<>2 AND foo!=bar AND baz<3.14159)}}  which doesn't match the parse tree - one should obtain:      {{... WHERE (refObjectId IS NULL OR flags<>2) AND foo!=bar AND baz<3.14159}}  This issue involves surveying all IR node classes and making sure that they render parentheses properly. {color:gray}(One way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. This would give us some IR that we can render to a string and reparse back into IR. If the rendering logic is correct, one should obtain identical IR trees).{color} Other possibilities that might explain the behavior above is that the input tree is somehow invalid or that {{WhereClause.prependAndTerm}} creates invalid IR."
2,Implement abstract base class for approximated or interpolated fields,"The user of an Approximate or Interpolate object doesn't care which of these they have, once the object has been constructed, and we should make these inherit from a common base class that only contains an interface for accessing the interpolated/approximated function while making no assumptions about its functional form.  The new class will represent a scalar field defined over an integer bounding box, and will have methods for evaluating the field at a point and creating an image of the field.  We could also consider giving it arithmetic interoperability with Image.  I don't have a strong candidate for the name; I've called it ""BoundedField"" on the HSC side but would like that to be revisited.  Adding derived classes to replace the functionality currently in the Approximate and Interpolate classes will be on a separate issue.  The main work on this issue is tweaking the design on the HSC side and getting signoff on the final design from LSST developers; the actual coding should be minimal, as it's just an interface and we already have a good starting point for it.  The HSC-side issue (which also includes work that is part of DM-1124) is here: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-796 and the associated git commits are here: https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796"
1,Use geom eups package for installing geometry,Use geom eups package instead of downloading geometry.py during Qserv configuration step.
1,"Qserv release (12.04) - final build, testing and cutting release",NULL
1,Simplify (script) install procedure,"Install procedure described in READMEs.txt is complex and error prone.  It could be encapsulated in two scripts :  1. the first for installing Qserv current version in the eups stack, following the LSST official install procedure, 2. the second, developer-oriented, for installing Qserv from a git repository to the eups stack installed during 1.  This two scripts logs could be colorized for better ergonomy.  "
2,Integrate sciSQL in eups,"In order to become compliant with eups, sciSQL install process may have to be refactored  :  - provide a scons build and install target which build and install sciSQL binary (.i.e. reduce waf tool features) - provide a configuration procedure (scisql-deploy.py) which install UDF in MySQL datadir, and deploy .so file in MySQL plugin directory.  Then it has to be packaged in eups format."
1,Replacing boost system lib with eups libs breaks scons build,"While detecting boost, Qserv build system checks for both system lib and then eups lib. This procedure use next code :  {code:python} class BoostChecker:     def __init__(self, env):         self.env = env         self.suffix = None         self.suffixes = [""-gcc41-mt"", ""-gcc34-mt"", ""-mt"", """"]         self.cache = {}         pass      def getLibName(self, libName):         if libName in self.cache:             return self.cache[libName]          r = self._getLibName(libName)         self.cache[libName] = r         return r      def _getLibName(self, libName):         state.log.debug(""BoostChecker._getLibName() LIBPATH : %s, CPPPATH : %s"" % (self.env[""LIBPATH""], self.env[""CPPPATH""]))         if self.suffix == None:             conf = self.env.Configure()              def checkSuffix(sfx):                 return conf.CheckLib(libName + sfx, language=""C++"", autoadd=0) {code}  and this last line run next gcc command :  {code:bash} g++ -o .sconf_temp/conftest_10.o -c -g -pedantic -Wall -Wno-long-long -D_FILE_OFFSET_BITS=64 -fPIC -I/data/fjammes/stack/Linux64/protobuf/master-g832d498170/include -I/data/fjammes/stack/Linux64/boost/1.55.0.1/include -I/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/include -I/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/include -I/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/include -I/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/include/xrootd -Ibuild -I/data/fjammes/stack/Linux64/anaconda/1.8.0/include/python2.7 .sconf_temp/conftest_10.cpp g++ -o .sconf_temp/conftest_10 .sconf_temp/conftest_10.o -L/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/lib -L/data/fjammes/stack/Linux64/protobuf/master-g832d498170/lib -L/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/lib -L/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/lib -L/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/lib -L/data/fjammes/stack/Linux64/boost/1.55.0.1/lib -lboost_regex-mt scons: Configure: yes {code}  As the ""-mt"" suffix is searched before the empty suffix, previous command succeed.In my example boost_regex-mt is a system lib. When launching ""scons build"", then CheckLib only looks for boost in /data/fjammes/stack/Linux64/boost/1.55.0.1/lib, not in /usr/lib/. This behaviour is eups-correct, but prevents to find boost_regex-mt.  In this example, a trivial solution is to reverse self.suffixes in python code, but a better solution would be to prevent g++ to use default search paths (e.g. : /usr/lib and /usr/include) in the second command. Is it possible to to it with scons ?  Mario, did you meet the same problem with sconsUtils ?    Thanks  Fabrice"
1,Update obs_decam for new CameraGeom,The obs_decam package worked on by Paul and Andy B. needs to be updated to reflect changes in the camera geometry.
3,Stretch: Data and test script specification for daily/automated QA/integration tests,"[I have renamed this Epic to better capture the current planned scope]          Per discussions I am pulling this Epic into 02C.01.02 [""my"" WBS]    The intent for it is to cover the science and scientific programming portion activities associated with setting up a daily automated QA/integration run.     The infrastructure activities to enable this  are covered by other Epics. It is a stretch because it will be primarily worked on by the Tucson Scientist, still to be identified ICW Tucson Scientific Programmer, still to be identified.     JK: In PMCS this would be New Hire LS6"
1,Identify Data Set for MiniProduction,NULL
1,Generate data for MiniProduction,"Assuming that the data needed for mini-production are to be simulated, the input files need to be created and simulated.  The input data then need to be put in a repo with appropriate calibrations."
1,Create scripts to run MiniProduction,Write command line script to run mini production.  The scripts should be able to be handed directly to the orca layer.
3,Eliminate local pixel indexing; always use parent instead,"As Jim Bosch notes in <https://dev.lsstcorp.org/trac/wiki/Winter2014/Bosch/Miscellaneous>:  Our image classes currently handle two different pixel coordinate systems which differ only by the offset commonly referred to as ""xy0"". The PARENT coordinate system puts the first pixel in the image at xy0; the LOCAL coordinate system always puts the first pixel at (0,0).  Our general rule has long been ""always pay attention to xy0"", which implies PARENT, but the Image class itself doesn't: in many operations, including subimage accessors and bbox getters, LOCAL is the default, whereas in others - particularly the pixel and iterator accessors - there isn't even an option to use PARENT. IMO this is the main source of image-offset bugs in the code.  We plan to address this issue as follows: * Modify the methods on image-like objects that return pixel iterators or locators based x and/or y index, so that they use parent coordinates. * Eliminate the ImageOrigin enum argument, which appears in image-like constructors, image-like getBBox methods. * Eliminate the image origin string argument in the butler's get and put methods for image-like objects.  At the end of this transition all indices will be PARENT indices; the concept of LOCAL will be gone. This will reduce ambiguity and eliminate an argument that is a source of confusion.  Furthermore, at Jim's suggestion and K-T's agreement, we plan to do this in two basic stages: * Rename methods or otherwise make changes that should break any code using LOCAL indices. Fix the code accordingly. * Rename methods back and eliminate the ImageOrigin argument. Fix the code accordingly."
1,Exception naming convention,The naming convention for exceptions in pex_exceptions is quite redundant.  This issue will make the convention more compact and update all packages that make use of pex_exceptions.
1,Evaluate moving to C++11 for .cc files,Check that {{C\+\+11}} works on .cc files.  Make {{C\+\+11}} the default in SconsUtils.
1,Improve afw::CameraGeom::utils code,Some of the utility code in CameraGeom was not completely ported in W13 and documentation is in need of updating.
0,Determine scope of XY0 convention update,It's unclear exactly how much effort will be involved in making a change to how the XY0 is used.  If the parent/child argument is removed completely this change could be quite invasive and wide reaching.
2,Does SWIG 3.x work with DM stack?,"This task is to evaluate whether we can use SWIG 3.x (and is it stable enough).  Assuming SWIG 3.x can be used with the stack, evaluate how much work will be involved in moving over to use it by default."
1,Create scripts to assess and report MiniProduction quality.,Write a command line script to assess the quality of the mini production run.  This will involve comparing output data to data produced using a standard stack.  The script will provide a report.
1,Create script to clean up after a MiniProduction run,The run of a mini production will produce an output repository.  It's likely that we will not want to save all output data.  A script to clean up and potentially save parts of the repo is needed.
0,Package log4cxx,"Fabrice, can you package log4cxx? I should have asked you earlier, sorry I waited so long, not it becoming urgent! Bill is almost done with his logging prototype and will be turning it into a real package, and we need to have log4cxx packages. Many thanks.  log4cxx version 0.10.0, which was released in 4/3/2008 but is still undergoing ""incubation"" at Apache. "
2,XLDB in Rio,Prepare for and attend XLDB-South America in Rio.
1,XLDB-2015 report,"Writing the report, most work done by Daniel, with input from Jacek and K-T."
1,Researching partnership opportunities,NULL
1,Restructure and package logging prototype,"Restructure and package log4cxx-based prototype (currently in branch u/bchick/protolog). It should go into package called ""log"""
1,Access patterns for data store that supports data distribution ,"Data distribution related data store includes things like. chunk --> node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). Need to understand access patterns - who needs to access, how frequently etc. "
0,research mysql cluster ndb,Checkout mysql cluster ndb from the perspective of data distribution - could it be potentially useful to store data related to data distribution?
1,Automated test should optionally ignore column headers,Some types of queries (like COUNT(*)) may return different column headers in qserv and mysql. This differences break our automated tests which dump and compare complete result including headers. It looks like we will not be able to guarantee that qserv can be made to return the same column headers as mysql except for providing aliases in the query itself. Running those queries without aliases is a legitimate use case so it would be nice to have an option in the test runner which ignores headers for some queries.
0,Disable failing test cases in automated tests,There are currently 4 test cases failing in out automated tests. Until we have a fix we want to disable them.
1,JOIN queries are broken,"Running a simple query that does a join:  {code} SELECT s.ra, s.decl, o.raRange, o.declRange FROM   Object o JOIN   Source s USING (objectId) WHERE  o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint; {code}  results in czar crashing with: {code} 2terminate called after throwing an instance of 'std::logic_error'   what():  Attempted subchunk spec list without subchunks. {code}  This query has been taken from integration tests (case01, 0003_selectMetadataForOneGalaxy.sql) "
1,Understand DCR amplitudes using realistic distribution of stars,NULL
0,Determine refraction amplitudes as a function of SED,NULL
0,Model refraction amplitudes as a function of SED,NULL
0,Determine DCR ampltiudes as a function of SED,NULL
0,Model DCR amplitudes as a function of SED,NULL
0,Determine requirements on atmospheric measurements to predict refraction/DCR,NULL
1,Put together requirements to model refraction/DCR for a given source,NULL
0,SQL injection in czar/proxy.py,"Running automated tests for some queries I observe python exceptions in czar log which look like this: {code} 20140529 19:47:19.364371 0x7faacc003550 INF <py> Query dispatch (7) toUnhandled exception in thread started by <function waitAndUnlock at 0x18cd8c0> Traceback (most recent call last):   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 78, in waitAndUnlock     lock.unlock()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 65, in unlock     self._saveQueryMessages()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 87, in _saveQueryMessages     self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp))   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/db.py"", line 95, in applySql     c.execute(sql)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute     self.errorhandler(self, exc, value)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler     raise errorclass, errorvalue _mysql_exceptions.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'r' AND sce.tract=0 AND sce.patch='159,3';', 1401410839.000000)' at line 1"") ok 0.000532 seconds {code}  I believe this is due to how query string is being constructed in czar/proxy.py: {code:py} class Lock:      writeTmpl = ""INSERT INTO %s VALUES (%d, %d, '%s', %f);""  # ...................             self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp)) {code}  If {{msg}} happens to contain quotes then resulting query is broken. One should not use Python formatting to construct query strings, instead the parameters should be passed directly to {{cursor.execute()}} method. "
1,Zookeeper times out,"I noticed running some queries, leaving system up and them returning few hours later and running more queries can result in:  {code} ZOO_ERROR@handle_socket_error_msg@1723:  Socket [127.0.0.1:12181] zk retcode=-4, errno=112(Host is down):  failed while receiving a server response {code}  It needs to be investigated (if we can reproduce) "
1,"Provide a ""stackfitCalib"" for coadds",Coadds have a Psf class that returns the proper sum of input PSFs at a point.  We need the same functionality for the Calib object associated with the Coadd Exposure.  This will require making a Calib a baseclass (it currently doesn't have a virtual dtor (although it does have virtual protected members)) 
0,PhotoCalTask doesn't return information about which stars were used in calibration,"The PhotoCalTask returns numpy arrays of the source and reference fluxes (and errors) of matched ""good"" photometric objects, typically stars.  However, while estimating the zero point, it clips outliers so the actual list of objects used is shorter.  Please add another output to the returned struct, a numpy Bool array ""good"", to indicate which objects are kept. "
0,Cleanup in core/examples and core/doc,#NAME?
0,Define filter for Epics and mapping of Epics to WBS,NULL
0,qserv have to use boost from stack,"To quote Jacek and KT: {code} Andy, re dm-751, KT says never use the system version.  J. {code}  So we need to switch qserv to eups-boost. This should be easy once DM-751 is done, just add boost to qserv.table. Then one can remove conditional part of {{BoostChecker}} which works with system-installed boost. "
1,Simplify copying tables while adding columns,"Currently, if I want to copy a table while adding a few columns (as specified by schema in the example) I need to do something like: {code}         cat = afwTable.SourceCatalog(schema)         cat.table.defineCentroid(srcCat.table.getCentroidDefinition())         cat.table.definePsfFlux(srcCat.table.getPsfFluxDefinition())         # etc.          scm = afwTable.SchemaMapper(srcCat.getSchema(), schema)         for schEl in srcCat.getSchema():             scm.addMapping(schEl.getKey(), True)          cat.extend(srcCat, True, scm) {code}  Please make this easier!  For example  - by adding a flag to the SchemaMapper constructor that automatically does the addMapping (should this be the default?)  - by making it possible to copy all the slots (maybe this'll be the case when the new alias scheme is implemented?).  Maybe we just need a new method: {code} cat = srcCat.extend(schema) {code} that does all the above steps."
1,Reimplement C++/Python Exception Translation,"I'd like to reimplement our Swig bindings for C++ exceptions to replace the ""LsstCppException"" class with a more user-friendly mechanism.  We'd have a Python exception hierarchy that mirrors the C++ hierarchy (generated automatically with the help of a few Swig macros).  These wrapped exceptions could be thrown in Python as if they were pure-Python exceptions, and could be caught in Python in the same language regardless of where they were thrown.  We're doing this as part of a ""Measurement"" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language."
1,Design Prototypes for C++ Algorithm API,"We've never really been happy with the new design for the C++ algorithm API, and Perry and Jim have a few ideas to fix this that need to be fleshed out.  Each of the subtasks of this issue will correspond to a different design idea.  Ideally, for each one, we'll try to do a nearly-complete conversion of the SdssShape algorithm (as a good example of a complicated algorithm) to see how these ideas work in practice."
1,Algorithm API without (or with optional) Result objects,"In this design prototype, I'll see how much simpler things could be made by making the main algorithm interface one that sets record values directly, instead of going through an intermediate Result object.  Ideally the Result objects would still be an option, but they may not be standardized or reusable."
0,add persistable class for aperture corrections,"We need to create a persistable, map-like container class to hold aperture corrections, with each element of the container being an instance of the class to be added in DM-740.  A prototype has been developed on DM-797 on the HSC side: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-797 and the corresponding code can be found on these changesets: https://github.com/HyperSuprime-Cam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/DM-797"
1,implement coaddition for aperture corrections,We need to be able to coadd aperture corrections in much the same way we coadd PSFs.  See the HSC-side HSC-798 and HSC-897 implementation for a prototype: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-798 https://hsc-jira.astro.princeton.edu/jira/browse/HSC-897 with code here: https://github.com/HyperSuprime-Cam/meas_algorithms/compare/d2782da175c...u/jbosch/DM-798 https://github.com/HyperSuprime-Cam/meas_algorithms/compare/c4fcab3251...u/price/HSC-897a https://github.com/HyperSuprime-Cam/pipe_tasks/compare/6eb48e90be12d...u/price/HSC-897a
1,reduce code- and object-duplication in aperture correction and PSF coaddition,"The aperture correction code to be added on DM-833 will likely not be as closely integrated with CoaddPsf as it could be, because the original design of CoaddPsf didn't anticipate the addition of other, similar classes.  We should work on allowing these classes to share code."
1,design Array fields for table version 1,"While we're trying to eliminate the need for compound fields in afw::table, Arrays present a few problems.  We could use FunctorKeys, the way we plan to use other compound fields, but here we need to guarantee that the per-element keys are contiguous, and we also be able to support views.  We also need to determine the naming scheme.  Finally, we need to make sure these work with aliases and slots."
0,Rewrite multiple-aperture photometry class,"We've never figured out how to handle wrapping multiple-aperture photometry algorithms.  They can't use the existing Result objects - at least not out of the box.  We should try to write a new multiple-aperture photometry algorithm from the ground up, using the old ones on the HSC branch as a guide, but not trying to transfer the old code over.  The new one should:  - Have the option of using elliptical apertures (as defined by the shape slot) or circular apertures.  - Have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons)."
1,"Rename methods that return pixel iterators and locators in image-like classes, and change to use parent indexing","Image-like classes have a large number of methods that return pixel iterators and locators based on row and/or column. We wish to change these to use parent indexing (making row and column relative to XY0). As the first step in doing this, rename all these methods and modify them to parent indexing. Renaming will help identify and fix all code that uses these methods."
0,Change code so ImageOrigin must be specified (temporary),"Image-like classes have a getBBox method and various constructors that use an ImageOrigin argument which in most or all cases defaults to LOCAL. As the first stage in cleaning this up, try to break code that uses the default as follows: * Remove the default from getBBox(ImageOrigin) so an origin must be specified. * Change the default origin of constructors to a temporary new value UNDEFINED  * Modify code that uses image origin to fail if origin is needed (it is ignored if bbox is empty) and is UNDEFINED.  Note: this is less safe than changing constructors to not have a default value for origin, because the error will be caught at runtime rather than compile time. However, that is messy because then the bounding box will also have to be always specified, and possibly an HDU, so it would be a much more intrusive change."
0,Change data butler I/O of image-like objects to require imageOrigin if bbox specified (temporary),"As part of making PARENT the default for image origin, change the data butler to require that imageOrigin be specified if bbox is specified when reading or writing image-like objects.  Note: this ticket turns out to be unnecessary, as all the few necessary change are done as part of DM-840."
3,Update code that uses pixel iterators and accessors,Fix all code that uses pixel iterators and locators as per the changes in DM-839. This affects roughly 60 files distributed over many packages.
0,Restore names of methods that return pixel iterators and locators,Restore the names of methods that return pixel iterators and pixel locators on image-like classes. (This is part of the final stage of eliminating LOCAL pixel indexing).
0,Eliminate ImageOrigin argument,Eliminate the ImageOrigin enum and argument from image-like classes.
0,Eliminate image origin argument from butler for (un)persisting image-like objects,Eliminate the image origin argument for butler get and put when dealing with image-like objects.
2,Change code to always specify image origin (temporary),"The next step after DM-840 is to change all code that uses image-like getBBox and constructors that take an image origin argument to always specify the origin (DM-840 will break any code that tries to use a default image origin).  This results in a working stack after DM-840.  Neither this nor DM-840 will be merged to master, though it can be used by HSC and other users as they transition to a default image origin of PARENT.  See DM-1176 for the final step."
1,Update code to use restored names for methods that return pixel iterators and locators,"For all code changed in DM-842, change it again to use the restored names for methods that return pixel iterators and locators.  This task is much simpler than DM-842 because it is merely renaming methods. Nonetheless, it touches many files in many packages."
2,Specify ImageOrigin = PARENT in all cases,"For all code that uses the ImageOrigin argument (either explicitly or using a bbox and the default value) change the code to use an explicit ImageOrigin of PARENT.  In other words, update all code to match the changes in DM-840 and DM-841.  This affects 40-ish modules in many packages. I am concerned about testing the modified code because we are missing unit tests for so many tasks. But we can do some data processing and check those results. I will want some help with that."
1,Eliminate use of ImageOrigin argument,"For all code that uses the ImageOrigin argument, eliminate the use of that argument. In other words, update all code to match the changes in DM-844 and DM-845,  This affects all the code affected by DM-848, plus any code that was already using ImageOrigin=PARENT, but is a simpler change."
1,duplicate column name when running near neighbor query,"Running a simplified version of near neighbor query on test data from case01:  {code} SELECT DISTINCT o1.objectId, o2.objectId FROM   Object o1,         Object o2 WHERE  scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1   AND  o1.objectId <> o2.objectId {code}  Result in an error on the worker:  {code} Foreman:Broken! ,q_38f9QueryExec---Duplicate column name 'objectId' Unable to execute query: CREATE TABLE r_13237cd4cfc9e0fa01497bcf\ 67a91add2_6630_0 SELECT o1.objectId,o2.objectId FROM Subchunks_LSST_6630.Object_6630_0 AS o1,Subchunks_LSST_6630.Object_6630_0 AS o2\  WHERE scisql_angSep(o1.ra_PS,o1.decl_PS,o2.ra_PS,o2.decl_PS)<1 AND o1.objectId<>o2.objectId; {code}  It is fairly obvious what is going on. ""SELECT t1.x, t2.x"" is perfectly valid, but if we add ""INSERT INTO SELECT t1.x, t2.x"", we need to add names, eg. something like ""INSERT INTO SELECT t1.x as x1, t2.x as x2"""
1,Passing error messages from czar to end user - design,NULL
2,Provide a detailed integration tests report,"Test output is very low level. Indeed only a verbose logfile and SLQ queries output are currently available. Furthermore failing queries (i.e. .sql.FIXME) aren't launched.  This ouput could be leveraged to a detailed html web report (using sbadmin for example : http://startbootstrap.com/templates/sb-admin/) which could :  - launch all queries, even failing ones, - print Qserv services status after each query, - execution time for each query, - information about MySQL and Qserv results differences, - href to Qserv services log files - all other interesting information about queries execution    "
0,apr and apt_util packages do not install shared library,"When we installed apr and apr_utils packages (as a dependency of new log4cxx package, see DM-772) we discovered that both these packages only build static libraries but no shared libs are installed. This is problematic if mixed with shared libs and we use shared libs everywhere else. We certainly need to build shared libs for these packages, this ticket is to follow up on this problem."
0,near neighbor does not return results,"A query from qserv_testdata (case01/queries/1051_nn.sql) runs through Qserv, but it returns no results, while the same query run on myql does return results.  The exact query for qserv is:   {code} SELECT o1.objectId AS objId  FROM Object o1, Object o2  WHERE qserv_areaspec_box(0, 0, 0.2, 1)  AND scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1 AND o1.objectId <> o2.objectId; {code}"
0,disable extraneous warnings from boost (gcc 4.8),"Compiling qserv on ubuntu 14.04 (comes with gcc 4.8.2) results in huge number of warnings coming from boost. We should use the flag ""-Wno-unused-local-typedefs""."
1,XLDB Rio - workshop report,NULL
1,XLDB - strategic positioning,"Discussions with strategic partners. Improving website and adding new context (community, speakers). 1-pager document"
0,W'14 newinstall.sh picks up wrong python?,"newinstall.sh fails with:  Installing the basic environment ...  Traceback (most recent call last):   File ""/tmp/test_lsst/eups/bin/eups_impl.py"", line 11, in ?     import eups.cmd   File ""/tmp/test_lsst/eups/python/eups/__init__.py"", line 5, in ?     from cmd        import commandCallbacks   File ""/tmp/test_lsst/eups/python/eups/cmd.py"", line 38, in ?     import distrib   File ""/tmp/test_lsst/eups/python/eups/distrib/__init__.py"", line 30, in ?     from Repositories import Repositories   File ""/tmp/test_lsst/eups/python/eups/distrib/Repositories.py"", line 8, in ?     import server   File ""/tmp/test_lsst/eups/python/eups/distrib/server.py"", line 1498     mapping = self._noReinstall if outVersion and outVersion.lower() == ""noreinstall"" else self._mapping                                  ^ SyntaxError: invalid syntax  Perhaps from running the wrong version of python.  Full script/log is attached. "
3,lsst_dm_stack_demo,"lsst-dm_stack_demo has obsolete benchmark files (circa Release 7.0)  which fail to serve the purpose of validating, for the user, the correct functioning of a freshly built Release v8.0 stack.   At the very least,  the benchmark files should be regenerated for each official Release. Tasks:   (1) Build the benchmark files for Release v8.0  (2) Debate (a) recommending the  use of 'numdiff'  to check if the output is within realistic bounds.   Or, (b) develop another procedure to better show how the current algorithms compare to the algorithms used at the benchmarked Release. (3) Depending on result of the debate on #2: for: (a) provide appropriate 'numdiff' command invocation in manual.; for (b) implement the new procedure."
1,Segmentation fault from writing dotted FITS header keywords,"Observed on HSC, but I'm not aware of any fix for this on the LSST side either.  {code:sh} pprice@tiger3:~ $ gdb python GNU gdb (GDB) Red Hat Enterprise Linux (7.2-60.el6_4.1) (gdb) r Starting program: /tigress/HSC/products-20130212/Linux64/python/2.7.6/bin/python  >>> from lsst.afw.image import ExposureF >>> exp = ExposureF(1,1) >>> exp.getMetadata().add(""A.B.C.D"", 12345) >>> exp.writeFits(""test.fits"")  Program received signal SIGSEGV, Segmentation fault. lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...)     at src/PropertySet.cc:746 746	        if (dj == _map.end()) { (gdb) w #0  lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...) at src/PropertySet.cc:746 #1  0x00002aaabac068dc in lsst::daf::base::PropertyList::deepCopy (this=0x1ddd4c0) at src/PropertyList.cc:74 #2  0x00002aaab7ba559e in lsst::afw::image::MaskedImage<float, unsigned short, float>::writeFits (this=0x1f1d8f0, fitsfile=..., metadata=<value optimized out>, imageMetadata=..., maskMetadata=..., varianceMetadata=...) at src/image/MaskedImage.cc:569 #3  0x00002aaab7b0a7f0 in lsst::afw::image::Exposure<float, unsigned short, float>::writeFits (this=0x1f1d8d0, fitsfile=...) at src/image/Exposure.cc:251 #4  0x00002aaab7b0a9fc in lsst::afw::image::Exposure<float, unsigned short, float>::writeFits (this=0x1f1d8d0, fileName=""test.fits"") at src/image/Exposure.cc:239 #5  0x00002aaab6b91614 in _wrap_ExposureF_writeFits__SWIG_0 (self=<value optimized out>, args=<value optimized out>) at python/lsst/afw/image/imageLib_wrap.cc:160786 #6  _wrap_ExposureF_writeFits (self=<value optimized out>, args=<value optimized out>) at python/lsst/afw/image/imageLib_wrap.cc:29886 	… {code}  This appears to only affect dotted keywords (which should be supported through use of {{HIERARCH}}, and even if they're not supported, this should never fail with a segfault)."
1,Optimize template engine used in configuration tool,"string.Template and string interpolation are quite weak, some additional feature would be welcomed :  - interpolation interprets wrongly ""%d"" in template files as template - string.template doesn't manage correctly non referenced template parameters present in template files."
1,Use an existing qserv_run_dir with a new Qserv instance/binary,"Here's what should be added to qserv-configure.py :  - edit $QSERV_RUN_DIR/admin/qserv.conf and change Qserv instance dir to current one (which qserv-configure.sh), - check compliance of QSERV_RUN_DIR with new Qserv instance (version check ?) and/or update configuration files, - re-initialize services, if needed, without breaking already loaded data. "
0,unset BASH_ENV in newinstall.sh or surrounding instructions,"Nutbar users such as myself may have BASH_ENV set, which can mess with your carefully constructed shell environments.  Unsetting BASH_ENV should help.  This could perhaps go in newinstall.sh, or the documentation alongside where we suggest they unset other variables: https://confluence.lsstcorp.org/display/LSWUG/Building+the+v8.0+LSST+Stack+from+Source"
0,Update PhoSim tutorial to use CatSim for creating instance catalogs,"Update the Process PhoSim Images tutorial (https://confluence.lsstcorp.org/display/LSWUG/Process+PhoSim+Images) to use CatSim to generate instance catalogs, once CatSim documentation is available. This will obviate the need for the helper script refCalCat.py. "
0,"SourceDetectionTask should only add flags.negative if config.thresholdParity == ""both""","The SourceDetectionTask always adds ""flags.negative"" to the schema (if provided) but it is only used if config.thresholdParity == ""both"".  As adding a field to a schema requires that the table passed to the run method have that field this is a significant nuisance when reusing the task.  Please change the code to only modify the schema if it's going to set it. "
0,Provide Task documentation for DipoleMeasurementTask,See Summary. 
1,Provide Task documentation for PsfMatchTask,See Description (it's currently called PsfMatch) 
0,Provide Task documentation for ImagePsfMatchTask,See summary
0,Provide Task documentation for SnapPsfMatchTask,See summary
1,Provide Task documentation for AssembleCcdTask,See summary
1,Provide Task documentation for IsrTask,See Summary
0,Provide Task documentation for SourceDeblendTask,See Summary
1,Provide Task documentation for CmdLineTask,See Summary
1,Provide Task documentation for RepairTask,See Summary
1,"How to write your own command line task, including how-to-retarget sub-tasks","Please provide documentation on how to write a command line task, and how to retarget tasks (e.g. reusing bits of IsrTask for a new camera)  The documentation should include a complete annotated example.  "
0,Improve install/configuration/tests documentation and migrate it to reST format,"This ticket propose to migrate README and README-devel to reST format (see http://sphinx-doc.org/rest.html). The output is located here : http://lsst-web.ncsa.illinois.edu/~fjammes/qserv-doc/  Furthermore this ticket wil integrate Andy S. DM-622 value-added remarks about Qserv embedded documentation. {quote} README.txt needs a bit of formatting, whole ""NOTE FOR DEVELOPERS"" is one long line which may need scrolling depending on what do you use to read the file, same applies to README-devel.txt The install procedure in README.txt implies that the whole stack has to be installed including eups. If people have some part of it installed already the it would probably be better to reuse existing stack. Shall we spit install instructions into ""Install eups (if not installed already)"" and ""Install qserv""? README-devel.txt says ""Once Qserv is installed..."", I don't think that we need or want to install whole qserv before we start development (what if qserv is not available yet for the platform I'm trying to test). What probably needed is installed dependencies, and this should be covered by the comments before 'setup -r .' {quote} "
0,"Photometric calibration uses a column ""flux"" not the specified filter unless a colour term is active","The photometric calibration code uses a field ""flux"" in the reference catalog to impose a magnitude limit.  If a colour term is specified, it uses the primary and secondary filters to calculate the reference magnitude, but if there is no colour term it uses the column labelled ""flux"" and ignores the filtername.    Please change the code so that ""flux"" is ignored, and the flux associated with filterName is used."
1,Replace getXXXKey for slots with returning functorKeys similar to existing compound Keys,"Make any changes in the Functor Key capabilities necessary to support getXXXKey, getXXXErrKey, and getXXXFlagKey for the 3 different types of slots.   Change these routines in Source.h.m4 to return FunctorKeys for both version 0 and version 1 tables.  Then fixup any compilation breaks on the C++ size which this causes.  Remove the version 1 specific accessors.   I am assigning this to Jim to confirm that the first part is done, then he can assign the remainder to Perry either by reassigning, or as a subtask."
0,Package xrootd-4.0.0rc3-qsClient2 with eups,NULL
0,Prevent conflict related to non-unique temporary files created during SciSQL install,"SciSQL install sometime fails with next message :  {code:bash} [1/2] MySqlScript: scripts/install.mysql [2/2] MySqlScript: scripts/demo.mysql Running testHtm                          : OK Running testSelect                       : OK Running testAngSep.py                    : OK Running testMedian.py                    : OK Running testPercentile.py                : OK Running testS2CPoly.py                   : OK Running testS2PtInBox.py                 : OK Running testS2PtInCircle.py              : OK Running testS2PtInEllipse.py             : OK Running docs.py                          : FAIL [see /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log]   9 tests passed, 1 failed, and 0 failed to run    One or more sciSQL unit tests failed make: *** [install] Error 1 -- CRITICAL: Error code returned by command : / u s r / l o c a l / h o m e / s a l n i k o v / q s e r v - r u n / u . f j a m m e s . D M - 6 2 2 - g 8 6 a 3 0 e c 7 2 a / t m p / c o n f i g u r e / s c i s q l . s h   $ cat /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log rm: cannot remove `/tmp/scisql_demo_ccds.tsv': Operation not permitted Failed to run documentation example:     rm -f /tmp/scisql_demo_ccds.tsv     ERROR 1086 (HY000) at line 4: File '/tmp/scisql_demo_ccds.tsv' already exists {code}  It looks like test uses non-unique temporary file name and someone already tried to run installation on the same host. "
0,Improve management of qserv-run-dir in start/stop scripts.,qserv-start.sh by default tries again to use /usr/local/home/salnikov/qserv-run/2014_05.0 directory. There may be a confusion if we have multiple run directories. Would it be better to install qserv start/stop scripts into run directory itself so that they don't need to guess anything?
3,Add Doxygen documentation on rebuilds,Master-branch doxygen documentation should be rebuild on every full master build.
1,Move qserv-testdata.py to qserv_testdata package,"Andy S. suggests this during DM-622 review, but this needs to be studied deeply, as qserv_testdata goal is to store large datafile, difficult to version in git."
0,Clearer and shorter output for qserv-configure.py,qserv-configure.py produces a lot of output which could be confusing if people try to look at it and understand it. It may be better to reduce it to something that just indicates that each step is completed successfully.
1,Buildbot email should state if the build used master only or included other branches,Buildbot build status report currently doesn't state if the build was only for master or included other branches requested by the user.
0,Use aliases to clean up table version transition,"The addition of schema aliases on DM-417 should allow us to clean up some of the transitional code added on DM-545, as we can now alias new versions of fields to the old ones and vice versa."
1,Move table versions to Schema,"We've added a version number to afw::table::BaseTable to help with the transition to a new approach with different naming conventions and FunctorKeys instead of compound keys.  However, it looks like Schema objects need to know about this version number as well, in order to change Subschema objects to split/join using underscores instead of periods.  That means we'll have to move the version down into the schema object, which may affect a lot of downstream code.  Other than touching a lot of code, the changes should be trivial."
0,measAlg.interpolateOverDefects doesn't accept a python list of Defects,The python binding of interpolateOverDefects should accept a python list of Defects as well as a swig-wrapped std::vector<Defect>; it doesn't (try using a python list in test818 in meas_algorithms/tests/Interp.py)  
0,Include aliases in Schema introspection,Schema stringification and iteration should include aliases somehow.  Likewise the extract() Python methods.
0,fix int/long conversion on 32-bit systems and selected 64-bit systems,"tests/wrap.py fails in pex_config on 32-bit systems and some 64-bit systems (including Ubuntu 14.04) with the following: {code:no-linenum} tests/wrap.py  ...EE.E. ====================================================================== ERROR: testDefaults (__main__.NestedWrapTest) Test that C++ Control object defaults are correctly used as defaults for Config objects. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 89, in testDefaults     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testInt64 (__main__.NestedWrapTest) Test that we can wrap C++ Control objects with int64 members. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 95, in testInt64     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testReadControl (__main__.NestedWrapTest) Test reading the values from a C++ Control object into a Config object. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 82, in testReadControl     config.readControl(control)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 212, in readControl     __at=__at, __label=__label, __reset=__reset)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 217, in readControl     self.update(__at=__at, __label=__label, **values)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 515, in update     field.__set__(self, value, at=at, label=label)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 310, in __set__     raise FieldValidationError(self, instance, e.message) FieldValidationError: Field 'a.q' failed validation: Value 4 is of incorrect type long. Expected type int For more information read the Field definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 184, in makeConfigClass     fields[k] = FieldCls(doc=doc, dtype=dtype, optional=True) And the Config definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 131, in makeConfigClass     cls = type(name, (base,), {""__doc__"":doc})   ---------------------------------------------------------------------- Ran 8 tests in 0.017s  FAILED (errors=3) {code}  There is a partial fix on u/jbosch/intwrappers; this seems to work for Ubuntu 14.04, but not on 32-bit systems."
0,qserv-configure.py is broken in master,"It looks like there was a bug introduced either during the merge of DM-622 with master or right before that. Running {{qserv-configure.py}} from master fails now: {code} $ qserv-configure.py    File ""/usr/local/home/salnikov/qserv-master/build/dist/bin/qserv-configure.py"", line 229     (""Do you want to update user configuration file (currently pointing                                                                       ^ SyntaxError: EOL while scanning string literal {code} I assign this to myself, Fabrice is on vacation now and we need to fix this quickly."
0,"Create a ""stub"" package that checks for system dependencies for Qserv","Create a special ""stab"" package that checks and reports missing dependencies."
1,setup standard aliases for frequently-used measurements,"Frequently-used measurement fields such as classification and pixel flags should have shortened aliases that can be used instead of their full, package-qualified versions.  In some cases, these may serve as a sort of slot (e.g. we may have multiple classifications algorithms someday).  In this issue, we should audit all current measurement algorithm fields that don't already have a slot that works for them and consider whether there should be a standard alias.  We also need to work out a system for defining these aliases, probably in the config for SingleFrameMeasurementTask."
0,Detailed documentation for meas_base tasks,We should follow RHL's example for detailed task documentation and document all meas_base tasks.
0,Documentation audit and cleanup for meas_base plugins,"Many meas_base Plugins and Algorithms have poor documentation, including several whose documentation is a copy/paste relic from some other algorithm.  These need to be fixed."
0,add base class for measurement tasks,"We should consider adding a base class for measurement tasks (SingleFrameMeasurementTask, ForcedMeasuremedTask) that includes the callMeasure methods.  I'm hoping this will help cleanup callMeasure and improve code reuse."
0,Add FunctorKey to replace Coord compound keys,"Unlike previous FunctorKey replacements, Coord compound Keys are used in the minimal schema for SimpleTable and SourceTable, which makes removing them problematic."
1,convert measurement algorithms in ip_diffim,ip_diffim includes a few measurement algorithms which need to be converted to the new framework.
1,convert measurement algorithms in meas_extensions_shapeHSM,"This is a low-priority ticket to replace the old-style plugins in meas_extensions_shapeHSM with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete."
1,convert meas_extensions_photometryKron to new measurement framework,"This is a low-priority ticket to replace the old-style plugins in meas_extensions_photometryKron with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete."
1,Finish the Log packaging,Finish the work started by Bill (DM-778)
0,allow partial measurement results to be set when error flag is set,We need to be able to return values at the same time that an error flag is set.  The easiest way to do this is to have Algorithms take a Result object as an output argument rather than return it.  We'll revisit this design later. 
0,.my.cnf in user HOME directory breaks setup script,"Presence of {{.my.cnf}} file in the user HOME directory crashes {{qserv-configure.py}} script if parameters in {{.my.cnf}} conflict with parameters in {{qserv.conf}}.  How to reproduce: * create .my.cnf file in the home directory: {code} [client] user     = anything # host/port and/or socket host     = 127.0.0.1 port     = 3306 socket   = /tmp/mysql.sock {code} * try to run {{qserv-configure}}, it fails with error: {code} /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: connect: Connection refused /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: line 13: /dev/tcp/127.0.0.1/23306: Connection refused ERROR 2003 (HY000): Can't connect to MySQL server on '127.0.0.1' (111) {code}  It looks like {{~/.my.cnf}} may be a left-over from some earlier qserv installation. If I remove it and re-run {{qserv-configure.py}} now it's not created anymore. Maybe worth adding some kind of protection to {{qserv-configure.py}} in case other users have this file in their home directory."
0,add query involving a blob to the integration tests,We need to add a query (or more?) to the qserv_testdata that involve blobs. Blobs are interesting because they might break some parts of the qserv if we failed to escape things properly etc. 
0,improve message from qserv_testdata,"Currently, when I try to run qserv-benchmark but qserv_testdata was not setup, I am getting  {code} CRITICAL Unable to find tests datasets. -- FOR EUPS USERS : Please run :    eups distrib install qserv_testdata    setup qserv_testdata FOR NON-EUPS USERS : Please fill 'testdata_dir' value in ~/.lsst/qserv.conf with the path of the directory containing tests datasets or use --testdata-dir option. {code}  It is important to note in the section for eups users that this has to be called BEFORE qserv is setup, otherwise it has no effect. "
0,make slot config validation more intelligent,"Slot config validation currently assumes that field names match plugin names, which is not always a safe assumption.  This can prevent certain algorithms from being used in slots.  We probably can't do this validation in Config.validate(); we need to check in the measurement Task constructor after the schema has already been constructed."
0,Make NoiseReplacer a context manager,"I think the API for NoiseReplacer could be made more idiomatic (and possibly safer) by turning it into a ""context manager"" (i.e. so it can be used with the ""with"" statement)."
2,Adapt integration test to multi-node setup v1,Following DM-595 we can start qserv in multi-node configuration. Next step is to be able to run integration tests in that setup. This needs a bit of understanding how to distribute chunks between all workers in a cluster and how to load data in remote mysql server.
0,rename config file(s) in Qserv,"Rename local.qserv.cnf to qserv-czar.cnf. It is quite likely there are some other config files that would make sense to rename. If you see some candidates, let's discussion on qserv-l and do the renames."
0,Modify assertAlmostEqual in ip_diffim subtractExposures.py unit test,"In unit test, the comparison     self.assertAlmostEqual(skp1[nk][np], skp2[nk][np], 4)   fails.  However if changed to    self.assertTrue(abs(skp1[nk][np]-skp2[nk][np]) < 10**-4)  which is the desired test, this succeeds.   This ticket will remove all assertAlmostEquals from subtractExposure.py and replace with the fundamental comparison operator of the absolute value of the differences."
1,Research Apache Mesos and Google Kubernetes,It'd be good to check out Apache Mesos and Google Kubernetes and see how relevant they are for Qserv / LSST Database 
0,Provide Task documentation for ModelPsfMatchTask,See Description (it's currently called PsfMatch) 
1,Migrate Qserv worker code to the new logging system,NULL
0,fix names of meas_base plugins to match new naming standards,Some meas_base plugins still have old-style algorithm names.
1,Remove use of compound fields in minimal schema,"If we want to ultimately remove compound fields, we need to remove the ""coord"" field from the SimpleTable/SourceTable minimal schema, and provide a different way to get ra,dec from a source."
0,remove temporary workaround in new SkyCoord algorithm,"SingleFrameSkyCoordPlugin is using the Footprint Peak, not the centroid slot.  According to comments in the code, this is a workaround for some problem with centroids.  This needs to be fixed."
0,Classification should set flags upon failure,"The classification algorithm claims it can never fail.  It can, and should report this."
0,Detailed documentation for GaussianCentroid,"We need more detailed documentation for the GaussianCentroid algorithm, in terms of how it actually computes the centroid.  We (Jim and Perry) have done what we can, but we need help from whoever actually wrote it (RHL, we think) to provide the rest.  In particular:  - Additional detail should be filled in in the class Doxygen for GaussianCentroidAlgorithm, in GaussianCentroid.h  - The ""noPeak"" flag field description and name should be compared to what the algorithm actually does with it.  It looks to me like it's a bit misnamed (and maybe shouldn't be considered an error condition at all, if we want to run this on difference images), but I'm not sure."
0,"convert GaussianFlux to use shape, centroid slots",We should cleanup and simplify the GaussianFlux algorithm to simply use the shape and centroid slot values instead of either computing its own or having configurable field names for where to look these up.
0,Detailed documentation for SdssCentroid,"We needed detailed documentation for how SdssCentroid actually works.  I think it involves fitting something like a symmetrized PSF model, but I'm sufficiently unsure of the details that RHL should write this, not me.  It should go in the Doxygen class docs for SdssCentroidAlgorithm in SdssCentroid.h in meas_base."
0,fix testForced.py,"testForced.py is currently passing even though it probably should be failing: it's trying to get centroid values from a source which has neither a valid centroid slot or a Footprint with Peaks (I suspect because transforming a footprint might remove the peaks).  Prior to DM-976, that would have caused a segfault; on DM-976, I've turned it into an exception, which is then turned into a warning by the measurement framework."
0,Fix incorrect eupspkg config for astrometry_net,"The clang patch from 8.0.0. version was (correctly) deleted. However, the patch identity was still left in the eupspkg config's protocol.  This will delete the last vestige of the formerly necessary clang patch."
0,fix warnings related to libraries pulled through dependent package,"This came up during migrating qserv to the new logging system, and it can be reproduced by taking log4cxx, see DM-983, essentially:  {code} eups distrib install -c log4cxx 0.10.0.lsst1 -r http://lsst-web.ncsa.illinois.edu/~becla/distrib -r http://sw.lsstcorp.org/eupspkg {code}  cloning log package (contrib/log.git), building it and installing in your stack, and finally taking the branch u/jbecla/DM-207 of qserv and building it.  The warnings looks like:  {code}/usr/bin/ld: warning: libutils.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libpex_exceptions.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libbase.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) {code}  and they show up when I build qserv package, and are triggered by the liblog. I suspect sconsUtils deal with that sort of issues, but since we have our own scons system for qserv it is not handled. Fabrice, can you try to find a reasonable solution for that? Thanks!"
1,make use of MeasurementDataFlags,"We started adding a system to allow algorithms to declare what kind of data they can run on, but never really put it in place.  To do this, we should:  - Add more flags (at least NO_CALIB).  - Pass these flags in pipe_tasks and other places with tasks that use measurement tasks as subtasks (for instance, we should set NO_WCS and NO_CALIB during calibrate.initialMeasurement, and COADD in processCoadd).  - Add checks for these flags in appropriate algorithms.  For instance, PsfFlux should fail in construction if NO_PSF is set, and PeakLikelihoodFlux should fail if PRECONVOLVED is not set."
0,qserv-version.sh produces incorrect version number,I have just installed qserv on a clean machine (this is in a new virtual machine running Ubuntu12.04) which got me version 2014_07.0 installed: {code} $ eups list qserv    2014_07.0    current b76 $ setup qserv $ eups list qserv    2014_07.0    current b76 setup $ echo $QSERV_DIR /opt/salnikov/STACK/Linux64/qserv/2014_07.0 {code}  but the {{qserv-version.sh}} script still thinks that I'm running older version: {code} $ qserv-version.sh 2014_05.0 {code}
0,"""source"" command is not in standard shell","{{qserv-start.sh}} script fails when installed on Ubuntu12.04: {code} $ ~/qserv-run/2014_05.0/bin/qserv-start.sh /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 4: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: source: not found /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 6: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: check_qserv_run_dir: not found {code}  It complains about {{source}} command. {{source}} is not standard POSIX shell command, it is an extension which exists in many shells. Apparently in older Ubuntu version {{/bin/sh}} is stricter about non-standard features.   To fix the script one either has to use standard . (dot) command or change shebang to {{#!/bin/bash}}. This of course applies to all our executable scripts."
3,W15 Refactoring of Qserv,"Refactoring Qserv code to make it cleaner, better, and most importantly more robust and resilient to failures.  JK: Refer to loading spreadsheet for PMCS assignments"
3,W15 Integration Testing of Qserv,"Improvements to the integration tests suite for Qserv, including making it more generic (it currently uses a hardcoded database name), integrating new partitioner, and adding a data set from one of the last data releases.  JK: Refer to loading spreadsheet for PMCS assignments"
3,W15 Design Metadata Store for production tracking (v1),"Come up with a design of Metadata Store that will track all information in the Science Data Archive (across Data Releases), for both image and database repositories  JK: Refer to loading spreadsheet for PMCS assignments"
3,W15 Initial version of image and file archive,"Build initial (alpha) version of system for tracking existing image data sets. (a) Build a web form where users enter information about existing data sets. (b) Design and build a MySQL backend (maybe through DataCat). (c) Implement crawler that automatically fetches metadata from FITS headers for data set registered by users.   Note that support for additional formats (eg config files) will be added in the future, not through this epic. Note that this initial version will not be production ready, we will make it more bullet-proof and feature rich during S15 cycle.  JK: Refer to loading spreadsheet for PMCS assignments"
3,W15 Butler (v2) for local data sets,Improvements and tweaks to the butler as needed based on the feedback from the Apps team  JK: Refer to loading spreadsheet for PMCS assignments
3,W15 Management of distributed Qserv databases and tables,"This includes tasks such as creating and deleting distributed databases managed by Qserv, creating and deleting partitioned tables based on the information that is stored in the CSS. Design and exploratory prototype.  JK: Refer to loading spreadsheet for PMCS assignments"
3,W15 Loading data into distributed Qserv-managed databases,"Improvements to the existing data loader, to simplify data loading into Qserv. That includes integration with information in the CSS. This version will include non-parallel, single-node loader.  Relevant document (describing a sketch of much more advanced version): https://dev.lsstcorp.org/trac/wiki/db/Qserv/DataLoading  JK: Refer to loading spreadsheet for PMCS assignments"
3,S15 Implement Query Mgmt in Qserv,"Initial version of system for managing queries run through qserv. This includes capturing information about queries running in Qserv. Note, we are not dealing with query cost estimate here, (it will be covered through DM-1490)."
0,eliminate confusing config side-effects in CalibrateTask,"CalibrateTask does some unexpected things differently if you configure it certain ways, because it perceives certain processing as only being necessary to feed other steps.  In particular, if you disable astrometry and photometric calibration, it only runs measurement once, because it assumes the only purpose of the post-PSF measurement is to feed those algorithms.  This (as well as poor test coverage) made it easy to break CalibrateTask in the case where those options are disabled a few branches back.  After conferring with Simon and Andy, we think the best solution is to remove this sort of conditional processing from CalibrateTask, which should also make it much easier to read.  Instead, we'll always do both the initial and final phase of measurement, even if one of those phases is not explicitly being used within CalibrateTask itself."
0,Create a permanent and accessible mapping of the BB# and the bNNN. ,Create a permanent and accessible mapping of the BB# and the bNNN.   The users are interested in the BB# since is is used to point to the STDIO file form the entire stack build. The bNNN is needed because the daily life of the developer revolves around the stack tagged alternately by the bNNN tags and/or the DM Release tags. 
3,"W15 Central State System, support for db/table/query metadata","Improvements to the CSS system to make it more thread safe, more compact representation of keys, and adding information that will be needed to handle distributed databases, tables and likely queries (if we decided to use CSS for Query management).  JK: Refer to loading spreadsheet for PMCS assignments"
3,W15 Image Cutout Service,First version of the images cutout service. It will rely on butler. The bulk of the work in this epic includes building a web-based front-end.  JK: Refer to loading spreadsheet for PMCS assignments
0,Add / improve tests and examples for Log package,NULL
0,init.d/qserv-czar needs LD_LIBRARY path,"With the addition of log we now need to find some shared libraries from stack. Current version of qserv-czar init.d script does not capture LD_LIBRARY_PATH, so we should add it there. "
0,Remove unnecessary pieces from qserv czar config,"The config file for the qserv czar has some items that are no longer relevant, and in this issue, we focus on the ones that are clearly the responsibility of our qserv css.  This ticket includes: -- removing these items from the installation/configuration templates -- removing these items from sample configuration files -- removing these items from the code that reads in the configuration file and sets defaults for these items -- fixing things that seem to break as a result of this cleanup.  danielw volunteers to assist on the last item, as needed.  "
0,"fix SubSchema handling of ""."" and ""_""","SubSchema didn't get included in the rest of the switch from ""."" to ""_"" as a field name separator.  As part of fixing this, we should also be able to simplify the code in the slot definers in SourceTable."
0,track down difference in SdssShape implementation,"The meas_base version of SdssShape produces slightly different outputs from the original version in meas_algorithms, but these should be identical.  We should understand this difference rather than assume its benign just because it's small."
3,S15 Data Distribution & Replica Mgmt Design,"This is a continuation of DM-779, continue research related to data distribution and replication. There are still many option questions: should it be centralized or peer-to-peer? Is bittorent useful? How much should be home grown vs off-the-shelf (and which off-the-shelf). There are no direct deliverables in FY2015, but it is a complex topic, so we should not wait until the last minute. This epic covers the R&D / design."
3,W15 Qserv Stability / bug fixes,"Tweaks, improvements, fixes to bugs discovered over the course of W15, to improve Qserv stability.  JK: Refer to loading spreadsheet for PMCS assignments "
0,move algorithm implementations out of separate subdirectory,"We should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms.  They should generally go into anonymous namespaces there.  After doing so, we should do one more test to compare the meas_base and meas_algorithms implementations."
0,audit and clean up algorithm flag and config usage,"Check that meas_base plugins and algorithms have appropriate config options and flags (mainly, check that there are no unused config options or flags due to copy/paste relics)."
0,switch default table version to 1,"Now that all tasks that use catalogs explicitly set the table version, it should be relatively straightforward to set the default version to 1 in afw.  Code that cannot handle version > 0 tables should continue to explicitly set version=0."
0,Switch default measurement tasks to meas_base,"We should set the default measurement task in ProcessImageTask to SingleFrameMeasurementTask, and note that SourceMeasurementTask and the old forced photometry drivers are deprecated."
0,create forced wrappers for algorithms,We have multiple algorithms in meas_base which could be used in forced mode but have no forced plugin.  We should go through the algorithms we have implemented and create forced plugin wrappers for these.
0,remove old forced photometry tasks,"After meas_base has been fully integrated, remove the old forced photometry tasks from pipe_tasks"
3,Measurement - Calibration and Ingest,"Create command-line tasks to transform raw measurement quantities (e.g. fluxes, positions in pixels) to global units (e.g magnitudes and positions in celestial coordinates).  Also requires interfacing with the measurement plugin system, as only plugins can be authoritative on how to transform their outputs.  JK: In PMCS this would be Bosch J 50% Gee P 50%"
0,convert afw::table unit tests to version 1,"Most afw::table unit tests explicitly set version 0.  We should change these to test the new behaviors, not the deprecated ones."
0,Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.,"Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.  It was found that the meeting recorded in: [https://dev.lsstcorp.org/trac/wiki/Winter2012/CodingStandardsChanges] failed to include two recommendations:   * recommended: 3-30: I find the Error suffix to be usually more appropriate than Exception. ** current: 3-30. Exception classes SHOULD be suffixed with Exception.  * recommended but not specifically included: Namespaces in source files: we should use namespace blocks in source files, and prefer unqualified (or less-qualified) names within those blocks over global-namespace aliases. ** Rule 3-6 is an amalgam of namespace rules which doesn't quite have the particulars desired. FYI: The actual vote was to:  ""Allow namespace blocks in source code (cc) files.""  To simplify the future audit, all other recommendations in that specific meeting were verified as installed into the standards."
0,add batch flag to newinstall.sh,In some cases (installing in some script environment) it is nice to be able to run without any interaction.  This will add a flag to tell the script to install without asking about anaconda and git.  It will assume the answer is 'yes' to any question it would have asked.
2,Organize the content of the DM Developer Guide,"The DM Developer Guide is presently a place-holder for information relevant to developers. It is time to re-organize the content, and to provide authoritative guidance to developers. This will involve contributions from various members of the DM Team, and likely some selected harvesting of the Trac/Wiki. "
0,Fix overload problems in SourceCatalog.append and .extend,"This example fails with an exception: {code:py} import lsst.afw.table as afwTable schema = afwTable.SourceTable.makeMinimalSchema() st = afwTable.SourceTable.make(schema) cat = afwTable.SourceCatalog(st) tmp = afwTable.SourceCatalog(cat.getTable()) cat.extend(tmp) {code}  Expected behavior is that the last line is equivalent to {{cat.extend(tmp, deep=False)}}."
0,Update the DMS and Astro Glossaries,"The DMS and Astro Glossaries in Confluence define a set of technical terms used in their respective domains. Some of the definitions are placeholders, and other terms used in the SWUG, DM Space, and DM Developer Guide have yet to be defined in one glossary or the other. It is time to update these docs."
1,Determine problem with Mac OS X processCCD output when compared to 'identical' dataset generated on RHEL6,The benchmark file for the summer2012 demo is generated on  RHEL6. When using the dataset to compare stack output generated on Mac OSX 10.8.5 (and 10.9)  there are significant deviations.   Find out where the problem arises...in the stack during processing or in the comparison.
1,Research how to kill query in mysql,"In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation.  The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented.  First step is to understand how mysql code is handling query killing."
0,Investigate HTCondor config settings to control speed of ClassAd propagation,"With default settings we do not have good visibility as to whether an updated ClassAd on a compute node (e.g., CacheDataList now has ccd ""S00"") will be in effect on the submit node in time for a Job to be matched to an optimal HTCondor node/slot.   There are several components (negotiator, schedd, startd) and their associated activities that could impact the time that it takes for a new ClassAd on a worker node to 'propagate' back to the submit side. We investigate these configuration settings to try to determine what thresholds for configuration settings are required to meet a given time cadence of job submissions."
1,Enhance SWUG chapter on measurement,"A basic description of measurement in the LSST Stack was created for the SWUG (see DM-692). However, this chapter lacks specifics about how the measurement algorithms work and does not yet mention some of the available algoritms. This task is intended to provide the next level of detail, with references to source-level descriptions that exist or are in progress. "
3,afw::table - finish interface transition,JK: In PMCS Bosch J and Gee P
3,Measurement - Convert Old Algorithms,JK: In PMCS Bosch J and Gee P
3,Measurement - Finish Framework Overhaul,JK: In PMCS Bosch J and Gee P
1,Design unit-test data package,Gather requirements for unit test datasets and determine how to organize and generate the data.
2,Generate test data,Write scripts to generate the test data (as needed) and run them.
1,Custom mapper for unit test data package,"We need a custom mapper for the test data package, so we can run unit tests against it without depending on any particular obs_* package."
3,Convert existing tests to use new package,"After the new unit test data package is available, we should convert all tests that currently use afwdata or obs_test to use the new package instead, and then remove obs_test and afwdata.  Should have subtasks and a re-estimate of effort after auditing how much there is to be done."
3,afw - Footprint Improvements,"This epic contains several improvements to the Footprint class, including API refactoring, performance-related reimplementations, and some new features.  JK: In PMCS this would be Bosch J and Swinbank J"
3,Galaxy Fitting - Shear Precision Experiments,"NB some of the below stories will actually be Lauren, not Perry.  Breakdown: pgee 58%; jbosch 21%; krughoff 6%; lauren 15%"
3,Minimal MultiFit system prototype,"Experiment with a framework for running ""multifit""-style fitting algorithms (i.e. fit a model convolved with the PSF to multiple exposures simultaneously).    Earlier prototypes (meas_multifit) have demonstrated executable multifit code on a single system: repeating that work is not the priority here. Rather, we will produce a concept for the framework for running multifit at scale using large datasets on a cluster. It is not a requirement that it be possible to run an at-scale multifit job and produce useful results by the end of this epic; however, there should be proof-of-concept code which demonstrates the structures within which future multifit work will be carried out. This will be supplied to both Science Pipelines and Process Middleware developers."
3,Galaxy Fitting - Sampling Algorithm Plugin,NULL
3,CModel robustness,"The CModel code is currently a mixture of work ported from HSC and some new development on LSST. It needs to be validated on real data (HSC, SDSS, CFHT). All known failure modes should be eliminated. Where possible, improve its performance."
3,Create utilities to allow camera testing team to use CameraGeom,"The camera team can use the CameraGeom classes to reduce lab data for testing purposes.  Since the camera is relatively flexible, a good way of doing this is to create a camera at runtime from the header keys in the files to be reduced.    JK: In PMCS this would be Krughoff S"
3,Make the API for ISR explicit,The run method of the IsrTask currently takes a dataRef which has getters for calibration products.  This makes the task hard to re-use because one needs a butler and because the interface is opaque.  This task will make the IsrTask API more transparent.  JK: In PMCS this would be Krughoff S
3,Re-write astrometry task to remove dependency on astrometry.net,"The astrometry task currently depends as astrometry.net as the only possible algorithmic back end.  This also has created a dependency on the astrometry.net file format.    This task will remove the dependency on astrometry.net file format and will implement the current HSC astrometry solver in the astrometry task.  This will focus on solving single chips, not the full focal plane.    Note that DM-167 (which was closed with reference to this ticket) requests that we split photometric and astrometric catalogues, and this is still needed.    JK: In PMCS this would be Krughoff S and Owen R"
3,Command-line Tasks - Unit tests for ProcessCcd,We can get a long way toward improved test coverage in pipe_tasks by having Unit tests for ProcessCcd.  JK: In PMCS this would be Owen R
2,Add numpy typemaps to afw interfaces so numpy types pass correctly to C++,Currently SWIGed interfaces do not handle numpy types correctly.  This can be fixed with the appropriate typemap.  JK: In PMCS this would be Owen R
3,Extend Exposure classes to contain background models,"Exposures carry many things: PSFs, FITS metadata, Detectors, etc.  Another object algorithms may want to access with an exposure is the set of background models subtracted from the exposure to that point.  This will involve porting afw.math.BackrgoundList to C++ and extending Exposure classes to contain the new BackgroundList object.  JK: In PMCS this would be Krughoff S"
1,Improve documentation of code from DM-70,"DM-70 was merged before its code documentation was completely ready, in an effort to not leave such a large amount of code un-merged.  This ticket exists as the second half of DM-70 to clean up its documentation."
3,Improve fault tolerance as demonstrated in nightly computing simulator,"Adjust simulator parameters, mechanisms, and implementation to minimize time to recover and unprocessed/unarchived data resulting from a machine or network failure.  For W15: Pietrowicz, S - 100% Start Oct 2014, finish Nov 2014  For S15 rollover: Pietrowicz, S - 12% Start March 2015, finish April 2015"
3,Simplify and refactor the Event Services API,"Remove old fixed metadata; refactor using more generic metadata interface.  For W15: Pietrowicz, S - 100% Start Dec 2014, finish Dec 2014  For S15 rollover: Pietrowicz, S - 11% Start March 2015, finish April 2015  "
3,Write a log4cxx handler that emits events for log messages,"Write a log4cxx handler that emits events for log messages.  For W15: Pietrowicz, S - 100% Start Jan 2015, end mid Jan 2015  For S15 rollover: Pietrowicz, S - 11% Start March 2015, finish April 2015"
3,requirements and specifications for initial version of inter-Task communication,"gather requirements and specification for Implementation of  a gather/scatter mechanism for a set of Tasks executing in parallel using the Event Services, providing for eventual extension to support MPI or other communication mechanisms.  Start mid Jan 2015, finish Feb 2015.    JK: In PMCS this would be Pietrowicz S"
1,Chebyshev approximation object for aperture corrections,"Using the interface defined in DM-740, implement a Chebyshev-based implementation to be used to interpolate aperture corrections across CCD-level interfaces.  A prototype is available on the HSC fork, which can be copied directly, modulo any interface changes on DM-740.  For more information, see the HSC Jira issue (which also includes the work associated with DM-740): https://hsc-jira.astro.princeton.edu/jira/browse/HSC-796 and the HSC git commits https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796"
1,avoid usage of measurement framework in star selectors,"At least one of the star selectors uses the old measurement framework system to measure the moments of a cloud of points.  With the new versions of all the measurement plugins, it should be much easier (and cleaner) to just call the SdssShape algorithm directly, instead of dealing with the complexity of applying the measurement framework to something that isn't really an image."
1,design new Footprint API,"This issue is for *planning* (not implementing) some changes to Footprint's interface, including the following:  - make Footprint immutable  - create a separate SpanRegion class that holds Spans and provides geometric operators does not hold Peaks or a ""region"" bbox (Footprint would then hold one of these).  - many operations currently implemented as free functions should be moved to methods  - we should switch the container from vector<PTR(Span)> to simply vector<Span>, as Span is nonpolymorphic and at last as cheap to copy as a shared_ptr.  The output of this issue will be a set of header files that define the new interface, signed off by an SAT design review.  Other issues will be responsible for implementing the new interface and fixing code broken by the change."
3,implement new Footprint API,"This issue is the implementation for DM-1126, including fixing code broken by the API changes.  This issue should be given subtasks for discrete pieces of work as part DM-1126, as it has a lot of story points.  Because it's likely all of this needs to be merged at the same time, it should probably be done on a single branch, unless some of the earlier work can be done in a backwards-compatible way.."
2,Span-based grow operations for Footprint,The current grow operation for Footprints is very inefficient for isotropic grows.  A better algorithm can be found in the attached paper.
3,Span-based topological set operations for Footprint,"Implement span-based overlap tests and spatial union, intersection, and difference operators for Footprints.  Should be split into subtasks; first task would be to come up with the interface, and then each operation to be implemented should have one subtask each.  If complete after DM-1126 and DM-1127, these operations should be implemented in the SpanRegion class, and Footprint should delegate to that."
2,refactor C++ Algorithm classes,"Using one of the prototypes of DM-828, refactor the C++ Algorithm code in meas_base and any other packages."
1,create PSF simulations for shapelet approximation test,"To determine the number and order of shapelet expansions needed to approximate the LSST PSF, we need simulations of LSST PSFs, presumably generated using PhoSim.  I think we want something like 50-100 extremely high-SNR stars, drawn from realistic distributions of anything that would affect the PSF, including position on the focal plane and center position within a pixel.  I only care about getting these out as postage stamps, so I'll leave the question of how best to run PhoSim to get these up to someone else.  While this is part of a mostly-Princeton Epic, I'm assigning this issue to Simon as he'd be able to do it much faster than I could.  I'll also let him review and update the story points estimate."
3,create galaxy simulations for shapelet approximation and truncation tests,"Using the PSF simulations generated in DM-1131 and the GalSim package, generate ~20k galaxies for each of 3 shear values for each input PSF.  Will need to be divided into subtasks - figure out what the outputs should look like, write the code to generate the simulations, estimate the time need to run the simulations, run the simulations at scale, etc. "
2,implement shear measurement driver for simulations,We'll need driver code to run the galaxy fitting algorithms on the simulations from DM-1132.  Efficiently saving all the samples could be challenging.
1,implement SDSS PSF residual trick,"SDSS galaxy fitting approximated the convolution of a galaxy model with the PSF as the convolution of the galaxy model with a simplified approximation to the PSF added to the difference between the PSF approximation and the true PSF.  We should do the same, as it'll be no worse than ignoring the difference between the PSF approximation and the true PSF, and it may greatly reduce the complexity needed in the approximation."
3,test how large pixel region used in galaxy fitting needs to be,"Using simulations built on DM-1132 and driver code from DM-1133, test different pixel region sizes and shapes, and determine at what point shear bias due to finite fit region drops below a TBD threshold."
2,test number/order of shapelet expansions needed to approximate PSF,"Run driver from DM-1133 on simulations from DM-1132 with different configuration for shapelet PSF approximation, increasing complexity until change in shear estimate drops below a TBD threshold."
3,Evaluate python/c++ documentation generation and publication tools ,"This epic related to documentation that is provided as part of normal development activities. The desire is to keep this documentation in and near the codebase as this is best practice for it being maintainable. At the other end, we wish to publish this documentation in a coherent and searchable way for users. A number of tools exist in this area and this item requires a preliminary evaluation to be made.   This is part of curating our documentation infrastructure.  [FE 75% DOC 100% starting August 20th] "
1,Demonstrate & iterate with team on documentation toolchain   ,"Following from DM-1137, this epic relates to demonstrating various options for documentation tools workflows to the team, gathering input as to the preferred solution, adopting a workflow, and defining any specific implementation choices.   This is part of curating our documentation infrastructure. "
3, Stack documentation infrastructure and migration,"[Epic retitled and bumped in points to reflect wider scope and change in resource allocation]    This epic containes work on migrating the documentation infrastructure  into sphinx (from doxygen, Confluence) subject to RFC, continuous deployment of documentation, read-the-docs or  similar presentation, standardisation via teplates of common  information, a proposal for CI-ing examples and tutorials, release and install note documentation, and visual  design and javascript development for UI/UX    An additional request to migrate Word-based design documentation has been accepted. LaTeX support is being investigated.     [JS 100%]    "
1,Investigate automatic MacOS X build/deploy,This item is to set up the same continuous integration process we have on Linux on a MacOSX test server.   JK: In PMCS this would be Economou F and New Hire LS3
3,Evaluate merits of alternative CI and RFC,"Evaluate whether we wish to continue buildbot development or use a different (or additional) continuous intergration system.  This is part of a continuous occasional process of evaluating whether our current toolchain choices are still meeting our needs.  [FE at 75%, JH at 75%]"
1,Regularise Nightly and Weekly builds ,"The intent here is to create two seperate automated deployment environments, one based on a nightly (or ad-hoc) build, one one  aslower cadence (eg weekly). This will allow us to do intergration/QA runs on a bleeding or trailing edge as required.   JK: In PMCS this would be Economou F and New Hire LS3"
3,Investigate candidates for Verification and Integration Data Sets,"The task here is to develop a data set that can be used both for continuous integration (build tests) and automatic QA (integration tests). We want to maximise the richness of the data set in terms of its usefulness, but minimise it in terms of its size. DN to co-ordinate contributions.     [DN 95%  FE 5%]"
1,Remove code made obsolete,NULL
0,Create a top-level qserv_distrib package,"qserv_distrib will be a meta-package embedding qserv, qserv_testdata and partition."
3,SUI: Research system framework for SUI development,"Current IPAC development utilizes GWT, is it the right system for LSST SUI in 2022? I think this will be an on-going activity for the first two years.     10% of Goldina, Zhang, Ciardi, Surace 20% of Roby, Rector, Ly, Wu 40% Groom"
3,W15 Metadata Store for production tracking (prototype),Build a first prototype of the Metadata Store.   JK: Refer to loading spreadsheet for PMCS assignments
2,Fast image search,A lot of users will search for images using some (often advanced) spatial criteria. Need to implement something similar to what we have in UDFs/SciSQL to efficiently support this class of search. Check GIS support in MariaDB
0,Fix example of IsrTask to be callable with data on disk,Currently the example of the IsrTask takes a fake dataref.  This is hard to use with real data.  In DM-1113 we will update IsrTask to not take a dataRef.  This will make it easy to update the example script to work with real data.  This ticket will also include removing from the unit tests any fake dataRefs that have become unnecessary as a result of DM-1299.  
0,Css C++ client needs to auto-reconnect,"The zookeeper client in C++ that the czar uses doesn't auto-reconnect. This is a capability provided in the kazoo library that qserv's python layer provides, but isn't provided in the c++ client.  The zookeeper client disconnects pretty easily: if you step through your code in gdb, the zk client will probably disconnect because its threads expect to keep running. zk sessions may expire too. Our layer should reconnect unless there is really no way to recover without assistance from the calling code (e.g. configuration is wrong, etc.).  This ticket includes only basic reconnection attempting, throwing an exception only when some ""reconnection-is-impossible"" condition is met."
1,"Minor problems in lsstsw, related to Qserv offline install procedure","- on lsstsw master branch tip, ./stack/Linux64/lsst/9.2/bin/newinstall.sh doesn't seems to be the last version (it still install eups-1.3.0 instead od eups-1.5.0)   - on Fedora19, flock from util-linux 2.23.1 doesn't support next options : {code:bash} $ flock -w 0 200 flock: timeout cannot be zero {code} but {code:bash} flock -w 1 200 {code} works,  - newinstall.sh : would it be possible to enable automatic answers to git and anaconda install questions. For example, in order to easily enable automatic install on 300 nodes clusters ? (cancelled : covered by DM-1078)  - loadLSST.sh appends automatically http://sw.lsstcorp.org/eupspkg to EUPS_PKGROOT, and if first url in EUPS_PKGROOT isn't available eups fails without trying next ones => this isn't compliant with offline mode and introduce a work-around in Qserv offline mode install scripts. Would it be possible to define a lightly different behaviour for loadLSST.sh ?  - In newinstall.sh, l. 167, if Python version isn't correct, then exit *with error code*. "
3,SUI web user interface prototype  ,"SUI team want to use this time to study the DM system and all the other design documents to come up with preliminary design of the SUI infrastructure with some prototyping along the way to help the proof of concept.  40% Wu 30%  Rector, Roby, Ly 10% Zhang, Goldina 80% Ciardi 90% Surace 60% Groom"
3,SUI Interface through Qserv with database and prototype ,Working closely with database group to define the interface needs between SUI and Qserv. Prototyping the functions will help us understand the interface better.  50% Rector 10% Ly 10% Wu
1,SUI:  Query and display LSST image ,Define the APIs for image query with database group.   Depend on the implementation of APIs  Exercise the image cutout service DM-1977 been developed in SLAC.     
3,SUI catalog query interface prototype,Define the APIs for image query with database group  Depend on the implementation of APIs  30% Goldina 20% Ly 10% Wu
3,SUI image visualization prototype (without searching LSST images),Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to get feedback from DM people and potential users of the tool.  20% Roby 30% Zhang
3,SUI 2D plot for catalog prototype,Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to get feedback from DM people and potential users of the tool.    30% Goldina 10% Ly
3,SUI catalog and image interactive visualization with LSST data,"Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to exercise the data access APIs developed by SLAC and get feedback from DM people and potential users of the tool.  20% Goldina, Zhang 10% Roby, Ly, Wu, Ciardi "
1,Cleanup SdssShape,"We should do a comprehensive cleanup of the SdssShapeAlgorithm class.  This includes removing the SdssShapeImpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class.  We should also ensure that the source from which the algorithm is derived is clearly cited -- that's Bernstein and Jarvis (2002, http://adsabs.harvard.edu/abs/2002AJ....123..583B); see also DM-2304."
1,Port meas_algorithms unit tests for plugins,"While test coverage isn't complete, there are many unit tests for measurement plugin algorithms in meas_algorithms that have not yet been ported to meas_base.  We should make sure any of these tests that aren't already covered in meas_base are moved over before we remove the meas_algorithms versions of things."
1,remove dependency between sconsUtils and eups,"sconsUtils currently depends on eups, (and as far as I understand it, it should not...)"
0,experiment with building MyISAM as a shared library,"Per Monty (phone call Aug 28, 2014), this should be easy, code to look at is in storage/myisam/mi_test*"
0,zookeeper port numbers should be configurable,"The test programs in core/modules/css/ has hardcoded zookeeper port numbers. That needs to be fixed, it needs to be configurable."
1,User-friendly install/configure/test scripts,"- stdout, stderr output should be colorized, whereas redirecting it into a file shouldn't."
3,FY19 Add Support for ForcedSource Table in Qserv,"ForcedSources will needs special handling  * ra/decl columns are not part of ForcedSource table, we will need them for partitioning  * we might want to store them in subchunks? "
1,Make default image origin PARENT in all cases,"After DM-840 is finished, and code updated for it (DM-846), make the new default image origin PARENT, remove the UNDEFINED image origin enum, and update at least some of the code that explicitly specifies PARENT (there is little harm in leaving this explicit, other than it does not set an optimal example)."
2,Implement sharable just-in-time subchunk management on the worker,"The current subchunk management on the worker is very simple. When a query fragment is selected for execution, it builds exactly the subchunks it needs, performs the queries, and destroys the subchunks. This presents a problem when we have concurrent queries that need the same subchunks. Thus an earlier query will destroy its subchunks (some/all of which are needed by another concurrent query) and cause the later queries to fail.  The suggested approach is as follows: * Each query, instead of building subchunks on its own, delegates responsibility for subchunk creation/deletion to another class (e.g. SubChunkManager, [but try to find a better name]). * Each query requests a subchunks and releases subchunks using a lock()/unlock() mechanism (in the SubChunkManager API), or wrapped in an interface like boost::lock_guard wraps boost::mutex where acquisition and deletion can be handled by scoping. (It might even be possible to use boost::lock_guard mechanism.) * Now SubChunkManager blocks calls to lock() until it has created the necessary subchunks and modified its records to note that these subchunks are needed by one additional query). A subsequent unlock() call from the client indicates that SCM can decrement its counter and destroy the appropriate subchunks.  I (danielw) think the data structure to manage this is straightforward and the code for creation/deletion of subchunks already exists, but it might not be obvious how to route the plumbing so that queries can request/release appropriately, to an instance (probably only one per worker instance) owned by an object somewhere else.  Please feel free to use a better or more maintainable approach. "
0,Channel was inactive for too long error,"There as an issue with the receiveEvent call that can cause an exception ""Channel was inactive for too long"".  The fix for this (according to the ActiveMQ users list and archives) is to either completely disable the inactivity monitor or to increase the inactivity limit to something extremely high.  The fix is adding:  ""wireFormat.maxInactivityDuration=0"" to the URL in establishing the connection.  There might also be a way of doing this directly in the ActiveMQ broker, but so far I haven't seen anything that would let me specify that."
1,Learn the OCS middleware,"OCS will deliver the OCS Middleware software in November 2014.  There will be a workshop held at SLAC the week of November 10th.    I spoke with K-T and we should be able to attend this remotely, if necessary.  We need to spend some time getting familiar with this software and how it will integrate with the Base DMCS for AP."
0,rewrite low-level shapelet evaluation code,"While trying to track down some bugs on DM-641, I've grown frustrated with the difficulty of testing the deeply-buried (i.e. interfaces I want to test are private) shapelet evaluation code there.  That sort of code really belongs in the shapelet package (not meas_multifit) anyway, where I have a lot of similar code, so on this issue I'm going to move it there and refactor the existing code so it all fits together better."
3,Write a transition plan to move gitolite and Stash repositories to GitHub,"As recommended by the SAT meeting on 2014-09-16, we need this document to promote the use of GitHub by other subsystems within the project and to understand the impacts on DM.  The plan should include, but is not limited to: * Whether and how the repositories should be reorganized. * How existing commit attributions will be translated. * Moving comments in Stash to GitHub"
0,There is a bug in the prescan bbox for megacam.,The bounding box of the prescan region in the megacam camera should have zero y extent (I think).  Instead it goes from y=-1 to y=2.  This is either a bug in the generation of the ampInfoTables or in the way the bounding boxes are interpreted.
0,exampleUtils in ip_isr is wrong about read corner,"https://dev.lsstcorp.org/cgit/LSST/DMS/ip_isr.git/tree/examples/exampleUtils.py#n95 Says that the read corner is in assembled coordinates.  This is not true, it is in the coordinates of the raw amp.  That is, if the raw amp is in electronic coordinates (like the lsstSim images) it is always LL, but if it is pre-assembled, it may be some other corner.  This should probably use the methods in cameraGeom.utils to do the image generation."
0,Support some mixed-type operations for Point and Extent,"The current lack of automatic conversions in python is pretty irritating, and I think it's a big enough issue for people writing scripts that we should fix it.  In particular, allow {code} Point2D + Extent2I Point2D - Extent2I Point2D - Point2I  Extend2D + Extent2I Extend2D - Extent2I {code} (and the respective operations in the opposite order where well defined) It would also be good to allow the all functions expecting PointD to accept PointI, but I'm not sure if swig makes this possible.  It's probably not worth providing C++ overloads for all of these functions (and to be consistent we should probably do all or none).  I realize that you invented these types to avoid bare 2-tuples, but I'm not convinced that we shouldn't also provide overloads to transparently convert tuples to afwGeom objects."
3,Revisit log integration with Xrootd,"The integration of logging with xrootd needs to be revisited, for both the czar and the worker, after a discussion with Xrootd devs about API changes. We want to accomplish 2 things:  * Logging from xrootd client should go through the qserv-czar's logger, so it can be saved in the same file and enabled/disabled as a component. * Logging from the xrootd server may also find a benefit in using our logger as a backend"
0,add ColumnView support to FunctorKeys,"FunctorKeys currently only work with individual records, but at least some could work with columns as well.  Need to add an interface for this and decide how to handle cases where it the FunctorKey doesn't support it."
2,Invert buffering for czar in row-based result handling,"The XrdSsi API performs some impedance-matching in buffering transfers. The current code (introduced in DM-199) doesn't leverage this because its flow model is based on the older mysqldump-based results passing. With dump files, we don't know the size of the fragments expected, so we are just passing buffers of bytes and building up a text blob to ingest. With the row-based protocol, we have sizes specified, hence the receiver can specify exactly what sizes of bytes are needed. Implementation of this ticket should simplify and reduce the code overall. "
0,anaconda is too outdated to work with pip,"The version of anaconda distributed with the stack is too outdated to be used with pip (and probably other things). The issue is an unsafe version of ssh.  A workaround is to issue this command while anaconda is setup: {code} conda update conda {code} Warning: it is unwise to try to update anaconda itself (with ""conda update anaconda"") because that will revert some of the changes and may result in an unusable anaconda.  I think what is required is an obvious change to ups/eupspkg.cfg.sh  The current version of anaconda is 2.0.1 based on http://repo.continuum.io/archive/  Note: there is no component for anaconda. I will submit another ticket."
0,cleanup order/grouping of header files,"We want: * header for the class * then system * then third party * then lsst * then qserv  We currently don't have the ""lsst"" group (with a few exceptions), and we call the last one ""local"" in most places."
0,makeMaskedImage leaks memory,"Calling afw.image.makeMaskedImage leaks memory when called from Python, because it returns a raw pointer without telling Swig %newobject.  To fix it, it'd be better to just have it return by value instead of by pointer, though this might involve fixing some downstream C\+\+ code (Python code should not be affected)."
1,compute linear parameter derivatives more intelligently in optimizer,"The numerical derivatives computed by the optimizer currently don't distinguish between the linear parameters (for which derivatives are trivial) and nonlinear parameters (for which they're hard), because we don't pass the information that distinguishes them to the object that computes the derivatives.  If we move the computation of derivatives from the Optimizer class to the Objective class, we should be able to compute the derivatives much more efficiently.  While this doesn't matter much when fitting single component galaxy models (because there's only one linear parameter in that case), it should matter quite a bit when fitting high-order shapelets to PSF models."
1,Refactor meas_base Python wrappers and plugin registration,"meas_base currently has a single Swig library (like most packages), defined within a single .i file (like some packages).  It also registers all of its plugins in a single python module, plugins.py.  Instead, it should:  - Have two Swig libraries: one for the interfaces and helper classes, and one for plugin algorithms.  Most downstream packages will only want to %import (and hence #include) the interface, and having them build against everything slows the build down unnecessarily.  The package __init__.py should import all symbols from both libraries, so the change would be transparent to the user.  - Have separate .i files for each algorithm or small group of algorithms.  Each of these could %import the interface library file and the pure-Python registry code, and then register the plugins wrapped there within a %pythoncode block.  That'd make the implementation of the algorithms a bit less scattered throughout the package, making them easier to maintain and better examples for new plugins."
0,Support multiple-aperture fluxes in slots,"We should be able to use multiple-aperture flux results in slots.  While this is technically possible already by setting specific aliases, it doesn't work through the usual mechanisms for setting up slots (the define methods in SourceTable and the SourceSlotConfig in meas_base).  After addressing this, we should remove the old SincFlux and NaiveFlux algorithms, as the new CircularApertureFlux algorithm will be able to do everything they can do."
2,Refine Butler prototype,A prototype of the new Butler is part of the S14 delivery.  This needs to be refined into a production package and the rest of the code needs to be ported to use it.
3,S15 Multi-node Multi-query Integration Testing Harness,Build end-to-end integration test harness for Qserv that will run queries on multi-node system.
1,LSE-68: Bring pull interface to CCB approval,"Bring a long-pending set of changes, primarily the adoption of the ""pull interface"" for all DM-Camera image requests, to CCB approval"
3,LSE-68: Major phase-3 details (W15),"Deepen ISD to cover: supply of crosstalk constants, deletion policy details, content of the new-data notification, availability of a pass-through tag in data, and other Phase 3 matters.  Edit ISD to ensure that it covers WFS and guider requirements as well.  Deliverables: * Marked-up LSE-68 with combination of DM proposals for Phase 3 details and, where that is not a realistic approach, specific questions for Camera DAQ team. * Bring a proposal to the CCB by the end of the cycle."
1,LSE-69: Bring Summer 2014 work to CCB approval,"Bring Summer 2014 work on LSE-69 to CCB approval, ideally by 20 October in time to be part of the CD-2 document package."
1,LSE-72: Bring Summer 2014 work to CCB approval,"Remaining work is to proofread the SysML-ization by Brian Selvy of the LSE-72 draft, do any required cleanup in conjunction with the OCS team, and advocate for LCR-202 (already exists) at the CCB."
3,Refine requirements and use cases for Level 3 facilities,"Refine the requirements and use cases for the three branches of Level 3 capabilities exposed to users:  * Level 3 programming toolkit (user reconfiguration / extension of DM pipelines and stack)  * Level 3 compute cycle delivery (user access to 10% of compute base)  * Level 3 data product storage    Deliverables:  * Refinement, if necessary, to Level 3 requirements in DMSR  * Flowed-down requirements as a separate document.  Sufficient detail to allow a breakdown of the deliverables in the three areas of Level 3 by annual release cycle through construction period."
2,LSE-72: Remaining Phase 2 details,"Sort out remaining Phase 2 details of LSE-72, especially:  * details of the configuration mechanism (yet to appear even in LSE-70), the publication of available configuration keys, the efficient publication of configuration contents, OCS-driven configuration ""knobbing"" * specific list of events to be published by DM * more specific EFD-query language * EFD deployment model (understanding of distributed design envisioned by OCS and its implications for the ICD)"
2,LSE-72: Phase 3 details,Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during Summer 15.
1,LSE-75: Bring Summer 2014 work to CCB,"Bring a small set of technical changes (e.g., pointing notice moved to OCS ICD) and the addition of PSF reporting to T&S into the SysML version of LSE-75 and submit a change request to get this approved."
1,LSE-75: Refine WCS and PSF requirements,Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.  Depends on the ability of the T&S group to engage with this subject during the Winter 2015 period.  Can be deferred to Summer 2015 without major impacts.  Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.
3,LSE-130: Bring Summer 2014 work to CCB approval,"Bring the current list-oriented version of LSE-130 into the camera-test-plan-oriented format requested by the Camera group, moving the old list(s) into LDM-272 for reference.  Negotiate what further work is required to make the document acceptable to the Camera group for CCB approval in time for CD-2.  Nominally this requires CCB approval by 20-Oct-2014 in order to have an approved document properly released to the CD-2 committee."
2,LSE-130: Make requests more quantitative,"Proceed, as enabled by input from the DM Apps group and the Camera, to make the individual data requests in LSE-130 more quantitative."
1,LSE-140: Bring Summer 2014 work to CCB approval,"Convert the existing LSE-140 draft to SysML, produce a docgen, and review with Jacques Sebag.  Bring to CCB meeting on 8 October under existing LCR-201."
0,Complete data entry of LSE-140 revised draft into EA,NULL
1,Risk Register refresh 1/2015,"Periodic review of DM risk register contents.  Covers preparation for a review expected at the end of January 2015, the only one during Winter 2015."
2,TOWG - Contributions to the Operations Concept Document,Covers contributions to the writing and editing of the Operations Concept Document during the Winter 2015 cycle.  Deliverables: * Timescales diagram and explanatory text * Framework for Nominal Operations chapter * Contributions to KTL's writing of the Data Processing Operations chapter * General proofreading
1,Install scisql plugin (shared library) outside of eups stack.,sciSQL plugin is currently deployed in eups stack (i.e. $MYSQL_DIR/lib/plugin) during configuration step. Nevertheless eups stack should be immutable during configuration step.  MySQL plugin-dir option may allow to deploy sciSQL plugin outside of eups stack (for example in QSERV_RUN_DIR).
1,add per-exposure callbacks for measurement plugins,"We should give measurement plugins an opportunity to do some work whenever we start processing a new Exposure.  This give them a good time to throw exceptions when there's something obviously wrong with the exposure (e.g. it's missing a Psf).  It also gives them an opportunity to do work that could be used to speed up per-source processing.  One specific case I have in mind is in shapelet PSF approximation for galaxy fitting, where it could be very valuable to use an initial fit to an average PSF to initialize (and speed up) the first to per-source PSFs.  One question here is how to to allow information to be passed from the per-Exposure method to the per-Source methods - we should not do that via plugin instance attributes, which means we probably want to have the per-Exposure method return an arbitrary object that would be passed to the per-Source methods.  Unfortunately, I don't see a way to avoid having that change the signature for those methods for all existing plugins, so this should be done relatively early, before we have too many user-contributed plugins."
2,"Reimplement CSS using JSON-packing, czar kazoo + c++ snapshotting phase 1","Transient cache for CSS, populated from python, no C++ interface to zookeeper. Pack multiple keys using json. Tenative packing spec is in DM-705  The v1 implementation transitions the zk access from the c++ layer into python, with code that understands how to unpack json-encoded data. This can coexist with the existing qserv_admin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qserv_admin. "
1,CSS design for query metadata v2,NULL
1,CSS design for query metadata v1,The goal of this ticket (and DM-1250) is to try to understand what kind of per-query metadata is necessary to provide client-transparent query processing in case when czar/proxy could die or be restarted.  Some relevant info is in the Trac: https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS/RunTimeState
1,Implement structure for DB/table metadata in CSS,NULL
2,Requirements gathering for Metadata Store,NULL
1,Metadata Store - design v1,Research potential off-the-shelf candidates. Propose initial version of metadata design. 
2,Metadata Store - experimental prototype v1,"This first prototype will involve capturing metadata from a small set of image files. * Create a new schema file in the cat package (lsstSchema4mysqlW15.sql) with definition of tables that will be used to capture image metadata (see Exposure tables in other schema files), and some basic structure for capturing information about the entire repository. Decide which keywords are standard, and which should go to flat key-value area * Pick a directory (or several) with images, candidates: /lsst7/dr7/runs, /lsst7/releaseW13EP, /lsst3/DC3/data/obs/ImSim/pt1_2, /lsst/DC3/data/pt1.2.3k/datarel/ImSim * create test database, load schema * extract metadata for all images in a given directory and load metadata from fits headers."
1,Metadata Store - design v2,NULL
1,Metadata Store - experimental prototype v2 (DataCat),"Integrate prototype v1 with Fermi's DataCat (e.g., reuse logic for reading headers using afw, store in DataCat). Experiment with foreign tables."
1,Update documentation and automatic install script w.r.t. Qserv 2014_09.0 release,Creation of qserv_distrib and distribution of Qserv via official LSST repositories have to be taken into account in Qserv documentation and automatic install script.
3,evaluation of Cinder as OpenStack storage cache in the LSST Middleware,"Deliverable: evaluation of Cinder as OpenStack storage cache in the LSST Middleware Daues G 100%"" "
3,"implementation of data movement in the production system in the existing hierarchy of NFS disk, condo, tape","Deliverable: implementation of data movement in the production system in the existing hierarchy of NFS disk, condo, tape Freemon M 50% "
1,incorporation of sizing model into data center requirements,Deliverable: incorporation of sizing model into data center requirements Freemon M 100% 
2,data center requirements document,Deliverable: data center requirements document Petravick D 10% 
0,completed governance of security plan for review,Deliverable: completed governance of security plan for review Petravick D 10% 
0,security plan october.,Deliverable: security plan Ephibian 10% 
1,addition of procurement of physical goods to contract,"Deliverable: addition of procurement of physical goods to contract Petravick D 5%, Gelman M 10% "
3,insertion of wide area simulator into test stand,Deliverable: insertion of wide area simulator into test stand Freemon M 100% 
1,report of traffic shaping,Deliverable: report of traffic shaping Freemon M 100% 
2,upgraded KVMs,Deliverable: upgraded KVMs Mather B 50% 
3,sizing model critique report,Deliverable: sizing model critique report Perez A 50% 
1,more efficient VMware infrastructure,"Deliverable: more efficient VMware infrastructure  W15: Glick B 50%, Elliott M 25%, Mather B 10% 38 SP estimated  S15: Mather B 40%, Glick B 25% 6 SP estimated"
3,technical roadmap critique report,Deliverable: technical roadmap critique report Perez A 50%
3,Upgrade NFS Storage Servers with new hardware,"Deliverable: new hardware, ZFS filesystem, 3x 100+ TB + 1 spare 100+ TB Elliot M 75% "
3,deployment plan for version 1 of OpenStack,"Deliverable: deployment plan for version 1 of OpenStack Glick B 75%, Elliot M 15%, Mather B 10%, Wefel P 10%"
3,Procure replacement development infrastructure,"Procure, install, test, and deploy hardware to replace existing LSST development cluster infrastructure.    Assignees: Bill Glick, Matt Elliot, Bruce Mather, Paul Wefel, Jason Alt  Duration: November - December 2015"
3,level of effort,"Deliverable: level of effort Wefel P 100%, Freemon M 5%"
3,level of effort,Deliverable: level of effort Voiciu L 100%
3,level of effort,"Deliverable: level of effort Petravick D 50%, Gelman M 50%, Glick B 30%, Mather B 50%"
2,written plan for next period epics,"Deliverable: written plan for next period epics Petravick D 50%, Gelman M 50%, Glick B 50%"
0,Fix Scisql deployment test error (doc.py),"Deployment test in tools/docs.py fails due to a wrong ""scisql_index"" path in scisql documentation.  Fortunately, qserv-configure.py doesn't stop on this error."
1,meas_base ResultMappers should be FunctorKeys,"The ResultMapper classes in meas_base should inherit from FunctorKey, and support bidirectional transfers involving the Result structs and records."
0,add Schema method to join strings using the appropriate delimiter,"Delimiters in Schema field names are version-dependent.  One can currently use {{schema[""a""][""b""].getPrefix()}} to join fields using the appropriate delimiter, but this is confusing to read."
0,multi-level replacement in Schema aliases,Schema aliases should support more than one level (i.e. an alias may resolve to another alias).
0,remove meas_extensions_multiShapelet from release packages/buildbot,NULL
0,"rename meas_multifit to meas_modelfit, and add to lsst_apps",NULL
0,Improve Startup of HTCondor Jobs,Adjust configuration parameters of HTCondor config and/or submission files to improve speed at which HTCondor jobs start in both the replicator pool and worker pool.
1,Improve worker fault tolerance of missing distributor data,"Instances of worker jobs might ask the Archive DMCS which distributor to connect to, only to connect to that distributor which has recently been rebooted, so the distributor might not have that information.   At this point, the Distributor could send an “expired” notice of some kind to the Archive DMCS to clear it’s cache, and also tell the worker that it has no file of the type it’s looking for.  The worker would then go back to the Archive DMCS. "
0,Propose and document a recipe to build Qserv in eups,In-place build is available and documented.
1,Identify test data and camera,To test processCcd we need to identify a set of data to process (possibly mocked).  This implies that there will also need to be a minimal camera to go with the minimal data.  This task is to identify the minimal data and find a location for it.
1,Implement designed tests for processCcd,Implement the designed tests with the installed data.
1,Generate use case,We should first sit down with a Camera team rep (Jim C. maybe) to define the tool they need.  How dynamic are the data?  What is the layout of the data?  We should also get some example test data to work with.
1,Sync test data headers with standards,The headers will need to be vetted against known FITS standards.  The headers should be valid and should not contain any non-standard keywords.  There should also be a census of FITS standards that may be used for defining sensor layout.
2,Implement a tool to generate a Camera from FITS images,Implement the tool to generate the camera object.  It should be a command line tool that will take a path to a file.  It should have an option to persist the generated camera.  It should also have the ability to plot the camera.
1,Verify use of the tool with the camera team,We will need to run the tool in the camera team's system to make sure the interfaces are as they expected.  It will also help to make sure that the tool addresses all of the cases.
1,Design the API,"The API will have to be able to accommodate data we know about, so will need to deal with reasonable missing data.  It should also not preclude extension.  This will need to be RFC'd since it is an API change."
2,Implement and test the new API,"Once the API is designed and signed off on, the API will need to be implemented and tested.  This will require updating all obs_* packages that use the current interface."
1,C++ code changes required for --std=c++11,"Some C++ code requires changes for modern C++ compilers if it is to be compatible with C++11 and the older standard. Here is my list, so far (ignoring the known issue of MacOS having two different standard C++ libraries)  Implicit conversion of shared_ptr to bool no longer works: <http://stackoverflow.com/questions/7580009/gcc-error-cannot-convert-const-shared-ptr-to-bool-in-return> In C++11, shared_ptr has an explicit operator bool which means that a shared_ptr can't be implicitly converted to a bool. This is likely to show up in a lot of packages. So far found in: - pex_policy - meas_algorithms  warning: adding 'int' to a string does not append to the string seen in daf_persistence  warning: 'va_start' has undefined behavior with reference types seen in pex_logging Trace.h and one other place. Fixed by not using references (and thus copying the arguments), rather than using pointers, to avoid changing the APIs. this is an old issue; but not all compilers warned about it: See <http://stackoverflow.com/questions/222195/are-there-gotchas-using-varargs-with-reference-parameters>  warning: 'register' storage class specifier is deprecated seen in: - boost 1.55.0.1 - Eigen 3.2.0 - a pex_logging .i file I silenced this warning in sconsUtils because it's too much trouble to upgrade boost and Eigen, and the warnings are very obtrusive  in daf_persistence: {code} src/DbStorageImpl.cc:87:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<1>::mysqlType = MYSQL_TYPE_TINY; {code} I have not figured out how to fix this one.  clang 6 is pickier about the order of instantiation: in afw (fixed in DM-1302) Many errors such as the following: {code} include/lsst/afw/table/Flag.h:27:8: error: explicit specialization of 'lsst::afw::table::FieldBase<lsst::afw::table::Flag>' after instantiation struct FieldBase<Flag> {        ^~~~~~~~~~~~~~~ include/lsst/afw/table/Key.h:49:39: note: implicit instantiation first required here {code} Fixed by including Flag.h in Key.h  clang 6 warns about ambiguous unsequenced modifications. Seen in shapelet, for example: {code} src/HermiteTransformMatrix.cc:107:59: warning: multiple unsequenced modifications to 'kn' [-Wunsequenced]         for (int kn=jn, koff=joff; kn <= order; (koff += (++kn)) += (++kn)) {                                                           ^          ~~ {code} Fixed by using a += ++b, a += ++b, since it was the cleanest solution I could find.  clang 6 is pickier about instantiation classes in the wrong namespace: in shapelet (fixed by not instantiating the anonymous classes): {code} src/MatrixBuilder.cc:945:1: error: explicit instantiation of 'lsst::shapelet::<anonymous namespace>::SimpleImpl' must occur in namespace '' INSTANTIATE(float); {code}  meas_algorithms produced this warning because the class member _wcsPtr was a reference (which was unecessary): {code} src/ShapeletKernel.cc:188:34: warning: binding reference member '_wcsPtr' to a temporary value [-Wdangling-field]         _interp(interp), _wcsPtr(wcsPtr->clone())                                  ^~~~~~~~~~~~~~~ {code}  clang 6 warns about expressions such as if (!a == 0) because the ! is applied to ""a"", not the result of the ""a == 0"". Fixed in the obvious way in several places.  meas_algorithms seg-faults on loading unless boost is built with C++11 support. See ticket DM-1361 "
0,Tests fail in shapelet when building on OS X 10.9,"When building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail:  {code} pugsley:lsstsw mjuric$ cat build/shapelet/tests/.tests/*.failed tests/testMatrixBuilder.py  .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMatrixBuilder.py"", line 310, in testConvolvedCompoundMatrixBuilder     self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14)  ---------------------------------------------------------------------- Ran 7 tests in 0.323s  FAILED (failures=1) tests/testMultiShapelet.py  ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMultiShapelet.py"", line 88, in testConvolveGaussians     self.compareMultiShapeletFunctions(msf3a, msf3b)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in compareMultiShapeletFunctions     self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareShapeletFunctions     rtol=rtolEllipse)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795)  ---------------------------------------------------------------------- Ran 7 tests in 0.131s  FAILED (failures=1) {code}  ===============  More info on pugsley.ncsa.illinois.edu:  	pugsley:lsstsw mjuric$ sw_vers 	ProductName:	Mac OS X 	ProductVersion:	10.9.5 	BuildVersion:	13F34  	pugsley:lsstsw mjuric$ clang -v 	Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn) 	Target: x86_64-apple-darwin13.4.0 	Thread model: posix  ============  The files are in {{/Users/mjuric/test/lsstsw/build/shapelet/}}."
0,Pre-CCB review of LSE-140 docgen,NULL
1,Edit agreed-upon changes into Word version of LSE-69,"A meeting around 9/26/2014 agreed on a set of revisions to LSE-69, with some language still needed from [~gpdf].  This action is to edit the tracked-changes Word version of LSE-69 containing the notes from that meeting into a final copy that can be reviewed by the Camera team and used as input to editing the SysML version of the ICD."
0,Create change request for LSE-69,Create a change request to bring LSE-69 up to date and capture the Summer 2014 work.
0,Enter LSE-69 update into EA as SysML,"Covers entering the contents of the LSE-69 update into EA as SysML, with associated updating of diagrams, and the creation of a docgen'ed version for CCB action."
0,Proofread docgen'ed version of LSE-72,Brian Selvy is producing a SysML version of the LSE-72 updated edited by [~gpdf].  The action here is to proofread the docgen of that version once it is ready.
0,Identify Conditions information in LSE-130 that is required for Alert Production,"LSE-69 declares that there are two categories of Conditions data (telemetry) required by DM from the Camera: those items that are needed for Alert Production (for which the AP components at the Base will need a whitelist, and for which the Camera has a tighter latency requirement), and those that are not (but are then presumably needed in DRP or other deferred productions).  It states that the subset needed for AP should be enumerated in LSE-130.  The action here is to create an initial version of that list."
0,Publish Qserv S14 version on lsst distribution server,"In order to publish this version please tag Qserv master tip with ""2014_09.0"" and then run: {code:bash} ssh lsstsw@lsst-dev # command below can't be runned in buildbot, as it doesn't support qserv_distrib build rebuild -t 2014_09.0 qserv_distrib # bXXX is provided by previous command publish -t qserv -b bXXX qserv_distrib publish -t 2014_09 -b bXXX qserv_distrib {code} "
1,advance to assigning tier-2 and tier-3 reliability levels ,Accommodated Ron Lambert's input on networking equipment. Assigned tier-2 and tier-3 levels to processing systems. 
0,Deploy LSST stack within OpenStack instances on ISL testbed,"Deploy the LSST Stack within OpenStack instances within the ISL testbed -- this could be for multiple flavors CentOS, Ubuntu, etc, and this could be done by pulling Docker Images to the instances.   There will also likely be some initial debugging of starting instances within the ISL platform as a new installation has been stood up Sept 2014. "
0,Create Docker Image / Dockerfile for LSST Stack for ubuntu,"Create an installation of the LSST Stack v9_2  within a Docker Image for ubuntu for easing the import of LSST software into an OpenStack instance,  We create  the image utilizing a Dockerfile to make systematic  the creation of such images. "
0,update expected results file in SDSS demo test,Update the expected outputs in the SDSS DM stack demo repo to match what we expect from the new meas_base framework.
0,Prepare quotes to order new VMware hardware & licenses,NULL
0,Setup temporary NFS datastore for VMs,"We expect the primary datastores will be local to the VM compute node, but this NFS datastore will allow us to live migrate a handful of VMs between compute nodes.  We may also use this or another datastore as place to create backup snapshots of particular VMs."
1,Setup new VM compute nodes,"Once the new VM compute nodes arrive, the hardware needs to be installed, ESXi installed, and networked for use by vSphere."
1,Expire Workers that receive no files,"In instances where the files the worker expected to get never arrive, there should be a way of the worker to recognize this (say, a timeout after waiting for the Archive DMCS for some time), and exit."
2,Configure and test new VM infrastructure,"This would be configuring to work vSphere, setting up permissions and groups, adding datastores, and testing the use of the new setup."
1,Automatic expiration of replicator jobs,"Replicator jobs that receive no data specifically for the visit, raft, and exposure sequence ID within a certain amount of time should self expire to prevent future jobs from running.  If this is not done, jobs will back up in the HTCondor queue."
1,Order new NFS servers,We are waiting on the UIUC Business Offices to provide us a new account number for ordering these servers.  As soon as we get that account number we need to order the hardware from KOI Computing.
0,Plan new NFS mounts & data organization,This is a task of deciding the new client mounts for the 3 new NFS servers.  These should match up with the new storage hierarchy and classification <https://wiki.ncsa.illinois.edu/display/LSST/Storage+Structure>.
2,Install new NFS servers,NULL
0,squash edge errors in SdssCentroid,"SdssCentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs.  Instead, it should catch the low-level exception and re-throw as MeasurementError, after defining a flag field for this specific failure mode."
0,address no-shape warnings in GaussianFlux,"GaussianFlux relies on the shape slot, and puts noisy warnings in the logs when the shape slot fails.  However, we probably don't want to add a new flag for GaussianFlux to indicate this failure mode, because it'd be entirely redundant with the shape slot flag.  We should figure out some other way to squash this warning - how we do that may depend on whether this is addressed before or after the C++ redesign.  We should also consider having GaussianFlux add an alias to the schema to point back at the shape slot flag, creating what looks like a specific flag for this failure while actually just being a link back to the shape slot flag.  That's probably not worth doing within the current C++ interface, however, as it'd require some unpleasant mucking around with ResultMappers."
0,resolve factor of two difference in GaussianFlux,"After changing the implementation of GaussianFlux to use the shape slot rather than estimate the shape itself by re-running the SdssShape code, Perry saw a 5-15% difference in the fluxes (I'm not sure of the sign).  The new behavior (using the shape) is consistent with what we'd have gotten with the old code when the little-used ""fixed"" config option was enabled (not surprising, as that just looked up the SdssShape measurement by name, instead of via slots).  I suspect the difference is coming in because of the factor of two between SdssShape's ""raw"" measurements - the actual Gaussian-weighted moments - and the factor of 2 it applies to make its measurements equivalent to ideal unweighted moments.  The correct weight function to use for GaussianFlux includes this factor of 2 (i.e. it's larger than the ""raw"" moments), and it's likely either the old code wasn't including this or the new code isn't.  We need to determine which one, and if necessary, fix the new code."
0,Test the creation of basic OpenStack instances on the new ISL testbed [IceHouse],"A new version & implementation of the ISL OpenStack testbed is up and running. The new cloud is using IceHouse, the ninth OpenStack release. We get started on this platform by verifying that basic instance creation is working.  We target the creation of an instance through the (Horizon) GUI interface,  and via the nova CLI. "
0,Create instance with a Floating IP Associated through the nova CLI,"We see that in working with the Horizon GUI, it is fairly straightforward to give an instance a public IP address by associating a Floating IP with the current local IP.    However, we will want to be able to accomplish this task both remotely and programmatically within workflow.  As a step towards this, we target the solution of this via the nova CLI."
1,Review advance doc to review with the data center working group,"Consult with M. Freemon on the sizing model given the generic description of the computing needed tier-3 and tier-2 support. Make a pass through the underlying data center standards doc for things  ""any competent designer should know""  for example wall plugs on separate circuits form computer circuits.  add that to the document, and call meeting of working group for review in late Oct."
3,Security Plan advancement for October,"get a draft Project Office detail plan consistent with the level of devleopment of the master plan. (ephiphian). Draft a data classification plan, refer to plan and classes of data in drat materials. Obtain some central place in LSST documentation framework to hold materials being drafted (if supported by the LSST system) Recieve feedback based on portion of plan submitted to LSST project office. Incorporate feedback. (if minor) re-plan (if major)"
1,Make QSERV_RUN_DIR scripts able to detect qserv install paths using eups,"Ticket related to Mario email (subject: [QSERV-L] Some points/actions from the discussion today) :  Making your ""qserv data"" directory independent of where qserv is installed   I think this is a big one, and largely independent of EUPS. You have a problem where you want to use one set of test data potentially with different qserv binaries (not at the same time, of course). I'd argue you should refactor the scripts generated by qserv-configure to either:   * get _all_ their information about various paths from a _single_ file, for example, from etc/paths.cfg.sh. Then you can easily regenerate just that file when you need to switch to a different qserv (or zookeper, or what not), or... * refactor the generated scripts to learn from the environment which binaries to run. I.e., if $QSERV_DIR is defined, use that qserv, etc. This will let you switch binaries by simply setup-ing the new one with EUPS.   The two are not mutually exclusive -- e.g., all of this logic could be in etc/paths.cfg.sh, and depending on whether this is a development build or a ""non-EUPS"" build, it can either pick up the paths from the environment or hardcode them.   Assuming you did that, your development loop may look something like this:   {code}     # assuming that qserv-configure.py has already been run     # in ../qserv-run       # do something with qserv-a clone     cd qserv-a     setup -r .     ... do some edits ...     scons       # now do the tests     cd ../qserv-run     ./bin/qserv-start.sh     ... do tests ...     ./bin/qserv-stop.sh       # now switch to qserv-b clone     cd ../qserv-b     setup -r .       # and do the tests again     cd ../qserv-run     ./bin/qserv-start.sh     ... do tests ...     ./bin/qserv-stop.sh  {code}  that is, as qserv-start picks up the relevant products from the environment, there's no need to rebuild/reconfigure the qserv-rundirectory each time. "
0,Read through log4cxx documentation and log.git code,Read through the log4cxx documentation and become familiar with how the log.git package is set up.
1,Write/configure tests with existing configurations and appenders,"In order to be come more familiar with how to use the log.git package, write some tests to see how existing configurations and appenders are used by the log.git package."
3,Write DM message appender class,Write DM message appender class to be used with log.git package.   This might entail writing a configurator class as well;  that depends on the investigation of how configurations/appenders are used.
0,Write Unit test for new DM message appender class,"Write unit tests for DM message appender class.  This might also require some tests for a configurator class, if that class is created."
1,Define gather/scatter mechanism for tasks with generic API,"Define gather/scatter mechanism for tasks with generic API.  This will use event services (DM Messages) initially, but we would like to support MPI or other communication mechanisms as well.  This will involve consulting with Paul about how the existing code is structured. "
1,OCS Middleware workshop,Attend OCS Middleware workshop.  This will probably have be done remotely because I have a personal conflict with that time that will prevent me from attending in person.
1,Write example programs for OCS Middleware,"Write some example programs to get familiar with the OCS middleware software. The OCS middleware will later be integrated with the AP, in the base dmcs and replicator jobs."
0,Refine Event base class to allow ActiveMQ filterable settings,"The current Event.cc base class needs to be refined to remove and old-style data release terms that aren't used anymore.  Plus, it needs to be easily extensible to allow other types of dictionaries of terms that will be used in the message headers to make them filterable on the server side."
1,Update tests to use unit test framework,The tests for this package predate the unit test framework that other package use.  Update the tests to uses the unit test framework and get rid of any duplicate  or obsolete tests.
2,Change marshaling code to use json,"The marshaling code for non-standard (i.e., non-filterable) components of messages is custom and not standard.  Change this to use JSON."
3,General cleanup of Events package,"There are some obsolete classes and code in the ctrl_events package that needs to be removed and/or updated.  PipelineLogEvent, for example.   That not only is no longer used, but it is applications specific, and should have been part of another package in the first place, subclassed from this package."
1,ORIGINATORID value can churn too quickly.,"The ORIGINATORID is a 64-bit word consisting of an IPv4 host address, 16-bit process id, and 16-bit local value.   In addition to the 16-bit process id not being standard across platforms (Mac OS >Leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits.  This could be fixed to changing ORIGINATORID to a 32-bit process id and a separate value for the local value, which would be specified together in the DM event selector.   I have to look into this more to see if this is a viable solution.  This might need to go to three separate values to future proof it (i.e., ipv6)."
0,"Install docker 1.1.2 in an ISL OpenStack CentOS  instance, perform basic checks","Install docker 1.1.2 in an ISL OpenStack CentOS 6.5 instance, and perform  basic checks such stopping and starting the docker daemon, changing default settings such as size limit of containers/images,  pulling  standard images from docker hub, starting containers from these images, etc. "
1,Re-arrange how Qserv directories are installed,"we already touched question about installation directory structure at a meeting, maybe we can improve things by re-arranging how things are installed      we are currently installing stuff into four directories: cfg, bin, lib, proxy     to make it look more standard and to avoid clash with qserv source directories we could move cfg and proxy to a different location (something like share/qserv to make it more root-install-friendly in case we ever want to install under /usr)     this change (if you want to do it) deserves separate ticket, do not do it in this ticket "
0,End-to-end demo  fails to exit with the correct status  when Warning would be correct.,"The output  of demo2012 results in an output file which is compared against a benchmarked file. Currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic"";  that  number is currently set to 11.  An example using that setting is: @ Absolute error = 5.9973406400e-1, Relative error = 9.1865836000e-4 ##2566    #:25  <== 29.7751550737 ##2566    #:25  ==> 29.7478269835  Additionally, the current use returns: a 'Fatal' error if the code itself fails to execute correctly; a ""Warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'Success' if the comparison meets all criteria.  It is noted that the buildbot 'Warning' color indicator is not currently being displayed when the comparison fails. That is a coding error.  This Issue will: * ensure that BUILDBOT_WARNING(S) causes the correct display color when appropriate.   This Ticket has been split into two parts. This is part 1.  Part 2 is DM-1379 "
0,improvements to PsfFlux,"While using it as an example for the redesign of DM-829, I came up with some ideas for how to improve PsfFlux's handling of edge/bad pixels:  - Add a flag indicating that at least one pixel was rejected  - Add thresholds (number? fraction of PSF model size?) for how many non-rejected, non-edge pixels we need to even attempt a fit."
0,Fix minor loose ends from new result plumbing,"Some of the more minor issues raised in comments from DM-199 were left undone prior to merging. This ticket addresses them.  For more information, please see:  https://github.com/LSST/qserv/pull/2 . "
0,Edit pull interface and other Summer 2014 work into LSE-68 in Word,"Deliverable: circulate a Word-based draft of LSE-68 in which the ""push"" interface is removed, and the ""pull"" interface is refined to include the guider and other Summer 2014 work.  Note that the use of ""pull"" for the guider applies whether or not the proposed guider redesign is accepted."
0,Avoid use of ~/.my.cnf (used by css watcher),"See https://jira.lsstcorp.org/browse/DM-1258?focusedCommentId=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29230.  {{~/.my.cnf}} is used by css watcher, an optional tool used for monitoring css. It is a symlink to $QSERV_RUN_DIR/etc/my-client.cnf.  The css watcher could use MySQL credentials located in ~/.lsst/qserv.conf (used by integration tests wich are a Qserv/MySQL client)"
1,"replace ""bad data"" flag in SdssCentroid","SdssCentroid has a ""bad data"" flag that doesn't actually convey any information about what went wrong.  This should be replaced with one or more flags that provide more information."
0,Errors in testHealpixSkyMap.py,"There is a failing unit test when healpy is supplied.  The problem is that the method boundary is not defined in the version of healpy we supply for the sims, however boundaries does exist.  If I replace boundary with boundaries, the test passes."
0,Create Docker Image / Dockerfile for LSST Stack for CentOS6.5,Make a Dockerfile for systematic generation of docker images using a Centos6.5 base image  containing the LSST Stack (v9_2 at the moment) and library dependencies.
0,Ensure that the partition package is C++11 clean and compiles on OSX 10.9,"The LSST buildbot infrastructure recently changed to building everything with --std=c++0x, which broke the partition package, and hence automated Qserv builds. While debugging this, I discovered that the partition package does not build on OSX 10.9, and considering how minimal its dependencies are, it really should. The OSX issue can be fixed by avoiding {{using boost::make_shared}}.  The partition package should be cleaned up to avoid all use of {{using}}. If we decide to use C++11 in Qserv, then the codebase should also be modernized (in particular, there are use-cases for static_assert, nullptr, etc... ). "
3,"Develop expanded and updated Information Security Program from NSF guidance/templates, using existing documents (e.g. LSE-99) as starting point ","Develop expanded and updated Information Security Program from NSF guidance/templates, using existing documents (e.g. LSE-99) as starting point.  Create master document and subordinate documents, allowing for existing AURA, site, and institutional polices to be incorporated as appropriate.  JK: In PMCS this is Petravick D and Ephibian (LeClair L)"
3,Investigate deblending in one band followed by using the resulting templates in all bands,"(See also [HSC-1025| https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1025])  One poor-man's approach to deblending multiple bands and visits is - Deblend in one band (or a combination, e.g. chi^2 band) using an SDSS-like algorithm that produces templates for each child - Take the templates (or possible model fits to those templates) and use them to deblend the objects in all other bands  It will probably be necessary to include undetected objects in this fit (cf. [HSC-1023|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1023]), using point-source models.  Note that this does not allow for the different seeing in each band.  This is not obviously catastrophic for reasonably wide blends as the total flux in each pixel is conserved, and if only one template is important near an object's centre the exact form of the template is unimportant."
3,Investigate deblending in one band followed by linear fits of models,"(Cf. [HSC-1024|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1024])  One poor-man's approach to deblending multiple bands and visits is  - Deblend in one band (or a combination, e.g. chi^2 band)  - Fit models in that band  - In all bands separately fit those models simultaneously to all object in the blend, allowing only a minimum of coefficients to float (ideally only amplitudes, but given the realities of real galaxies the bulge and disk will need to both be fit, or the Sloan Swindle components, or the Lensfit Swindle, or ...).  The correct PSF for each image would be used.  It will probably be necessary to include undetected objects in this fit (cf. [HSC-1023|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1023]), using point-source models.  It is not clear how much of a poor-man's solution this actually is, or whether it will work quite well."
1,Merge Footprints from different bands/epochs,"The current concept of the deblender assumes that the inputs are  - A merged set of Footprints that define which pixels are part of the blend  - A merged set of Peaks within that merged Footprint  Please generate these merged Footprints (which will be defined in (x, y) coordinates in the tract/patch coordinate system). "
1,Generate a master list of Objects given detections in multiple bands/epochs,"Once we've detected Sources in multiple bands we need to merge the positions to generate Objects.  This is a little complicated (or at least messy):  - The positions have errors  - If the seeing is different in different visits, objects may be blended in some but not all exposures  - If we use more than one detection pass (e.g. smoothing when looking for faint objects, not smoothing for bright) this has similar-but-different consequences (but we should probably deal with in the per-band processing)  - Objects move, so even if the positions are within the errors the motion may still be detectable "
0,Submit LCR for LSE-68,"Create an LCR, including a summary of changes, for LSE-68."
0,Edit LSE-68 changes into EA,NULL
1,Improve output from integration tests,"Currently the integration tests print pages and pages of output, it is hard to see what failed and how. It'd be nice to clean that out, and instead, print some short summary, perhaps in a form of a summary table what queries has run, what succeeded, what failed etc. HTML format might be a reasonable way to display it."
0,Eups 1.5.4 requires each new shell to source the eups setups.sh,"eups v 1.5.4 requires each new shell to source ...eups/../bin/setups.sh.  This requires the buildbot scripts: runManifestDemo.sh, create_xlinkdocs.sh, be updated to individually  do that task.  Add  demo2012: bin/demo.sh . "
2,"Reimplement CSS using JSON-packing, czar kazoo + c++ snapshotting phase 2","The v1 implementation (DM-1249) transitions the zk access from the c++ layer into python, with code that understands how to unpack json-encoded data. This can coexist with the existing qserv_admin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qserv_admin. v2 scope will be defined further, once DM-1249 enters review.  Update: Phase 2 includes removal of c++ zk code and unification of Qserv css modules into a single place."
0,Design CSS schema to support database deletion,"Need to implement deleting databases. Deliverable: a design of the system that will be capable of deleting a distributed database including all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later."
0,Improve documentation of pixel systems in obs_lsstSim,"There is not much documentation of the coordinate systems in use by CameraGeom.  This is by nature documentation that is instrument specific, so should go in the obs_ package for each instrument."
0,"Create suite of Dockerfiles / docker images for LSST Stack for ubuntu, CentOS","Building on issues DM-1317 and DM-1375 where initial images and Dockerfile's were constructed, we can now use these Dockerfile's as prototypes to extend the set of Dockerfiles & images.  We observe that by making  simple (scriptable) edits to the initial Dockerfile, we can run 'docker build' to make docker images for several combinations of OS and base compiler gcc version."
1,Prepare a SL6x openstack image for with Qserv development environment,Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests.  IN2P3 Openstack platform offer next virtual machines :  {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name              | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1  | m1.tiny           | 512       | 0    | 0         |      | 1     | 1.0         | True      | | 15 | cc.windows.small  | 4096      | 20   | 0         |      | 2     | 1.0         | True      | | 16 | cc.windows.xlarge | 8192      | 50   | 0         |      | 4     | 1.0         | True      | | 2  | m1.small          | 2048      | 10   | 20        |      | 1     | 1.0         | True      | | 3  | m1.medium         | 4096      | 10   | 40        |      | 2     | 1.0         | True      | | 4  | m1.large          | 8192      | 10   | 80        |      | 4     | 1.0         | True      | | 5  | m1.xlarge         | 16384     | 10   | 160       |      | 8     | 1.0         | True      | | 6  | cc.lsst.medium    | 4096      | 20   | 40        |      | 2     | 1.0         | False     | | 7  | cc.lsst.large     | 16384     | 20   | 160       |      | 8     | 1.0         | False     | | 9  | cc.lsst.xlarge    | 40000     | 20   | 160       |      | 20    | 1.0         | False     |  cc.lsst.xlarge would allow a quick build/test of new Qserv release.
0,Add return code for integration tests,"Integration test have to returns non-zero when failing. This will ease use of CI or debugging tools ({{git bisect}}, buildbot)."
1,Stand up netem server with Dell PE 1950,NULL
0,Create persistent volume of Cinder block storage and attach to instance,"We create a persistent volume of Cinder block storage and attach to working instance.  When it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance.  We want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed. "
0,Document how to switch Qserv or dependency in eups,"DM-1338 may allow to switch Qserv or dependency version using eups, without having to re-configure Qserv (i.e. without any change in QSERV_RUN_DIR).  There may have conditions for this to work. That's why it's reasonable to get some user feedback on DM-1338 before documenting this feature."
0,Add return codes for qserv-check-integration.py and qserv-testunit.py,NULL
1,Improve Qserv Configuration Procedure,"Based on input from review of DM-1338, and discussions at hangout https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29: implement -keepdata option.  * --keepdata will preserve qserv_meta.conf and my.cnf. Regarding the latter, this is necessary because user might have customized data_dir. * The structure of qserv_meta.conf and my.cnf must remain the same between existing version of Qserv and the to-be-configured version of Qserv. It is users's responsibility to ensure the structure did not change. Note in particular, the template files (with the exception of my.cnf) should not be customized"
0,Process sample sdss data with LSST Stack in a Docker container in OpenStack,"Process sample sdss data, starting with the lsst_dm_stack_demo, with LSST Stack in a Docker container in an OpenStack instance. "
2,SUI define local hardware needs to host the test DB and application servers,"We need have Qserv and the test DB in IPAC to do development and test locally, to access the  data through Qserv APIs."
1,"SUI install Qserv and test DB, enable access to catalogs for SUI team members",Install Qserv and test DB in local hosts.  Enable SUI team members to access the catalogs. 
2,State diagram for jobs in progress,Build a state diagram showing job progress throughout a run.
0,afw tests use the same name for a dummy file: test.fits,"The afw package  uses a file named: test.fits in multiple testers.  If the user sets up the build to use multiple CPU (-j #), then there is the risk that the shared filename will be affected by more than one tester at a time.   In the case which provoked this Issue, the tester: testSimpleTable.py, reported that the file was missing.  A simple rerun managed to get past the error.  I recommend that the different testers use uniquely named demo files."
1,Github Transition: Naming conventions for repositories,"The Simulations team has requested that repos in general and the numerous DM repos in particular are prefixed in a way that would make fitering them out of searches and display easy (for example, their repos are prefixed sims_*)  This would be an evident useability aid to DM developers and outside contributors too.   Obtain a decision on how to allow users to quickly isolate repositories they are interested in. "
1,Github Transition: Storage of large data files,Github currently has fixed 100MB per blob or 1GB per repo limits. Sims has at least one file in its repo whose history exceeds this (sims_meas) and this has been raised as an issue before.  Obtain a decision on a suitable way forward for the time being that would allow the upload of the repositories on Github. Experienced to be reviewed in a few months time to consider whether has proved satsifactory.
1,"Github Transition: Stash-stored pull requests, extraction",Extract comments from Atlassian Stash made during pull requests /code reviews and their associated SHA1s if it all possible.   
0,"Github Transition: pull request discussion, retention - proof of concept","Store review comments / PR discussion into relevant git repositories  Code & process to do this on a continuous basis post-transition will be a different issue, "
0,Test absence of individual components,"Test absence of individual components of the AP simulator.  Bring each down, run the system, and restart just those components to see if the system still operates as expected"
0,check if Qserv compiles with C++11,NULL
1,Fine tune czar and worker database initialization,"For now, Qserv databases are the same on the master and on the worker. It means that czar tables are created on the worker and vice-versa.  This ticket aims at creating the right tables at the right places, and also at sharpening permissions on these tables.  This is done by splitting configuration script qserv-czar.sh, which configure a mono-node instance, in two scripts: - qserv-czar.sh, for configuring the czar - qserv-worker.sh, for configuring workers.  see https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29  Please note that this code can't be validated with mono-node integration tests (whereas it doesn't break them): indeed it requires a mono-node installation with 2 MySQL instance, one for the worker and one for the master. Updating the mono-node configuration and integration tests with such a feature would make them far more complex.  A quick and dirty, hard-coded, testbed is available in u/fjammes/DM-1446-test. It has been used successfully to test this ticket.  This ticket will also be validated during next Qserv install on in2p3 clusters."
3,Improve spatial-selection flexibility by parsing ptInSphBox-like syntax instead of qserv_areaspec_box,"Note: it is not clear that we should do this.  This is an idea to change the syntax for spatial area selection. Currently, we have SELECT...FROM...WHERE qserv_areaspec_box(...) ...  This forces the area selection to apply the box (or appropriate) cut on the tables referenced in the FROM list that are partitioned, with those columns. It would be nice to allow spatial selection based on tables (and columns?) explicitly specified by the user, and then compute the appropriate chunk coverage and WHERE clause conditions and predicates. This is a pretty big deal, and would affect all spatial-select user queries, though the compatibility can be mitigated by supporting both syntaxes."
0,Move code for mock images into afw so it reusable.,"There is some code in the exampleUtils in $IP_ISR_DIR/examples that could be of wider use.  Specifically there is code to generate mock darks, flats, and raw data from a mock camera.  There is also code to generate a mock dataRef.  It could be used more widely if moved someplace else.  Russell suggested afw.cameraGeom.utils."
0,newinstall.sh should check that python2 is available,The standards now suggest using: {code} #!/usr/bin/env python2 {code} instead of {code} #!/usr/bin/env python {code} This doesn't work with (at least) Mac OSX 10.9 system python.  newinstall.sh should check if this works and suggest a fix (creating a symlink).
0,Detect lua version in admin/templates/configuration/etc/init.d/mysql-proxy,NULL
0,Add data versions to Zookeeper,"Track versions of data inside zookeeper, and detect from Qserv code if Qserv code is compatible with given format of data."
1,LOE - Week ending 10/31/14,NULL
1,LOE - Week ending 10/24/14,NULL
1,finish porting meas_algorithms unit tests,NULL
1,C++ Redesign -- Result definition for custom algorithms,Additions to Jim's redesign to make it easier to define custom results.
0,Add NaN check to PixelFlags,The test of PixelFlags in measureSources.py (from measAlg) requires a check to be sure that the center inputs are not NaN.
0,SdssShape shiftMax config item is being ignored,The code we ported from meas_algorithms sets the maxShift to 2 without regard to the config item which is supposed to set that value.
0,Design Review prep for C++ redesign,Write up the design developed on DM-829 and push it through review.
0,Github Transition Plan: Reverse mirror for beta-tester repositories,"Test the reverse mirror for beta-testers.  Straw man:  1. Break the mirror for anybody beta-testing github workflow 2. Mirror back to new gitolite area: ""mirror"")  Method:  https://help.github.com/articles/duplicating-a-repository/  "
2,Rewrite obs_test,"I want to use obs_test to test processCcd, but it contains no imaging data. I will replace the current data with new imaging data (a portion of one LsstSim CCD), add a camera and rewrite the mapper (subclassing CameraMapper).  This also requires rewriting the unit tests in obs_test and in one other package that uses obs_test."
1,Execute multi-platform lsst_dm_stack_demo test with Fig orchestration,"In DM-1431 we demonstrated processing of sample data within Docker containers within an OpenStack instance.  Data was processed for containers based on CentOS6.5, Ubuntu 13.10, Ubuntu 14.10.  We consider a multi-platform ""testing"" scenario, where we process the sample data in numerous containers based on various platforms/OSs all simultaneously on an OpenStack instance.   This scenario entails starting up and managing multiple Docker containers.  Fig (http://www.fig.sh) is a new tool for starting up and managing services via Docker containers.   It is a common use case with Docker to have individual components of an application each run separately in a container (resulting in numerous  containers running on a node or cloud env, i.e., a 'Docker stack'), with the containers having linkages/dependencies to be managed. Fig may be used to encode the relationship between the containers in the Docker stack, and to start up such a set of containers.  We install and examine Fig, and apply Fig to our multi-platform ""testing"" scenario.   "
0,Fix 2014_09 documentation,Replace {{NEWINSTALL_URL=http://sw.lsstcorp.org/pkgs/}} with: {{NEWINSTALL_URL=http://sw.lsstcorp.org/eupspkg/}}  and {{eups distrib install qserv_distrib 2014_10.0}} with:  {{eups distrib install qserv_distrib -t 2014_10}}
1,Secure MySQL root password in configuration templates,"MySQL password in written in multiple file during configuration procedure. One single file (QSERV_RUN_DIR/tmp/my.cnf) should be used, and removed at the end of configuration procedure. qserv-meta.conf also contains MySQL password and should be also secured (move password to qserv-configure.py cmd line?)."
1,Study fig to manage Qserv cluster,http://www.fig.sh/  coordinate with @GregDaues who is also investigating fig.
1,Buildbot master takes exception when exiting from mail notifier after dynamic email sent.,Buildbot master exits without posting the required statically-addressed email notification if a dynamically-addressed was sent.   This fix needs to ensure that the required (by buildbot specification) static email is sent even if it has to be directed to a dead-letter box (which it is).
3,Discover/learn what others are doing in astronomy software,"Attend the annual  ADASS conference to keep up with the software development in the astronomy community.  Trey Roby, Tatiana Goldina, Xiuqin Wu plan to attend the ADASS 24."
2,SUI: study other plot packages,Firefly uses clientsidegchart at https://code.google.com/p/clientsidegchart/ for XY plot. This package has not been updated since 2010. We need to find out more about other plotting packages and consider if we need to switch. The candidates are:  1. flot plotting library for JQuery. http://www.flotcharts.org/ 2. Highcharts JS at http://www.highcharts.com/
1,Finalize the Design of Query Metadata,Fine-tune the design outlined in DM-1251 (metadata for capturing information about running queries in Qserv.).
1,Modify czar to interact with query metadata,"Modify czar code: it should interact with the query metadata (store information for long running queries, retrieving)."
3,FY18 Design and Implement Query Cost Estimate,"Design and implement system that will estimate query cost. In particular, we will need to know if the query is interactive, or should be scheduled on shared scan, which shared scan etc. Estimating cost will likely involve looking at number of chunks involved, number of joins, and complexity of math operations. Consider extending API and allow users to specify a hint. "
1,Add queries on partitioned table for Qserv integration test dataset #3,"Qserv test dataset #3 runs only queries on non-partitioned tables. Adding queries on partitioned table would increase drastically the coverage of these tests.  Please note that a long-term solution could be to launch all the *.FIXME queries and to have a more detailed report. For example define queries which must pass for the integration test to succeed, and test queries which may pass and log their results in a report."
0,Allow newinstall.sh to run in batch mode without installing Anaconda,"Installing Qserv on a cluster without internet access requires newinstall.sh to run in batch mode without installing Anaconda (whose install requires internet access). So Anaconda should have a {{-anaconda=yes|no}}. Usefulness of git {{-git=yes|no}} option, against full batch option answering ""yes"" everywhere could also be studied."
1,Package Qserv mono-node instance in Docker,Learn Docker basics and then package a Qserv mono-node instance.
1,Package Qserv master and worker instance in Docker,Learn Docker basics and then package a Qserv mono-node instance.
0,confusing error message when enabling unregistered items in RegistryField,"pex_config seems to split out this confusing error message when trying to enable (i.e. append to .names) a registry item that doesn't exist: {code:hide-linenum}   File ""/home/lam3/tigress/LSST/obs_subaru/config/processCcd.py"", line 51, in <module>     root.measurement.algorithms.names |= [""jacobian"", ""focalplane""]   File ""/tigress/HSC/LSST/lsstsw/anaconda/lib/python2.7/_abcoll.py"", line 330, in __ior__     self.add(value)   File ""/tigress/HSC/LSST/lsstsw/stack/Linux64/pex_config/9.0+26/python/lsst/pex/config/configChoiceField.py"", line 72, in add     r = self.__getitem__(value, at=at) AttributeError: 'SelectionSet' object has no attribute '__getitem__' {code}"
0,Support new version of newinstall.sh,newinstall.sh now creates loadLSST.bash instead of loadLSST.sh. This has to be taken in account in Qserv automated install script: qserv-install.sh and in Qserv documentation.
0,Reverse image slicing doesn't work as expected,"The lsst.afw.image image-like classes support slicing, but reverse slicing does not work as expected. Here are some examples: {code} from lsst.afw.image import ImageF im = ImageF(10, 10) im[:,:].getDimensions() # is (10,10) as expected im[0:10, 0:10].getDimensions() # is (10,10) as expected im[::-1, ::-1] # should be the data with x and y reversed, but fails with: Box2I(Point2I(-2,-2),Extent2I(12,12)) doesn't fit in image 10x10 im[9:1:-1, :].getBBox() # starts at 0,0, as expected, but is 10x10 instead of 9x10 {code}"
0,Remove unnecessary config/install steps,"Some existing installation configuration steps are not needed any more. For instance, we don't need $RUN/q or $RUN/result any more-- these paths are handled directly by qserv code, so the filesystem lookup shouldn't ever take place.  Completion of this ticket should remove unnecessary steps and concepts from installation, simplifying future maintenance. "
0,C++ Standards Rule 3-9 needs rewrite,"Jim Bosch reviewed Section 3 of the C++ Standard.  He noted this change which I felt warranted SAT clarification:  3-9: This is a confusing conflation of two entirely different concepts: it seems to discourage all global variables, regardless of whether they're in a namespace, but only to discourage free functions when they aren't in namespace. If that reading is correct, I think both are sensible recommendations, but they need to be clarified, and probably split up. If the reading should be that all free functions are discouraged, that'd be a terrible rule we violate all over the place. If the reading should be that global variables are only discouraged when not in a namespace, that needs to be clarified (and IMO it could be bumped up to a complete prohibition).  Refer to: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685 Rule 3.9.   The current Rule states: 3-9. Global variables and functions SHOULD be avoided and if used MUST always be referred to using the '::' operator. _________________________________________ ::mainWindow.open(), ::applicationContext.getName(), ::erf(1.0) __________________________________________ In general, the use of global variables should be avoided. Consider using singleton objects instead. Only use where required (i.e. reusing a framework that requires it.) See Rule 5-7 ( .https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706#C++Statements-5-7 ).  Global functions in the root namespace that are defined by standard libraries can often be avoided by using the C++ versions of the include files (e.g. ""#include <cmath>"" instead of ""#include <math.h>""). Since the C++ include files place functions in the std namespace, ""using namespace std;"", which is permitted by Rule 5-41, will allow these functions to be called without using the '::' operator. In cases where functions are only available in the C include files, the '::' operator must be used to call them. This requirement is intended to highlight that these functions are in the root namespace and are different from class methods or other namespaced free functions. "
0,calling extend with a SchemaMapper should support positional arguments,"Calling {{catalog.extend(other, mapper)}} isn't equivalent to {{catalog.extend(other, mapper=mapper)}} because the second argument is the boolean {{deep}}.  When a SchemaMapper is passed as the second argument, we should recognize it for what it is."
0,sconsUtils fails to identify Ubuntu's gcc,"On Ubuntu 12.04, gcc --version says: {code:hide-linenum} gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3 Copyright (C) 2011 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. {code}  This apparently isn't quite what sconsUtils expected, because it says: {code:hide-linenum} scons: Reading SConscript files ... Checking who built the CC compiler...(cached) error: no result CC is unknown version unknown {code}  Happily, everything seems to work anyway, as the fall-back options for the unknown compiler work fine with this one."
1,Use eups swig instead of system swig during Qserv build,"Qserv build system use system swig, it should be fixed to use eups swig. "
0,"Add support for ""SET @@session.autocommit""","The SUI team is using JDBC connector and queries like  ""SET @@session.autocommit = {0}"".format(switch)  are derailing everything. We should add support for these queries. "
2,"check out FIrefly package, build and run an applicaiton","Get familiar with FIrefly package, build and run an application.  Understand the coordinate grid overlay on an image. "
0,Fix confusing error message,"Selecting from a table that does not exist, e.g. something like:  select count(*) from whdfd  produces a strange error:  ERROR 4110 (Proxy): Qserv error: 'Unknown error setting QuerySession' "
0,"Fix ""stripes and substripes must be natural numbers"" bug","on user side:  {code} ERROR 4110 (Proxy) at line 1: Qserv error: Unexpected error: (<class 'lsst.qserv.czar.config.ConfigError'>, ConfigError(), <traceback object at 0x26b8b90>) {code}   in czar log:  {code} 1114 22:30:10.155 [0x7fafcefab700] ERROR root (app.py:370) - Unexpected error: (<class 'lsst.qserv.czar.config.ConfigError'>, ConfigError(), <traceback object at 0x26b8b90>) Traceback (most recent call last):   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 360, in __init__     self._prepareForExec()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 437, in _prepareForExec     self._addChunks()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 535, in _addChunks     self._computeConstraintsAsHints()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 529, in _computeConstraintsAsHints     self.pmap = self._makePmap(self.dominantDb, self.dbStriping)   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 478, in _makePmap     raise lsst.qserv.czar.config.ConfigError(msg) ConfigError: ""Partitioner's stripes and substripes must be natural numbers.""  {code}  To reproduce:  {code} qserv-check-integration.py --case-no 01 --load {code}  then {code} mysql --port 4040 LSST -e ""select count(*) from Object"" {code}   "
0,Switch readMetadata' return value from PropertySet to PropertyList,"It'd be useful if readMetadata would return PropertyList instead of PropertySet, because in many situations order matters. For details, see discussion in DM-1517"
0,Investigate halos around stars,"Andrew Becker reports on dm-users: {quote} We have been using the LSST stack to reduce CFHT data at UW, and have come across a potential bug in the DM software.  I don't see how this could be something intrinsic to the data, instead it seems like it could be a bug in the software triggered by its application to new data.  I have a how to reproduce at NCSA at /lsst/home/becker/CFHT.  Follow commands in the SETUP file to get an up-to-date version of the stack (b449) and a modified version of ip_isr (a hack to avoid overscan correction) and a private branch of obs_cfht (u/krughoff/ossos_support).  If you run the commands in SOURCE_ME it will run the processCcd on a single CCD, in 2 modes, yielding 2 output directories.  The second call merely sets the bin sizes to be the same amongst all the various background subtractions, but is sufficient to trigger the bug.  A basic script included in the directory differences the 2 output calexps (a straight -=) and ds9 is used to view the diff.  If you pan to e.g. 1845,546 you'll see a feature like attached in the difference (out1 on the left, out2 in the middle, diff on the right).  This can be seen in various other parts of the image, as irregular annuli around bright blended objects.  It suggests to me that during measurement some substamp manipulations are leaking into the original image.  But you can kind of see in the center image that the second run seems to trigger the issue. {quote}  And then: {quote} So to summarize:     export LSSTSW=~lsstsw    source ~lsstsw/bin/setup.sh    setup lsst_apps -t b449    setup -k -r /nfs/lsst/home/becker/LSST/DMS/obs_cfht    setup -k -r /nfs/lsst/home/becker/LSST/DMS/ip_isr    setup -k -r /nfs/lsst/home/becker/LSST/DMS/processFile    cd /nfs/lsst/home/becker/CFHT   will set up your environment.  The following is sufficient to recreate the bug, and create postIsrCCD images for processFile:     processCcd.py input/ --id visit=1612606 ccd=8 --output out1/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True     processCcd.py input/ --id visit=1612606 ccd=8 --output out2/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512   Following are the calls I made to processFile.py (using my modified version of the processFile.git package), and which return images whose difference is exactly 0.  So I don't really get whats happening with processFile, but it doesn't seem to be accepting my command-line config changes.     processFile.py out1/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp1.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False     processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp2.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False  detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512     processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp3.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False  detection.background.binSize=1024 calibrate.detection.background.binSize=1024 calibrate.background.binSize=1024 {quote}"
1,Update processFile to use new measurement framework,"processFile.py uses the old measurement framework in meas_algorithms, but all the other components expect it to be using the new measurement framework in meas_base."
1,Draft security risks into the Center's template,"The Cyber security center has provided a risk template consistent with their templates,  The Center attempted to populate their templates with material from LSE 99  to the templates,   The impedance mis match was too large, The way forward is seem as  attempting a high level decomposition of risk into the templates."
0,Identify potential KVM hardware,Identify potential KVM hardware that would meet our needs.  e.g. a current version of the Avocent or Dell KVMs used at NCSA.
0,Reorganize docker image repositories and align with github,"A heterogeneous collection of docker images have been accumulating within the public docker repository  daues/lsstdistrib . Such a heterogeneous collection prevents the assignment of a ""latest"" tag to allow users to easily obtain the most recent image for a particular item (detailed version numbers, labels currently required.)     Thus we should break out the single repository into multiple repositories where are ""latest"" tag will be effective.  We also make  github repositories of matching names to hold the Dockerfiles which produced images (a common pattern for github/dockerhub usage, especially with automated builds; so we start this practice.) "
1,Test if IPMI can replace KVM,Can we use IPMI in place of a KVM?  Can we reliably do the following across our various server vendors?  - power cycle hung systems  - view and use text console  - view and use remote gui console  - boot from a remote ISO
0,Create a LSST base CentOS image,NULL
0,Document use of LSST image for CentOS,NULL
2,Gather Open Stack needs/requirements from DM team,NULL
0,Assist DM team in accessing test open stacks,NULL
0,Fix qserv_testdata documentation,"qserv_testdata relies on sconsUtils, and its build procedure has to be clearly documented."
0,Add support for mysql JDBC driver,"SUI which rely on JDBC fail because they internally issue some queries that are not yet supported by Qserv. Need to patch it (in the short term), and add proper support (in the long term). This story covers the patching only. The queries that upset Qserv are listed below.   {code} SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect'  SELECT @@session.auto_increment_increment  SET NAMES latin1  SET character_set_results = NULL  SET autocommit=1  SET sql_mode='STRICT_TRANS_TABLES' {code}"
1,Add support for BIT columns,"The mysql/SchemFactory code has incomplete logic for dealing with columns that have BIT type - in particular, it doesn't properly handle the length of a bit field, and it sets the SQL type to ""BIT?"", which causes table creation failures later on. Fixing this requires modifying SchemaFactory, and making sure that bit values are  transmitted properly."
1,Add support for transmitting [VAR]BINARY column data,"The code that pulls data out of a {{MYSQL_ROW}} and puts it into a protobuf {{RowBundle}} does not handle binary data correctly. See [_fillRows|https://github.com/LSST/qserv/blob/master/core/modules/wdb/QueryAction.cc#L217] for the relevant code.  The issue is that the generated {{add_column(const char*)}} member function of {{RowBundle}} treats the input as C-style NULL-terminated strings. But in the case of {{BINARY}} column data (and {{VARBINARY}}/{{BLOB}} variants, maybe also {{BIT\(n\)}}), the contents can contain embedded NULLs. We are currently using such columns for user defined types in MySQL (e.g. image bounding polygons), so it's important to get this right. On the protobuf side the fix is as simple as calling {{add_column(const char* value, int size)}} instead. I'm not a MySQL C API expert, but the size will presumably have to be obtained/derived from the corresponding {{MYSQL_FIELD}}.  "
2,Drawing speed will detect and optimize for large datasets,NULL
0,Finished Background conversion,NULL
3,Set up GIT hub for ipac firefly,NULL
1,Span-based shrink operations for Footprint,"Analogous to DM-1128, but shrinking rather than growing footprints."
0,"Install docker in an ISL OpenStack Ubuntu instance, perform basic checks",NULL
3,Prototype HTM-based spatial binning to visualize large number of catalog sources,"Currently we are using a generic 2-d binning algorithm, that is finding minimum and maximum values of the two columns to be visualized and bins the values into 2-d grid with the specified number of grid cells.  This algorithm distorts data in the pole regions and whenever data are on both sides of ra=0. More smart binning based on a spatial index is necessary when reducing the number of (ra,dec) entries intended for visualization.  "
2,Resolving QServ database configuration/connectivity issues,"To start the development I should be able to connect to the QSERV development database via VPN and run simple queries.   Ideally, I'd like to be able to connect to QSERV with JDBC, view the data, run spatial queries. I also need an access to data dictionary to interpret data."
2,Evaluate SDSS catalog access,"SDSS allows two ways to access their catalog data: via HTTP Post service, accessible to anonymous users, and via CasJobs services, which require having an account. Evaluate how SDSS catalog data can be accessed from an application by prototyping single and multiple target searches accessing SDSS services. "
3,FY17 Add Support for User Upload Tables for Qserv,"Users will need to be able to upload a list of ""things"" to workspace, then request ""repeat a given query for each ""thing"" from the uploaded list"". Example of ""things"": ra/dec points, ra/dec points + distances, object ids, object names, bounding boxes, etc. In IPAC terminology, this is called ""table upload queries"". Note, we will need to assign some sort of unique id to such list. Results must contain information which result correspond to which ""thing"". E.g, if user asks for neighbors near (1,1), (4,4), it is not enough to just return list of neighbors, we need to tell which neighbor is for which point.  In Qserv, that means that user will upload a special table with these ""things"", and then issue a query that involves join against that table.   We need to evaluate and agree on how generic these upload tables can be.  We might need to do some special optimizations (e.g., related to spatial searches). "
3,FY17 User Upload Tables for ImgServ,"Users will need to be able to upload a list of ""things"" to workspace, then request ""repeat a given query for each ""thing"" from the uploaded list"". Example of ""things"": ra/dec points, ra/dec points + distances, object ids, object names, bounding boxes, etc. In IPAC terminology, this is called ""table upload queries"". Note, we will need to assign some sort of unique id to such list. Results must contain information which result correspond to which ""thing"". E.g, if user asks for neighbors near (1,1), (4,4), it is not enough to just return list of neighbors, we need to tell which neighbor is for which point. In Qserv, that means that user will upload a special table with these ""things"", and then issue a query that involves join against that table.  We need to evaluate and agree on how generic these upload tables can be.  We need to decide on format / location: csv? Table in database?   We might need to do some special optimizations (e.g., related to spatial searches)."
2,A better coordinate grid overlay,Some of the grid lines are not drawn right and the labels are not in the locations.
2,Research Javascript Frameworks: General Overview,"Begin looking into JS frameworks. Start to look into AngularJS, try to write some sample code.  Attempt to understand the main concepts. Gat an overview of the others out there.  "
2,Work with the database group to define initial concepts for backend interface API ,We are starting to gel around some ideas. 
1,Improve management of integration test datasets description,"Currently data set description (i.e. data file extension, compressed extension, schema file extension) is hard-coded in python/lsst/qserv/tests/datareader.py  This could be improve (standard format for test dataset, meta service, meta-configuration file, ...)"
1,Implement drawing only active tab with new data model,NULL
1,LOE - Week ending 11/07/14,NULL
1,LOE - Week ending 11/14/14,NULL
1,LOE - Week ending 11/21/14,NULL
2,LOE - Week ending 11/28/14,NULL
1,Create integration test case using data duplicator,Integration tests should provide a new test case which use sph-duplicate in partition package.
0,Setup Qserv for SUI tests,Setup Qserv on lsst-db2 with and load some reasonable data set (perhaps PT 1.2). One potential caveat: we need to setup access for some accounts that are ideally other than our internal qsmaster.
1,Test deblended CModel colors,"Using HSC data, examine color-color and color-magnitude diagrams with deblended CModel magnitudes.  Investigate outliers by looking at images and deblended HeavyFootprints."
1,Basic validation of LSST pipeline on HSC data,Get the pipeline running on HSC data to the point where nothing is obviously wrong.
1,"add support for ""freeze-drying"" measurement failures","We should make it easy for users to inspect and capture problems in measurement algorithms into an on-disk format that allows them to be reproduced later with minimal setup (ideally, the package would require no access to the original data or configuration).  While this issue is mostly concerned with capturing problems in measurement algorithms, an extension to capture deblender failures should be considered as a future extension."
0,Support Mac OS in scisql-deploy.sh,cf. Andy Connolly message: {quote} Just as an FYI on my mac scisql-deploy.py was looking for 0.3.4/lib/libscisql-scisql_0.3.so but there is only 0.3.4/lib/libscisql-scisql_0.3.dylib {quote}
2,Sanitize AstrometryTask interface,Currently the AstrometryTask and Astrometry class share work.  E.g. distortion is done in AstrometryTask but matching is done in Astrometry.  AstrometryTask also makes assumptions about what fields are available in the solver config.    The AstrometryTask interface should be sanitized so that it can be used as a thin wrapper for calling any astrometry solver.  Top level config params should go in the AstrometryTaskConfig and solver level work should be done in the solver class and configured at the solver level.
1,Rework Astrometry class,"The Astrometry class shares information upstream with AstrometryTask.  This should be factored out so that the Astrometry class can be configured and called via a single well known method (solve?).  One thing the Astrometry class that is needed by down stream processing (PhotoCal) is to match sources.  This is currently a private method, but should be made public so that  it can be used without running AstrometryTask."
0,Move photocal out of meas_astrom,It is confusing that photocal is in meas_astrom.  I assume that is historical.  I think it could probably live in pipe_tasks.
1,Implement replacement for A.net index files,"Astrometry.net index files are hard to generate and hard to read.  We need another, more flexible, more standard system for storing reference files.  We should also be able to read FITS files and other formats, but having a standard format with the utilities to create and query them is a must.  Coming up with a format that satisfies astrometry.net's solver may be too hard, because a.net requires quads, which a non-blind solver may not need. However, a format that we can convert to something suitable for a.net would probably suffice (conversion would presumably be a separate task that is run once).  It will be easier to identify a suitable format once we have identified at least one solver other than than a.net that we wish to implement or adapt. hscAstrom appears to use a catalog of star positions, which is nice and simple."
2,Implement a replacement solver to the current A.net solver,"This should be further refined.  The solver will be required to work with several input formats.  It will only be required to solve in the in the presence of a reasonable starting point with reasonable pointing errors.  Failure should be graceful.  If multiple, equivalent solutions are found, this should be reported (for situations including perfect grids of sources)."
0,Qserv spatial restrictor names are case sensitive,"The SQL grammar treats Qserv spatial restrictor names case insensitively, but {{qana/QservRestrictorPlugin.cc}} does not, which means that one must use e.g. {{qserv_areaspec_box}} rather than {{qserv_areaSpec_box}}. We are loose with case in a lot of our wiki pages, so we really should fix this to avoid confusing users. Also, case insensitivity is consistent with MySQL behavior for UDF names."
1,Research how to integrate different image metadata stores with DataCat,"Data Release Production will generate highly structured image metadata (exposure* tables). If we decide to use DataCat (e.g., for keeping more ad hoc metadata), the question arises if/how to integrate all this together: * should we integrate all exposure* tables from all releases into DataCat? (eg via foreign tables) * should we keep them distinct, and integrate at higher level (e.g., Metadata Service)"
1,Design system for tracking existing images/files,"We have a lot of files/images already brought in or generated through Data Challenges done to date. We need a system for cataloging them. This story will define such system, eg, sketch of UI, underlying technology used (DataCat, plain mysql, schema etc)."
0,Define structure of web form for collecting metadata about existing data sets,Web alpha version of the form (using django or Fermi Java webservices code) that collects input from users about data repositories. Authentication not covered in this version.
1,Implement FITS header crawler and integrate it with the form,Implement crawler that walks through registered repos (through the form) and loads metadata from fits headers into the mysql backend
1,Research and experiment with building form for capturing user input,"Need to build a form that will be used to capture user input about existing image repositories (users will be registering their files/repositories). Options to consider: python-based django, java-based system that is part of Fermi DataCat. "
0,Break down & discuss DM-1074,"I will take the lead on DM-1074. First step will be to sit with [~jbosch], get a feeling for what needs to be done, and sketch out a set of stories."
0,Convert test_qservAdmin.py into a real unit test,"Need to turn ./admin/tests/test_qservAdmin.py into a real unit test. In the past it was broken and it went unnoticed, see DM-1395"
0,Remove check for stack dir write access in qserv-configure.py,"qserv-configure.py checks for write access to stack dir, this should be replace by check for read access, or removed.  fjammes@qserv-build-server-xlarge:~/src/qserv$ qserv-configure.py --all INFO: Qserv configuration tool =======================================================================  WARNING : Do you want to erase all configuration data in /home/fjammes/qserv-run/2014_10.0 ? [y/n] y INFO: Copying template configuration from /home/fjammes/stack/Linux64/qserv/2014_10.0/cfg/templates to /home/fjammes/qserv-run/2014_10.0 INFO: Creating meta-configuration file: /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Reading meta-configuration file /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Defining main directory structure INFO: No write access to dir /home/fjammes/stack/Linux64/qserv/2014_10.0 : [Errno 13] Permission denied: '/home/fjammes/stack/Linux64/qserv/2014_10.0/write_tester' "
2,Research Javascript Frameworks: Understand Angular & React,Write some prototype code with Angular and then try to do the same thing with React
1,Clean up multi-component Footprints,"Following the landing of DM-1545, it's possible for an erosion operation on a footprint to cause it to split into multiple components and for peaks which were previously inside the footprint to not fall inside any of those components.  Here, we should provide a ""clean up"" operation that takes a multiple-component footprint and splits it into a set of contiguous footprints with appropriate peak lists."
0,init script fails to start xrootd after crash,"I think we saw this issue in the past, not sure it was actually fixed back then or just was to reintroduced one more time.  After crash of xrootd the regular {{etc/init.d/xrootd start}} fails to start it: {code} [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists                         [FAILED]   see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd start Starting xrootd: (already up)                              [  OK  ] [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists                         [FAILED]   see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid {code} I can start it with {{restart}} but I think that {{start}} should detect that xrootd is dead ({{status}} does that) and start service correctly.  This issue may exist for other services but I did not check. "
2,Design of calibration and ingest system,Produce a confluence page describing the approach to be taken to solve DM-1074. Ensure that all the relevant folks have reviewed that page and are happy. Break down DM-1074 into stories appropriate to that design.
0,Determine if Astrometry class is desired,"The question is whether the Astrometry class is the thing that is overridden or if the AstrometryTask has component configurables that are overridden.  Also, determine location default implementation."
0,Add support for c-style comments in front of queries sent to qserv,"Connecting to qserv from java fails because the jdbc driver inserts comments. ""/* ... */"" in front of queries (example pasted below). The fix involves removing the comments at the proxy level   {code} SQLException: Qserv error: 'ParseException:ANTLR parse error:unexpected token: /:'  Query being executed when exception was thrown: /* mysql-connector-java-5.1.34 ( Revision: jess.balint@oracle.com-20141014163213-wqbwpf1ok2kvo1om ) */SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' {code}"
0,Prevent collisions in /tmp related to scisql deployment,Deploying scisql involves creating a file in  /tmp. The file never gets removed. This can cause the following error when Qserv is installed later on the same machine: {code} ERROR: failed to open output file /tmp/scisql_demo_htmid10.tsv for writing {code}  We should switch to using a temporary file instead of file with fixed name.
0,Qserv scons scripts do not pick up the version of swig provided by eups,"See the summary. The consequence is that the Qserv integration tests fail on systems that provide swig 2.x+ - there seems to be some implicit dependency on SWIG 1.x. The reason may be that swig is getting confused about shared_ptr to objects that are defined in one module, but used in another (recent swig reorganization split czar and css into two separate swig modules)."
0,Sanitize configs,"Some solver specific information is stored in the AstrometryTask config.  Further, the solver config is accessed inside the AstrometryTask run method.  This mixing of information make it hard to make the framework pluggable.  Solver configuration should be confined completely within the solver class (whether it be part of the Astrometry class or a configurable of its own)."
0,Add unit tests to test c-style comments in/around queries,I should have thought about it when doing DM-1601 but I didn't... it'd be good to add test queries that test comments before/after/inside query.
0,Move meas_algorithms unit tests to meas_base framework,The following tests in meas_algorithms need to be ported to the meas_base framework:  measure.py psfSelectTest.py testPsfDetermination.py ticket2019.py testCorrectFluxes.py (though this cannot be done until the algorithm exists)
1,Research off-the-shelf solutions for harvesting metadata,Relevant links:  * http://www.ivoa.net/documents/SIA/20141024/index.html - there’s a list of metadata in the document  * CAOM (Common Archive Object Model): http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/caom2/
1,Integrate Metadata Store prototype v1 with cat and db  ,"Integrate prototype developed through DM-1255 with cat and db as appropriate (don't hardcode schema, don't hardcode credentials, etc)."
2,Experiment with DataCat foreign tables,"This is a placeholder story, we should break it down into more smaller stories..."
0,Add support for mysql JDBC driver (v2),"MySQL client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in DM-1539 are not sufficient."
1,Design and implement CSS structure for distributed Qserv setup,For management of the distributed databases/tables we need info in CSS about all workers and tables. The info will be created by data loader and updated by replicator which do not exist yet. For this issue we need to provide python API which can fill the same information in CSS so that we can build and test other pieces needed for this epic.
1,Implement remote host access for management framework,To manage remote workers we need a way to access services on remote machines that run workers. Services may be something like mysql (which we would prefer to run without TCP port open) and optionally xrootd. This ticket will implement Python module which will hide a complexity of doing things like ssh/port forwarding/authentication from the client.
1,Client library for accessing distributed database/table information from CSS,"Provide Python interface for accessing information in CSS which is relevant to distributed management, such as database/table/node data. This interface can be used also by data loader and replicator."
2,Implement distributed database creation,Implement Python library which creates databases on all active workers based on info from CSS.
2,Implement distributed table creation,Implement Python library for creating mysql tables on all active workers.
0,Move CSS documentation close to code ,"CSS documentation about the structure is currently in trac (which is readonly), at https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS. We need to move it close to the source code, e.g., to a doc file."
0,Add unit test to to verify zookeeper versioning works,"This is a follow u pto DM-1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled."
0,Add unit test for queries from DM-1539,Need to add unit test for queries listed in DM-1539
0,Qserv should report it's version,"It should be possible to determine which version of qserv we are running by looking at information from log files. So, in practice, we probably should generate in scons a unique id (from sha from git?) and compile it into the code."
0,SciSQL should report its version,It should be possible to determine which version of scisql we are running.
0,Build 2014_12 Qserv release,The title says it all. Please open ticket for next release when closing this one.
3,"Integrate metaserv, imgserv and dbserv with Data Access Services","* Create dbserv for handling database related web-based queries, support running queries through web (initial version).  * Integrate dbserv, metaserv and imgserv RESTful API into the webserv - want to run all services (meta, image, db) under one server.  * Also want to be able to run them separately, so have some handy servers for each service * Proof of concept for supporting multiple formats (html, json)"
0,Adopted/Retired RFCs are not counted as resolved,"Also, marking an RFC as Adopted brings up a box with a message applicable to the Closed status."
0,New RFCs should result in dm-devel E-mails and HipChat postings,Email notices of new RFCs filed with a DM component go to dm-devel Email notices of new RFCs filed with a DM component go to Data Management chat room Email notices of all new RFCs go to Bot: RFC room 
0,Build 2014_11 Qserv release,"The title says it all. Please open ticket for next release when closing this one.  Tasks to do:  - publish last buildbot build under a temporary eups-tag (""qserv-dev"")and test it, if it works fine: - create git-tags for Qserv and dependencies - publish the release with eups-tags ""qserv"" and ""YYYY_MM"" - generate and publish the doc for release ""YYYY_MM"" - update release number in Qserv code and set ""YYYY_MM+1"" as release in dev and ""YYYY_MM"" as stable release (update {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}}) - generate and publish the doc for release ""YYYY_MM+1""  - look at the doc - commit  {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}} with current ticket number   This procedure should be validated and documented."
1,Update build process for Firefly opensource,Update build and deploy related scripts to reflects the changes affected by open sourcing of Firefly.
0,Remove redundant CORS headers from Firefly's http response,Make sure CORS related headers are not sent when the Origin header is missing.  Firefox does not like it.
1,Research popular web development technologies,Research popular web development technologies to prepare Firefly for the future with the focus on front-end framework. 
1,"Document ""getting started"" procedure for new stack developers","Document a procedure for building the stack on a new system in a way that is appropriate for both project members and external contributors.  This can be linked as a ""getting started"" guide from http://dm.lsst.org/."
1,LOE - Week ending 12/5/14,NULL
1,preparation work for FIrefly open source,"1. discuss with IPAC director and managers to open source Firefly 2. study the major open source license,Apache, GPL, BSD 3-clause, MIT.  3. final decision: BSD-3 clause"
1,Use an OpenStack instance to run an HTCondor Central manager,Use an OpenStack instance to run an HTCondor Central manager
3,W16 Research technologies for Data Access,"Research technologies potentially useful for Data Access / Database. This epic covers Apache Mesos, Google Kubernetes, MaxScale, Serf, Consul, and MemSQL."
3,The existing FITS reader class needs to be refactored to improve the performance(1),#NAME?
1,Extend data loading script to support multi-node setup,"Implement simplest use case for data loading with one or more worker database separate from czar database. Simplest means minimal functionality in what concerns access to workers, just assume for now that we can connect to every worker directly using regular TCP connection. It should be possible to just add a list of worker nodes as an option to loader script and send the chunks in some random order to the workers in that list. Of course chunks for different tables should end up on the same host, so some form of chunk management is needed (use for now whatever is defined in CSS doc on trac)."
0,Working with SLAC on definition of metadata store,"Follow up metadata store schema development to ensure SUI will be able to use it. Define the fields that should go into Data Definition table. Define the fields that must be present in the image metadata table, which SUI will be searching."
0,hipchat support for maigically urlifying docushare documents,"It would be generally use full if references to official documents in hipchat (or it's successor) automagically generated urls for official document handles.  Eg: Document-1234, LSE-123"
1,LOE - Week ending 12/12/14,NULL
0,Add git bisect tool for Qserv repos,"{code:bash} fjammes@clrlsst-dbmaster-vm:~/src/qserv (u/fjammes/DM-627 $%) $ qserv-test-head.sh -h  Usage: qserv-test-head.sh [options]    Available options:     -h          this message     -q          quick: only rebuild/install new Qserv code,                 and perform test case #01    Rebuild from scratch, configure and run integration tests against   a Qserv git repository.   Pre-requisite:     source loadLSST.bash     setup qserv_distrib -t qserv     setup -k -r ${QSERV_SRC_DIR}    Can be used with 'git bisect' :     cd ${QSERV_SRC_DIR}     git bisect start     git bisect bad     git bisect good git-commit-id     git bisect run /home/fjammes/src/qserv_testdata/bin/qserv-test-head.sh {code}  Code is in DM-627 ticket branch."
0,Aperture Flux back into an abstract class,"The last change to ApertureFlux to make it work with the new C++ design changed ApertureFluxAlgorithm into an instantiatable class.  However, I have now figured out how to make this work with SWIG while still allowing measure and fail to be defined by default at the ApertureFlux level..  So this issue is to put things back in order."
0,Statistics tests use a constant image,I just noticed that the tests for Statistics (clipped mean etc.) use a constant image.  We should be testing a Gaussian field (although that makes the tests a little trickier) 
0,czar log4cxx link/load bug,"Under ubuntu 14.04 (at least), the czar falls over at load time with an unresolved sym for typeinfo for log4cxx::helpers::ObjectPtrBase while loading the css python wrapper shared lib."
1,Make qserv dependencies build on OS X with clang,Fix anything necessary for qserv dependencies to build on OS X with clang.  Note -- making qserv itself build is more complicated and may require a separate ticket.
0,fix dependency problems in obs_subaru scons scripts,"When building obs_subaru with -j4, it often tries to build files related to the defects before the main Python module is built, resulting in import errors (because the scripts it invokes depend on the main Python module).    We need to rewrite the SCons scripts to ensure this dependency is captured.    A preliminary look indicated that this is not entirely trivial, and I'll have to remind myself a bit of how some things in SCons work to get it done, so I'm putting this off for a future sprint.    In the meantime, the workaround is to build obs_subaru with no parallelization."
1,Understand historical written docushare materials dealing with the operational security environment.,"researched the docushare traversing the plans for materials to be embedded in the OCS and similar systems, as well as extant operational plans. The goal was to understand how to separate the the security responsibilities of development, what security constraints ought to be but on items that are delivered, and what to tell the camera and telecscope teams  to mender ea smooth integration with IT security systems upon delivey and integration of their sub systems in Chile."
0,Install PgMySQL and use to connect to local Qserv.,"Used ""pip"" to install it. ""Conda"" should work as well. Therefore, it should be easy to make it part of the delivered system: VM, container, tar file, after the fact download, etc. It has documentation, uses the MIT license, under active development and available from PyPI. DB connection is straight forward and requires little experience to get meaningful work done."
0,Create SQL code to read Qserv into Python Pandas data frame,"This works well, at least for a simple case. You can move directly from a query statement to a Pandas data frame for analysis in just a few lines of code. Here is the start of an iPython Qserv session showing how easy it is.  In [6]: import pandas as pd In [7]: import pymysql as db  In [8]: conn = db.connect(host='lsst-db1.ipac.caltech.edu',port=4040, user='qsmaster', passwd='', db='LSST')  In [11]: df = pd.read_sql(""select deepCoaddId, tract, patch, ra, decl from DeepCoadd"", conn)  In [12]: df Out[12]:     deepCoaddId  tract   patch        ra      decl 0      26607706      0  406,11  0.669945  1.152218 1      26673242      0  407,11  0.449945  1.152218 2      26804242      0   409,2  0.011595 -0.734160 3      26673154      0   407,0  0.449945 -1.152108 … "
1,Explore queries for Qserv database.,"Use a local database here at IPAC with 5 Qserv tables in it. Looked at several Python query interfaces. Used the pymysql interface for testing because it's pure Python and because I found reports suggesting it was almost as fast as the mysqldb interface that requires C language libraries.  Ad hoc queries can be constructed in three lines of code, so useable in a science environment.  Found a couple of bugs in Qserv that were reported."
1,Begin looking at how Python Pandas can be used for LSST data analysis.,"Pandas is well integrated with the other parts of SciPy: numpy, matlibpy, etc.  It’s a good candidate for data analysis, especially where time series are involved. However, there are no multidimensional columns, poor metadata support for FITS files and a need to use masks instead of NaN values. These may, or may not, be problems.  There is a 400 page book about Pandas, so it will take some further time to learn its value, especially with astronomical data in different situations."
0,Allow SWIG override for broken SWIG installations,"Dependency on SWIG 2.0+ was introduced into Qserv, and this broke Qserv building on systems relying on SWIG 1.3.x.  This ticket introduces basic code to override SWIG_LIB on those systems to allow use of the broken installation (some SWIG search paths are fixed during its build process otherwise)."
2,Data ingest scripts cleanup,NULL
0,Identify dead code,NULL
1,Write documentation for SSI interface,NULL
2,multi-error utility class,"QueryAction::Impl currently has a handful of private members/methods related to maintaining a collection of errors that occurs while processing a query (_addErrorMsg, IntString, IntStringVectory, _errors, etc.)  It would be useful to split this stuff out into a separate, re-usable utility class, and extend, then extend with some additional functionality (output stream operator, subclass from std::exception, maybe capture __LINE__ and __FILE__, integrate with logging, etc.)  [Fabrice: discuss requirements and basic design w/ Fritz and/or Daniel]"
0,Minor bug in a test,"tests/centroid.py has a bug in testMeasureCentroid: ""c"" is undefined in the following bit of code: {code} if display:     ds9.dot(""x"", c.getX(), c.getY(), ctype=ds9.GREEN) {code}"
2,"Implement ""unlimited"" result size handling","DM-854 exposed an issue in handling large results. Result rows are returned from worker to czar in protobufs messages. However, protobufs messages should not be larger than some number of megabytes, according to protobufs documentation. IOW, protobufs is not designed to handle messages on the order of hundreds of megabytes. There may be some code in the protobufs implementation that does not scale beyond messages of a few megabytes. Hence, in order to send larger amounts of result rows, we need to use multiple messages. The current protobufs definition for result messages includes a placeholder for chaining result messages, but there is no code on the czar or worker that implements result chaining.   The scope of this ticket is to implement message chaining for results on the worker and on the czar, in such a way that it places no limits on the overall number of result rows (if there is a limit, it should be no smaller than quadrillions and be well-documented).  "
1,Define interfaces for Data Access Services,NULL
1,Implement interfaces for Data Access Services,"Implement proof of concept, skeleton of the prototype. The work will continue in follow up stories in February and in S15."
1,Integrate image cutout service interfaces with butler,NULL
2,Finish image cutout service implementation,Define appropriate interfaces and connect them with the RESTful API (see DM-1695).
1,Finish metadata store prototype,* A quick proof-of-concept prototype of loading tool for loading database-information into metadata store.   * Handling mysql credentials through auth file in home dir instead of hardcoded values.
1,Create read only OpenStack volume and execute processing scenario,Create read only OpenStack volume and execute processing scenario
0,Clone OpenStack volume for use against multiple instances,Clone OpenStack volume for use against multiple instances
1,Examine fqdn/hostname assignment for OpenStack instance,Examine fqdn/hostname assignment for OpenStack instance
3,S15 Implement Database & Table Mgmt,"Continuation of DM-1036, making the code for managing distributed databases and tables more feature reach, including features such as deletion."
3,S15 Run Large Scale Qserv Tests,Run large scale tests to uncover unexpected issues and bottlenecks.
3,S15 Tune Qserv,Fix scalability and performance issues uncovered through large scale tests DM-1704
1,S15 Analyze Qserv Performance,"Final analysis of Qserv performance, measure KPIs. Based on LDM-240, we are aiming to demonstrate:  * 50 simultaneous low volume queries, 18 sec/query  * 5 simultaneous high-volume queries, 24 h/query  * data size: 10% of DR1 level.  * Continuous running for 24 h with no software failures.  "
3,S15 Refactor Qserv,"Ongoing refactoring of Qserv - code cleanup, tightening interfaces etc."
3,W16 Improve Query Coverage in Qserv,Currently Qserv supports only a limited subset of queries. We need to make sure it supports all queries that users need to run.
1,Implement result sorting for integration tests,"We need to be able to sort results, because we can't always rely on ORDER BY. So we need a formatting per query in the integration tests (sort result for some, don't sort for others etc.)    The following queries have been disabled because we don't have result sorting, so once it is implemented, we will need to re-enabled them prior to closing this ticket:  {code}  case02/queries/0003_selectMetadataForOneGalaxy_withUSING.sql  case02/queries/3001_query_035.sql  case02/queries/3008_selectObjectWithColorMagnitudeGreaterThan.sql  case02/queries/3011_selectObjectWithMagnitudes.sql  case02/queries/3011_selectObjectWithMagnitudes_noalias.sql  {code}"
0,ValueError in lsst.afw.table.Catalog.extend(),"{code} from lsst.afw.table import BaseCatalog, Schema  s = Schema() c1 = BaseCatalog(s) c2 = BaseCatalog(s)  c1.extend(c2) {code}  The above fails, saying:  {code} Traceback (most recent call last):   File ""test.py"", line 7, in <module>     c1.extend(c2)   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/afw/10.0+3/python/lsst/afw/table/tableLib.py"", line 6909, in extend     _tableLib.BaseCatalog_extend(self, iterable, deep) ValueError: invalid null reference in method 'BaseCatalog_extend', argument 3 of type 'lsst::afw::table::SchemaMapper const &' {code}"
3,"S15 Improve MetaServ: RESTful, Basic Image Search, DDL, Config Files","Implement beta version of the Metadata Service. This version will support basic image search, DDL, config files, and RESTful interfaces for MetaServ and Database (Qserv)."
3,"S15 Add Support for Image Stitching and Rotation, RESTful APIs","Implement image stitching and rotating, including RESTful APIs."
1,S15 Image & File Archive v2,System for tracking existing image data sets integrated with metadata services.
1,Integrate MetaServ with Schema Browser,"Schema browser displays detailed info about schema, including custom fields like UCD, units etc. This information is stored as comments embedded in the master version of the schema (in ""cat"" repo). Currently we are generating ascii from the master schema for schema browser, and we load it into mysql, then schema browser reads it from mysql. This story involves changing schema browser such that it will read the schema information directly from MetaServ."
0,Disable query killing,Apparently killing a query through Ctrl-C is confusing xrootd. Disable query killing (which seems to be only partly implemented).
3,Implement query killing through Ctrl-C,Need to properly implement query killing through Ctrl-C
0,Make secondary index for director table only,"Following discussion on qserv-l, we only need to generate ""secondary"" index for director table, no other table is supposed to have it. Need to modify data loader to recognize which table is director table and generate index only for that table. "
3,S15 Improve Query Coverage in Qserv,"Query coverage in the qserv integration testing is very limited, we have been turning off more and more queries and we were making the qserv code and the data loader more strict. This epic covers work (fixes and improvements) related to * re-enabling test queries marked as ""fixme"" (when it make sense, some queries are for features that are not implemented yet) * adding more queries to test interfaces and features that are implemented but are not currently tested."
2,LOE - Week ending 12/19/14,The System Administration team at NCSA worked on the following LOE tasks this week: - RMA'ed RAID card for lsst-dev <https://jira.ncsa.illinois.edu/browse/LSST-599> - Updated user's updated public ssh keys <https://jira.ncsa.illinois.edu/browse/LSST-614> - Rebuilt failed drive in lsst-dbdev2 <https://jira.ncsa.illinois.edu/browse/LSST-610> - Increasing drive size of lsst-eval <https://jira.ncsa.illinois.edu/browse/LSST-611> - Researched & repaired lsst20 NFS issues <https://jira.ncsa.illinois.edu/browse/LSST-591> - Upgraded lsst-xfer to 10G networking
1,stack build fails on gcc 4.8 with opt=3,"The stack fails to build with gcc 4.8, with a test failure in meas_base (though a similar problem on the HSC fork suggests the problem is actually in afw).  [~darko] reports that on OpenSuse 13.1, the failure goes away when compiling with opt=1 instead of the default opt=3, indicating that the problem is overly aggressive optimization.  This, and the fact that a traceback of the HSC-side failure implicates MaskedImage::getXY0, leads me to guess that something is going wrong with the alignment of the Eigen data members in afw::geom::Point.  Until that theory is disproven, this probably belongs in my court, though I'd be happy to let someone steal it from me if they're interested in working on it."
2,Stabilize Firefly Repository,#NAME?
3,Research Javascript Frameworks: Work toward future proposal,Take prior research and come up with rough firefly migration proposal.  Include how to use a hybrid system for foreseeable future. Write some prototype code.
0,fix table file handling of MANPATH in dependencies,"As discussed on DM-1220, the table files for:  - mysqlproxy  - protobuf  - lua  - expat should have the MANPATH entry removed entirely, while:  - xrootd should have "":"" added to the end of its MANPATH value, to allow the default paths to be searched as well."
1,Fix error on duplicate result_id_m table while launching qserv integration tests,"Next command:  {code} qserv-check-integration.py --case=01 --load;  qserv-check-integration.py --case=02 --load {code} Fails, most of the time, with next error in logs: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-627 *+)⟫ cat ~/qserv-run/2014_12/var/log/qserv-czar.log | grep result_1211906_m 0103 16:38:18.576 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:445) - InfileMerger table qservResult.result_1211906_m is ready 0103 16:38:18.686 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:18.721 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 0103 16:38:24.716 [0x7fe3cf7fe700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1050: Table 'result_1211906_m' already exists Unable to execute query: CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:438) - InfileMerger error: Error creating table (qservResult.result_1211906_m) 0103 16:38:34.672 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1054: Unknown column 'QS1_COUNT' in 'field list' Unable to execute query: CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:34.674 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 {code}  So it seems test case #01 create table .result_1211906_m, and then test case #02 try to re-use this name for an other query.  Qserv result tables aren't cleaned (here Qserv has be stopped): {code} ls  ~/qserv-run/2014_12/var/lib/mysql/qservResult/ db.opt                result_1211336_m.MYI  result_1211340_m.MYI  result_1211343_m.MYI  result_1211346_m.MYI  result_1211382_m.MYI  result_1211385_m.MYI  result_1211902_m.MYI  result_1211905_m.MYI result_1211334_m.frm  result_1211337_m.frm  result_1211341_m.frm  result_1211344_m.frm  result_1211347_m.frm  result_1211383_m.frm  result_1211386_m.frm  result_1211903_m.frm  result_1211906_m.frm result_1211334_m.MYD  result_1211337_m.MYD  result_1211341_m.MYD  result_1211344_m.MYD  result_1211347_m.MYD  result_1211383_m.MYD  result_1211386_m.MYD  result_1211903_m.MYD  result_1211906_m.MYD result_1211334_m.MYI  result_1211337_m.MYI  result_1211341_m.MYI  result_1211344_m.MYI  result_1211347_m.MYI  result_1211383_m.MYI  result_1211386_m.MYI  result_1211903_m.MYI  result_1211906_m.MYI result_1211335_m.frm  result_1211339_m.frm  result_1211342_m.frm  result_1211345_m.frm  result_1211381_m.frm  result_1211384_m.frm  result_1211901_m.frm  result_1211904_m.frm result_1211335_m.MYD  result_1211339_m.MYD  result_1211342_m.MYD  result_1211345_m.MYD  result_1211381_m.MYD  result_1211384_m.MYD  result_1211901_m.MYD  result_1211904_m.MYD result_1211335_m.MYI  result_1211339_m.MYI  result_1211342_m.MYI  result_1211345_m.MYI  result_1211381_m.MYI  result_1211384_m.MYI  result_1211901_m.MYI  result_1211904_m.MYI result_1211336_m.frm  result_1211340_m.frm  result_1211343_m.frm  result_1211346_m.frm  result_1211382_m.frm  result_1211385_m.frm  result_1211902_m.frm  result_1211905_m.frm result_1211336_m.MYD  result_1211340_m.MYD  result_1211343_m.MYD  result_1211346_m.MYD  result_1211382_m.MYD  result_1211385_m.MYD  result_1211902_m.MYD  result_1211905_m.MYD {code}  Changing _idCounter to next value in appInterface.py: {code:python} self._idCounter = int(time.time() % (60*60*24*365) * 10) {code} solves the problem, but a deeper explanation will allow to bring a more robust fix."
0,Build 2015_01 Qserv release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
2,refactor fftools viewer to have derived viewers / start LSST SUI git repo,Do the following:  - refactor fftools to allow for specialized viewers. - make a generic fftools viewer. - make an irsaviewer package in ife-new that will build. - generalize the catalog search and add factories to access the lsst search process. 
0,Have newinstall.sh check itself against distrib version,We want to alert people who are just using a newinstall.sh they have lying around (old or hacked up or...) that they are not using the official server version.  
1,Migrate qserv modules (python code) to new logging system,NULL
0,deblender artifacts in noise-replaced images,"We still see noise artifacts in some deblended images on the LSST side when running the M31 HSC data.  They look like the result of running NoiseReplacer on HeavyFootprints in which the children can extend beyond the parents.  This was fixed on the HSC side on DM-340 (before the HSC JIRA split off), and I *think* we just need to transfer the fix to LSST."
2,Catch-all epic for essential fixes during DM-W15-4,NULL
0,CSV reader for Qserv partitioner doesn't handle no-escape and no-quote options properly,"Both the no-quote and no-escape CSV formatting command line options should not have a default value, as specifying any value turns off field escaping and quoting. Furthermore, when quoting is turned off, the reader incorrectly treats embedded NUL characters as a quote character."
0,Fix SWIG_SWIG_LIB empty list default value,"See Serge message to Qserv-l ""xrootd premature death"": {quote} However, there are bigger problems. First of all, master doesn’t build for me. I get this error:    File ""/home/lsstadm/qserv/SConstruct"", line 104:     env.Alias(""dist-core"", get_install_targets())   File ""/home/lsstadm/qserv/SConstruct"", line 90:     exports=['env', 'ARGUMENTS'])   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 609:     return method(*args, **kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 546:     return _SConscript(self.fs, *files, **subst_kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 260:     exec _file_ in call_stack[-1].globals   File ""/home/lsstadm/qserv/build/SConscript"", line 39:     canBuild = detect.checkMySql(env) and detect.setXrootd(env) and detect.checkXrootdLink(env)   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 225:     xrdLibPath = findXrootdLibPath(""XrdCl"", env[""LIBPATH""])   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 213:     if os.access(os.path.join(path, fName), os.R_OK):   File ""/home/lsstadm/stack/Linux64/anaconda/2.1.0/lib/python2.7/posixpath.py"", line 77:     elif path == '' or path.endswith('/'):  which is caused by the fact that env[“LIBPATH”] looks like:  [[], '/home/lsstadm/stack/Linux64/antlr/2.7.7/lib', '/home/lsstadm/stack/Linux64/boost/1.55.0.1.lsst2/lib', '/home/lsstadm/stack/Linux64/log4cxx/0.10.0.lsst1+2/lib', '/home/lsstadm/stack/Linux64/xrootd/4.0.0rc4-qsClient2/lib', '/home/lsstadm/stack/Linux64/zookeeper/3.4.6/c-binding/lib', '/home/lsstadm/stack/Linux64/mysql/5.1.65.lsst1/lib', '/home/lsstadm/stack/Linux64/protobuf/2.4.1/lib', '/home/lsstadm/stack/Linux64/log/10.0+3/lib']  The first element is [], which comes from https://github.com/LSST/qserv/blob/master/site_scons/state.py#L173 where a PathVariable called SWIG_SWIG_LIB is given a default value of []. I can fix the build by changing the default to an empty string… but I don’t know enough scons to say whether that’s the right thing to do. Can one of the scons gurus confirm that’s the right fix? {quote}"
1,make lsst-sui & firefly git repo / make an lsst viewer,NULL
1,Update auto build tool to work with new split repositories ,"After the repository split, changes are required to get the auto build tool to work properly. Firefly and Firefly based applications are built using Gradle system.  "
1,Create an integration test case with GB-sized data,"It's difficult to load manually data in Qserv, so a way to do that is to use integration test framework to automatically do this.  Big data file won't be stored in git, but the user wil lhave to retrieve them manually, and the test case won't be executed by integration tests."
0,Provide input data for exampleCmdLineTask.py,"{{pipe_tasks/examples/exampleCmdLineTask.py}} reads data from a repository. The comments in {{pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py}} suggest that  {code} # The following will work on an NCSA lsst* computer: examples/exampleCmdLineTask.py /lsst8/krughoff/diffim_data/sparse_diffim_output_v7_2 --id visit=6866601 {code}  There are a few problems with that:  * External contributors don't have access to {{lsst*}}; * Even though that data exists now, it's unclear how long it will remain there, or what steps are being taken to preserve it; * The mention of this data is fairly well buried -- it does appear in the documentation, but it's certainly not the first thing a new user will stumble upon.  At least the first two points could be addressed by referring to a publicly available data repository. For example, the following works once {{afwdata}} has been set up:  {code} examples/exampleCmdLineTask.py ${AFWDATA_DIR}/ImSim --id visit=85408556 {code}  Although this has the downside of only providing a single image."
1,Export SUI data (DC_W13_Stripe82_subset),"- import sui.sql.bzip2.out (produced by Serge) into MySQL for DeepSource and DeepForcedSource tables: - remove columns chunkId and subChunkId for each chunk table - merge all chunk table into the main table - join DeepSource and DeepForcedSource to add coordinates of DeepSource (director) object in DeepForcedSource table. then dump  DeepSource and DeepForcedSource  to files DeepSource.csv and DeepForcedSource.csv {code:sql} SELECT f.*, COALESCE(s.ra, f.ra), COALESCE(s.decl, f.decl) FROM DeepForcedSource f LEFT JOIN DeepSource s ON (f.deepSourceId = s.deepSourceId) INTO OUTFILE '/db1/dump/DeepForcedSource.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n'; {code} - Load this file using Qserv loader.  A sample should be made and tested first to validate this procedure. This sample could be added in qserv_testdata"
1,overhaul slot and alias system,"While working on DM-1218 and DM-464, I've grown quite dissatisfied with the current state of the slot and alias mechanisms, and we now have a concrete proposal for larger-scale changes on RFC-11.  Unfortunately, I don't think we'll be in a good position to do much about this until we've completed the transition to meas_base and removed the old measurement framework in meas_algorithms."
0,move SourceRecord/Table/Catalog to meas_base,"We can make address a lot of dependency issues if we move the Source classes to meas_base, because we'll no longer have low-level code (e.g. afw::table persistence) in the same module as very high-level code (e.g. slots).   It will also put all the slot code in the same place, instead of spreading it across two pacakges.  This should be straightforward, except that we'll have a lot of downstream code to (trivially) change, and there's a good chance Swig will get confused somewhere along the way."
2,Remove in-memory support of old-version afw::table objects,"After removing the old measurement framework in meas_algorithms, we should also end support for version=0 Schemas in memory, and instead convert version=0 Schemas to version 1 when we unpersist them."
3,afw::table - post-transition improvements,Breakdown: jbosch 40%; swinbank 60%
3,Measurement - Framework Improvements,"Add new features in meas_base that are desirable, but not required to replace functionality in meas_algorithms.  Breakdown: pgee 70%; jbosch 30%"
0,Support DDL in MetaServ - design,"DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves designing the procedure involving loading DDL information into MetaServ. We need to be ready to support a variety of scenarios: * we are getting already preloaded database, need to just load metadata about it to metaserv (we might have the original ascii file with extra information, or not) * we are starting from scratch, need to initialize database (including loading schema), and need to load the information to the metaserv * we already have the database and metadata in metaserv, but we want to change something (eg. alter table, or delete table, or delete database)."
0,move executionOrder from plugin config class to plugin class,"We originally put the executionOrder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework.  But it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change."
2,Create a search processor to do cone/box search on a QSERV catalog,"Create a search processor which accepts cone and box spatial constraints, queries a catalog, stored on QSERV (DeepSource), and returns the relevant rows as an IPAC table or RawDataSet.  The implementation is querying DeepSource catalog using qserv_areaspec_circle and qserv_areaspec_box spatial restrictors. "
1,"Read SUI requirements, send a list of questions to the group scientist","Read the requirements document on Archive Browser and Query Tools, make a record of unclear items, send questions to our scientist (D. Ciardi) "
0,Add support for running unit tests in qserv/admin,"This came up during review of DM-370: ""We do not run tests scripts in admin/ during regular build, SConscript in admin/ does not support that unfortunately."" This story involves tweaking SConscript to enable running unit tests automatically."
1,Setup work environment / test builds for refactored repositories,"After the repository was split into firefly and new-ife, it was necessary to understand the changes, check for inconsistencies, test builds, set up new IDEA project, etc. "
2,Obtain and use catalog dd (data definition),"Get catalog metadata, which should include column description, units, and type. Use it in tool tips and possibly to create flexible constraints.  A temporary solution is to get metadata from an internal lsst_schema_browser_S12_lsstsim database on lsst-db.ncsa.illinois.edu  A permanent solution would be querying catalog metadata from metadata store."
1,XYPlotter should be caching and restoring plot metadata,"For efficiency, XYPlotter is designed to create up to 4 cards. When the card limit is exceeded, a previously created card is reused to plot catalog data for the current catalog. When card is reused the previous plot metadata (like column selections, grid option, etc.) are lost."
0,Remove optimisation flag management in partition SConstruct,"- eups build with -g and -O3 by default.  - Developpers can build their sources with next options:  scons debug=false opt=3  - Nevetheless partition SConstruct add -O2 option if debug is disabled, which is orthogonal. "
3,The existing FITS reader class needs to be refactored to improve the performance(2),#NAME?
1,fix faint source and minimum-radius problems in Kron photometry,"This transfers some improvements to the Kron photometry from the HSC side:  - HSC-983: address failures on faint sources  - HSC-989: fix the minimum radius  - HSC-865: switch to determinant radius instead of semimajor axis  - HSC-962: bad radius flag was not being used  - HSC-121: fix scaling in forced photometry  The story points estimate here is 50% of the actual effort, as the work (already done) also benefited HSC."
1,Fix errors in parsing or rendering nested expressions,Qserv has errors rendering nested expressions in predicates of the WHERE clause. It is unclear whether the problem is in constructing the predicate representation or in rendering the representation (or both).  Example: {code} SELECT  o1.objectId FROM Object o1  WHERE ABS( (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) -              (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) ) < 1 {code} Yields: {code} SELECT o1.objectId FROM LSST.Object_100 AS o1 WHERE ABS((VALUE_EXP FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.gFlux_PS)-FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.rFlux_PS)))<1 {code}  I probably left out a more general implementation one/both of those parts of query parsing/analysis.
0,Add rotAngle to baseline schema,"Add ""rotAngle DOUBLE"" to every table that has image ra/decl.  "
3,Implement using multiple disk spindles,"Qserv should be able to take advantage of multiple disk spindles (JBOD-type architecture). In practice that means either relying on something like native mysql partitioning, or tweaking loader so that it can distribute chunks across multiple disks (and patch symlinks in mysql data_dir).  "
1,OpenStack automation via Python scripts : Launch an Instance,"In our introductory work with OpenStack we have been utilizing the Horizon GUI interface for first steps, followed by the use of command line tools (the 'CLI') (e.g., nova, cinder, etc) as shown in DM-1334  DM-1700 , DM-1701. While it is possible to write automation scripts that utilize the CLI, an approach based on 'pure' Python scripting would fit more seamlessly into the LSST software development process. Enabling OpenStack automation via Python offers the opportunity to integrate provisioning of resources into the overall flow of LSST workflow & processing (e.g., DRP.)  The OpenStack services expose native Python APIs that expose the same feature set as the command-line tools.  The required python packages (python-keystoneclient, python-novaclient, ..) are installed on the head node 'vlad-mgmt' of the NCSA ISL OpenStack, and so initial Python scripting can be executed/tested there.  A first Python script will perform required authentication and launch an instance. "
1,OpenStack automation via Python scripts : Software installation/test on an LSST node,"The set of packages that will enable us to write against the native Python APIs of the OpenStack services is  {code} python-keystoneclient python-glanceclient python-novaclient python-quantumclient python-cinderclient python-swiftclient {code}  We begin testing these in DM-1787  on the NCSA OpenStack head node, but eventual use within LSST orchestrating workflow would entail these being installed on LSST nodes in the LSST stack.  In this issue we perform a basic installation of these packages into the system space on an LSST node/VM for testing.  These are managed in github, and we install these via 'pip install' onto an LSST VM for initial tests."
1,"Experiment with afwtable, meas_base, pipe_{tasks, base}",NULL
1,Produce detailed prototype & accompanying documentation,NULL
1,Produce straw-man prototype,NULL
0,Update documentation and automatic install script w.r.t. new newinstall.sh script,newinstall.sh script has evolved and breaks Qserv install procedure.
1,Pull distEst package into obs_subaru,"Reducing HSC data requires an estimate of the distortion, which is provided by the HSC package distEst.  This can be pulled into obs_subaru to consolidate code and reduce dependencies.  I propose to treat distEst as legacy code, which means I will pull it into obs_subaru without major changes to the code style."
1,Use anonymous NCSA rsync server to distribute large test datafiles.,"DM-1755 has been done before this feature was available. It uses rsync over ssh which require use to have an ssh-key on lsst-dev.  NSCA rsync server can now be accessed with next syntax: {code:bash} rsync -av lsst-rsync.ncsa.illinois.edu::qserv/qserv_testdata/datasets/case04/data/DeepSource.csv.gz . # LIST FILES AVAILABLE IN THE MODULE NAMED ""qserv"" rsync lsst-rsync.ncsa.illinois.edu::qserv  # To add content to this module/group, you can copy files into the following path:   /lsst/rsync/qserv/ {code}  rsync over ssh feature should be kept, in order to distribute private data are distributed."
1,Improve test coverage for case04,"Most of queries used in GB-sized case04 return empty results.  These queries should be added: {code:bash} fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT count(*) FROM DeepForcedSource"" +----------------+ | SUM(QS1_COUNT) | +----------------+ |       33349940 | +----------------+ fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT * FROM DeepForcedSource LIMIT 1"" {code}  But, even better, SUI team could provide some more interesting query to Qserv team in order to improve case04 quality."
0,Package flask,"The Data Access Webservice APIs are relying on flask, so we need to package flask according to the LSST standards. For my initial testing, I just run ""sudo aptitude install python-flask"".  "
0,Regression testing of AP Simulator,Run the AP simulator to make sure that none of the changes to the ctrl_events package break anything.
0,Regression testing of Orca,Do some test runs using Orca to make sure Orca still works after the the changes to ctrl_events.
0,remove unused local typedefs,gcc 4.8 now warns about locally-defined typedefs that aren't used.  We have a few of these in ndarray and afw::gpu that should be removed.
1,S15 Explore Qserv Authorization,"Explore authorization centrally: use information generated by parser. Either generate dummy query and run on mysql that runs near czar, or use info produced by parser to determine if user is authorized.  Note, we want to limit this to ~1 week, just to reveal potential problems, or do a quick proof of concept."
1,Study the current SUI requirement ,Study the current requirement carefully to make sure they all make sense and we can do it.
1,Study the current SUI requirement ,NULL
0,Study the current SUI requirement,NULL
1,Study the LSST data products document and give a summary to the team,NULL
0,Study the current SUI requirement,NULL
2,Study SUI requirement and summarize all the input from other team memebers,NULL
0,segfaults in ip_diffim on gcc 4.8,"I'm seeing test segfaults in ip_diffim on gcc 4.8, similar to those resolved on DM-1725, but with no similar smoking gun yet.  Preliminary indication is that the problem is actually in meas_algorithms."
1,Prepare initial content,Prepare LSE-130 content as far as possible without input from the new collimated-projector calibration plan.
1,Determine LSE-130 impact of collimated projector calibration plan,"During a working meeting with Robert Lupton and Chris Stubbs, determine the impact on LSE-130 of the introduction of the collimated projector for calibration."
1,Prepare draft of LSE-130 for Camera and CCB review,Produce a reviewable draft of LSE-130 based on decisions on calibration operations
0,Support Camera CD-2 (mainly re: LSE-130),"Provide slides and other information needed for CD-2, mainly relative to the open questions around LSE-130"
1,Support LSE-130 review by CCB (mainly Camera),"Respond to comments, perform revisions to LSE-130 as necessary based on feedback from CCB review of the document"
0,Convert LSE-130 to SysML,"Following CCB recommendation of approval of LSE-130 draft, convert Word draft to SysML and provide a docgen to Robert McKercher for final posting. "
1,Create and post docgen of LSE-68,"To support discussions with the Camera, post a provisional docgen of LSE-68 to the appropriate Confluence page.  Use knowledge from EA training to improve template."
0,Support completion of final document,"Based on CCB approval of LSE-72 on 10 October, support the completion of the final copy of the document for posting on Docushare."
0,Complete LSE-140 work as needed to produce final document,Complete any review-driven revisions of LSE-140 and support the CCB meeting and following final document preparation.
0,LSE-140: Collect desired changes for future release,"Prepare for a future revision (Phase 3) of LSE-140.  Collect issues to be addressed in the revision.  Determine if any affect Phase 2 scope (which would require a prompt revision).  It is not anticipated that there will be an actual revision of LSE-140 during the Winter 2015 cycle, because additional detail on calibration requirements will not be available in time."
1,Fix czar assertion failure,"Reported by Tatiana: I am encountering this once in a while.   qserv-czar.log  python: build/rproc/ProtoRowBuffer.cc:69: int lsst::qserv::rproc::escapeString(Iter, CIter, CIter) [with Iter = __gnu_cxx::__normal_iterator<char*, std::vector<char, std::allocator<char> > >, CIter = __gnu_cxx::__normal_iterator<const char*, std::basic_string<char, std::char_traits<char>, std::allocator<char> > >]: Assertion `srcBegin != srcEnd' failed.  Czar is dead and qserv stops responding after that.  ----  For more details, search in qserv-l archives mails with ""czar assertion failure"" subject: https://listserv.slac.stanford.edu/cgi-bin/wa for complete description."
0,Define issues to be addressed,"Work with TCS contacts (Jacques Sebag, Paul Lotz, etc.) to define the principal issues"
0,Produce draft of LSE-75 with agreed revisions,"Produce a draft of LSE-75 with the following agreed revisions: * remove reference to advance notice of pointing, now in LSE-72 * add reference to PSF reporting"
2,"Catch-all epic for essential fixes during DM-W15-1,2,3 ",NULL
2,Create primary calibration plugins,NULL
1,Develop General Acceptable Use Policy,Don Petravick and Lee LeClair
1,Develop Information Classification Policy,"Don Petravick, Lee LeClair"
1,Develop Incident Response Policy,"Don Petravick, Lee LeClair"
1,Develop Data Management Sub-Project Plan and Risk Table,"Don Petravick, Frossie Economou"
1,Develop PMO Sub-Project Plan and Risk Table,"Lee LeClair, Iain Goodenow"
1,Develop EPO Sub-Project Plan and Risk Table,"Don Petravick, Frossie Economou"
1,Develop Camera Sub-Project Plan and Risk Table,"Don Petravick, Richard Dubois"
1,Develop Telescope and Site Sub-Project Plan and Risk Table,"Lee LeClair, German Schumacher"
1,Package ISP Documents into LSST Standard Format for Control and Delivery,Robert McKercher
1,Deblend post-merge objects,NULL
1,Task-level processing for merged objects,NULL
1,"Fix query error on case03: ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata"" ","Xrootd prevents the worker to return more than 2MB data.  On GB-sized data: {code} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch  -e ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata""                                                                                                                                                                     ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case03_qserv/1234567890): 20150123-16:27:45, Error merging result, 1420, Result message MD5 mismatch (-1) {code}  On integration test case 04: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-1841 *)⟫ mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case04_qserv  -e ""SELECT * FROM DeepForcedSource""   ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case04_qserv/6970): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=2 Resource(/chk/qservTest_case04_qserv/7138): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=3 (-1) {code}"
0,Permit PropertySets to be represented in event payloads,"In the old marshalling code, property sets were representable within the payload of the event.   This was removed in the new marshalling scheme.   There are things (ctrl_orca) that still used this, so this needs to be added to the new marshaling code.  At the same time, new new filtering code can not allow this to be added, because the JMS headers only take simple data types."
0,Test Qserv on SL7,Needed to run Qserv on CC-IN2P3 cluster.
1,Coordinate implementation of web form for collecting data about existing data sets,"The form is being implemented by the DataCat team. Need to coordinate (including with the NCSA team which parts are covered by which team), test, fine tune etc."
2,Add support for large results in XrdSsiRequest::GetResponseData,GetResponseData needs to handle data sets beyond 2 MB. The problem is discussed in more details in story DM-1841
2,SUI work with DB team to define the image query API,IPAC SUI team will work with SLAC database team to define the image query APIs.  IPAC needs to make sure the APIs are sufficient to satisfy the UI needs. SLAC will implement them. .
0,Study RESTful API and work with SLAC team  to define image query APIs,NULL
0,"Discuss, review, and define image query APIs with SLAC team",NULL
0,Image query API discussion and review,NULL
0,Image query API discussion ,NULL
0,Image query API discussion,NULL
3,SUI propose a structure definition for user workspace,Workspace is an integral part of SUI. We want to start the discussion and definition of workspace concept and structure.     SUI team had several discussions and Xiuqin presented the results at the DM AHM at SLAC. The slides and the discussion notes are here: https://confluence.lsstcorp.org/display/DM/Workspace+discussion
1,Specify mechanism for periodic (nightly/weekly) build distribution,NULL
1,Implement nightly/weekly release automatic distribution - Part I,"This ticket covers code in sqre-codekit to do migrate as much of the process of special machines, and git tag repos on the basis of eupspkg manifests."
1,Github Transition Plan: Write document for CCB,NULL
3,Release engineering W15 bucket,Bucket epic for activities related to stack releases during W15
2,Publish v10_0 release,NULL
0,Update documentation for v10_0 release,All done bar obtaining some release notes. 
2,Workflow tool improvements w15 bucket,"Bucket epic for w15 for improvements with JIRA, Hipchat, etc "
1,JIRA project for RFCs,NULL
2,Implement Github transition plan,NULL
1,Review existing Level 3 documentation,Review existing requirements in this area.  Find all relevant existing project-controlled and other key documents.
1,Document as-is Level 3 requirements and conceptual design,"Produce a single jumping-off point for documentation on all aspects of Level 3, on Confluence.  Ensure that flowdown for existing Level 3 requirements in SysML is modeled.  Describe the high-level conceptual design."
0,tighten control over heterogeneous DictFields,"DM-1218 added support for DictFields with heterogeneous item types, which probably allows a bit too much freedom (the rest of pex_config is much more strongly-typed).  Instead of passing None to allow any type to be used, we should pass a tuple of supported types."
1,Define JSON Results for Data Access Services,"As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This story covers defining structure of JSON results for Data Access Services (dbserv, imgserv, metaserv) "
3,SUI requirement refinement to define many unclear areas,"In the current SUI requirement document, many areas are not clearly defined. We want to put more description and definition for those areas, and  identify and define the missing functions.  The goal is to generate a requirement document for DM review and put it under version control. "
3,SUI User workspace specification,"At the 2015 Deb DM AHM at SLAC, SUI led a discussion of workspace. https://confluence.lsstcorp.org/display/DM/Workspace+discussion  We want to continue the discussion, understand the user needs, identify the DM groups involved.  The goal is to generate a document to capture the functions requirement of workspace.     The first version of document is here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41783931"
3,SUI 2D data visualization (XY plot),Better algorithm in spatial binning to visualize large number of catalog sources Plot histogram for tabular data Plot basic light curve 
3,SUI the alert subscription system specification,"Identify parties involved in the alert system generation, broadcast, subscription.  Understand the flow of the alert from generation to notifying users. Understand the requirement for SUI subsystem ""Alert subscription and notification"". "
3,SUI infrastructure implementation,Identify the hardware resources needed at NCSA for short term development and  Set up the basic git repository and build system Explore multi resolution images display for background iamge
3,SUI functional design ,"understand current use cases, collect and define  more use cases. Design the major functions of major components in SUI, mainly Firefly package."
3,SUI web interface and Python interaction,"To facilitate  users to interact with Firefly visualization components in IPython notebook, to allow users to control the display with Python script. "
3,"Collect, understand, and define more use cases",This is an on-going effort. The collected use cases will be posted at confluence page https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41784036. 
1,Implement RESTful interfaces for Database (GET),"Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""GET"" type requests only, ""POST"" will be handled separately."
1,Improvements to web form ,NULL
2,organize the workspace discussion and present a good proposal,SUI team had several discussions and Xiuqin presented the results at the DM AHM at SLAC. The slides and the discussion notes are here: https://confluence.lsstcorp.org/display/DM/Workspace+discussion
1,summarize WEBDAV capabilities and past experience using it,"WEBDAV could be a candidate for managing the user workspace.  summarize its capabilities and past experience, collect some use cases will help us to make a better decision."
0,Contribute to the workspace capability discussion ,"This include past experience, collection of use cases. "
0,HDF5 file format study,"Xiquin, Loi, Trey, and myself discussed HDF5 as a default format to return result set and metadata from lower-level database services vs. traditional IPAC table. Here is the summary:  Advantages of IPAC Table format  - Simple and human-readable, contains a single table - Fixed length rows (easy to page through) - Supported by many astronomical tools  - Provides a way to pass data type, units, and null values in the header - More metadata can be added through keywords (attributes)  Disadvantages of IPAC table format   - Steaming can not be started before all data are received – need to know column width before the table can be written (csv is better alternative) - Only alpha-numeric and '_' characters are allowed in column names (small subset of available characters) - Only predefined datatypes and one attribute type (string) - ASCII representation requires about twice as much storage to represent floating-point number data than the binary equivalent.  Advantages of HDF5  - Can represent complex data and metadata (according to LOFAR, good to represent time series) - Structured data, arbitrary attribute types, datatypes can be combined to create structured datatypes - Flexible datatypes: can be enumerations, bit strings, pointers, composite datatypes, custom atomic datatypes - Access time and storage space optimizations - Partial I/O: “Chunked” data for faster access - Supports parallel I/O (reading and writing) - Built-in compression (GNU zlib, but can be replaced with others) - Existing inspection and visualization tools (HDFView, MATLAB, etc.)  Disadvantages of HDF5  - Complex - Tuned to do efficient I/O and storage for ""big"" data (hundreds of megabytes and more), not efficient for small reads/writes. - Requires native libraries (available in prepackaged jars, see below) - Not human readable - (?) Not yet widely supported by astronomical tools (counter-examples: AstroPy, IDL, more at hdfgroup site)  Tools and Java wrappers:  * JHI5 - the low level JNI wrappers: very flexible, but also quite tedious to use. * Java HDF object package - a high-level interface based on JHI5. * HDFView - a Java-based viewer application based on the Java HDF object package.  * JHDF5 - a high-level interface building on the JHI5 layer which provides most of the functionality of HDF5 to Java. The API has a shallow learning curve and hides most of the house-keeping work from the developer. You can run the Java HDF object package (and HDFView) on the JHI5 interface that is part of JHDF5, so the two APIs can co-exist within one Java program. (from StackOverflow answer, 2012)  * NetCDF-Java is a Pure Java Library, that reads HDF5. However, it's hard to keep pure java version up-to-date with the standard, does not support all the features.  A way to set up native libraries (3rd option from JHDF5 FAQ):      ""Use a library packaged in a jar file and provided as a resource (by putting the jar file on the class path). Internally this uses the same directory structure as method 2., but packaged in a jar file so you don't have to care about it. Jar files with the appropriate structure are cisd-jhdf5-batteries_included.jar and lib/nativejar/.jar (one file for each platform). This is the simplest way to use the library.""       "
3,S15 Butler (v3),Improvements and tweaks to the butler as needed.
2,Research support for integrated view of all databases,"Metadata Store needs to contain information about all databases (data release databases, Level 1, Level 3 user databases)."
1,Add support for IPAC table format,Implement support for result formatting in IPAC table format.
2,Research supporting cutout from images with overlaps,Research producing cutout from images with overlaps.
3,SUI refactor Firefly package,"The team has studied and researched the web UI framework (DM-1148) in W15. A new framework will be decided in February 2015. Many classes in Firefly package need to be refactored to use the the new framework. This epic in S15 will be our first attempt for it. Developers will be involved in this effort: Trey Roby, Loi Ly, Tatiana Goldina, Lijun Zhang, Xiuqin Wu"
1,Design CSS schema to support table deletion,"Table/chunk deletion can be an extended process as some worker nodes may be temporarily down. We need to define a process and its supporting structures in CSS to allow gradual deletion of individual chunks and full tables.  Deliverable: a design of a system capable of deleting a distributed table (all chunks, all replicas). It should be possible to create a table with the same name after deletion."
0,Modify CSS structure to support table deletion,"Modify CSS structures to support DROP TABLE, as defined in DM-1896."
1,Consistency checking for table data CSS ,"CSS data on tables/chunks/nodes is supposed to be consistent at all times. Would be nice to have a tool that verifies consistency, probably including checking actual worker state."
0,Tool to dump CSS information,CSS information tree may become large and it would be nice to have a tool to examine that tree or parts of it. Something that dumps the tree in user-friendly way and allows filtering or summarizing.
1,Worker management service - design,"We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker  servers, probably HTTP-based. Data loading, start/stop should be handled by this service."
1,Re-implement data loading scripts based on new worker control service,Once we have new service that controls worker communication we'll need to reimplement WorkerAdmin class based on that.
0,Implementation of calibration transformation framework,"Following DM-1598 there will be a detailed design and prototype implementation for the calibration & ingest system. This issue covers cleaning up that code, documenting it, having it reviewed, and merging to master."
1,Continued footprint improvements,A redesigned API and support for topological operations within the Footprint class.  This continues the work started in DM-1107 in W15.  Breakdown: jbosch 15%; swinbank 85%
0,QSERV issues when working with scisql_s2PtInCircle,"This problem has been adressed un u/fjammes/DM-1841.  Here's Tatiana report:  {quote} This error happens on all DeepForcedSource queries. (It happens on DeepSource too, but not always.)  [2015-01-15 11:02:29] [Proxy][4120] Error during execution: -1 Ref=1 Resource(/chk/LSST/6970): 20150115-11:01:05, Complete (success), 0, Ref=2 Resource(/chk/LSST/7138): 20150115-11:02:19, Complete (success), 0, Ref=3 Resource(/chk/LSST/7140): 20150115-11:02:18, Error merging result, 0, Ref=4 Resource(/chk/LSST/730 (-1)  Query examples:  select * from DeepForcedSource where ra>0.4 and ra<0.6 and decl>0.9 and decl<1.1;  select * from DeepForcedSource where scisql_s2PtInCircle(ra, decl, 0.5, 1.1, 0.138) = 1; {quote}"
2,Outline an expandable Python framework for advanced users ,"Outline an expandable Python framework for use by advanced users in interactive or batch mode. Many users should be able to contribute software to the framework following simple API guidelines. Also look at configuration and delivery systems for the software. Start by looking for existing Python software that could be used in the areas of: - Database & file access - Data analysis frameworks - Display of data,, especially in an Astronomical context. - Reading and writing data in different formats - Graphing data locally or over the web - Science and astronomy data analysis - Event interfaces - VO functionality and connecting with existing software like DS9,, Aladin, Topcat, etc. - Ways to integrate modules with an LSST focus like 	AGN 	Large Scale Structures 	Galaxies 	Local Volume 	Solar System 	Astrostatistics 	Stellar Pops 	Strong Lensing 	Supernova 	Transients 	Weak Lensing 	Camera - Configuration and delivery system  "
3,Backport HSC multi-band deblend processing,"Breakdown: lauren 60%; price 40%  We need to transfer the recent HSC-side multi-band deblender changes to the LSST side, including all HSC issues in this query: https://hsc-jira.astro.princeton.edu/jira/issues/?filter=11603"
3,Interface design for full focal plane PSF estimation,"Breakdown: swinbank 40%; jbosch 30%; rhl 30%  This is mostly design work, but I'd like the goal to be a new Python command-line task that repeats as much of the current ProcessCcd as necessary to run full focal plane PSF estimation, and would serve as a starting point for a future-proof visit processing script.  Some issues include:  - Gather requirements from e.g. DESC people as to needed inputs and data flow.  - Get details of what camera/telescope systems will provide, and figure out how those related to what DESC needs.  - Design classes for camera/telescope engineering data and wavefront information.  Figure out how they'll be managed on disk (stored with Exposure, part of CameraGeom, with flats, biases, etc.).  - Discuss parallelization needs with middleware team, and determine a way forward that lets us design interfaces using future parallelization schemes that don't exist yet.  - Design Python task interface for full focal plane PSF estimation.  - Implement command-line task that calls the full focal plane PSF estimation task.  - Implement placeholder PSF estimation subtask that just uses existing PSF determiners on single CCDs.  This may be too ambitious for the 40 story points we've allocated, and should discuss either adding more effort or reducing the scope."
3,"DRP DM-S15-1,2,3 Bugs and Papercuts",Breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%
3,"DRP DM-S15-4,5,6 Bugs and Papercuts",Breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%
2,Prototype command interaction with Firefly,NULL
3,Research Javascript Frameworks: Finish new framework proposal,NULL
2,Create a deployable installation package for Firefly,"With Firefly being open-source, we should provide a simple all-in-one installation package so a user can quickly setup and deploy an instance the Firefly Tools web application."
2,Fine-tune data access interfaces,"Brought up by Gregory via comments on the API page.  Data release selection in queries: I see that the /db/... queries take a ""?"" query parameter ""db"" with an example value of ""DR1"", i.e., a data release selector.  A couple of remarks: * Will this query parameter be provided for all the Level 2 image data products, e.g., for retrievals of coadded images? **If so, then it needs an equivalent to the M4 ""GET /meta/v0/db/<type>"" query. * I assume the ""db"" query parameter defaults to the most recent data release. * Will the M4 query return an indication of which ""?db="" value is the current default? * I assume that the numeric-identifier components of the various paths are unique only within a single data release.  That means that eventually, in user documentation, we should make sure that they understand that they can't scan through different releases' versions of the same image (for example) just by varying the ""?db="" parameter.   * Are the identifiers also unique within a particular type (i.e., ""raw"", ""template"", ""coadd"", ""calexp"", etc.)?  -----  Distinguishing L1 and L2 versions of reprocessed data products  Since most or all of the L1 data products will be regenerated in each data release, the catalog and image APIs should presumably allow the user to distinguish between the two.  I see how this could be done for catalogs - the ""?db="" parameter presumably allows selecting something like ""L1"" (for the actively updated Level 1 database) in addition to the above-documented ""DR1"", ""DR2"", etc.  Will the L2 table names for the reprocessed L1 data products be generally expected to be the same as for L1?  (Barring the discovery of a serious issue that requires revision of the schema for the reprocessing.) How will the L1 and reprocessed-L1 image data products be distinguished?"
0,Fix missing virtual destructors,The compiler is warning about some derived class hierarchies that are lacking virtual destructors.  We should add at least empty implementations to the base classes of these hierarchies.
1,write Unit test and validation classes to validate the FITSreader refactoring ,NULL
0,Address misc. compiler warnings,"Fix places where compiler is warning about some things we are doing on purpose and which we don't intend to change.  This helps keep compiler noise down so its easier to notice ""real"" warnings."
1,update shapeHSM wrappers to latest external version,"The HSM shape code has undergone many improvements and bug fixes as part of being included in the GalSim package, and we've recently included those in the HSC fork of meas_extensions_shapeHSM (HSC-129, HSC-1093).  We should transfer those changes to the LSST side before tackling DM-981 (or at least before finishing it).  The story point estimate here is for the work already done on the HSC side (with the usual 50% factor for shared work).  The transfer to the LSST side should be essentially no effort.  To the extent that EVM cares about this, the credit should go to [~price], even though I ([~jbosch]) am doing the transfer."
1,Make unit tests use shared libraries,"Many (all?) unit tests are currently built as static executables which include all needed object files. This has several issues associated with it: - many files are compiled twice, once as *.os files for shared libraries, second time as *.o file for unit tests - unit tests do not test actual code in the shared libraries but instead separately-built copy of the same code  We should change our procedure and make unit test to link against shared libraries to avoid these problems."
0,Setup network for IPMI,NULL
1,Setup IPMI bastion hosts,NULL
0,Document how to use IPMI with LSST infrastructure,NULL
1,Base configuration of NFS servers,install and configure OS
2,Test new NFS servers,Test to confirm servers are configured optimally
3,Deploy first of the NFS servers,NULL
1,LOE - Week ending 12/26/14,NULL
1,LOE - Week ending 1/9/15,NULL
1,LOE - Week ending 1/16/15,NULL
1,LOE - Week ending 1/23/15,NULL
1,LOE - Week ending 2/6/15,NULL
1,LOE - Week ending 2/13/15,NULL
1,LOE - Week ending 2/20/15,NULL
1,LOE - Week ending 2/27/15,NULL
2,Define instrumental inputs to PSF estimation,"Define inputs needed to build physically-motivated PSF models beyond what's contained in the image data and the current CameraGeom.  This includes:  - static engineering data from lab tests  - slowly-varying engineering data (measured daily, weekly, etc.)  - per-visit auxiliary data from telescope and camera (including, but not limited to wavefront sensor data).  The focus here should be on APIs, not the technical details of the data; we want to define class hierarchies (perhaps some polymorphic ones) that can be used to pass this data around in the future.  We also want to identify and characterize any preprocessing that needs to be done before they can be used for PSF estimation.  Experts who should be consulted include [~gpdf] (who is in charge of making sure the camera and telescope interfaces to DM are well-defined), camera/telescope people he recommends, and people from the LSST DESC who have worked on physical PSF estimation and know something about the things they'll want (e.g. Michael Schneider, Aaron Roodman).  This is essentially a requirements-gathering task, so the output should be a confluence page, not code."
2,API for instrumental inputs to PSF estimation,"Following the requirements-gathering in DM-1939, come up with classes that can be used to pass the needed inputs to the PSF estimation code, and at least sketch out roughly how they will need to be managed by the butler and loaded by the framework code.  The output of this ticket should be an API design in confluence (possibly on the same page used for DM-1939), and an associated RFC."
2,Organize SUI design discussions,Organize the SUI team for web UI discussions to capture as much functions and components as possible.  A draft design document should be produced out of those discussions. 
3,Test data development and HSC stack integration,"Breakdown: price 20%; lauren 80%  This epic is focused on general stack testing and integration on HSC data, with a goal of getting LSST-side reductions of HSC data to the same level of quality  and robustness currently present on the HSC fork of the stack, which will involve a combination of backporting minor fixes from the HSC codebase and tuning parameters for LSST-side algorithms that supercede their HSC-side counterparts.  This will allow science-level algorithms to be tested using HSC data, and will provide functionality important for science-grade tests on other real data (such as the ongoing CFHTLS reprocsessing at IN2P3).  This effort will be focused on single-epoch processing (i.e. ProcessCcdTask), with coadd-level processing a stretch goal dependent somewhat on the completion of astrometric calibration work at UW."
1,HSC backport: convert Peak to PeakRecord,"This issue covers transferring all changesets from [HSC-1074|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1074] and its subtasks, as well as:  - An RFC to propose the API change, and any requested modifications generated by the RFC.  - Additional fixes to downstream code that's broken by this change (HSC-side changesets should be present for most of downstream fixes, but perhaps not all)."
1,HSC backport: guarantee consistent handling of peaks in deblender,This issue covers transferring changesets from:  - [HSC-134|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-134]  - [HSC-1109|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1109]  - [HSC-1083|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1083]  
1,HSC backport: multiband processing for coadds,"This issue includes transferring changesets from many HSC issues:  - HSC-1060  - HSC-1064  - HSC-1065  - HSC-1061  Most of this is in multiBand.py in pipe_tasks, but there are scattered changes elsewhere (including updates to camera mappers to include the new datasets, for which we'll need to modify more than just obs_subaru).  However, before we make these changes, we'll need to open an RFC to gather comments on the design of this task.  We should qualify there that this is not a long-term plan for consistent multiband processing (which we'll be starting to design on DM-1908), but a step towards better processing in the interim.  Note: while I've assigned this to [~lauren], as I think it will be very helpful for her to get familiar with this code by doing the transfers, the RFC will have to involve a collaboration with [~jbosch], [~price], and Bob Armstrong, as we can't expect someone who wasn't involved in the design to be able to write a document justifying it."
1,HSC backport: low-level Footprint merge code,"This is a transfer of changesets from the follow epics:  - [HSC-1020|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1020]  - [HSC-1075|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1075]: only the afw changes  Because this is purely an addition to the interface, and we're planning to redesign that interface in DM-1904, I don't think we need an RFC here."
2,Add support for async request cancellation to xrdssi,NULL
3,S15 Implement Query Cancellation,Add support for query cancellation.
1,Add abstraction in czar for unit tests,Add hooks in czar that will let us build unit tests.
1,Unit test for query cancellation,NULL
0,"Change log priority for message ""Unknown column 'whatever' in 'field list'""  ",Next message should be logged with ERROR priority:  {code} 0204 15:08:03.748 [0x7f1f4b4f4700] INFO  Foreman (build/wdb/QueryAction.cc:250) - [1054] Unknown column 'whatever' in 'field list'   {code}
0,Post meas_base move changes to Kron,"These are to note leftovers from DM-982.  They could be done in a single issue. 1.  I commented code out referring to correctfluxes, but it will need to be restored once it is available in the new framework.  2.  Jim asked me to replace the computeSincFlux which is currently in PsfImage.cc in meas_algorithms with a similar call in meas_base/ApertureFlux.cc.  I did not do this because it became rather complicated, and can just as easily be done when the meas_algorithms routine is moved or removed.  Basically, the templating in ApertureFlux is on Pixel type, whereas in meas_algorithms it is on ImageT (where ImageT is not necessarily a single class hierarchy -- e.g., Image and MaskedImage).  So I left this for now."
3,HSC backport: deblended HeavyFootprints in forced photometry,"This is a transfer for changesets for [HSC-1062|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1062].    Unlike most of the HSC backport issues for multiband deblending, these changes will require significant modification the LSST side, because we need to apply them to the new forced measurement framework in meas_base rather than the old, HSC-only one in meas_algorithms and pipe_tasks.    Also include [HSC-1256|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1256], [HSC-1218|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1218], [HSC-1235|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1235], [HSC-1216|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1216]."
2,International Network Design and Implementation,FIU/Amlight is expected to provide a full capacity 3 x100 Gbps link by FY17.  
2,REUNA will provide a “pre-operations” link between La Serena and Santiago,REUNA will provide La Serena - Santiago links at full capacity 100 Gbps link by FY17 and a 40 Gbps secondary link by FY20.  This link will support testing and development prior to that time.
3,Chilean National Network Design and Implementation,Refer to REUNA MREFC sub award contract for deliverable details.  This covers non-contract work by the AURA/LSST.
3,S15 Data Distribution & Replica Mgmt Prototype,This epic covers building an initial prototype of the Data Distribution and Replication system. The design is covered through DM-1060
2,Prepare data set for large scale tests,"Load data set for large scale tests (duplicate, partition, load)"
1,Run large scale tests,Coordinate running large scale tests
1,Parallelization requirements for PSF estimation,"We need to gather algorithmic ideas for how full focal plane PSF estimation will work from a parallelization and data flow standpoint, and discuss with the middleware team how these should be handled from an interface standpoint.  Questions include:  - Will we need to do significant cross-CCD image processing or require significant memory for these tasks?  If so, should we structure this via message passing between CCD-level processes, or scatter-gather?  - Assuming a scatter-gather approach, will we need multiple scatter/gather iterations when processing a single visit?  - How much data will be passed between threads/processes?  Would this include complex serializable objects, or just POD arrays?  - Will different PSF estimation plugins will have different parallelization requirements?  Or, can we define the plugin interfaces at a low-enough level that parallelization can be handled by the framework?  If plugins do need to control their own parallelization, how do we make parallelization interfaces accessible to the plugins?"
1,Python interface for full-visit PSF estimation,"Create a Python interface for a pluggable PSF estimation system that supports algorithms that will operate over full images.  This should include a sketch of how a calling command-line task, and placeholders for parallelization interfaces that may not yet be finalized.  The output of this issue is a completed RFC on the design."
1,Command-line driver and placeholder implementation for PSF estimation,"Create a command-line task that makes use of the new PSF estimation interface, duplicating as much of ProcessCcdTask's functionality as necessary to provide the inputs to PSF estimation (I expect this new task to ultimately replace ProcessCcdTask).  This may have to include workarounds or temporary implementations for parallelization features that are not yet available.  Create a simple PSF estimation placeholder that simply uses existing single-CCD PSF-determiners."
3,CModel flux validation and testing,"Investigate the performance of the new version of the CModel code on various test datasets, including HSC data (following DM-1942), SDSS, and possibly CFHT data.  Breakdown: lauren 100%"
1,Create a kind of Wcs that encapsulates a TAN WCS and a distortion model,We can simplify the astrometry solver if we have a Wcs that encapsulates a pure tangent-plane WCS and a distortion model that takes converts between PIXELS and TAN_PIXELS. This is useful because at the early stages of processing raw data we have a TAN WCS from the telescope control system and a pretty good estimate of distortion from the optical model (represented in the camera geometry).  The result will be a Wcs whose sky<->pixel transformation is a reasonable approximation of reality (and a much better approximation than the tangent-plane WCS that is currently available. This will potentially eliminate a significant amount of confusing code that attempts to correct for distortion by manually applying the optical distortion model.
0,Build 2015_02 Qserv release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
0,"Fix enclose, escape, and line termination characters in qserv-data-loader","Add this string to mysql loader 'LOAD DATA INFILE' command:   {code}  q += ""ENCLOSED BY '%s' ESCAPED BY '%s' LINES TERMINATED BY '%s'"" % (enclose, escape, newline) {code} and add params in cfg file."
3,S15 Implement Fully-RESTful Data Access Web Service,"Improvements to the skeleton of Data Access Web Service built in W15. Make all responses fully RESTful (including results, and errors, setting response headers). Fine-tune API, add support for versioning, unit testing."
1,Research and Document API Versioning,"Research and document versioning of the RESTful API (through flask blueprints). In particular, need to understand how to avoid code duplication between different versions of API."
1,Add SQLite-based v0.1 unit testing for metaserv,"Add unit tests for the RESTful flask based API. I think it'd be most useful if we could  a) load some test data into underlying metaserv  b) run programmatically things like ""curl -H accept:text/html http://127.0.0.1:5000/meta/v0/db/L2/DC_W13_Stripe82/tables"" etc (more examples in dax_*serv/README.txt) and verify that we got what we expected. Do it for both json and html."
1,Add error handling for webserv,Add basic error handling for the RESTful flask-based API
1,Improve security for mysql in python,"Revisit all code that talks to mysql from python to use parameter bindings instead of direct string substitutions. In practice: {code} conn.execute(""SELECT * FROM t WHERE name=%s"" % theName) {code} should be replaced with {code} conn.execute(""SELECT * FROM t WHERE name=%s"", (theName,)) {code}  For details, see http://mysql-python.sourceforge.net/MySQLdb.html#some-examples "
0,Fix JDBC timestamp error,JDBC driver returns an error on next query:  {code:sql} sql> select * from Science_Ccd_Exposure [2015-02-06 13:39:37] 1 row(s) retrieved starting from 0 in 927/970 ms [2015-02-06 13:39:37] [S1009] Cannot convert value '0000-00-00 00:00:00' from column 32 to TIMESTAMP. [2015-02-06 13:39:37] [S1009] Value '[B@548997d1' can not be represented as java.sql.Timestamp {code}
3,Redesign/Refactor WCS and Coord,"%50 KSK, %50 RO Currently WCS is mutable and Coord objects are heavyweight.  Refactor WCS to be immutable and make Coord less heavyweight.  Include lists of Coord objects.  It's possible astropy could inform in that area.  Also, remove TanWcs in favor of TanSipWcs since TanWcs can have SIP terms."
3,Update analysis tasks: diffim and snap combination,"50% KSK 50% RO  A small amount of this work is in 02C.03.01 The diffim task needs to be looked at.  It needs to be updated to use current mechanisms.  It should also be refactored to split out some of the procedural code into methods.  In a similar task, the lsstSimIsrTask needs to include a real snap combine step.  Currently, one of the snaps is dropped on the floor.  For this round just implement naive snap addition and morphological CR rejection."
3,Define API for Stack Astrometric Calibration,"70% RO 30% KSK This should also include a minimal implementation.  This should be done with an eye toward photometric calibration.  Prerequisite: Get multi epoch (multi-band?) catalogs of centroids from some trusted source (CFHT, HSC?).  1. Load all stars that overlap a patch for all epochs. 2. Associate all stars on each chip.   2a. Implement K-D tree 3. Fit model for rigid chip system + optics + atmosphere.  Eigen for sparse model fitter.   3a. Allow for external catalog.   3b. This could include a class to fit XYTransforms 4. Turn result into WCS.  So maybe a down scope for a single cycle epic is to get matches and interfaces for models and solvers. "
3,Research DCR in the context of DiffIm including possible algorithms for mitigation.,"It is not clear how template coadds will be built.  This includes understanding the data necessary to generate a template for the entire sky.  This epic is to identify possible techniques as well as the risks associated with each technique.    This does not need to pin down the exact algorithm or the specific selections, but should inform what further development is necessary to avoid putting alert generation at risk."
3,Refactor Approximate and Interpolate classes,100% RO A base class for this will be created in DM-740 as a part of epic DM-85.    This epic is to implement the classes to replace the original functionality.
3,SQuaRE Support,50% KSK 50% RO SQuaRE has asked that we leave 20 SP free per Cycle to help out.
0,Story point display and roll-up in epic display,"I understand that there is a pending request to display the story points for individual story issues in the mini-table in which they are displayed for an epic.  It would also be useful to see a rolled-up total of the story points for the defined set of stories - so that, among other things, this could be compared to the story point value for the epic.  Ideally the story points for the roll-up might be displayed as ""nn (mm)"" where nn is the total points and mm is the number of points remaining to do (or done already - I don't care which as long as the definition is clear)."
2,Define faulty/consistent states and recovery process,"Whiteboard session(s) to gather a list of invariants / principles that define a consistent state, a list of fault conditions, and the steps in failure recovery."
2,Data transport mechanism for data distribution,"Decide on a transport mechanism for data (bit torrent, scp, or ?). We must take into consideration whether the data source matters in this choice (e.g. tape vs known good node), as well as how to identify that a data source is the correct one (e.g. via checksums and sequence numbers)."
3,Architecture for failure detection and resolution,"Come up with an architecture for detecting failure or non-nominal conditions (e.g. under replication). The core question to resolve is whether we go with a distributed approach, or with centralized control."
2,Research existing theory and prior art,"Peruse the distributed systems literature for prior approaches to this problem, and examine existing system implementations for components/ideas we could reuse."
2,Document data distribution/replication plan,"Produce an overview document that explains our definitions, architecture and strategies for dealing with data distribution and replication. "
2,Define strategy for adding and removing data ,Define a strategy (push/pull distributed/decentralized) for recognizing incoming data and cleaning up/removing stale/deleted data. Is data ingest just another form of failure recovery?
1,Define data distribution/replication testing strategy,"Once we decide on a design for data distribution / replication, we should come up with a test plan."
3,Package Reorganization (Science Pipelines),"Breakdown: jbosch 50%, swinbank 50%"
0,switch ndarray to external package,There is already an external ndarray project on GitHub (we've been using a fork of that).  We should merge the forks and switch to using the external package. 
0,"merge ""basics"" packages","Create detailed RFC and implement merge of base, utils, and daf_base.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
0,separate pex_exceptions from base and rename,"Remove dependency on base from pex_exceptions and rename to just ""exceptions"" (after RFC).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
0,Move Wcs from afw::image to afw::coord,Create RFC and implement move of Wcs from afw::image to afw::coord.  See https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
0,move Jarvis/shapelet code to legacy package,"Create RFC and remove Jarvis/shapelet package from meas_algorithms, into new legacy sci package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
0,"move Psf, Kernel code to new afw::convolution subpackage",Create detailed RFC and implement move for these packages.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
0,split pipe_base into command-line and non-command-line components,"Create detailed RFC and implement package split, to separate basic Tasks (to be used as e.g. subtasks) from CmdLineTask and ArgumentParser.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
1,Implement image stitching,"This story involves implementing code that stitches images, simple case that does not involve tract boundaries. More advanced case in covered in separate ticket. We will need to determine WCS information for the target images."
0,Create interface and utility package for single-frame/forced processing,Create detailed RFC and implement move of interface and utility code from multiple existing packages to new package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
1,Design and implement RESTful API for image stitching and rotation,NULL
0,Split PSF estimation and PSF model code into separate package,Create detailed RFC and implement move of concrete PSF estimation code and Psf subclasses from meas_algorithms to separate package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
0,rename packages with minimal reorganization,"meas_deblender, ip_isr, meas_astrom, meas_modelfit, and ip_diffim do not require major refactoring to fit into the new package reorganization, but they should be renamed (with sci_prefixes).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
0,Split measurement plugins into separate packages,"Create detailed RFCs and implement splitting measurement plugins into separate package.  May want one package for extremely basic plugins, always-used plugins (PixelFlags, TransformedCentroid, etc).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
0,Split coaddition code and single-frame/forced command-line drivers,This should split all content in pipe_tasks into two packages (aside from what may have been removed in previous issues).
2,Research how to support L3,Research implications of having to deal with updatable Level 3 data.
1,Architecture for supporting small non-partitioned tables,"Some tables, like Exposure, provenance, will not be partitioned, and the current plan is to either replicate them on each node, or store on shared file system. Need to decide how it will be dealt with."
2,Research software deployment on the qserv cluster,Discuss and decide how to deploy and upgrade Qserv software
1,Investigate procedures for package reorganization,e.g. develop script to handle bulk namespace changes.
3,DRP S15 support for SQuaRE,Breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%
3,FY15 Key Performance Metrics,"Collect data, compile scripts, perform measurements as necessary to report figures in respect of the FY15 key performance metrics.  Breakdown: lauren 50%; rhl 50%"
3,Support Exposure Use Cases,Development in support of Exposure Use Cases
1,Implementing stitching multiple patches across tract boundaries in a coadd,NULL
2,refactor afw Swig to improve build times,"I have an idea for how to improve Swig build times that we should get vetted (and possibly improved upon) by a true Swig expert (even if that costs a bit of $$).  This issue includes vetting that idea (splitting up classes into multiple per-package module builds), getting it through the RFC process, and implementing it in afw."
1,Add image-query related KPIs to the plan,"Existing plan in LDM-240 does not mention image related KPIs. Need to come up with a road map, and propose KPIs. This should be synchronized what realistically NCSA cluster can deliver in any given FY."
3,FY19 Setup Database for Deep Drilling,NULL
3,FY18 Setup Calibration Database,"Need to think through issues related to supporting calibration. Schema, requirements that will require special optimizations."
3,S17 Build Prototype of AlertProd and L1 User Database,"Build a working, non-optimized prototype of the [Alert Production and L1 User Database|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog].    Deliverable: working, non-optimized prototype of AlertProd Database."
3,X16 Revisit Design of AlertProd & L1 Db,"Revisit the [design of Alert Production Database and Level 1 User database|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog], including schema, indexing, partitioning, synchronization, replicating, fail over. Verify that the latest requirements match the design. This epic will likely involve experimenting and light-weight standalone prototyping related to parts of the system that might be non-trivial to scale or to implement.    Deliverable: a refreshed design document for Alert Production and L1 User Database."
3,FY17 Design Internal DRP Database,"Internal DRP DB will be used to store  * all bookkeeping (provenance, what run what did not, etc) * intermediate data products (might be larger than final data products),  * a subset of data (what we need by DRP), eg foorprints of objects  Internal DRP DB might need its own spatial engine.  It is expected that SDQA will run on that database.   Need to think through issues related to supporting internal Data Release Production. Schema, requirements that will require special optimizations.  Need to define reliability requirements.  In limited cases pipelines might want to use internal db (instead of files). Example: select all objects from a given region. Need to understand query load and complexity coming from DRP."
3,FY18 Revisit Design of L3 Support in Qserv,Need to think through issues related to supporting Level 3 databases
3,FY19 Design Next-to-database Data Analysis System,Need to design the system that will allow users run their own custom data analysis next to database.
3,FY19 Implement Next-to-database Data Analysis,Need to implement the system that will allow users run their own custom data analysis next to database.
3,W16 Improve Data Provenance Design,"We have a detailed design of the Provenance, described at https://dev.lsstcorp.org/trac/wiki/db/Provenance. Work covered by this epic involves:  1. Revisiting the design and tweaking it as necessary. In particular:  * Describing in more details interactions with key data producers (DRP, AlertProd, Calibration, L3 data brought in by users).  * Estimating the size of provenance data  * Considering querying the provenance data    2. Evaluating existing off-the-shelf provenance systems/tools.    Deliverable: a document describing data provenance architecture / schema supported by a standalone proof-of-concept prototype."
2,Catch all epic for essential fixes in Science Pipelines DM-W15-5,NULL
1,server side preparation for  histogram plot (1),Convert necessary code to make it possible for a JavaScript component to place a JSON request to the server and to parse the resulting RawDataSet.  
2,Client side plot display for histogram,"- Create a React JavaScript component, which takes the data and renders histogram.  - Make it possible to call this component from GWT code, using experimental JsInterop technology in GWT 2.7"
3,"SUI Investigate L3 data/tools requirements, evaluate potential tools ",There are many overlap areas in L3 data analysis tools with the general science user tools. We want to identify those requirements and needs to help making SUI components adaptable for L3 data production and analysis.
3,Start requirements gathering for pipeline QA visualization needs  ,Gather use cases for pipeline QA visualization tools. We want to build the SUI components in such a way that they could be used to support QA needs. 
3,SUI Build the visualization components that could be used independently,"Currently we identified three basic components: Image visualizer,  tabular data display,  2D XY Plot. All three could share the data model and provide inter activities between the components.  "
3, Integration and test monitoring architecture Part I,"[retitled to better capture cycle scope]    Develop and deploy a layer to capture the outputs, initially numeric,  of integration testing afterburners such as sdss_demo, hsc_demo, and  others developed this cycle. Also capture meta-information such as  execution time and memory footprint. Propose log format to standardise  production of such informations. Investigate notification system based  on trending away from expected values. Investigate data provisioning  of integration tests such as storage of test data in GithubLFS.    [75% JMP 25% JH]        "
3, Firefly-based data display for SQuaSH - Part I,"[Epic retitled to better reflect cycle scope]    This epic covers work relating to working on the visualisation side of  the Science QA Analysis Harness. It is a timeboxed effort to come up  to speed with Firefly in particular, evaluate it against our needs,  and provide any feedback to the Firefly team. Some prototyping of  visualising integration dataset products will also be involved.     [AF 100%]         "
3,Maintain list of OSes that pass build and integration testing ,"Provide an automatiically generated and updated pages showing operating systems that are successfully building  and integrating the stack from source.   [FE at 75%, JH at 75%]"
3,Specify system for performing CI on Docker stack containers,Investigate how we can CI first-party Docker containers with runnable stacks    [JH 100%]
3,Release engineering  Part One,"Bucket for public stack releases  [FE at 75%, JH at 75%]"
3,Miscellaneous service support improvements,"JIRA, comm toos,  etc for DM and  non-DM teams (indicate)    In order to avoid fractional story points, some 0    [FE at 75%, JH at 75%]"
0,Attend Scale 13x conference,"Attend database talks, in particular the MaxScale proxy talk (http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale?utm_campaign=north-american-trade-shows&utm_source=hs_email&utm_medium=email&utm_content=16099082&_hsenc=p2ANqtz-_MFjfxvpCdmV_Ax2RKDdOGypHPQ85UL-UMuy0eRs_MrlJ2qJVp-MXx-g7_-dAQsq0trpA61hkZrzO-3gp6bKVkpK52fQ&_hsmi=16099082).  If anyone has questions they would like me to ask, please post them here as well.  I will post notes to this issue. "
0,Data loader should always create overlap tables," We have discovered that some overlap tables that are supposed to exist were not actually created. It looks like partitioner is not creating overlap files when there is no overlap data and loader is not creating overlap table if there is no input file. Situation is actually symmetric, there could be non-empty overlap table but empty/missing chunk table. When we create one table we should always make another as well. "
1,Clean up QuerySession-related code in czar,"(created in response to DM-211) This ticket should address the following inelegancies in the qserv-czar.   * QuerySession->QueryPipeline. The ""session"" abstraction has moved to a better place. The iterator portion should be shifted into its own separate class, though perhaps still associated with QueryPipeline. The iterator portion's new home should be amenable to eventually moving the actual query materialization to the worker, though we shouldn't introduce new abstractions until we are actually ready to move the substitution/materialization to the worker.  * QueryContext needs to be split into incoming external QueryContext and a sort of QueryClipboard for passing information between analysis/manipulation plugins. Eventually, I imagine a chain/tree of them attached to the select statements themselves in order to represent subquery scope nesting (which is complicated to represent and to reason about--nesting and the resulting namespace resolution is tricky), but I don't think we should try doing the chaining in the first phase. For this ticket, create QueryClipboard to hold the portion for interchange between analysis plugins. Query analysis plugins would then pass this object (which points at an immutable? QueryContext) between themselves. QueryClipboard probably should live in qana, QueryContext in query (unless there is a good reason to move it).   "
0,Rename TaskMsgFactory2,Rename TaskMsgFactory2 to TaskMsgFactory.    Please see DM-211 for more information.
2,Port fault-recovery testing code to XrdSsi,"Please see DM-211 for the origin of this ticket.  BillC put in code to introduce random errors in query dispatch as part of working on code to recover from faults. In the port to the XrdSsi API, we did not port this code. This story is to introduce the ability (compile-time configurable, if not command-line or dynamically configurable) to simulate these sorts of faults to exercise the fault-recovery (confined to retrying on transient-ish failures) code."
0,Creates overlap tables even if empty while loading data,"Query execution expects all chunk and overlap tables to exist, even if they are empty. In short term, that means loader should: * look at all chunks and add corresponding overlap chunks, * look at overlap chunks and add missing empty chunk table "
3,FY17 Implement Data Verification Tool,"Need a tool for verifying whether data is in consistent stage (e.g., right after loading, after some upgrades, in general at any given time).  The list of things to check include: * empty chunk file, * xrootd exported DB, * data tables * overlap tables, * data_0123456789 tables * chunkId, subChunkId columns existence  Some of the above can be automatically fixed on the spot when problem is discovered."
0,Add test case to catch missing empty chunks or overlaps,Discussed at db hangout 2015-02-18.   We need a use case to test for missing empty overlap chunk tables and/or empty chunk tables.
3,FY17 Design L2 Catalog Swap/Release Automation,Need to think through the issues related to releasing L2 catalog / swapping a new one in place of an old one
3,FY17 Build AP-ready Data Provenance System,"Improvements to the first version of the standalone prototype built through DM-2042. Discussions with the Application Team on capturing provenance and integrating DRP with the provenance system. Add scaffolding / unit tests that will simulate data producers, in particular DRP.  Deliverable: DRP-ready System for capturing provenance."
3,FY18 Integrate AlertProd with Data Provenance,Integrate Alert Production with the Provenance system.
3,FY17 Implement Async Queries in Data Access Web Services,"Work includes: * asynchronous requests, request status, retrieving results for dbserv and imgserv"
3,FY19 Implement Partial Query Results,NULL
2,S17 Improve ImageServ,NULL
3,W16 Add Support for Multi-table Shared Scans,Implement multi-table shared scans. Ensure that shared-scans are not delaying interactive queries. The baseline architecture of the shares scans are described in [LDM-135 Shared Scanning|http://ldm-135.readthedocs.org/en/master/#shared-scanning].
3,F16 Qserv KPMs,NULL
3,F17 Run Large Scale Qserv Tests,NULL
3,F18 Run Large Scale Qserv Tests,NULL
3,FY19 Implement Missing Features in Qserv for L3,Need to think through issues related to supporting Level 3 databases
3,FY20 Improve Design of Next-to-database Data Analysis,Need to implement the system that will allow users run their own custom data analysis next to database.
3,FY18 Demonstrate Fault Tolerance,NULL
3,FY18 Implement Basic Resource Mgmt for DB,"Includes things like query throttling per user for all databases (L1, L2, L3)"
3,FY19 Optimize Resource Mgmt for DB,NULL
3,FY18 Implement Basic Resource Mgmt for Images,NULL
3,FY19 Optimize Resource Mgmt for Images,NULL
3,W16 Distributed Loader - Research,"In production, we will need a distributed loader that will be capable of loading entire data set produced by DRP within 24-48 hours. This epic involves researching all the needs, requirements and constraints, and exploring what the best architecture for a distributed loader would be. Related doc: [LDM-135 §8.15.2|http://ldm-135.readthedocs.org/en/master/#data-loading]"
3,W16 Distribution and Replica Mgmt Prototype v2,"This epic involves building a complete, working prototype of the Qserv data distribution and replica management."
3,FY18 Implement L2 Catalog Swap/Release Automation,Need to think through the issues related to releasing L2 catalog / swapping a new one in place of an old one
3,FY17 Add Support for Managing Per-user Access for DB,NULL
3,FY18 Add Support for Managing Per-user Access for Image and File Archive,NULL
3,FY18 Integrate Qserv with EFD,NULL
0,Port metaREST.py to db,"metaREST_v0.py in metaserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. "
0,Port dbREST.py to db,"dbREST_v0.py in dbserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. "
1,Long term database work planning,Long term planning (updating LDM-240).
0,Package andyH xssi fixed version (>2MB answer pb) in eups,"See DM-1847 - Andy made a patch, it'd be good to the xrootd we use for our stack."
3,FY18 Revisit L2 Catalog Schema,Revisit the baseline schema
3,FY18 Implement Internal DRP Database,Implement Internal DRP Database as designed in DM-2038
3,FY17 Improve ImageServ,NULL
3,FY19 Demonstrate Qserv Fault Tolerance,Including multi-master failover
3,FY19 Optimize Partitioning Granularity,"We have been always talking about having ~20K chunks per table, and it was driven primarily by spreadsheet-based analysis. We need to look in more details into that, and perhaps even change the model if needed, e.g., introduce  different partitioning for larger tables, like ForcedSource."
3,FY17 Improve Query Coverage in Qserv,Currently Qserv supports only a limited subset of queries. We need to make sure it supports all queries that users need to run.
3,FY18 Improve Query Coverage in Qserv,Currently Qserv supports only a limited subset of queries. We need to make sure it supports all queries that users need to run.
3,"FY17 Support Explain, Show, List Commands","Implement [explain|http://dev.mysql.com/doc/refman/5.0/en/explain.html] and [show|http://dev.mysql.com/doc/refman/5.0/en/show.html] commands for Qserv. Also, commands such as ""list tables"" will need to be intercepted and overloaded. "
3,FY19 Implement Multi-master for Qserv,NULL
3,FY19 Make Database Secure,"Revisit security issues, such as sql injections, detecting and shielding from DoS attacks, etc."
3,FY19 Build/Setup Basic Qserv Monitoring,frontend/worker health monitoring (and management?)
3,FY18 Implement Failover for L1 Database,"Need to implement and test failover - a failure of the master copy of L1 database, and automatic fail over to a replica. The design of the Alert Production L1 database is covered [here|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog]."
3,W16 Optimize Secondary Index - Research,"Work on the secondary index (objectId --> chunkId / subChunkId mapping). This needs to be scalable to 40B entries. Since we are planning to ingest all data from DRP in <2 days, building should take <2 days. This epic involves researching applicable technologies (including experimenting with most promising ones). Deliverable: proposed technology / architecture along with measures performance at production scale (40 B entries). "
3,FY18 Add Support for Non-partitioned Tables,"Non partitioned tables will need special attention. Options include: a. replicating them on each worker node b. keeping them on a shared file system c. federating  Need to thing through these issues, pick the best architecture and implement it."
3,FY17 Revisit Qserv Deployment on Cluster,NULL
3,FY18 Design Qserv Software Upgrading,Need to understand how to do software update for Qserv 
3,FY19 Improve Query Coverage in Qserv,NULL
3,FY20 Improve Qserv Monitoring,frontend/worker health monitoring (and management?)
0,Resolve compiler warnings in new measurement framework,"When building {{meas_base}}, or any other measurement plugins which follow the same interface, with clang, I see a bunch of warnings along the lines of:  {code} In file included from src/ApertureFlux.cc:34: include/lsst/meas/base/ApertureFlux.h:197:18: warning: 'lsst::meas::base::ApertureFluxAlgorithm::measure' hides overloaded virtual function       [-Woverloaded-virtual]     virtual void measure(                  ^ include/lsst/meas/base/Algorithm.h:183:18: note: hidden overloaded virtual function 'lsst::meas::base::SimpleAlgorithm::measure' declared here:       different number of parameters (4 vs 2)     virtual void measure( {code}  This is an artefact of a [workaround for SWIG issues|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390]; the warnings aren't indicative of a fundamental problem, but if we can avoid them we should.  While we're at it, we should also fix:  {code} include/lsst/meas/base/ApertureFlux.h:233:1: warning: 'ApertureFluxResult' defined as a struct here but previously declared as a class       [-Wmismatched-tags] struct ApertureFluxResult : public FluxResult { ^ include/lsst/meas/base/ApertureFlux.h:65:1: note: did you mean struct here? class ApertureFluxResult; ^~~~~ struct {code}"
2,W16 Understand Async Queries in Qserv,"Understand how disruptive the changes related to implementing asynchronous queries will be for Qserv.    Delivarable: brief description outlining changes needed, with story point estimate."
0,Add parameter binding to db interface,NULL
0,Validate user query in dbREST,Need to validate query (from security standpoint that user enters through rest api.
1,Support DDL in MetaServ - implementation,"DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves building tools that will load the DDL schema into MetaServ. Design aspects are covered in DM-1770."
0,"Add meas_extensions_shapeHSM to lsstsw, lsst_distrib","meas_extensions_shapeHSM has just been resurrected from bitrot, and should be included in our distribution.    Contrary to DM-2140, it should probably not be included in lsst_apps, as it's not clear we want to add a dependency on tmv and GalSim there."
1,General OpenStack Learning,NULL
3,Setup spare test hardware for OpenStack testing,NULL
1,Test Ubuntu OpenStack,NULL
1,Test Mirantis OpenStack & Fuel,NULL
1,"Figure out OpenStack networking (vLAN, routing, etc)",NULL
3,Figure out OpenStack integration with LDAP,NULL
0,Log fails on uniccode string,"Log is currently failing if we pass unicode string, it is easy to reproduce by doing: log.info(u""hello""). It fails with:  {code}   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 103, in info     log("""", INFO, fmt, *args, depth=2)   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 94, in log     _getFuncName(depth), frame.f_lineno, fmt % args)   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/logLib.py"", line 648, in forcedLog_iface     return _logLib.forcedLog_iface(*args) TypeError: in method 'forcedLog_iface', argument 6 of type 'std::string const &' {code}"
0,Data loader crashes on uncompressed data.,"Vaikunth just mentioned to me that the is a crash in data loader when it tries to load uncompressed data: {noformat} root - CRITICAL - Exception occured: local variable 'outfile' referenced before assignment Traceback (most recent call last): File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 312, in <module> sys.exit(loader.run()) File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 248, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 168, in load return self._run(d atabase, table, schema, data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 192, in _run     files = self._gunzip(data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 388, in _gunzip     result.append(outfile) UnboundLocalError: local variable 'outfile' referenced before assignment {noformat}  It looks like we never tested loader on uncompressed data and there is a bug in handling uncompressed data. "
0,Add support for JSON - define structure,"As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This includes defining the exact format, and implementing it. This story covers defining the format."
1,Implement Image Response for ImgServ,"This story covers implementing proper response, and the header metadata for the fits image response."
0,Setup webserv for SUI tests,"We need to setup a service (eg on lsst-dev) that can be used by the IPAC team to play with our webserv/metaserv/dbserv/imgserv.  The server runs on lsst-dev machine, port 5000. To ssh-tunnel, try: {code} ssh -L 5000:localhost:5000 lsst-dev.ncsa.illinois.edu {code}  An example usage:  {code}   curl 'http://localhost:5000/db/v0/query?sql=SHOW+DATABASES+LIKE+""%Stripe%""'   curl 'http://localhost:5000/db/v0/query?sql=SHOW+TABLES+IN+DC_W13_Stripe82'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.DeepForcedSource'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.Science_Ccd_Exposure'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+deepForcedSourceId,scienceCcdExposureId+FROM+DC_W13_Stripe82.DeepForcedSource+LIMIT+10'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+ra,decl,filterName+FROM+DC_W13_Stripe82.Science_Ccd_Exposure+WHERE+scienceCcdExposureId=125230127'   curl 'http://localhost:5000/image/v0/raw/cutout?ra=7.90481567257&dec=-0.299951669961&filter=r&width=30.0&height=45.0' {code} "
1,Refactor Geom class in Firefly,The Geom class was ported from C code 20 years ago.  It needs to refactor to comply with Java OO design.  
1,Review at DM leadership team meeting,"review document  with Kantor, KT, Hobblit, and Lambert,  including prep time "
1,Refactor document for that specifications are clearer,1) Have one basic definition of racks and other components in the specifications.  2) Fully write up first full draft  specification for the supporting material  handing area.
0,receive / process comments  from Jeff Barr a,"receive edits from Jeff Barr,  accept the formatting and mechanical l edits. Compose separate email list issues related to non LSST tenants in the  room.  "
0,Investigate  Commerical  vendor to deal with comments on requirements. ,process email discussion about the need to liaison with the putative Chilean design contractor.    Kantor suggests a contractor to support requirements may be apropos. 
1,Work inside NCSA to connect procurement contract Modification to OSPRA contract officet,"work Jeff's proposal until it reached university contract officer. -- January meeting  -- clarify  purchasing rules -  Internal discussion of property management, -  General work within  contract modification process. "
1,Implement JSON Results for MetaServ and DbServ,"Implement JSON results for Metadata Service (see all M* in https://confluence.lsstcorp.org/display/DM/API),  and Database Service (see all D*) as defined in DM-1868"
0,Disable testDbLocal.py in db if auth file not found,tests/testDbLocal.py can easily fail if required mysql authorization file is not found in user home dir. Skip the test instead of failing in such case.
2,Adapt integration test to multi-node setup v2,Following DM-595 we can start qserv in multi-node configuration. Next step is to be able to run integration tests in that setup. This needs a bit of understanding how to distribute chunks between all workers in a cluster and how to load data in remote mysql server.
2,Worker management service - impl,"We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker  servers, probably HTTP-based. Data loading, start/stop should be handled by this service."
2,Implement worker-side squashing,"In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation.  The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented."
1,Migrate Qserv to external sphgeom,"Migrating qserv to the new c++ geometry API required porting a fair amount of code from the python layer and updating the plumbing in the czar. During implementation, the sphgeom was in the process of finding a home, so the sg code was temporarily placed under core/modules.  This ticket covers: * removing core/modules/sg * updating code to point at the external sphgeom * updating build-logic to properly depend on and link with external sphgeom."
3,S17 Design Prototype EFD Schema for DRP,"The epic involves understanding the structure of the EFD database produced by the Engineering and Facility team, and designing schema that will be best suited for Data Release Production. Note that the original EFD database may not even be in MySQL, there were discussions to store it in Postgresql.  Deliverable: Alpha version of the EFD database schema for DRP with ""real"" data loaded (if available)."
3,FY18 Design DRP-ready EFD Schema,NULL
1,Move astrometry_net wrapper code from meas_astrom to meas_astrometry_net,We would like to remove all astrometry.net wrapper code from meas_astrom and put it in a new package with a name such as meas_astrometry_net.  This will also require moving any abstract task base classes into a lower-level package such as meas_astrom.
3,FY17 Data Loader for Large Tables with No Position Information,"We need to load some tables (e.g., ForcedSource) that lack director positioning, we will only have the director's primary key. The general case is very expensive (lookup position and chunk for each position), however the fact such tables will be spatially-ordered when loading helps."
1,Update the astrometry.net astrometry solver to use the new standard schema,"DM-1576 provides a new astrometry solver and a new schema for reference objects. However, the old astrometry.net astrometry solver still uses the old schema. It would be wise to convert the old solver to the new schema so that the match list returned by it is in standard format."
2,Large scale test planning,"Need to come up with a plan which data set we will use for large scale tests, and how we will produce it."
0,Documentation for data loader,"Vaikunth had some ""expected"" troubles playing with data loader options for his DM-1570 ticket. Main issue I believe is the absence of the documented use cases and their corresponding data loader options. I'll try to add a bunch of common use cases to RST documentation and also verify that all options behave as expected."
1,Define command line tasks for pre-ingest transformation,"DM-1903 provided a command line task which would transform a {{src}} catalogue into calibrated form. Here, we build on that to provide command line tasks for all source catalogues which will need to be ingested; will include at least {{deepCoadd_src}}, {{goodSeeingCoadd_src}}, {{chiSquaredCoadd_src}}."
2,"Provide transformations for ""big three"" measurements","Provide standard calibration transformations for each of shape, flux and centroid and make sure they are returned as the default transformation for all algorithms measuring those quantities."
1,Add assertXNearlyEqual to afw,"We often want to compare two WCS for approximate equality. afw/image/testUtils has similar functions to compare images and masks and I would like to add one for WCS    This ended up being expanded to adding functions for many afw classes (not yet including image-like classes, though existing functions in image/testUtils for that purpose should probably be wrapped or rewritten on a different ticket)"
1,Ensure proper functioning of HSC distortion correction within obs_subaru,There may be some discrepancy between the pixel units being passed to distest.cc compared to what it is expecting (units of pixels).  This needs to be investigated further and remedied in such a way that all other representations (e.g. in camera.py) are consistent with the other obs_XXX representations.
2,Create form framework in React,We want to create a new frame work for entering data for forms and dialogs. This is in javascript based on React.js.  This is the first step in our javascript conversion.
1,Prototype HTM-based spatial binning to visualize large number of catalog sources,See story DM-1551.
0,Build 2015_03 Qserv release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe. 
1,Add typemaps for numpy scalars,"Add typemaps so that we can use numpy scalars to call C++ functions that take plain old scalar types (e.g. float, double or int). At present attempting to pass numpy scalars will fail unless the type is one of a restricted subset, e.g. float and numpy.float64 succeed but numpy.float32 is rejected as being an incompatible type, and similarly for integer types."
1,Acquire development data,"We'll need a reference set of data to work against.  This could be SDSS, CFHT, or simulated.  Should be 10? epochs with realistic atmospheric conditions taken at similar airmass and hour angle.  Single band is fine for now."
1,Produce task API,"This will require a new task, so will require a new interface and associated RFCs.  The interface should take an arbitrarily large stack of catalogs with or without a reference catalog.  It should return a stack ow WCSs that map from the individual coordinate systems to the reference."
1,Break down monster DM-1108 stories,"[~pgee] -- After finishing the measurement work, your next priority is to get started on DM-1108. However, the stories you have been assigned there are currently too big for useful scheduling (20-30 SPs is a mini-epic; we're looking for less than 10 SPs per story). The first task therefore is to work with [~jbosch], and others if required, to break them down and come up with a set of stories which usefully reflect the work which needs to be done."
3,Deploy and test network emulation for nightly processing testbed,"Deploy and test network emulation for nightly processing testbed.    Assignees: Paul Wefel, Steve Pietrowicz, James Parsons  Duration: January - February 2016"
3,Alert Production Simulator,"Start March 2015, finish July 2015 Pietrowicz S - 100%"
3,Complex Event Processing,"Start May 2015, finish June 2015"
2,OCS Software,"Start July 2015, finish August 2015 Pietrowicz S - 100%"
2,Configuration Management (Puppet),"Start March 2015, finish May 2015 Mather B - 40%"
2,Setup qserv prototype for qserv & SUI teams,"Start July 2015, finish August 2015 Glick B - 25%   Qserv requirements: - SUI will be testing against lsst10 (or IPAC qserv) for now ?   SUI requirements:  Xiuqin's 'short term' version: - 1 VM - SUI build server     - 4GB memory and 200GB hard disk should be good enough. - 1 VM - Apache server as a proxy and web front end     - 4GB memory and 100GB hard disk should be enough     - port 80 accessible from outside - 2 VMs - Tomcat servers      - each has 16GB memory, access to 1TB of shared hard disk     - Port 8080 should be open for Apache server to access     - Port 8009 should be open to each other so they can replicate cache. (First 2 VMs are not absolutely needed. We can always use one of the hosts in number 3 to do build and host Apache server.)  Trey Roby's 'long term' (in 2+ years) SUI requirements: - 2 vm/machines for Tomcat servers, they are fairly large       - each 100 GB mem       - each 16 processors       - 1 TB disk space shared and accessible between. - 1 vm/machine for Web Server, can be small 	- 16 GB mem       - 30 GB disk       - 4 processors - 2 small vm/machines for playing around with workspaces/L3 concepts       - each has 8 GB mem       - each 10 GB disk       - 4 processors       - Can share the Tomcat servers disk  space"
3,Storage Policies and Alignment,"Start March 2015, finish August 2015 Freemon M - 100%"
3,File System Research and Prototyping,"Start March 2015, finish September 2015 Freemon M - 100%, Glick B - 25%, Daues G - 40%, Elliot M - 25%"
3,File Management Technology,"Start March 2015, finish September 2015 Daues G - 40%, Freemon M - 100%"
1,Understand GPFS and commercial filesystems between data centers,"Start May 2015, finish June 2015 Petravick D - 50%, TBD from SET group"
3,Update Sizing Model,"Start March 2015, finish September 2015  Alt J - 50%, Petravick D - 10%"
3,Base Data Center Requirements,"Start March 2015, finish May 2015 Petravick D - 50%"
1,Start understanding inheritability and reusability of dataset types,"In order to allow for on-the-fly Task creation of dataset types, the essentials of each type need to be encapsulated in code.  That code should be reused across all similar dataset types, and there are opportunities for inheritance and specialization, particularly in cases like simple file-oriented mappers.  Investigate this by prototyping a number of possibilities."
3,Wide-Area Network Work,"Start March 2015, finish September 2015 Wefel P - 25%"
3,LOE - S15 (sys admin),"Glick B - 25%, Mather B - 40%, Elliot M - 25%, Freemon M - 100% Wefel P - 25%"
3,LOE - S15 (management),"Petravick D - 50%, Gelman M - 50%"
2,LOE - S15 (misc),All NCSA team
1,LOE - Week ending 3/6/15,- backup issues with lsst-stor141 (https://jira.ncsa.illinois.edu/browse/LSST-632) - setup jumbo frames on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-628) - crashplan reconfig on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-629)
1,LOE - Week ending 3/13/15,- crashplan issue with lsst-netem (https://jira.ncsa.illinois.edu/browse/LSST-633) - yum/glibc issue with lsst-dbdev1 (https://jira.ncsa.illinois.edu/browse/LSST-631) - account for Jacques Sebag (https://jira.ncsa.illinois.edu/browse/LSST-624)
3,LOE - Week ending 3/20/15,- lsst-dbdev2 drive failure (https://jira.ncsa.illinois.edu/browse/LSST-636) - account for Colin Slater (https://jira.ncsa.illinois.edu/browse/LSST-634) - disable Robyn Allsman's accounts (https://jira.ncsa.illinois.edu/browse/LSST-623)
3,LOE - Week ending 3/27/15,NULL
3,LOE - Week ending 4/3/15,- researched buildbot slowness on lsst-dev <https://jira.lsstcorp.org/browse/DM-2388> - researched buildbot slowness on lsst-dev <https://jira.ncsa.illinois.edu/browse/LSST-638>
3,LOE - Week ending 4/10/15,NULL
3,LOE - Week ending 4/17/15,NULL
2,LOE - Week ending 4/24/15,"- Researched how to monitor network drops, errors, etc <https://jira.ncsa.illinois.edu/browse/LSST-641> - Opened up SUI ports on lsst-dev <https://jira.ncsa.illinois.edu/browse/LSST-651> - Moved /nfs/admin/ to /condo/admin/ - Review of EL 6.x kernel security patch - Fixed jumbo frame issues, primarily with old VMs that needed new version of NIC <https://jira.ncsa.illinois.edu/browse/LSST-652>"
1,Prototyping with Puppet,NULL
3,Test Puppet with base configuration manifests,NULL
3,Develop use cases for TOWG,"Start March 2015, finish May 2015 Petravick D - 50%, Glick B - 25%, Gruendl R - 5%"
3,ISO Work,"Start March 2015, finish September 2015 Withers A - 25%"
0,Extend API: expose cursor,Extend API to expose cursor. This was brought up by Andy in DM-2137. 
1,Define ntermediate plan for MacOSX builds, We have  1. Obtain a dedicated colo OSX server  2. Have done some testing using the SQuaRE vagrant-sandbox harness  It is therefore a plausible avenue forward to do at least a nightly build/deploy/intgeration-test on OSX pending more extensive arrangements requiring purchase of hardware.  
2,Github transition for DM,DM's transition for code repositories to Github is complete.  Outstanding are data repositories; a cleanup of contrib/externals; and supporting the Stash move. 
1,Workflow improvements for Sims / PST projects,New wokflow for Sims Merge of Opsim and CATsim New workflow for PST 
3,Prototype automated system for release preparation builds, Prototype an environment that allows automatic   - Provisioning of a VM for a certain OS - Install the Stack prerequisites for that OS - Build the stack via newinstall.sh from the production server - Run integration tests (in the curent case the sdss test  https://github.com/lsst-sqre/sandbox-stackbuild
3,"Galaxy Fitting via ""ngmix""","Provide wrappers that let us run Erin Sheldon's [ngmix|https://github.com/esheldon/ngmix] as part of the DM pipeline.  Issues so far only cover getting the a single-frame (visit or coadd) version of the code running.  ngmix can also to simultaneous fitting to multiple exposures, but it's not yet clear how we'll want to handle the I/O and that interacts with a future multifit plugin framework."
0,Implement SExtractor's SPREAD_MODEL,"The new SExtractor star/galaxy classifier, SPREAD_MODEL, is popular with everyone who has tried it, and should be simple to implement by building on code in meas_modelfit.  See definition and discussion here: http://arxiv.org/abs/1306.4446"
0,Define common interface for star/galaxy classifiers,"We need some common fields for star/galaxy classifiers so they can participate in a slots-like mechanism once we have several of them.  Most of these can produce a floating point number between 0 and 1 (but sometimes it's not limited to that range), and it's rarely a true probability.  We may want to make a boolean that results from a threshold on these be the common interface, but we don't necessarily want to hard-code such a threshold into the processing either - especially when we could also use a FunctorKey to get a boolean from the floating point value."
1,add third-party package builds for ngmix dependencies,"In addition to numpy and scipy, ngmix depends on the emcee and statsmodel packages.  While it can build without them, we probably want the full functionality.  I also see some undeclared dependencies on the ""esutil"" and ""fitsio"" packages (all from esheldon's GitHub), and there may be a few more dependencies on some of his own packages.    This issue includes creating a third-party build for ngmix itself."
2,Add SFM plugin for ngmix MCMC sampling,"Add an SFM plugin for ngmix MCMC fitting, as in the example in the ngmix README.    This should depend on DM-5429 (or a suitably configured modelft_ShapeletPsfApprox) for PSF approximation.    For now, we should just take the mean of all parameters in the MCMC samples and write those to the record, as we currently don't have any way to save all of the samples.    Testing and tuning this algorithm to get it working well should be deferred to another issue.  The only requirement here is that it be able to run without crashing (even if that means setting the number of samples small)."
1,make a simple build for Firefly package,We want to have a out of box build for users of Firefly package. It will include a simple Firefly viewer. 
0,Allow eups xrootd install script to be relocatable,"xrootd lib/ directory should be s relative symlink to lib64, no a full path link."
0,Setup in2p3 cluster for Qserv team,- create accounts - update umask on stack  to each account - provide easy ssh config if possible - setup up build procedure (each developer can build Qserv using tag git and 'git' version is set up by default on all the Qserv if ti exists)
0,remove PSFAttributes,"PSFAttributes has long been deprecated, and we just need a little more work to remove it:  - Add an effective area accessor to the Psf interface, and implement it in ImagePsf.  - Replace usage of PSFAttributes with usage of Psf accessors.  This may require a little work if code depends on the details of how the shape was calculated, as PSFAttributes provided support for more algorithms than we will going forward."
0,Improve build system for sphgeom,NULL
3,Create pilot condor jobs,"Create long-running jobs to reduce the startup time for new HTCondor jobs.   This can be implemented as a parent/child, or as a on_exit_remove=false directive in HTCondor.  I suspect it will be a combination of the two."
1,Implement task switching between work job machines,AP requires that jobs are handed off to different worker job clusters as the previous set of images is being worked on.
3,Refactoring,The initial prototype of the AP simulator needs to be refactored to improve how tasks are handled by the components for further development.
2,Implement API for reading simulated camera data,"Currently this is generated by the replicator and sent to the distributor.  The idea where is to put the API in place so that the data will be transferred from outside of the replicator to it, and then passed on."
1,Implement file transfer API ,Create file transfer API so we can easily test different types of file transfer mechanisms to/from the AP.
0,Move VMs to Docker containers,We anticipate being able to move from the VMs that we currently use to using docker.  This will require some coordination with Greg Daues to see how HTCondor is configured.  
1,Unify logging strategy for python scripts,"- add -vvv option  - remove default value for configuration file in logger, provide it at each script level (i.e. integration test, data loader).   - if it exists, provide configuration file option explicitly to all called submodules which uses it.    See admin/python/lsst/qserv/admin/logger.py  {code:python}   14 def get_default_log_conf():                                                                                                                                                                  15     default_log_conf = ""{0}/.lsst/logging.ini"".format(os.path.expanduser('~'))                                                                                                               16     return default_log_conf                                                                                                                                                                  17                                                                                                                                                                                              18 def add_logfile_opt(parser):                                                                                                                                                                 19     """"""                                                                                                                                                                                      20     Add option to command line interface in order to set path to standar                                                                                                                     21     configuration file for python logger                                                                                                                                                     22     """"""                                                                                                                                                                                      23                                                                                                                                                                                              24     parser.add_argument(""-V"", ""--log-cfg"", dest=""log_conf"",                                                                                                                                  25                         default=get_default_log_conf(),                                                                                                                                      26                         help=""Absolute path to yaml file containing python"" +                                                                                                                27                         ""logger standard configuration file"")                                                                                                                                28     return parser                                                                                                                                                                            29                                                                                                                                                                                              30                                                                                                                                                                                              31 def setup_logging(path='logging.ini',                                                                                                                                                        32                   default_level=logging.INFO):                                                                                                                                               33     """"""                                                                                                                                                                                      34     Setup logging configuration from yaml file                                                                                                                                               35     if the yaml file doesn't exists:                                                                                                                                                         36     - return false                                                                                                                                                                           37     - configure logging to default_level                                                                                                                                                     38     """"""                                                                                                                                                                                      39     if os.path.exists(path):                                                                                                                                                                 40         with open(path, 'r') as f:                                                                                                                                                           41             logging.config.fileConfig(f)                                                                                                                                                     42         return True                                                                                                                                                                          43     else:                                                                                                                                                                                    44         logging.basicConfig(level=default_level)                                                                                                                                  45         return False    {code}"
0,Document HOW-TO setup-up krb5 for easy cluster access,"{code:bash} su aptitude install krb5-user # edit /etc/krb5.conf w.r.t ccage one # then as desktop user kinit ssh ccqservxxx {code}  /etc/krb5.conf {code:bash} [libdefaults] 	default_realm = IN2P3.FR  ... 	allow_weak_crypto = true   ... [realms] 	IN2P3.FR = { 		kdc = kerberos-1.in2p3.fr:88 		kdc = kerberos-2.in2p3.fr:88 		kdc = kerberos-3.in2p3.fr:88     		master_kdc = kerberos-admin.in2p3.fr:88     		admin_server = kerberos-admin.in2p3.fr     		kpasswd_server = kerberos-admin.in2p3.fr     		default_domain = in2p3.fr {code}  sshconfig: {code:bash} Host ccqservbuild GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName ccqservbuild.in2p3.fr #ProxyCommand ssh -W %h:%p cc   Host ccqserv1* GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName %h.in2p3.fr ProxyCommand ssh -W %h:%p ccqservbuild {code}"
0,Fix problems with mysql timeout,We added some code for supporting reconnecting (see https://dev.lsstcorp.org/trac/ticket/3042) but clearly not enough to recover from connection timeouts. This needs to be addressed.
0,The TAN_PIXELS cameraGeom coordinate system should be with respect to the center of the focal plane,"The TAN_PIXELS cameraGeom coordinate system (the position on a detector if there is no optical distortion) is presently defined with respect to the center of the detector -- i.e. a star at the center of the detector will have the same position in PIXELS and TAN_PIXELS coordinates. That is a mistake. TAN_PIXELS should be defined with respect to the center of the focal plane, since it then reflects the effects of having optical distortion or not.  Fixing this will help meas_astrom match stars. The effects of not fixing it are making the matcher search farther for a fit. As long as we allow sufficient offset in the matcher config the current system will work, but it is not ideal."
0,Implement connection pool,"Implement a class that manages a connection pool, and optionally, if configured, restarts connection as needed in case of timeout."
0,Switch to using db connection pool,"Switch to using the db connection pool. Note, in addition to getting auto-reconnect, in metaserv that would handy if we need to talk to multiple database servers simultaneously."
2,Participate in design process,"Participate and guide the SUI design process, generate charts and documents as appropriate"
2,Move javascript code into firefly repo and begin creating a real input form,NULL
2,Work with Camera & Pipeline team to spec out  proof of concept tools,NULL
1,Personnell requisitions ,"Work though recruiting for software effort.  Investigated and filled the ""kenton"" recruiting pattern at NCSA -- (few explicit requirements, many desirable)  Began discussion to break down hires for ""2nd"" floor  work -- to be in the LSST group v.s support groups -- ADS and Doug's group"
0,Arrange for commercial object store presentation,"arrage for presentations next week w.r.t commercial object store.  The vendor in question is know to NCSA and has claims to have produced a commercial object store having both NFS,  GPFS  and swift interfaces. "
0,Begin WBS review ,Begin  comprehensive review of the WBS.   Forced on overall framework and begin  workflow systems  
0,Security officer orientation,begin orientation of LSST ISO ALEX Withers. 
0,Internet2 TIER investigation,NULL
0,Unable to start cmsd on Qserv worker node,"Some build issues have qlready been fixed in commit: 9dd378829e8751a6852356967411c20580e2a1c3  Here's the log:  {code:bash} [fjammes@ccqserv101 ~]$ cat /qserv/qserv-run/var/log/worker/cmsd.log 150309 21:19:46 9794 Starting on Linux 3.10.0-123.8.1.el7.x86_64 Copr.  2004-2012 Stanford University, xrd version v20140617-203cf45 ++++++ cmsd worker@ccqserv101.in2p3.fr initialization started. Config using configuration file /qserv/qserv-run/etc/lsp.cf =====> all.adminpath /qserv/qserv-run/tmp =====> xrd.port 1094 =====> xrd.network nodnr Config maximum number of connections restricted to 4096 Config maximum number of threads restricted to 2048 Copr.  2007 Stanford University/SLAC cmsd. ++++++ worker@ccqserv101.in2p3.fr phase 1 initialization started. =====> all.role server =====> ofs.osslib libxrdoss.so  =====> oss.localroot /qserv/qserv-run/xrootd-run =====> cms.space linger 0 recalc 15 min 10m 11m =====> all.pidpath /qserv/qserv-run/var/run =====> all.adminpath /qserv/qserv-run/tmp =====> all.manager ccqserv100.in2p3.fr:2131 =====> all.export / nolock The following paths are available to the redirector: w  /   ------ worker@ccqserv101.in2p3.fr phase 1 server initialization completed. ++++++ worker@ccqserv101.in2p3.fr phase 2 server initialization started. Plugin Unable to find  required version information for XrdOssGetStorageSystem in osslib libxrdoss.so ------ worker@ccqserv101.in2p3.fr phase 2 server initialization failed. 150309 21:19:46 9794 XrdProtocol: Protocol cmsd could not be loaded ------ cmsd worker@ccqserv101.in2p3.fr:1094 initialization failed {code}"
0,Read through Don's SCADA notes and comment,NULL
2,"Revisit db and dbPool, separate connection from utilities","Revisit whether we need something better than a very basic db connection pool.    It may be worth looking at http://docs.sqlalchemy.org/en/rel_0_9/core/pooling.html (or even sqlalchemy in general). Note the Pooling Plain DB-API Connections section - one can use sqlalchemy pooling independently of the other library features.    Separate utilities like createDb(), dbExists() and such from core part that deals with connections / sqalchemy."
3,Improve Webserv/Metaserv,"Work includes implementing features requested by SUI (schema metadata, units etc)"
1,Support metadata for databases without description,"The metaserv should be able to support databases for which we don't have the ascii schema with descriptions and special tokens (ucd, units etc). This story involves implementing it. In practice, the metaserv/bin/metaBackend will need to be extended to implement ""ADD DB"""
1,Measurement transforms for Flux,Provide calibration transforms for flux measurements to magnitudes.
1,Measurement transforms for centroids,Provide calibration transforms for all algorithms measuring centroids.
0,Measurement transforms for shapes,Provide calibration transforms for algorithms measuring shapes.
0,Update dev quick-start guide to new git repositories,The quick-start documentation for developers still points to the old git repositories. The RST document needs to be updated to the GitHub repos.
0,obs_test's table file is out of date,"obs_test's table file is somewhat out of date. Problems include:  - afw is required but missing  - meas_algorithms and skypix are used by bin/genInputRegistry.py, which is only used to create the input repo so these can be optional  - daf_persistence is not used  - daf_base is only used by bin/genInputRegistry.py, so it can be optional (though it is presumably setup by daf_butlerUtils in any case)"
1,Improve xssi API to send a few bytes with the message informing the client that a response is available on  the server,"This would allow Qserv no to send the first protobuf header as a xrootd in-band message, and save some resources (network and CPU due to xrootd/TCP/IP encapsulation)"
0,Clarify expectations for unauthenticated user data access,"h4. Short version:  Clarify what existing community practices, notably including VO interfaces, appear to rely on the availability of unauthenticated access to information in astronomical archives.  h4. Details:  At the February DM All Hands, [~frossie] raised an objection when it was mentioned that there is a presumption that all user access to LSST data through the DM interfaces (as opposed to through EPO) will be authenticated.  We don't appear to have ever documented an explicit requirement that all access be authenticated.  The basic controlling requirement is OSS-REQ-0176, ""The LSST Data Management System shall provide open access to all LSST Level 1 and Level 2 Data Products, as defined in the LSST System Requirements and herein, in accordance with LSSTC Board approved policies. ..."", which was a carefully crafted indirection at a time when the policy for non-US/Chile access was still being developed.  However, this presumption has been around for a long time.  It is inherent to the project policy that access to the non-Alert data will be limited to individuals who are entitled to it.  No matter what we think the final policy might be, we do have to design a system that can be consistent with this policy.  [~frossie] stated that the astronomical community relies on certain types of data and metadata - she mentioned coverage maps, among others - being available through unauthenticated interfaces.  This ticket is to ask her (and others) to collect documentation of those existing practices, so that we can figure out what the expectations may be and how to respond to them in our design."
0,Remove deprecated merging code: rproc::TableMerger,"rproc::TableMerger seems to be replaced with rproc::InfileMerger, so this class could certainly be removed easily. "
1,Revisit exceptions in db module,Revisit db/python/lsst/db/exception.py. Perhaps get rid of the numbers.
0,KT reading list for operational requirements,NULL
0,Observatory site requirements reading,NULL
1,"Setup hosts for SUI (2x Tomcat, Apache, and build)","Xiuqin's 'short term' version: 1 VM - SUI build server 4GB memory and 200GB hard disk should be good enough. 1 VM - Apache server as a proxy and web front end 4GB memory and 100GB hard disk should be enough port 80 accessible from outside 2 VMs - Tomcat servers each has 16GB memory, access to 1TB of shared hard disk Port 8080 should be open for Apache server to access Port 8009 should be open to each other so they can replicate cache. (First 2 VMs are not absolutely needed. We can always use one of the hosts in number 3 to do build and host Apache server.)  The 2 Tomcat servers are larger than we can currently support as VMs.   We've discussed repurposing 2 of the older LSST ""cluster/condor"" nodes (e.g. lsst14 & lsst15) for this purpose.  But, ideally these could be implemented with the new vSphere hardware if the timeframe works."
0,"review """"data center in a box"" mali.  Recover consultant's contact into ","reviewed the data center in a  box, recovered consultant's name prior to drafting a SOW."
0," attend DDN WOS briefing, write summary note. ","as described above.  Summary note is attached. also looked for use of this product in DOE labs, who would be consumers  of LSST data.  Discovered that it had been investigated for use in HEP a few years earlier, but that is was not adopted because, at that time the hardware and software were coupled."
0,misc for week of march 9,finalize job descriptions. Meet with kantor additional hour of  orientation for the ISO. group meeting  Misc.
1,Simplify interactions with XrdOss,"The qserv code is still using the old ssi scheme for the cmsd, this needs to be rewritten. For  details, see  https://listserv.slac.stanford.edu/cgi-bin/wa?A1=ind1503&L=QSERV-L#3"
1,Setup IRODS zone on ISL OpenStack,"We begin an examination of iRODS for managing data collections. We perform initial testing using resources available on NCSA's ISL OpenStack.  To mock up a zone or 'data grid' managed by iRODS, we set up an ICAT server, a resource server (this is a data storage resource that does not run the central database), and a client host. "
1,Save iRODS installations/servers as docker images,"We install and configure iRODS servers (an ICAT server, a resource server, a client host) in docker and make images, pushing the results to a docker hub repository. "
1,develop/propose storage policies,NULL
2,develop/propose storage procedures,NULL
1,develop/propose storage implementation,NULL
0,Reprise SDRP processing metrics,"In support of an SDRP-based science talk of Yusra AlSayyad, we spent some cycles gathering/summarizing processing middleware results and metrics from the US side of processing of the Split DRP.  This information from notes, logs, databases, etc provided contextual information on the processing campaign that produced the SDRP science results. "
1,Use parallel ssh to manage Qserv on IN2P3 cluster,IN2P3 sysadmin won't manage Qserv through puppet. So Qserv team has to provide ssh scripts to do this.  
1,Integrate changes from Events code review,NULL
0,Move afw_extensions_rgb functionality into afw proper,See RFC-32 
0,(In)equality semantics of Coords are confusing,"Viz:  {code} In [1]: from lsst.afw.coord import Coord In [2]: c1 = Coord(""11:11:11"", ""22:22:22"") In [3]: c1 == c1, c1 != c1 Out[3]: (True, False) In [4]: c2 = Coord(""33:33:33"", ""44:44:44"") In [5]: c1 == c2, c1 != c2 Out[5]: (False, True) In [6]: c3 = Coord(""11:11:11"", ""22:22:22"") In [7]: c1 == c3, c1 != c3 Out[7]: (True, True) {code}  {{c1}} is simultaneously equal to *and* not equal to {{c3}}!"
0,useValueEquality and usePointerEquality fail to fail,These SWIG macros return a class instead of raising an exception instance when the equality operation fails.
0,Add unit tests to SchemaToMeta,"Add unit tests, also improve variable names as suggested by K-T in comments in DM-2139"
1,Install and learn to use iPython notebook,NULL
2,Participate in design discussion,Participate  in the design discussions three times weekly for two to three months. 
2,Participate in design discussion,Participate in the design discussions three times weekly for two to three months. 
2,Participate in design discussion,Participate in the design discussions three times weekly for two to three months. 
0,Identify the hardware resources needed at NCSA for short term development ,Supply the hardware resources needed at NCSA for short term development. It is captured in DM-2327  
0,make PixelFlagsAlgorithm fully configurable,"PixelFlagsAlgorithm currently hard-codes the mask planes it considers.  This should be fully configurable instead.  It also overloads the ""edge"" flag to mean both ""EDGE mask plane was set"" and ""centroid was off the edge of the image"".  These should be different flags.  We may also want to have this algorithm use SafeCentroidExtractor.'  Finally, the algorithm is woefully undertested."
0,standardize handling of missing peaks in centroiders,GaussianCentroid has a NO_PEAK flag that it sets when there is no Peak to use as an input.  SdssCentroid does not.  This behavior should be standardized.  Maybe we should use SafeCentroidExtractor here?
0,RGB code introduces dependency on matplotlib,"While the new RGB code looks like it's just calling NumPy, NumPy is actually delegating to matplotlib under the hood when it writes RGB(A) arrays.  It also turns out that code is broken in matplotlib prior to 1.3.1 (though that shouldn't be a problem for anyone but those who - like me - are trying to use slightly older system Python packages).  I think think this means we should add an optional dependency on matplotlib to the afw table file, and condition the running of the test code on matplotlib's presence (and, ideally, having the right version).  I'm happy to do this myself (since I'm probably the only one affected by it right now)."
0,Revisit the choice of using flask,"We should quickly revisit if flask is the right choice for us.  Related: reportedly, our simple flask-based webserver is using more CPU in an idle state than expected. It might be useful to profile things, and look into that. "
1,run lsstswBuild.sh in a clean sandbox,"The ""driver"" script, lsstswBuild.sh, used by the buildbot slave on lsst-dev to initiate a ""CI run"" has a number of environment assumptions (binaries in the $PATH, paths to various components, hostnames, etc.).  This prevents it from [easily] being invoked on any other host.  As lsstswBuild.sh builds a number of packages that are not in the lsst_distrib product, the os level dependencies for these other products need to be determined.  In addition, the current version of lsstswBuild.sh and related scripts on lsst-dev are not version controlled."
1,Move QuerySession::_stmtParallel from query::SelectStmtPtrVector to query::SelectStmtPtr,"QuerySession::_stmtParallel is a vector but it seems only it's first element is used, so storing it in a vector doesn't seem necessary.  Code can be easily simplified here. This should lead to mode understandable code.  QuerySession public members and method comments could also be improved here."
1,run lsstswBuild.sh under Jenkins on EL6,* Demonstrate lsstswBuild.sh being invoked by jenkins on EL6 (same OS as lsst-dev). * Experiment with a single build slave attached to a jenkins master * Investigate configuration management of jenkins builds.
1,Improve logger use in qserv,"Qserv logger must be easily configurable. Next technique, based on log4cxx documentation allows to do it easily.  Example:  In QuerySession.cc, initialize a static logger: {code:c++} namespace lsst { namespace qserv { namespace qproc {  LOG_LOGGER QuerySession::_logger = LOG_GET(""lsst.qserv.qproc.QuerySession""); {code}  then use it in Query session member functions:  {code:c++}         if (LOG_CHECK_LVL(_logger, LOG_LVL_DEBUG)) {             std::ostringstream stream;             _showFinal(stream);             LOGF(_logger, LOG_LVL_DEBUG, ""Query Plugins applied:\n %1%"" % stream.str());         } {code}  And use log4cxx.property to easily configure, AT RUNTIME, logging for each Qserv module class:  {code} # logger for all module will inherit this one log4j.logger.lsst.qserv=ERROR # this could be generalized to all Qserv modules: log4j.logger.lsst.qserv.qproc=INFO # can also be done at the class level for advanced debugging #log4j.logger.lsst.qserv.qproc.QuerySession=DEBUG {code}  And then in the log: {code} /home/qserv/qserv-run/2015_03/var/log/qserv-czar.log:0319 17:08:48.786 [0x7f208da93740] DEBUG lsst.qserv.qproc.QuerySession (build/qproc/QuerySession.cc:118) - Query Plugins applied: {code}  This proposal is a draft and should be improved before implementing it."
0,evaluate NCSA proposal to investigate CEPH in the context of NCSA Integrated systems lab,"The integrated systems lab (ISL) is the orgianizational vehicle used to investigate pre-production technologies at NCSA.   Since  We still lack the ability to procure goods,  I evaluated and commented on an ISL proposal to investigate the CEPH file system for its properties as an alternative to the LSST baseline file system GPFS. "
0,revise and circulate data center requirements note,"Reconvert the ~10 TBD's  in the priori version of the note  — I’ve kept the stipulation that end of service-lifee stuff will leave these spaces  but added an appendix that “this is what the central space should provide”  My understanding is  there are now discussions on whether that central space will exist of not.   The requirements  can be promoted to center requirements no central space is evient. — The maximal average weight for a rack was computed from LDM-144 and is given.  — There are more cu ft estimates for  the need to dispose of dunnage and packing material. — TBD’s w.r.t overhead cabling are specificed. — The non- LSST requrements are in there, and have been as far as I am able to ascertain them from   champaign urbana.   Ron has been stating requirements as “rows”   I have never fixed row length  thinking  that is for the designer to do.   We’ve stated that rows are shareable, but racks are not. so what’s in the docs is definitive unless/until non LSST requirements can be stated in the same terms.     ""Shall support 16 racks for the NOAO tenant"". is what I had.  power requirements as per  the common space, becasue we want a maximally flexible space. "
1,Management,"Meetings -- Monday CAM meeting, Friday standup and infrastructure.  Internal NCSA group meeting,  Internal NCSA ""comp pol"" technical coordination meeting.  Screen existing candidate pool for likely people to fill opening,  Interviewed one person checked references + Misc."
0,Retrieve HSC engineering data,"HSC data becomes public 18 months after it was taken, so data taken during commissioning are now available.  We would like to use this data for testing the LSST pipeline.  It needs to be downloaded from Japan."
0,Make sure the command-line parser warns loudly enough if no data found,A user recently got confused when calling parseAndRun didn't call the task's run method. It turns out there was no data matching the specified data ID. Make sure this generates a loud and clear warning.
0,migrate package deps from sandbox-stackbuild to a proper puppet module,There is a growing list of known package dependencies in the sandbox-stackbuild repo and a need to use this information for independent environments (such as CI).  This list of packages should be lifted out into an independent puppet module that can be reused.
1,Implement data loading in worker manager service,This is a separate ticket for implementation of the data loading part of the worker management service (started in DM-2176). Some ideas and thoughts are outlined in that ticket.
0,Build testQDisp.cc on ubuntu,testQDisp.cc needs flags -lpthread -lboost_regex to build on ubuntu.
0,Errors need to be checked in UserQueryFactory from QuerySession objects,UserQueryFactory doesn't check its QuerySession object for errors after setQuery. Thus it continues setting things up after the QuerySession knows the state is invalid.
2,Migrating to GWT 2.7,"To use JavaScript Interop functionality, we need to migrate to GWT 2.7"
2,React components for Form,"To familiarize myself with React and to prepare the ground for moving to React-based user interfaces, we need to create React components for the form. This story includes CheckboxGroup, RadioGroup, and Listbox input fields."
3,FY17 Enable DC Analyses through Qserv,Load data challenge data into Qserv and enable analytics of the DC data through Qserv.
3,FY18 Enable DC Analysis through Qserv,Load data challenge data into Qserv and enable analytics of the DC data through Qserv. 
3,FY17 Fix Qserv Bugs,Bucket epic for unexpected bug fixes.
3,FY18 Fix Qserv Bugs,Bucket epic for unexpected bug fixes.
3,FY19 Fix Qserv Bugs,Bucket epic for unexpected bug fixes.
3,FY20 Fix Qserv Bugs,Bucket epic for unexpected bug fixes.
3,W16 Butler (v4),NULL
0,Allow qserv-admin.py to delete a node,"Registered workers in CSS with qserv-admin.py are currently not able to be removed (no DELETE NODE type command). Also, changing node status from ACTIVE to INACTIVE needs to be fixed."
0,Change integration test user from root to qsmaster,Currently integration tests use root account as default user - this should be changed to qsmaster for the future.
1,investigate configuration management for jenkins,The most popular Puppet module for managing Jenkin's {code}jenkinsci/puppet-jenkins{code} is able to create a master and build slaves but is missing the functionality to manage several master configuration options that otherwise require manual setup.  We need to investigate the difficulty of managing a Jenkin's master configuration values in an idempotent manner.
1,convert Statistics to use ndarray natively,"The Statistics class predates ndarray, and hence uses some hackish Image-class emulators/wrappers to deal with 1-d arrays.  It'd clean things up considerably to have it use ndarray under the hood, and have the Image-based interfaces interact via their ndarray views."
0,Data loader script crashes trying to create chunk table,"Vaikunth discovered a bug in data loader when trying to load a data into Object table: {noformat} [CRITICAL] root: Exception occured: Table 'Object_7480' already exists Traceback (most recent call last):   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 318, in <module>     sys.exit(loader.run())   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 254, in run     self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 171, in load     return self._run(database, table, schema, data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 209, in _run     self._loadData(database, table, files)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 586, in _loadData     self._loadChunkedData(database, table)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 653, in _loadChunkedData     self._makeChunkAndOverlapTable(conn, database, table, chunkId)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 727, in _makeChunkAndOverlapTable     cursor.execute(q)   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 176, in execute     if not self._defer_warnings: self._warning_check()   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 92, in _warning_check     warn(w[-1], self.Warning, 3) Warning: Table 'Object_7480' already exists {noformat} It looks like I did not do enough testing after my recent improvement in creating chunk tables. It tries to create the chunk table with ""CREATE TABLE IF NOT EXISTS ..."" but that actually generates ""warning exception"" on mysql side when table is already there. Need to catch this exception and ignore it."
1,Implement authentication mechanism for worker management service,We need some reasonable security for access to new worker management service. It should be lightweight and not depend on complex things that require infrastructure. Something based on a shared secret should be adequate for our immediate needs and likely for the long term.
1,Document API for worker management service,"New worker management service exposes its API as an interface to RESTful web service. Many or all ""methods"" will be wrapped into some sort of Python API, but it would still be useful to document every web service ""methods"" independently. There is basic documentation in the design document (https://dev.lsstcorp.org/trac/wiki/db/Qserv/WMGRDesign), this needs to be extended with detailed description of what those methods do and what kind of data they accept and return.  This story involves selecting the right tool."
0,Improve support for Python modules in Scons,"it seems we have two tools to manage python modules:  - site_scons/pytarget.py and - site_scons/site_tools/pymod.py (grep for InstallPythonModule) used by admin tools. We could unify this, isn't it?"
0,Weighting in photometric calibration is incorrect,"Dominique points out that the zero point calibration uses errors not inverse errors to calculate the zero point.  git annotate reveals: bq. 24c9149f python/lsst/meas/photocal/PhotoCal.py (Robert Lupton the Good 2010-12-13 05:03:12 +0000 353)     return np.average(dmag, weights=dmagErr), np.std(dmag, ddof=1), len(dmag)  Please fix this.  At the same time, we should add a config parameter to soften the errors. "
0,Create transitional duplicate of Span,"One challenge in switching from {{PTR(Span)}} to {{Span}} in {{Footprint}} is that Swig won't generate wrappers for {{std::vector<Span>}} (or any other container) if {{%shared_ptr(Span)}} is used anywhere else in the codebase.  So, to allow both the old {{Footprint}} class and the new {{SpanRegion}} to coexist (temporily), we need to have two {{Span}} classes, one wrapped with {{%shared_ptr}} and one wrapped without it.  Since we don't want to disrupt the old {{Footprint}} class yet, we should call the new Span something else, and make it the one that's wrapped without {{%shared_ptr}}.  This ticket can be considered complete once we have a unit test demostrating a usable Swig-wrapped {{std::vector<NewSpan>}} while all old {{Footprint}} tests continue to pass."
1,Implement SpanRegion core functionality,"Implement the core of the SpanRegion class, as prototyped in RFC-37.  This includes the following:  - The private implementation object and copy-on-write utilities (see Schema for an example of copy-on-write, but note that SpanRegion's implementation object can be private, while Schema's is not).  - All STL container methods and typedefs, and their Pythonic counterparts.  - All constructors and assignment operators, except for SpanRegionBuilder.  This includes the ability to detect and fix overlapping Spans.  - All simple accessors.  - {{isContiguous()}}  - The shift and clip methods."
0,Implement SpanRegion+ellipse operations,"Implement the following SpanRegion operations:  - Construct from an ellipse - note geom::ellipses::PixelRegion; this should do most of the work.  - Compute centroid - see old Footprint implementation  - Compute shape (quadrupole moments) - see old Footprint implementation  One complication here is that this will introduce a circular dependency between afw::geom and afw::geom::ellipses.  That's easy to address at the C++ level, but it's tricky in Python (which package imports the other?)  I'll be emailing dm-devel shortly to start a discussion on how to address this problem."
1,Implement SpanRegion applyFunctor methods,"Implement methods that apply arbitrary functors to pixels within a SpanRegion, as described on RFC-37.  The only tricky part of this implementation will be the ""traits"" classes that allow different target objects to interpreted differently.  I'd be happy to consult on this; I have a rough idea in my head, but it needs to be fleshed out."
0,Add aperture corrections to meas_extensions_photometryKron,"When transitioning {{meas_extensions_photometryKron}} to the new measurement framework, aperture correction was omitted pending the completion of DM-85. It needs to be re-enabled when that epic is complete."
0,Make qserv server-side log messages more standard,"Qserv server-side Python logging appears to mostly use a common format: ""{{%(asctime)s %(name)s %(levelname)s: %(message)s}}"".  It also mostly uses a common date format: ""{{%m/%d/%Y %I:%M:%S}}"".  But I see instances of: * ""{{%(asctime)s %(levelname)s %(message)s}}"" * ""{{%(asctime)s - %(name)s - %(levelname)s - %(message)s}}"" *  ""{{%(asctime)s \{%(pathname)s:%(lineno)d\} %(levelname)s %(message)s}}"" * and now, after DM-2176, ""{{%(asctime)s \[PID:%(process)d\] \[%(levelname)s\] (%(funcName)s() at %(filename)s:%(lineno)d) %(name)s: %(message)s}}""  Unless these are used in very different contexts, it will aid automated log processing for them to be more standardized.  In addition, the date format is unacceptable as it does not use RFC 3339 (ISO8601) format and does not include a timezone indicator (which means the default {{datefmt}} is insufficient).  This must be fixed.  See also DM-1203."
1,Fork GREAT3 sim code and integrate with LSST stack,"Get the GREAT3 simulation code running with LSST-provided third-party packages of Python, GalSim, etc, and figure out where we're going to put our modified scripts on GitHub:  - Do we just put things in a fork of the great3 repo, or do we have other repos layered on top of a fork of the great3 repo?  (I think probably the latter, but we should determine how many repos, and for what purposes.)  - Where in GitHub space do we put them (lsst?  lsst-dm? user spaces?)  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues)."
0,Increase postage stamp size in simulation scripts,The GREAT3 simulations have a fixed postage stamp size (though this may differ between branches).  A first step at modifying the simulation scripts to meet our needs would be to try to change the postage stamp.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).
1,Create simulation script with different constant PSF per galaxy.,"Modify the GREAT3 simulation scripts to create a branch in which each galaxy gets a different constant PSF, rather than one constant PSF per subfield or a spatially-varying PSF that spans multiple subfields.  This could be done by modifying the control/ground/constant branch or the variable-psf/ground/constant branch, or creating an entirely new branch, or anything else (since we don't actually need multiple branches in our simulations).  At this point, the source of the PSFs doesn't really matter - as long as we have a class that can provide a different one to every image.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues)."
1,Draw simulated PSFs from a library of on-disk files,Modify the simulation code to draw PSFs at random from a library of on-disk files (whose format and on-disk layout should be specified here).  The PSFs chosen should be deterministic via a random number generator seed specified via config.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).
0,Reading an Exposure from disk aborts if the Psf is of an unknown type,"Attempting to read an Exposure (in this case via the butler) fails if the PSF class isn't available.  An exception would be reasonable, but an assertion failure is not.  Running the attached script on tiger-sumire with bq. setup python anaconda; setup -T v10_1_rc2 lsst_apps; setup -j distEst -t HSC; setup -j -r ~/LSST/obs/subaru  {code}  WARNING: Could not read PSF; setting to null: PersistableFactory with name 'PsfexPsf' not found, and import of module 'lsst.meas.extensions.psfex' failed (possibly because Python calls were not available from C++). {0}; loading object with id=4, name='PsfexPsf' {1}; loading object with id=28, name='CoaddPsf' {2} python: src/table/io/InputArchive.cc:109: boost::shared_ptr<lsst::afw::table::io::Persistable> lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive&): Assertion `r.first->second' failed. Aborted {code}"
0,"Cherry-pick ""fix makeRGB so it can replace saturated pixels and produce an image"" from HSC","HSC-1196 includes fixes and test cases for {{afw}}. After review on HSC, they should be checked/merged to LSST."
1,Port HSC-side functionality to allow showCamera to display real data via the butler,"One of the things that exists on the HSC side of things but not LSST is the ability to use showCamera to create full-focal-plane mosaics.  Please convert the code to run with the new cameraGeom    Not only is this generically useful, but it's part of the effort required to make the DM-side visualisation work for the Camera group  "
0,iRODS test: Replicate data between servers,"A fundamental feature of using iRODS would be to prevent file loss/corruption incidents by replicating data to different physical servers, possibly in geographically disparate locations. We verify that we can replicate data within out test zone/grid."
0,iRODS test:  Virtual collection ,"iROD manages data as a 'virtual collection', that is, one can have a single logical/virtual view of a collection of files (the appearance of a single file system/tree) while the data with the collection is stored on separate physical servers. We demonstrate this by creating a collection with data targeted/uploaded to different physical resources."
0,iRODS test: Register data in place,"In our first tests of iRODS, we have used ""iput"" to load data into iRODS cache spaces (the iRODS Vault).  For large collections already in a well known location on a server, one may want to leave the data in place but still manage it with iRODS. To do this one can use ""ireg"" to register the data with IRODS without the upload process."
1,"iRODS usage, devel survey",Read up on current IRODS usage and development track. 
1,Fix and test CheckAggregation,"{code:C++} class CheckAggregation { public:  CheckAggregation(bool& hasAgg_) : hasAgg(hasAgg_) {}  inline void operator()(query::ValueExpr::FactorOp const& fo) { if(!fo.factor.get()); {code}  - return is missing here. .get() is not needed, shared_ptr is like regular pointer which is convertible to bool, so whole thing should probably be: if (! fo.factor) return;  - We should have a unit test to show us we have problem here "
1,"Fix query ""SELECT * FROM Object o, Source s WHERE  o.objectId = s.objectId AND o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint""","Next query fails:  {code:bash}  mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case01_qserv -e   ""SELECT *   FROM Object o, Source s   WHERE  o.objectId = s.objectId   AND    o.objectId = 390034570102582   AND    o.latestObsTime = s.taiMidPoint""  {code}    It seems there's several problems here:    * objectId field is duplicated, zookeeper could be used to know all the fields involved by * in a query, but then it has to know each columns.  * subChunkId and chunkId are also duplicated, this isn't the case in the plain-mysql query.    This duplicated columns prevent the creation of the result table on the czar.  "
0,v10_1_rc2 build test,"Test v10_1_rc2 + tickets/DM-2303 on el6, el7, fedora 21, ubuntu 12.04, & ubuntu 14.04.  Results to be reported in http://ls.st/faq ."
1,Write additional test for duplicate fields check,"Alongside:  {code:C++} BOOST_AUTO_TEST_CASE(getDuplicateAndPosition) {code}  Add test for: - no duplicate strings - triplicate - more than one string duplicated  and alongside:  {code:C++} BOOST_AUTO_TEST_CASE(SameNameDifferentTable) {code}  test more than one duplicated column, and a column duplicated more than twice."
1,Fix cmsd-server logger configuration,"cmsd-server logger configuration is incorrect:  see cmsd.log on Qserv worker:  {code:bash} Plugin loaded unreleased QservOssGeneric unknown from osslib libxrdoss.so log4cxx: Could not instantiate class [org.apache.log4j.XrootdAppender]. log4cxx: Class not found: org.apache.log4j.XrootdAppender log4cxx: Could not instantiate appender named ""XrdLog"". log4cxx: No appender could be found for logger (QservOss). log4cxx: Please initialize the log4cxx system properly. QservOss (Qserv Oss for server cmsd) ""worker"" {code}"
0,Fix interface between QservOss and new cmsd version,"QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} "
1,Replace toString() function,"See [~salnikov] comment:    Fabrice, anything is possible in C++, if you can define toString() for vectors it should also be possible to define some other construct to format vector into a stream :)  My objection to toString() is based on couple of of observations:      most of the time in our code converting complex objects to string is done to push them to streams or to logging system (logging is also usually based of streams)     methods like toString() are usually implemented using temporary streams.  So if you write code like cout << toString(vector) or LOGF_DEBUG(""vector: %1%"" % toString(vector)) it is very inefficient because it creates temporary stream and temporary string(s).  To make it more efficient you have to define operator<<() which is implemented without using toString(). Then you could implement toString() based on operator<<() but I'd argue that you should avoid it. In case you really need to convert to string there semi-standard tools which already do the same for types that have operator<< defined (like boost::lexical_cast), but again most of the time you only need operator<< as you don't want to mess with strings.  If you want to know how to implement operator<< for vector (or any container) here is the sketch of what I would do (there might be simpler ways):  {code:c++} namespace detail {     template <typename Cont> struct _ContInserterHelper {         const Cont& cont;     };     template <typename Cont> std::ostream& operator<<(std::ostream& out, const _ContInserterHelper<Cont>& cins) {         out << ""["";         const Cont& cont = cins.cont;   // this is container itself         // print container elements with separators         return out << ""]"";     } } template <typename Cont> detail::_ContInserterHelper<Cont> ContInserter(const Cont& cont) {     return detail::_ContInserterHelper<Cont>{cont}; } {code}  And after that you can do:  {code:c++} std::vector<int> v; std::cout << ContInserter(v); {code}  And this has no overhead or any temporary objects created. "
1,investigate github oauth integration for jenkins ,"We need a means of authenticating and authorizing users to interact with the CI system.  The current seem of using an htpasswd file with buildbot is a hassel both for end user and administratively.  Jenkin's has support for ldap and there is a plugin available for github oauth.  Administratively, and it terms of reliability, it may make more sense to be coupled with github than a a new DM or the exist LSST LDAP instance."
0,uncaught exceptions in GaussianFlux,"{{SdssShapeAlgorithm::computeFixedMomentsFlux}}, which is used to implement {{GaussianFlux}}, now throws an exception when the moments it is given are singular.  That shouldn't have affected the behavior of {{GaussianFlux}}, as it contains an earlier check that should have detected all such bad input shapes.  But that doesn't seem to be the case: we now see that exception being thrown and propagating up until it is caught and logged by the measurement framework, resulting in noisy logs.  We need to investigate what's going wrong with these objects, and fix them, which may be in {{SdssShape}} or in the {{SafeShapeExtractor}} {{GaussianFlux}} uses to sanitize its inputs."
1,Participate in April design process,Most work here was with designing firefly tools API related details.
1,Prepare firefly for GitHub,NULL
1,Finsh pushing firefly to GitHub,NULL
2,Begin actual conversion of parts of firefly to pure javascript,NULL
2,Develop next gen Firefly JavaScript API Tools,NULL
2,Develop external http api that can control Firefly viewer,NULL
3,Implement client side of mask layers in FITS image Viewer,NULL
1,Prepare v10_1 release candidate,Candidate is v10_1_rc2 based on EUPS tag b949
0,lsstsw ./bin/deploy needs LSSTSW set to install products in the right place,"I  cloned lsstsw into ~/Desktop/templsstsw and cd'd into it and typed ./bin/deploy and was shocked to find it installed everything into ~/lsstsw, leaving an unsable mess: some files were in templsstsw and some in ~/lsstsw.  The short-term workaround is to manually set LSSTSW before running ./bin/deploy, but this should not be necessary; bin/deploy should either set LSSTSW or not rely on it. I don't recall this problem with earlier versions of lsstsw; I think this is a regression.  For now I updated the instructions at https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool but I look forward to being able to revert that change."
1,Implement stitching multiple patches across tract boundaries in a coadd v2,"* Find region that returns multiple tractPatchLists for testing.  * Request region via central point (RA, Dec) with width and height definable in arcseconds and pixels.  * May be extend web interface to other data sets, and/or good seeing SkyMaps. "
2,Qsev Documentation,NULL
1,Turn on C++ 11 flag for Qserv,NULL
1,Handle all exceptions coming from worker,NULL
0,Build 2015_04 Qserv release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Design API and RFC design,"Use the HSC implementation of the base class as a point of reference for designing an integrated Approximate and Interpolate class.  The design take into account Chebyshev, spline, and Gaussian process mechanisms.  Want to take into consideration client code.  I.e. it shouldn't make current consumers more complicated (background and aperture correction to name two).  RFC the designed API."
2,Edit background class,Make fixes to background class to use new approximate/interpolate class.
1,Fix-up any code that uses approximate/interpolate,The background matcher is one area where the approximate/interpolate class will be used.  This story will find all places (including examples and unit tests) where the old approximate/interpolate mechanisms are used and update them to use the new interface.
0,Delete old approximate/interpolate classes,"Once all updates are done to code and unit tests pass with the new approximate/interpolate interface, the old ones should be completely removed."
0,"Justify level of staff at La Serena, to the level of justifying for office space",help with the specifications for the buildings in La Serena. The request si enough prove to justify office space for the DM administrators. and for  other staffing needed for DM.  This work will span two weeks and is due this Thursday  April 9.    This story is for the orienting work - -kickoff phone call.
0,recieve and begin to process document from SET about scalability of CEPH,IN the context of ISL investigations into stogie systems the SET group has produced a document  that goes into the scaling of the meta data services.  The concern is that there is a central meta data service ins CEPH.   Began to process this analysis and to think about feasibility of testing program.
1,management for week March 30.,"Investigated invoicing fro storage condo -- appears to be annual fee, OK by Jeff. Investigated attaching  effort breakdown to invoke -- this seems hard as U of I invoicing occurs at quite a distance (procedural) distance from the NCSA business office.  Decided to look at improvements in recording effort in Jira so as to be able to generate report. -- Capture all actuals.  Business office transition  support is transitioning from Matt S. to new person.  Review AMCL sides,  kept tradition generating exponentially more comments, but reduced the exponent.  Process to bill out effort applied to project, but not in standing assignments in the staffing plan.  Internal strategy meeting about agenda items w.r.t VAO given Rap Plante is leaving NCSA. Prep for DM leadership meeting --  synergies at NCSA.  "
0,security weekly meeting ,"met with the ISO, looking for ways to more actively engage.  Idea was to focus on the SCADA enclave, and the need was to engage with  German Etc"
2,Draft SCADA security plan,NULL
0,Initial survey of Datacat for LSST ,"Jacek, Brian Van Klaveren have sent along some initial overview/description of their work on Datacat;      https://confluence.slac.stanford.edu/display/~bvan/LSST+Datacat+Overview  We start examining this in the context of our studies of managing data collections at NCSA."
0,shapelet unit tests attempts to access display on failure,"When tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used.  These plots should be disabled, as they obscure the real error when the display is not available."
2,prepare v10_1_rc3 release candidate,Need for rc3 identified 
0,Fix g++ 4.9 return value implicit conversion incompato,"g++ 4.9 enforces the ""explicit"" keyword on type conversion operators in return value context.  This mean bool checkers along the lines of  bool isValidFoo() { return _smartPtrFoo; }  require an explicit cast to compile under g++ 4.9 with -std=c++0x.  There were a handful of these in our code; found and fixed."
3,run jenkins builds on multiple platforms,"Demonstrate a Jenkins build matrix running lsstswBuild.sh on a number of platforms including; el6, el6, f21, u12.04, & u14.04."
3,Mountain - Base fiber path design and installation method,Design path and installation method for Mountain - Base fiber cable.  Path will run from Cerro Pachon to Cerro Tololo to AURA gate.  Installation method will define where the fiber cable will be on poles or underground.
0,Improve db.createTable,"DM-2417 revealed that the current implementation of createTable in db module behaves differently that mysql: mysql will issue a warning if table exists, and db module will fail with an error. We should make the db behave similarly to how mysql behaves. "
0,Doxygenize db,The db module needs to be doxygenized.
0,Optimize support for many identical database schemas - design,It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include coming up with a plan how to implement it.
0,Optimize support for many identical database schemas - impl,"It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include implementing a clean solution, proposed through DM-2504"
0,Document structure of our custom ddl ascii schema,Need to better document what is supported / accepted by schemaToMeta.py. We are currently relying on cat/sql/baselineSchema.sql as the guide.
1,Information exchange between processes - research,"We need to identify a reliable and fast way to exchange information between processes (for example, cmsd and xrootd).   This story involves understanding key requirements (structures, scale), and researching what mechanism would be best).   Deliverable: short narrative describing key requirements, and proposed mechanism, including a sketch of the design."
1,Information exchange between processes - implementation,"Implement system for information exchange between cmsd and xrootd, per instructions in DM-2507"
2,Research feasibility of using SQLite as backend to the db module,This story involves plugging in SQLite and dealing with issues that arise as a result of using SQLite in places that depend on the db module.
0,The distance field of match lists should be set,"The meas_astrom AstrometryTask returns a match list that has distance = 0 for all elements. Neither the matcher nor the WCS fitter are setting this field, and both ought to."
3,FY17 Integrate Web Services with NCSA Authentication System,We need to integrate Data Access Web Services with Authentication mechanisms used by NCSA.
3,W16 Improvements to db,Improvements to Db wrapper.
0,Migrate to new WBS for 02C.06,Migrate to the new WBS structure for 02C.06. Work include: * revisiting wbs assignment for all epics * updating [S15 planning 4 DB team|https://confluence.lsstcorp.org/display/DM/S15+planning+4+DB+team] * updating ldm-240 spreadsheet * updating associated budget accounts * tweaking [build-ldm240.py|https://github.com/jbecla/experimental/blob/master/build-LDM-240.py]
0,"Catch ""address in use""","I noticed when running integration tests, it failed with the error pasted below. It'd be good to catch it and print something useful. I am not entire sure what port number is in use, and what to kill...   {code}   File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 89, in <module>     sys.exit(main())   File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 85, in main     app.run(host)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/flask/app.py"", line 772, in run     run_simple(host, port, self, **options)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 710, in ru n_simple     inner()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 692, in in ner     passthrough_errors, ssl_context).serve_forever()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 486, in ma ke_server     passthrough_errors, ssl_context)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 410, in __ init__     HTTPServer.__init__(self, (host, int(port)), handler)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 419, in __init__     self.server_bind()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/BaseHTTPServer.py"", line 108, in server_bind     SocketServer.TCPServer.server_bind(self)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 430, in server_bind     self.socket.bind(self.server_address)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/socket.py"", line 224, in meth     return getattr(self._sock,name)(*args) socket.error: [Errno 98] Address already in use {code}"
0,Add a CFHT-based post-build integration test to the sandbox build,"From [~boutigny]    I have installed some simple stack validation tools working on CFHT data in {{/lsst8/boutigny/valid_cfht}}    Here is the content of the README file :    ------------------------------------------------------------------------------------------------------------------------  This directory contains a set of utilities to validate a stack release with CFHT data    At the moment, only validation plots for the astrometry are produced    Directories :  -------------  rawDownload     : contain raw CFHT images (flat, dark, bias, fringe,... corrected)  reference_plots : contain reference plots corresponding to the best results obtain so far.    Files :  -------  setup.cfht       : stack environment setup  valid_cfht.sh    : run processCcd taks on the cfht images     valid_cfht.sh init : create the input/ouput directories, ingest raw images and run processCcd     valid_cfht.sh      : without the ""init"" argument, runs processCcd assuming that the directory structure exists and that the raw images have been ingested.  valid_cfht.py    : run some analysis on the output data produced by valid_cfht.sh  processConfig.py : configuration parameters for processCcd  run.list         : list of vistits / ccd to be processed by processCcd    Requirements :  --------------  obs_cfht : tickets/DM-1593  astrometry_net_data : SDSS_DR9 reference catalog corresponding for CFHT Deep Field #3  ------------------------------------------------------------------------------------------------------------------------    Basically it produces a set of plots stored in a png image that can be compared to a reference plot corresponding to the best results obtained so far with stack_v10_0    I hope that this is useful. Just be careful that I wrote these scripts with my own ""fat hand full of fingers"" and that it is just basic code from a non expert. If it is useful, I can certainly add more plots to validate the psf determination, photometry, etc.    Comments, suggestions and criticisms are very welcome."
1,Check for Qserv processes at configuration tool startup,Configuration tool has to check for Qserv processes before removing configuration directory (which may contains init.d scripts for these running processes)
1,Proof of concept Python APIs to access Firefly components,The pipeline needs to visualize the images using Firefly. We want to provide a few Python APIs for proof of concept that we could do this in Python and IPython notebook. 
0,Update repo.yaml for first set of Sims Stash repo moves,The repos.yaml file needs to be updated with correct repository locations once SIM-1074 is completed.
1,Implement distributed database deletion,"Implement database deletion based on the process defined in DM-1396. Need to deal with situations like worker is offline - might need some infrastructure e.g., running something in background to act when affected workers come back online.  Deliverable: a demonstration of system that deletes a distributed database: user issues ""drop database x"" and all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later."
1,Ensure we can delete/create table with the same name,Test / ensure that we can create a table with the same name as the table we just deleted.
2,Margaret's mgmt. activities in March,Catch-all story for LOE activities in March 2015.  20 working days - 7 days vacation = 13 actual days  13 days * 2 SP/day * .5 FTE = 13 SP.
0,Tom Durbin on board as a consultant for the Base site data center.,"Prep/attend/follow through for meeting with Tom Durbin, the facility manager of the National Petascale Computing Facility, to discuss participating as  consultant to the project as it finds a design contractor, and as the design matures.    "
1,Mgt activity summary for week of April 6,"- Made inquires about the status of materials contracting - ball in AURA's court. - Prepare for visit to Lyon.  Consult with atoll, researched collaborative structures,  articulated and vetted hardware process, made some slides for the visit. - Spent time thinking about VO protocols and such in prep for the DM F2F discussion. - Edited Job descriptions for the ADS department ,who will recruit for our systems engineers to include LSST, and LSST concerns. - Management / leading by walking around  N.B. Margaret in CAM training (or associated travel)  Tu-F. N.B Don was off 1 1/2 days  "
1,Resolve outgoing port issues on Blue Waters/Cray systems ,"pro data system scaling tests on cray system were limited by the number of outgoing ports on a cray node. The limitation had been  ~20 ports, participated in Tests of new system software,limit relaxed to at least ~2000 in tests. Likely greater."
0,discussed the request from the SUI group for authentication guidance ,"responded to ticket from Jacek, on behalf, I think  of the SUI group asking for guidance on authentication at NCSA.   So far, consulted with Alex Withers,  contemplating the extent of policies so far (not much) an authentication mechanism worth investigtaing and likely policies.  Drew figure for discussion, wrote up in hip chat."
0,Remove version attribute from Schema,"Remove the Schema attribute and its getters and setters.  This change won't be something we can merge to master on its own, as it doesn't provide backwards-compatible FITS reading that will added in future tasks."
1,Rewrite afw::table FITS reading to be more flexible,"In order to support backwards-compatible FITS table reading, we need to break the current assumption that everything we need to know about how to read a Record from a FITS file is contained in the Record's Schema.  This issue involves that refactoring, without actually adding the backwards compatibility support."
0,Backwards compatibility for reading compound fields from FITS,"Read old-style afw::table compound fields in as scalar fields, using the new FunctorKey conventions."
0,Backwards compatibility for reading slots and measurements from FITS,Rename fields to match the new slot and measurement naming conventions.
1,Contextual error handling,"There are cases when an empty result might have different errors than the top error, and it would be good to unwrap the context in which the error occured. Example: GET /meta/v0/db/L3/joe_myDb/tables/Object, the result might be empty because the database does not exist, or the v0 is not a supported version, etc."
1,RESTful python client,Develop basic abstractions for restful apis in a python client
1,Research Ceph file system,"Research Ceph as possible networked filesystem for LSST usage to replace NFS. Estimate spending 10-20 hours of work with result being a wiki page of suggestions, limitations, etc.  (Implementation will be a different task, presuming we want to implement.)"
1,Python APIs for Firefly ,We need Python APIs to interface with Firefly visualization components.  This is the first set of many functions.  
0,ctrl_events build issue,Had a problem where ctrl_events was having build issues.
0,LaTeX support in Doxygen broken,"LaTeX markup in Doxygen documentation ought to be rendered properly for display in HTML. It isn't: it's just dumped to the page as raw text. See, for example, [the documentation for {{AffineTransform}}|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_15_07.01.28/classlsst_1_1afw_1_1geom_1_1_affine_transform.html#details]."
0,Host.cc doesn't find gethostname and HOST_NAME_MAX under el7,el7 gives an error that it can't find HOST_NAME_MAX.
1,Fix again interface between QservOss and new cmsd version,"QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} "
0,The string repr of Coord should show the coordsys and angles in degrees,"The default string representation of Coord (e.g. std::cout << coord in C++ and str(coord) in Python) is to show class name and a pair of angles in radians.  It would be much more useful if the default display showed the angles in degrees, as that is what people are used to. Also, it would be very helpful if the display included the name of the coordinate system. This is especially needed for the base class, as it is quite common to get shared_ptr to Coord and have no idea what coordinate system it is.  At present there is a lot of code that unpacks the angles and explicitly displays them as degrees to get around this problem. But it seems silly to have to do that."
0,ANetAstrometryTask's debug doesn't fully work,"{{ANetAstrometryTask}}'s debug code calls (deprecated) method {{Task.display}}, which raises an AttributeError on this coce:  {code}  try:      sources[0][0]  except IndexError:              # empty list      pass  except (TypeError, NotImplementedError): # not a list of sets of sources  {code}  "
1,xrootd can't be started via ssh,"{code:bash} qserv@clrinfopc04:~/src/qserv$ ssh localhost -vvv ""~qserv/qserv-run/2015_02/etc/init.d/xrootd start"" ... debug3: Ignored env _ debug1: Sending command: ~qserv/qserv-run/2015_02/etc/init.d/xrootd start debug2: channel 0: request exec confirm 1 debug2: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug2: channel 0: rcvd adjust 2097152 debug2: channel_input_status_confirm: type 99 id 0 debug2: exec request accepted on channel 0 Starting xrootd.. debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0 debug2: channel 0: rcvd eow debug2: channel 0: close_read debug2: channel 0: input open -> closed {code}  Here ssh command freeze, it is possible to lauch xrootd with this (example) script: {code:bash} set -e set -x  . /qserv/run/etc/sysconfig/qserv export QSW_XRDQUERYPATH=""/q"" export QSW_DBSOCK=""${MYSQLD_SOCK}"" export QSW_MYSQLDUMP=`which mysqldump` QSW_SCRATCHPATH=""${QSERV_RUN_DIR}/tmp"" QSW_SCRATCHDB=""qservScratch"" export QSW_RESULTPATH=""${XROOTD_RUN_DIR}/result"" export LSST_LOG_CONFIG=""${QSERV_RUN_DIR}/etc/log4xrootd.properties""  eval '/qserv/stack/Linux64/xrootd/xssi-1.0.0/bin/xrootd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/xrootd.log -n worker -I v4 &'  echo ""SCRIPT STARTED"" {code} and the same problem occurs. So the problem seems to be with xrootd, and not the startup scripts.   "
0,Remove most compound fields from afw::table,"Remove all Point, Moment, Coord, and Covariance compound fields.  Array fields should be retained for now; it's not clear if we want to remove it or not, or how to handle variable-length arrays if we do."
0,Create and advertise Firefly mailing list,Create an IPAC mailing list for all users of Firefly.  Advertise it to the interested communities (including the LSST Camera group) and through the Github site.  The mailing list firefly@ipac.caltech.edu has been created and all the interested partied have been subscribed to the list.
1,Make dbserv async,NULL
0,Vectorize methods for locating objects on detectors,vectorize _transformSingleSys and _findDetectors in afw.cameraGeom so that the sims_coordUtils method findChipName (which finds the chips that an object lands on) runs faster.
2,Migrate qserv code to reworked db/dbPool,"Migrate code to the new implementation of SQLAlchemy-based Db module, including removal of DbPool."
3,Chilean Network LOE ,NULL
3,Addressing File corruption in iRODS 3.3.1,In this issue we examine how file corruption would be detected and repaired with iRODS tools and rules/microservices.
0,read and understood proposal to consider CAS/crowd system ,"the FERMI telescope has an authentication system based on CAS/Crowd. The  benefit of the system is that it can be use as an authentication system for both web and command line.      Download materials, and acquire the  understanding from a review of documentation . Discuss with the ISO,  propose discussion for vTony's visit to NCSA (may 21)."
1,management activities for week of April 13,"Read proposed  ""Hardware"" contract amendment, sent marked up comments to Julie Robinson, U of I contract negotiator.  Major points are that Hardware is not descriptive of all purchases  needed to fulfill SOW.  The procurement approval process needs spelling out. Detailed guidance in comments inserted into contract.  Along with M. Gelman met with the NCSA business people to fully understand the U  of I invoicing process, and the information in the existing business processes. prior to inventing processes for the  supplementing the U of I invoice with the more detailed annotations (hours by WBS) agreed to in the LSST contract.  Obtained help from the NSCS IT group. Documented in tow page note.   Met concerning seemingly large amount of effort to respond to hip chat take about slowness in the NCSA development system.     Miscellaneous and meetings.  "
1,Research BeeGFS file system,"Research BeeGFS as possible networked filesystem for LSST usage to replace parts of NFS. Estimate spending 10 hours of work with result being a wiki page of suggestions, limitations, etc. (Any implementation will be a different task, presuming we want to implement.)  http://www.beegfs.com/content/  BeeGFS (formerly FhGFS) is a parallel cluster file system, developed with a strong focus on performance and designed for very easy installation and management. If I/O intensive workloads are your problem, BeeGFS is the solution.  Likely not good replacement for formal/managed data, but perhaps great option for shared scratch file systems."
0,"Calling AliasMap::get("""") can return incorrect results","It looks like empty string arguments can cause AliasMap to produce some incorrect results, probably due to the partial-match logic being overzealous."
1,Implement user-friendly template customization,"Qserv configuration tool has to be improved to allow developers/sysadmin to easily use their custom configuration files (with custom log level, ...) for each Qserv services.    An optional custom/ config file directory will be added, and configuration files templates which will be here will override the ones in the install directory.    This should be thinked alongside configuration management inside Docker container."
0,log4cxx build failure on OS X,[~frossie] writes:  {quote} I have a log4cxx failure on a Macp while building lsst_distrib. Attaching file in case someone has any bright ideas for me in the morning {quote}
1,Research MaxScale as a mysql-proxy replacement,"We have been told by Monty that MaxScale is the replacement of the mysql-proxy. Based on DM-2057 the sentiment is that it won't work for our needs. We should very briefly document what our needs are, how we use the proxy now, and if we think MaxScale is not good-enough, say it why, and discuss with Monty and his team."
3,Purchase of network equipment for use in Chile,"jkantor, rlambert"
3,Base LAN Network Design ,"Design of the network at the Base to to provide services for the ""tenants"" Telescope, Camera and DM"
3,Base Network LOE,NULL
3,Design the Network from NCSA to Ampath in Florida,NULL
1,Comparison of ALMA and Reuna/AURA costs on National link,NULL
0,Comparison of ALMA summit to base link with LSST,NULL
0,Remove obsolete hinting code in proxy,Remove now dead code related to sending hints from proxy to czar
1,Client API for new worker management service,"We have new worker management service which has HTTP interface, now we need to provide simple way to access it from Python basically wrapping all HTTP details into simple Python API. "
0,Change repos.yaml for next set of Simulations Stash repos,The next set of Simulations Stash repository migrations is laid out in SIM-1121.
1,Symlink data directory at configuration,"We decided to introduce symlinks in order to protect data. This is in particular useful when we need to reinstall qserv, but we have valuable, large data set that we want to preserve. This story introduces symlinks to data: when Qserv is reinstalled, only the symlink is destroyed, and the data stay untouched."
3,Fiber installation on AURA property from Gate to Pachon,AURA and Reuna oversee Telefonica in installing the fiber from the AURA Gate to Cerro Tololo to Cerro Pachon.
1,afw.Image.ExposureF('file.fits.fz[i]') returns the image in 'file.fits.fz[1]' ,It seems that afwImage.ExposureF ignores the extension number when this is passed on as part of the filename and uses the image in extension number 1. This is not the case with afwImage.MaskedImageF which correctly uses the input extension number passed in the same way.  The problem has been checked on OSX Yosemite 10.10.3 with  the is illustrated in  the following code https://gist.github.com/anonymous/d10c4a79d94c1393a493  which also requires the following image in the working directory: http://www.astro.washington.edu/users/krughoff/data/c4d_130830_040651_ooi_g_d1.fits.fz 
3,FY18 Integrate DRP with Data Provenance,"Integrate the Data Provenance system with the DRP. This includes capturing hardware and software configuration, as well as dependencies between data sets.  Deliverable: System capable of capturing provenance for DRP."
1,'eups distrib install flask -t qserv' fails on Ubuntu 14.04,"Qserv now depends on Flask, so this blocks all Qserv install which rely on eups.  Comman below works with system-python but not with anaconda:  {code} qserv@clrinfoport09:~/stack/EupsBuildDir/Linux64/flask-0.10.1/flask-0.10.1⟫ python setup.py install --home /home/qserv/stack/Linux64/flask/0.10.1                                                                   running install Traceback (most recent call last):   File ""setup.py"", line 110, in <module>     test_suite='flask.testsuite.suite'   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/core.py"", line 151, in setup     dist.run_commands()   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 953, in run_commands     self.run_command(cmd)   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 972, in run_command     cmd_obj.run()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 73, in run     self.do_egg_install()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 82, in do_egg_install     cmd.ensure_finalized()  # finalize before bdist_egg munges install cmd   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized     self.finalize_options()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 274, in finalize_options     ('install_dir','install_dir')   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 298, in set_undefined_options     src_cmd_obj.ensure_finalized()   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized     self.finalize_options()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install_lib.py"", line 13, in finalize_options     self.set_undefined_options('install',('install_layout','install_layout'))   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 302, in set_undefined_options     getattr(src_cmd_obj, src_option))   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 105, in __getattr__     raise AttributeError, attr AttributeError: install_layout {code}"
3,FY18 Integrate Calibration Pipe with Data Provenance,NULL
3,FY19 Integrate L3 with Data Provenance,Integrate L3 (images and databases) with Data Provenance.
1,Package flask dependencies,"We packaged flask (see dm-1797) and we are using it via eups, but we have not packaged flask dependencies, and we are still relying on anaconda to get them. This story involve packaging the dependencies."
0,HSC backport: recent Footprint fixes,This is a backport issue to capture subsequent HSC-side work on features already backported to afw.  It includes (so far) the following HSC issues:   - [HSC-1135|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1135]   - [HSC-1129|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1129]   - [HSC-1215|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1215]
3,W16 Qserv Refactoring,Refactoring of Qserv as found necessary in W16.
3,FY17 Qserv Refactoring,NULL
3,FY18 Qserv Refactoring,NULL
0,API key case study,NULL
3,Margaret's mgmt. activities in April,"- Weekly DMLT phone meetings - Weekly security meetings - Weekly local group meetings - T/CAM training meeting - 1 week w/ travel - Attended leadership meetings about NCSA/UIUC receiving and inventory policies and procedures and grant management for project managers  - Prepared slides for AMCL - Updated LDM-240 milestones for FY16+ - Created invoice breakdown (incorrectly!) - Prepared March Technical Progress Report - Prepared Travel Expense Report for T/CAM meeting - Attempted to update S15 Staff Plan in PMCS - Reviewed risk registry with Don - Met with Julie Robinson & AURA to work on/discuss procurement contract amendment   - Met with Kaylyn and Alan to discuss invoicing, billing process and timeline, WBS activity code breakdown in MIS - Met with Jay and Kaylyn about staff planning and budget - Met with Nathan to figure out how to track a lot of this EV stuff by downloading to a local database and integrating MIS information - Developed swimlane diagram to understand roles and responsibilities of reporting - Started working on Gantt chart to wrap my head around tracking resource loading and activity progress"
0,weekly liaison with ISO,"Discussed ""should we piggyback signing of the LSST AUP with a capability offered by OSG""? with Alex.  Additional understanding of Authorization/Authentication."
1,Management activities for week of april 21,"Met with Julie Robinson, the Illinois contract negotiator, w.r.t. aura ""hardware"" contract amendment.  re-drafted long paragraph i AURA section, breaking it down into separate items for each party, and addressed what I see a grave flaws so that a discussion could be held.  Did work w Margaret do arrange for business proscesst discussion relating to monthly reporting to LSST appended to invoices, basic   Interviewed Martin Paegert (one day visit).  Further work on other other matters relating to open requisitions of people. Other work on personal matters  Responded to comments about the NCSA WBS and overall project WBS not being aligned.  on LLDM-240 -- provided example of working one case -- scheduled for next wee'k LT.  Misc."
2,LOE - Week ending 5/1/15,#NAME?
0,Drop PK on overlap tables in data loader,NULL
1,Reimplement Data Loader Using Worker Mgmt Service,"Current loader depends on ssh, need to switch to the new service, http based."
1,Add version stamping in czar and ssi service,"DM-2547 will introduce compile-time version generation of a header file that has macros defining version strings. Ideally, each running process using qserv code (e.g., czar, cmsd, xrootd, and mysql-proxy), and perhaps one-shot binaries (loader?) would print version information when logging to improve debuggability.  DM-2547 focused on the osslib plugin (libxrdoss) for the cmsd. The next important processes are the czar and xrootd (libxrdsvc). This story covers inclusion of version identifiers (w/ commit hash) in the czar and xrootd logs. Hopefully this will end any confusion about versions when reading log files sent from colleagues."
1,Modify czar to support table deletion,"Czar needs to handle table deletion. In practice that means mysql proxy should let DROP TABLE queries through, and czar should modify appropriate table-related metadata structures in CSS. This is part of work proposed in  DM-1896.  "
0,Design Basic Watcher,"Design watcher, including its interactions with other components (mysql, css, etc). In the near term, the watcher will handle deleting tables and databases."
0,Implement DROP table in watcher,Implement DROP table using the watcher designed in DM-2623.
0,Create service for managing watcher,We need to be able to start/stop the watcher implemented through DM-2624. This story involves extending our scripts for starting various qserv services to manage watcher.
1,Add support for configuring multi-node integration tests,"The multi-node integration test software produced through DM-2175 has hardcoded node names. This story will allow user to configure it. Current plan is to pre-set integration test for several different configurations, e.g., single-node, 2-node, 8-node (and maybe eg 24-node), and user would supply node names through a configuration file."
0,Integration test succeeds when individual tests fail,"Integration test behaves strangely, it always succeeds even though there may be tests that fail. Here is what I ge when I run individual case: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-check-integration.py -i 01 -l ............... 2015-04-28 11:26:12,137 - lsst.qserv.tests.benchmark - ERROR - MySQL/Qserv differs for 4 queries: 2015-04-28 11:26:12,138 - lsst.qserv.tests.benchmark - ERROR - Broken queries list in /usr/local/home/salnikov/qserv-run/2015_04/tmp/qservTest_case01/outputs/qserv: ['0001_fetchObjectById.txt', '0003_selectMetadataForOneGalaxy_withUSING.txt', '0003_selectMetadataForOneGalaxy_classicJOIN.txt', '0003_selectMetadataForOneGalaxy.txt'] 2015-04-28 11:26:12,138 - root - CRITICAL - Test case #01 failed {noformat}  But if I run integration test it says everything is OK: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-test-integration.py ................... ok  ---------------------------------------------------------------------- Ran 5 tests in 160.058s  OK {noformat}  There are actually messages about failed test in the output but you have to look very closely not to miss them. "
0,Fix build for gcc 4.7.2 and gcc 4.8.2,#include <condition_variable> is missing in threadSafe.h
1,Document configuration tool main use cases,"- Document main use case for qserv-configure.py: install Qserv master/worker node with externalized data directory  - Hide complex configuration options?  {code} Configuration steps:   General configuration steps    -d, --directory-tree  Create directory tree in QSERV_RUN_DIR, eventually                         create symbolic link from QSERV_RUN_DIR/var/lib to                         QSERV_DATA_DIR.   -e, --etc             Create Qserv configuration files in QSERV_RUN_DIR                         using values issued from meta-config file                         QSERV_RUN_DIR/qserv-meta.conf   -c, --client          Create client configuration file (used by integration                         tests for example)  Components configuration:   Configuration of external components    -X, --xrootd          Create xrootd query and result directories   -C, --css-watcher     Configure CSS-watcher (i.e. MySQL credentials)  Database components configuration:   Configuration of external components impacting data,   launched if and only if QSERV_DATA_DIR is empty    -M, --mysql           Remove MySQL previous data, install db and set                         password   -Q, --qserv-czar      Initialize Qserv master database   -W, --qserv-worker    Initialize Qserv worker database   -S, --scisql          Install and configure SciSQL {code}  "
2,"Use WebSocket for communication between client and web server, proof of concept",Research and proof of concept code to use Web Socket for two-way communication between client and web server. 
2,implementation of Web Socket for two-way communication between client and Web server ,Implementation of web socket to be used as the two-way communication method between client and web server. 
1,"refactor the image stretch code for better, simplified  organization ",NULL
1,add new image stretch algorithm to Firefly visualization ,"There is a need to include two new stretch algorithms, which are asinh and power law gamma.  The algorithm is as follow: * asinh ## input        zp: zero point of data        mp: maximum point of data        dr:  dynamic range scaling factor of data.  It ranges from 1-100,000        bp: black point for image display        wp: white point for image display ## calculate rescaled data value        rd = dr *(xPix - zp)/mp ## calculate normalized stretch data value         nsd = asinh(rd)/asinh(mp-zp) ## calculate display pixel value        dPix = 255 * (nsd-bp)/wp       Note: The bp, wp values specify how far outside of the scale data one wants the image to display.  By default, setting bp=0 and wp=dr.    * power law gamma ## input \br        zp: zero point of data        mp: maximum point of data        gamma: gamma value for exponent ## calculate rescaled data value        rd = xPix - zp ## calculate normalized stretch data value         nsd =  rd^(1/gamma) / (mp0zp)^(1/gamma) ##  calculate display pixel data value         dPix = 255 * nsd       "
0,"Provide a function to return the path to a package, given its name","As per RFC-44 we want a simple function in utils that returns the path to a package given a package name. This has the same API as eups.getProductDir, but hides our dependence on eups, as per the RFC."
1,Update code to use the function provided in DM-2635,As per RFC-44: update existing code that finds packages using eups.getProductDir or by using environment variables to use the function added in DM-2635
1,Run large scale tests,Coordinate running large scale tests.
0,missing dependencies in scons builds,"ndarray and afw have some headers generated via m4, and while those are built when the package is installed, if someone just tries to build other targets, they aren't - leading to build failures.  We also need to add a dependency from the ""lib"" target to the ""python"" target, because we can't link the Python libraries against the C++ library until it's built.  That needs to be changed in sconsUtils. "
1,Migrate Qserv to ssi v2,ssi v2 including comple objectification of the interface. Need to migrate qserv to the new interface.
0,Switch to using shpgeom and remove duplicate code,Qserv is currently relying on a copy of the spherical geometry code (in core/modules/sg) instead of relying on the sphgeom module. This needs to be cleaned once we sort out the build issues with sphgeom (DM-2262).
0,TOWG attendance ,remote operations discussion. 
1,MGT for balance of April ,"recruiting for open positions Work on accounting infrastrucutre. ""hardware"" contact - -   outline to Jeff over the phone what is coming  work through  inventory infrastructure for materials for La Sereba  review budget and effort projections.  hear file system invesitigations meeting. "
2,Histogram options,"We have studied histogram options supported by Exoplanet Archive (http://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=planets)    We'd like to support similar options for our Histogram plot. The options are:    - Column selection  - Axes options:         Linear, Log, Linear reversed, Log reversed        Range selection (Auto, Manual)  - Binning options        Min, Max, Number of Bins        (These can be assigned automatically: nBins = sqrt(nPoints))  "
3,Prepare external http api for Firefly viewer for beta use,NULL
1,Fix thread leak in Qserv,Qserv is currently leaking a thread per query. Executing a simple query list select count(*) from Object in a loop results in everything hanging after qserv is up to 67 threads.
1,Prepare next gen Firefly JavaScript API Tools for beta,NULL
2,Look into current transient alert event systems.,#NAME?
0,Configuration mechanism for GalSim galaxy generation,"This is an additional script for great3sims to allow simple configuration of the great3sims.run().  Most of the parameters which need to be set are in great3sims.constants.py, though some additional command line parameters may be needed for the run method."
1,Build Psf Libraries from PhoSim Images,"Takes output provided by Debbie from PhoSim runs and use them to create libraries of Psfs.  Warp to remove camera distortion if necessary.  This issue does not include figuring out what different categories of Psfs are required, but all of the process issues should be covered in this issue."
0,Categorize Psfs and Distributions Required from PhoSim,"Request a full focal plane of Psf images. Write code to allow them to be stored in a way which allows us to sample randomly from a full focal plane.  There will be multiple such focal planes, so we also need to be able to pass the information to the measurement algorithm which will allow us to categorize measurements by visit.  This will be done in the Psf Library building code, and will then be passes to the measurement algorithm through the great3sims code which constructs the data for the measurement algorithm."
1,Produce HSC Psf sample for use in algorithm testing,"Produce a set of well distributed Psfs from HSC data.  As long as the Wcs info is also provided, the code to warp them should have been done in a separate issue."
1,Alternative parameterized Psfs from PhoSim,"Michael Schneider has suggested that he can do a better job of creating realistic Psfs from Psf models which he is working on, and which he intends to integrate into GalSim.  These are intriguing, but depend on work which hasn't been done yet.  When these models are fully available in GalSim and supported through the yaml configuration interface, we should work with them.  But this is currently an ""as time permits"" issue."
1,Prototype test harness for testing measurement algorithms,"This is a relatively simple task, which will take the Galaxy images from the great3sims modifications and run measurement algorithms on the individual postage stamps.  The result will be a catalog of the measurement outputs, cross-references against the galaxy and psf parmeters used for a given postage stamp.  To do this, we need to combine information from the galaxy catalog and psf catalog into an input catalog for the algorithm.  A source needs to be created for each galaxy which will contain at least the galaxy centroid and footprint relative to the postage stamp.  The postage stamp with Psf appended and the above source much be fed to the measurement algorithm"
1,Do time tests running measurement algorithms against sample galaxies,"Jim has suggest that we use cmodel to run these tests, since he is not committing to completing a complete shape measurement algorithm during the next sprint.  So we will do our timing test using cmodel and shapelet approximation, and switch to the new algorithm from Jim when it is available."
1,Find an adequate process platform for shape measurement tests,"This issue requires an estimate of how many measurements  we will need to run during S 15.  And it also needs an estimate of how long it will take to measure a single galaxy.  We should be able to guess how many galaxies are required to do an accurate assessment of a single parameterization of the shape measurement algorithm.  We probably cannot accurately estimate how much of the parameter space of the shape measurement algorithm we will have to explore.   The total amount of processing required should tell us whether this can be done with simple multi-core systems, or if a more sophisticated parallel process environment is required.  There is a large additional task if ordinary multi-core processing isn't adequate, so this task may spawn a rather large additional issue."
3,proof of concept types and providers for managing jenkins security settings,"Proof of concept level implementation of native puppet types and providers for managing jenkins users, security realm, and authorization strategy."
1,Create Analysis code for Constant Shear Tests,"For any test of shear measurement vs. input shear (where input shear is constant), plot the measured shear vs. input shear and fit the multiplicative bias m and additive bias c."
3,puppet types & providers for puppet security management,"The current puppet-jenkins (and ansible, and chef) can not fully control jenkins users and security realm / authorization strategy settings.  We should develop a proof of concept level limitation of naive puppet types and providers for users, security realm, and authorization strategy."
2,Analyze bias vs. postage stamp size of galaxies,"Vary the postage stamp size of simulated galaxies and access the effect of that size on the shear bias.  This task will not require additional galaxy image generation, as the intent is to generate all the galaxies at a size which is liberally larger than the likely point where bias does not change with size.  James Jee has indicated that 48 pixels on the LSST scale is not large enough enough for this bias to converge.  It seems likely that we will need to generate our images at 64 or 96 pixels to get beyond this limit."
2,resolve communication between JavaScript component and java server, We are writing the web application client side code in JavaScript. JS interop will make it much easier to use GWT with JavaScript libraries. This task is to resolve the issues that may rise with this technology since it is new in GWT2.7.  
0,Build 2015_05 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
0,Build 2015_06 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
2,Get meas_mosaic working on HSC data with LSST stack,"We have an old, bitrotted version of meas_mosaic on the LSST side, created in a failed attempt to get it running on LSST PhoSim data.  Now that we're making a serious effort to get HSC data running through the LSST pipeline, we'll need to get it running with the LSST pipeline at least on HSC data, which will probably involve just merging everything from the HSC side over, and then fixing it until it builds and runs.  For this issue, we'll assume that we're going to use the Eigen backend for the matrix solver, rather than the MKL one we use on the HSC side.  That will make it much slower (since MKL is multithreaded and we can't make Eigen multithreaded for just meas_mosaic), but hopefully still usable."
1,Research Serf and Consul,Serf: https://serfdom.io  Consul: https://www.consul.io  
1,Investigate loading of binary data,"The binary entries in qserv_testdata are stored as binary values in text files and there is no reason to believe that they are being read into the database correctly, see qserv_testdata/datasets/case01/data/Science_Ccd_Exposure.tsv.gz.     Binary data from text files needs to be in hex format along with whatever other changes need to be made to reliably load the data into the database. Note that this story involves just investigating, it is not yet clear how much work will be needed to properly implement it. This story will help up understand the effort needed."
0,Fix default LOAD DATA options,"Integration tests in multi-node produced the following error during data loading: {code} 2015-05-01 17:03:03,030 - lsst.qserv.admin.dataLoader - CRITICAL - Failed to load data into non-partitioned table: Data truncated for column 'poly' at row 60 2015-05-01 17:03:03,031 - root - CRITICAL - Exception occured: Data truncated for column 'poly' at row 60 {code}  The default options for MySQL LOAD DATA need to be fixed for this."
3,v11.0 release,NULL
1,Fix race condition in userQueryProxy,"In UserQuery_kill, depending on timing, the call ""uqManager.get(session)->kill()"" can fail if kill is called more than once by user, because the session might get deleted by the earlier kill. To simulate this, I modified the code to delay the second kill as follows:  {code} void UserQuery_kill(int session) {     static int killNo = 0;     killNo ++;     LOGF_INFO(""EXECUTING UserQuery_kill(%1%), %2%"" % session % killNo);     if (killNo > 1) {         sleep(10);     }     uqManager.get(session)->kill(); } {code}  We need to revisit if other functions in this class might suffer in similar way."
0,Add missing empty-chunk-path on Ubuntu 14.04,QSERV_DATA_DIR/var/lib/qserv wasn't created on Ubuntu 14.04 and this was breaking loader script. It was working on SL7 for unknown reason. Creation of the directory has been added to qserv-czar config script.
0,Fix case05 3009_countObjectInRegionWithZFlux freeze,This prevents 2014_05 release to pass integration tests.
3,F17 Experiment with Non-Partitioned Tables,"Test sharing of unpartitioned tables between worker nodes. This is something we claimed would work if we simply stuck them on a SAN, but never tested. Now is a good time to find out whether it actually works. If it fails, we need to re-think that part of the design. Shall we put the unpartitioned on a set of std replicated mysql nodes and attach them to the worker mysqld via the connect engine? Probably worth it to reconsider the overall architecture so that this can be integrated more elegantly (and maintainably) than just bolting on more moving parts.    This epic involves:  a) revisiting the numbers and checking if we could simply replicate non-partitioned tables on all nodes  b) estimating realistic load on the non-partitioned tables  c) playing with bringing the non-partitioned tables through mechanisms such as [connect engine|https://mariadb.com/kb/en/mariadb/connect-table-types-mysql-table-type-accessing-mysqlmariadb-tables/]    If replicating on all nodes turns out to be too costly, we will arrange appropriate test bed (at NCSA?) and do the testing in S16 cycle.    "
1,Clean up FITS binary table writing,"FITS binary table is being refactored by necessity on DM-2534, and while there's no similarly urgent need to clean up the writing code, we should do it at some point, as the refactoring of the read code broke some symmetries and made it even harder to follow the writing code that it already was."
0,Review with Telefonica revised path Tololo - Pachon,A meeting in Santiago  with Reuna and Telefonica to discuss the difference in price for the new path from Tololo to Pachon
0,May 1 management,Met with A. Withers first cut read of scada plan. Contact session AURA -- explain gross contract changes accepted i principle)
0,Rule for automatic replication in iRODS,"Maintaining extra copies/replicas on separate resources is an important tenet in iRODS, with this practice considered key for prevention of data loss. The automatic replication of files upon ingest can be encoded via a system rule, so that data is preserved as a inherent part of storing in iRODS."
1,Revisit mysql connections usage in integration tests,"Recent changes in integration tests require too many connections. We need to understand what changes that is now requiring so many connections, and fix it."
1,Revisit mysql connections from worker,"Revisit the code that handles mysql connections in qserv. At the moment Qserv will maintain a connection per chunk-query, up to a hardcoded limit (GroupScheduler: 4, ScanScheduler:32).  Also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit)."
3,Prototype Ceph Deployment,"Deploy Ceph on spare storage servers, with particular emphasis on deploying Ceph FS. This should likely take 8-15 story points over a period of about 3 weeks. This does not include a production deployment of Ceph for LSST. It is intended to help us gain insight into requirements for the initial production deployment. The story will primarily consist of effort from M. Elliott & W. Glick, with secondary effort from M. Freemon and B. Mather. "
0,Research GPFS Server for Performant Access to Condo Storage,Work with NCSA SET to figure out requirements for LSST GPFS Server access to our Condo storage. Implementation will be a different story. Expect this to take 3-6 story points over the next couple of weeks.
1,Fix connection leak,"Fix connection leak: 1 connection is leaking per chunk-query, in practice ~30+ connections for a query that touches many chunks.  It is a real blocker, and we need to fix it asap."
1,Final cleanup of Query cancellation code,"The query cancellation code that went in through DM-1716 works fine, however we feel it'd be good to do another pass and double check we are applying the cancellation consistently. Some potential places to clean: 1. in ccontrol/UserQuery.cc we changed the semantics of discard() 2. QueryRequest needs some cleanup: it'd be better to call Finished() from one place  More regarding the former (from DM-1716 PR): ""if a query is cancelled, none of the cleanup below happens in discard() anymore -- presumably we are now waiting for object deletion to do the cleanup.  If object deletion is sufficient to do this cleanup, do we need discard() at all anymore? It would be best if cleanup always occured in the same place rather than having two different control paths for it?""  Regarding the latter - see comment in https://jira.lsstcorp.org/browse/DM-1716"
1,Fix memory leak in Executive,"There is a memory leak, most likely in Executive, related to _requesters. It looks like the ~MergingRequester() is never during normal operations (it is called when there are abnormal conditions and different parts of the code are triggered).   As a result _infileMerger kept inside MergineRequester is not called either, which results in 2 connection leaks per query."
1,Latin America Infinera rep to give presentation of equipment,NULL
3,FY17 Research technologies potentially useful for Data Access,NULL
0,Understand race condition in Executive::_dispatchQuery,"Inserting a log (presumably just a delay) in Executive::_dispatchQuery after the new QueryResource but before the Provision call causes queries to fail.  The particular test query was ""select count(*) from Object"" on test case 01."
1,Convert the ds9 interface to follow RFC-42,"RFC-42 (provide a backend-agnostic interface to displays) being accepted, please implement it.    For now, provide compatibility code so that the old way (import lsst.afw.display.ds9) still works.  "
0,Mutex use before creation,"qana/QueryPlugin.cc contains a static boost::mutex, that is used by static class member functions to register plugin implementations. Its constructor is not guaranteed to be called before the static registerXXXPlugin (see e.g. qana/AggregatePlugin.cc) instances use it to register plugin classes."
1,Migrate boost:thread to std::thread,We are mixing boost and std threading libraries. This should be cleaned up - use std:thread consistently everywhere.
0,Migrate boost::shared_ptr to std::shared_ptr,"We are mixing boost and std shared_ptrs. This should be cleaned up - use std:shared_ptr consistently everywhere. In a few places we have other types of pointers, (e.g weak_ptr). Migrate these too."
0,Add missing includes unistd.h for gcc 4.9.2,NULL
0,Fix connection leak (2nd iteration),Fix connection leak (and memory leak and thread leak) -- we are leaking 2 per query.
1,Add test involving many chunks,"It might be useful to add a test to the integration test suite that involves a large number of chunks per node. I think I'd try something like 200-300 chunks. I'd 1. add case06 2. get one table, say Object from case05 and configure partitioning to ensure we have 200-300 chunks. 3. Run several queries that touch all chunks."
0,Upgrade EUPS used by lsstsw,"As discussed, bump it up when you get a chance please. "
0,Migrate boost::scoped_ptr to std,"We have a few places where we are using boost::scoped_ptr. Given we migrated shared_ptrs, we might want to move scoped_ptrs too (most likely to std::unique_ptr)."
0,Revisit design of query poisoner,"As we discovered through DM-2698, poisoner tends to hold onto query resources even after the query completes. We should revisit whether than can be redesigned and improved, so that when query finishes, all resources related to that query are immediately automatically released. This story involves just the planning part, implementation will be done through separate stories."
2,LOE - Week ending 5/8/15,NULL
1,LOE - Week ending 5/15/15,NULL
1,LOE - Week ending 5/22/15,NULL
2,LOE - Week ending 5/29/15,NULL
0,Package Python requests package,To complete DM-2593 we need to package and install `requests` as a separate package instead using one from anaconda.
0,Build should fail if node.js is not present,"Problem: I built Firefly by mistake w/o having node on my path. The build didn't signal any errors, but generated an unusable webapp that wouldn't load.  Expected behavior: the build should have failed and warned the user that node.js is missing."
0,Fix a few more g++ 4.9.2 compatos,"Some of the recent boost -> std changes don't compile/link under gcc 4.9.2, because of some poor #include hygiene (including <thread> when we should include <condition_variable>, not explicitly including <unistd.h>, etc.)  Also, -pthread linker option is required when using std::thread under gcc 4.9.2. "
2,Generalize / Simplify Facade ,"Daniel started thinking about simplifying Facade, here is some unfinished code from him  {code} /// Unfinished. Planned to be a re-thinking of Facade that collapses some /// genericity and simplifies things using the assumption of running on a /// snapshot. class FacadeSnapshot : public Facade { public:     StringMap _map; // Path --> key      FacadeSnapshot() {     }      virtual bool containsDb(std::string const& dbName) const {         if (dbName.empty()) {             LOGF_DEBUG(""Empty database name passed."");             throw NoSuchDb(""<empty>"");         }         string p = _prefix + ""/DBS/"" + dbName;         bool ret =  (_map.find(p) != _map.end());         LOGF_DEBUG(""containsDb(%1%): %2%"" % dbName % ret);         return ret;     }     virtual bool containsTable(std::string const& dbName,                                std::string const& tableName) const {         if (!containsDb(dbName)) {             throw NoSuchDb(dbName);         }         if (tableName.empty()) {             LOGF_DEBUG(""Empty table name passed."");             throw NoSuchTable(""<empty>"");         }         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" + tableName;         bool ret =  (_map.find(p) != _map.end());         LOGF_DEBUG(""containsTable returns: %1%"" % ret);         return ret;     }     virtual bool tableIsChunked(std::string const& dbName,                                 std::string const& tableName) const {         if (!containsTable(dbName, tableName)) {             throw NoSuchTable(dbName + ""."" + tableName);         }         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" +                tableName + ""/partitioning"";         bool ret =  (_map.find(p) != _map.end());         LOGF_DEBUG(""%1%.%2% %3% chunked.""                    % dbName % tableName % (ret?""is"":""is NOT""));         return ret;     }     virtual bool tableIsSubChunked(std::string const& dbName,                                    std::string const& tableName) const {         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" +             tableName + ""/partitioning/"" + ""subChunks"";         StringMap::const_iterator m = _map.find(p);         bool ret = (m != _map.end()) && (m->second == ""1"");         LOGF_DEBUG(""%1%.%2% %3% subChunked.""                    % dbName % tableName % (ret ? ""is"" : ""is NOT""));         return ret;     }     virtual bool isMatchTable(std::string const& dbName,                               std::string const& tableName) const {         LOGF_DEBUG(""isMatchTable(%1%.%2%)"" % dbName % tableName);         if (!containsTable(dbName, tableName)) {                 throw NoSuchTable(dbName + ""."" + tableName);         }         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" + tableName + ""/match"";         StringMap::const_iterator m = _map.find(p);         bool ret = (m != _map.end()) && (m->second == ""1"");         LOGF_DEBUG(""%1%.%2% is %3% a match table""                    % dbName % tableName % (ret ? """" : ""not ""));             return ret;     } #if 0     virtual std::vector<std::string> getAllowedDbs() const {     };     virtual std::vector<std::string> getChunkedTables(std::string const& dbName) const;     virtual std::vector<std::string> getSubChunkedTables(std::string const& dbName) const;     virtual std::vector<std::string> getPartitionCols(std::string const& dbName,                                                       std::string const& tableName) const;     virtual int getChunkLevel(std::string const& dbName,                               std::string const& tableName) const;     virtual std::string getDirTable(std::string const& dbName,                                     std::string const& tableName) const;     virtual std::string getDirColName(std::string const& dbName,                                       std::string const& tableName) const;     virtual std::vector<std::string> getSecIndexColNames(std::string const& dbName,                                                          std::string const& tableName) const;     virtual StripingParams getDbStriping(std::string const& dbName) const;     virtual double getOverlap(std::string const& dbName) const;     virtual MatchTableParams getMatchTableParams(std::string const& dbName,                                                  std::string const& tableName) const;   private: #endif }; {code}"
0,Add config file for test dataset 04 tables,"Following the changes to default LOAD DATA settings in DM-2679, two tables in test case 04 need to have a config file to include their in.csv format."
1,optimistic matcher may match the same reference object to more than one source,"The optimistic pattern matcher in meas_astrom, adapted from hscAstrom, does not check if reference objects have been used before when finding the reference object nearest to each source. As a result the same reference object may be matched to more than one source. This should not happen."
1,Log xrootd client debug messages in Qserv czar,"xrootd client print it's debug messages to stdout. This ticket aims at redirecting them to Qserv logger, if possible."
0,Build a DiscreteSkyMap that covers a collection of input exposures,"This is essentially a rehash of the old trac Ticket #[2702| https://dev.lsstcorp.org/trac/ticket/2702], originally reported by [~jbosch], which reads:  ""I'd like to add a Task and bin script to create a DiscreteSkyMap that bounds a set of calexps specified by their data IDs. This makeDiscreteSkyMap.py could be used instead of makeSkyMap.py when the user would rather compute the pointing and size of the skymap from the input data than decide it manually.""  The work was done by [~jbosch] & [~price] and exists on branch {{u/price/2702}} in {{pipe_tasks}}, but it was never merged to master.  I plan to simply rebase the commits in that branch onto master."
0,"Remove #include ""XrdOuc/XrdOucTrace.hh"" from Qserv code","See next emails:  Hi Fabrice,  Absolutely!  Andy  On Wed, 13 May 2015, Fabrice Jammes wrote:  > Hi Andy, > > Thanks, > > In my understanding, you're ok if I remove the existing > #include ""XrdOuc/XrdOucTrace.hh"" > from Qserv source code. I'll do it soon. > > Have a nice day, > > Fabrice > > Le 12/05/2015 23:41, Andrew Hanushevsky a écrit : >> Hi Fabrice, >> >> Well, no. We have a long-standing approach that qserv should not depend on anything outside of XrdSsi public interfaces. This is the only way to easily protect sqserv code from infrastructure changes. So, I would not. If you want to copy something like that for >> >> qserv please do, it's simple enough. But in the end qserv needs to be self-contained in that it does not depend on xrootd code just the public ssi interfaces. >> >> Andy >> >> -----Original Message----- From: Fabrice Jammes >> Sent: Tuesday, May 12, 2015 9:06 AM >> To: Andrew Hanushevsky >> Subject: About xrdssi client logging >> >> Hi Andy, >> >> Hope you're doing well. >> Could you please tell me if its usefull to include >> #include ""XrdOuc/XrdOucTrace.hh"" >> in our xrdssi client code? >> >> Indeed client seems to only print DBG macro output, that's why I was >> wondering if XrdOucTrace was only use on the server side. >> If yes, I will remove it from our client. >> >> Thanks, and have a nice day, >> >> Fabrice "
0,Make ANetAstrometryTask more configurable,"The current ANetAstrometryTask has a solver that is not easy to retarget. This makes testing with hscAstrom needlessly difficult. My suggestion is to make the solver a true Task instead of a task-like object, and make it retargetable using a ConfigurableField instead of a ConfigField. This is very easy to do because the solver is already a task in all but name. "
0,sandbox selection of newinstall.sh source url,"Frossie would like the ability to control the source URL for the newinstall.sh script in sandbox-stackbuild.  The newinstall.sh installation logic needs to be migration to the puppet-lsststack module, converted into a defined type, and have unit+ acceptance tests written for it."
1,Second Review with Chris Smith AURA head,Went over the process relating to AURA and NSF
3,Design of the summit network computer facility,jeff 
0,Add clear message when integration test fails,"Integration test fails without printing a clear message at the end, and for now a query is broken: 0011_selectDeepCoadd.txt but it isn't printed at the end of tet output."
1,Fix case04/0011_selectDeepCoadd.txt,"It seems --config=/path/to/table.cfg param can be duplicated (see dbLoader l.77 and QservDbLoader l. 87)    Futthermore there is an enclosing pb and it can be solved for this query by passing correct cfg table (which in.csv.enclose correct parameter), but then next query fails: it seems some cfg parameters of table.cfg aren't managed correctly by the loader in plain MySQL mode.     This need further investigations."
1,Allow lsst/log library to log PID on the C++ side,lsst/log should be able to log application PID
0,db 10.1+4 tests randomly fail with python egg installation error,"The unit tests for DB seem to fail at random and always pass on a second build attempt.  My hunch is that multiple tests are running in parallel all attempting to install the mysql module but I haven't investigated.  {code}                   db: 10.1+4 ERROR (0 sec). *** error building product db. *** exit code = 2 *** log is in /home/build0/lsstsw/build/db/_build.log *** last few lines: :::::  [2015-05-15T19:12:35.557258Z] scons: done reading SConscript files. :::::  [2015-05-15T19:12:35.558276Z] scons: Building targets ... :::::  [2015-05-15T19:12:35.558409Z] scons: Nothing to be done for `python'. :::::  [2015-05-15T19:12:35.570007Z] makeVersionModule([""python/lsst/db/version.py""], []) :::::  [2015-05-15T19:12:35.686733Z] running tests/testDbLocal.py... running tests/testDbRemote.py... running tests/testDbPool.py... failed :::::  [2015-05-15T19:12:35.695011Z] passed :::::  [2015-05-15T19:12:35.698811Z] passed :::::  [2015-05-15T19:12:35.706360Z] 1 tests failed :::::  [2015-05-15T19:12:35.706703Z] scons: *** [checkTestStatus] Error 1 :::::  [2015-05-15T19:12:35.708443Z] scons: building terminated because of errors. {code}  {code} [root@ip-192-168-123-151 .tests]# cat * tests/testDbLocal.py  Traceback (most recent call last):   File ""tests/testDbLocal.py"", line 53, in <module>     from lsst.db.db import Db, DbException   File ""/home/build0/lsstsw/build/db/python/lsst/db/db.py"", line 49, in <module>     import MySQLdb   File ""build/bdist.linux-x86_64/egg/MySQLdb/__init__.py"", line 19, in <module>        File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 7, in <module>   File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 4, in __bootstrap__   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 937, in resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1632, in get_resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1662, in _extract_resource   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1003, in get_cache_path   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 983, in extraction_error pkg_resources.ExtractionError: Can't extract file(s) to egg cache  The following error occurred while trying to extract file(s) to the Python egg cache:    [Errno 17] File exists: '/home/build0/.python-eggs'  The Python egg cache directory is currently set to:    /home/build0/.python-eggs  Perhaps your account does not have write access to this directory?  You can change the cache directory by setting the PYTHON_EGG_CACHE environment variable to point to an accessible directory.  tests/testDbPool.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-test.txt' not found. tests/testDbRemote.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-testRemote.txt' not found. {code}"
1,Write example-based documentation for multiband processing,"The multi-band coadd processing tasks we're porting over from the HSC side don't have the high-quality example-based documentation we typically provide for Tasks on the LSST side, so we need to write it from scratch."
1,Improve selection criteria for sources,"Dominique Boutigny has demonstrated that one reason the new astrometry task is working so poorly is that it is not selective enough about which sources it uses. This ticket is to be used to improve that situation.  Another problem Dominique discovered is that the TAN-SIP WCS fitter needs to be iterated to work properly, and that work may also be done on this ticket. "
1,Configure NCSA LSST Perfsonar Host,NULL
0,Administrative - 6-2015,Meetings and reporting and such
1,Create LSST wiki documentation for LHN effort,NULL
0,Onboarding Humberto,Efforts in helping new employee Humberto come up to speed in his role as lead on perfsonar deployments
1,Avoid leaking memory allocated by mysql_thread_init,"mysql/MySqlConnection.cc contains the following comment: {code}     // Dangerous to use mysql_thread_end(), because caller may belong to a     // different thread other than the one that called mysql_init(). Suggest     // using thread-local-storage to track users of mysql_init(), and to call     // mysql_thread_end() appropriately. Not an easy thing to do right now, and     // shouldn't be a big deal because we thread-pool anyway. {code}  The comment is not really correct with regards to thread pooling. Instead, each rproc::InfileMerger has an rproc::InfileMerger::Mgr which contains a util::WorkQueue that spawns a thread, and so we are failing to call mysql_thread_end at least once per user query. This has been verified using the memcheck valgrind tool. "
3,Mountain - Base fiber implementation,"Aquire, install, and test fiber connecting Mountain - Base.   The fiber will follow a path along roads from Cerro Pachon to Cerro Tololo and down to the AURA gate, where it will connect with Telefonica fiber bundle to La Serena."
2,Improve management of ColSchema.hasDefault and ColSchema.defaultValue,"Managing default values in protobuf and result table isn't optimized for now. Indeed all values are packed in protobuf, whereas default values could be removed from protobuf messages (in .QueryAction::Impl::_fillRows())  It is interesting to monitor performance when packing default values, and when not, and then improve the code related to default value management, or completely remove it."
3,AURA Traffic Utilizing the Fiber Link at 10Gbps,Obtain equipment in order to light the fibers up to 10Gbps and run live traffic for Tololo and Pachon over the link
1,Fix ORDER BY in integration test query case03 0019.1.0 ,"ORDER BY fails sometimes for unknown reason, see  datasets/case03/queries/0019.1.0_selectRunDeepSourceDeepcoaddDeepsrcmatchRefobject.sql.FIXME"
2,investigate decomposition of stack build into independent packages,"In order to obtain per package build time, test time, coverage, or virtually any per component metric ,the CI build needs to be decomposed from a single large integrated build into per package jobs with an overall work flow representing the dependency graph.    This is also needed for binary artifacts to be passed between builds step and/or binary packages."
1,sconsUtil has a hard dependency on EUPS for both tests and installation,"After some discussion on Data Management, its clear that sconsUtils is a hard requirement on EUPS for both tests and installation.  It was decided by RFC-44 that tests should not depend on EUPS.  However, I'd argue that sconsUtils should also not depend on EUPS as any package that uses sconsUtils (the virtual entirety of the stack) can not build or run tests without the presence of EUPS.  The current situation is that the complete stack has a hard dependency on EUPS.    Attempting to build sconUtils without the presence of EUPS.  The tests fail attempting to import the eups module.  {code}  $ SCONSUTILS_DIR=. scons -Q  Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Unable to import eups; guessing flavor  Doxygen is not setup; skipping documentation build.  ImportError: No module named eups:    File ""/home/vagrant/sconsUtils/SConstruct"", line 9:      scripts.BasicSConstruct.initialize(packageName=""sconsUtils"")    File ""python/lsst/sconsUtils/scripts.py"", line 106:      SCons.Script.SConscript(os.path.join(root, ""SConscript""))    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 609:      return method(*args, **kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 546:      return _SConscript(self.fs, *files, **subst_kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 260:      exec _file_ in call_stack[-1].globals    File ""/home/vagrant/sconsUtils/tests/SConscript"", line 5:      import eups  {code}    Attempting to bypass the test failures:  {code}  [vagrant@jenkins-el7-1 sconsUtils]$ rm -rf tests  [vagrant@jenkins-el7-1 sconsUtils]$ SCONSUTILS_DIR=. scons -Q install  Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}"
0,sconsUtil install target does not respond to either force=True or --force,"I've been unable to figure out how to bypass the install 'force' check, but have confirmed that this is the correct expression by commenting it out:    https://github.com/lsst/sconsUtils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsUtils/installation.py#L92    {code}  $ SCONSUTILS_DIR=. scons -Q force=True install   Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}  "
1,Improve SIP fitting,"Dominique Boutigny says ""the tan-sip fitter is very sensitive to bad matches. This is a weakness of the fitter and I think that it could (should) be rewritten in such a way to reject the outliers internally.""  This has resulted in iteration in the matching (DM-2755), which should be unnecessary (or at least minimised).    Additionally, it seems the SIP fitter fits for x and y in subsequent iterations, which can confuse users.    We should:  1. Make the SIP fitter fit for x and y concurrently.  2. Add rejection iterations in the SIP fitter.  3. Remove or minimise the iterations in the matching."
0,Fix races in BlendScheduler,"_integrityHelper() from wsched/BlendScheduler inspects a map of tasks and is sometimes called without holding the corresponding mutex. My theory is that it is observing the map in an inconsistent state, leading to assert failure and hence worker death, and finally to hangs/timeouts on the czar."
1,HSC backport: allow for use of Approximate model in background estimation,This issue involves transferring changesets from the following HSC issues:    - [HSC-145|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-145]  Investigate approximating rather than interpolating backgrounds  - [HSC-1047|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1047] Background object cannot be loaded with butler  - [HSC-1213|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1213] Set background 'approximate' control settings when background control is created.  - [HSC-1221|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1221] tests failing in ip_diffim  - [HSC-1217|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1217] Verify backgroundList IO works properly when Approximate is enabled in background control - HSC JIRA    The Approximate (Chebyshev) approach greatly improves the background subtraction around bright objects compared with the interpolation scheme currently in use (which over-subtracts near bright objects).
0,Fix race in Foreman,The Foreman implementation passes a TaskQueue pointer corresponding to running tasks down to the task scheduler without holding a lock. This means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.
0,Document and test how to log PID via lsst/log,NULL
0,push PID in lsst/log MDC in a C++ plugin (for xrootd),NULL
0,Firefly Tools API: Add advance region support,"Firefly Tools API: Add advance region support  Improve firefly's region functionality to support a ""dynamic region"".  Data can be added or removed from this region by API calls.  Allow any amount of region lines to be added or removed.  Make sure performance is good.  Also, document the current Firefly region support."
1,Control firefly viewer tri-view mode,"When table data is add to Firefly Viewer, control whether it goes into tri-view or just overlay data on FITS, or just does an XYPlot, etc"
0,Add Firelfy Tools API controlled Pan and Zoom,NULL
1,FFTools python wrapper: make launch Browser smarter. ,"FireflyClient.launchBrowser() needs to send an event to the server who will attempt to guess if there is an existing connection.  It will not be launch in that case.  This way it can be called every time without creating tons of tabs, who are all talking to the same channel.    Also, launchBrowser really should not return until the tab is ready to receive events from the websocket channel.  Both of these feature are going to take some thought on how to do.  This is a multi-threaded problem on both the client and the server.    "
0,"FFTools api, wrapper: upload region file from memory like fits file",NULL
1,Footprint dilation performance regression,"In DM-1128 we implemented span-based dilation for footprints. A brief test on synthetic data indicated that this was a performance win over the previous version of the code.    In May 2015, this code was merged to HSC and applied to significant quantities of real data for the first time. A major performance regression was identified:    {quote}  [May-9 00:26] Paul Price: processCcd is now crazy slow.  [May-9 00:29] Paul Price: Profiling...  [May-9 00:40] Paul Price: I'm thinking it's the Footprint grow code...  [May-9 00:44] Paul Price: And the winner is…. Footprint construction:  [May-9 00:44] Paul Price: 2    0.000    0.000  702.280  351.140 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:191(makeSourceCatalog)         2    0.005    0.002  702.274  351.137 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:228(detectFootprints)       15    0.001    0.000  698.597  46.573 /home/pprice/hsc/afw/python/lsst/afw/detection/detectionLib.py:3448(__init__)       15  698.596  46.573  698.596  46.573 {_detectionLib.new_FootprintSet}  [May-9 00:53] Paul Price: If I revert HSC-1243 (""Port better Footprint-grow code from LSST""), then the performance regression goes away.  @jbosch @jds may be interested...  {quote}    The source of the regression must be identified and resolved for both HSC and LSST."
1,rename CameraMapper.getEupsProductName() to getPackageName() and convert to abstract method,"Per discussion on this PR related to DM-2636:  https://github.com/lsst/daf_butlerUtils/pull/1#issuecomment-104785055    The CameraMapper.getEupsProductName() should be renamed to getPackageName() and converted to an abstract method.  This will eliminates a runtime, and thus ""test time"", dependency on EUPS.  As part of the rename/conversion, all subclasses that are not already overriding getEupsProductName() will concurrently need to have getPackageName() implemented."
0,meas_modelfit not in full-stack doxygen build,"I'm fairly certain meas_modelfit is included in lsst_apps and hence in CI, but id doesn't seem to be included in the LSST Doxygen build:    http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/search.php?query=modelfit"
0,Make the new astrometry task the default task,"The new astrometry task should be the default astrometry task, but we need to make sure it is good enough first."
1,Improve behavior of new matcher on highly distorted fields,"The optimistic matcher used by the new astrometry task probably does not handle highly distorted fields well. The issue is that it tries to match in X-Y space, and if that has significant curvature then the match is not optimal.    I suggest matching in RA/Dec space, as per Tabur's original algorithm (on which this code is based). This is simple and easy to understand    An alternative is to use the old technique of ""undistorting"" source and reference object positions before matching. This works, but is complicated, difficult to understand and adds an unnecessary step.  "
1,Fiber from Tololo to Pachon,Preparation for fiber cable from Cerro Tololo to Cerro Pachon
1,Various fixes for broken code within display=True clauses and/or using --debug,Running with the display and/or debug options turned on is revealing many instances of code that is now suffering from bit rot.  This ticket will be used to track those encountered while trying to debug issues arising while porting HSC code and running processing tasks on real data.
1,Implement HSC improvements to Colorterm,"Paul Price recommended some HSC changes for the Colorterm class. To quote Paul: changed Colorterms so it's not a global, and it can now be configured using Config. See https://github.com/HyperSuprime-Cam/meas_astrom/blob/master/python/lsst/meas/photocal/colorterms.py    This sounds useful. Note that the HSC colorterms.py is in meas_astrom but as of DM-1578 the LSST version is in pipe_tasks."
0,Tests for daf_butlerUtils should not depend on obs_lsstSim,Currently two of the tests in {{daf_butlerUtils}} depend on {{obs_lsstSim}}. They will never run in a normal build because {{obs_}} packages can not be a dependency on {{daf_butlerUtils}}.    After discussing the options with [~ktl] the feeling is that {{ticket1640}} should be rewritten to remove the dependency and {{ticket1580}} can probably be removed.
0,Review with Contractors preparing fiber path,Hold conversations with the two major fiber laying contractors to prepare the path from Tololo to Pachon with a trench
0,Document NCSA Wide Area Network status now and in the near future,Write up a document explaining how the wide area network is evolving at NCSA.
3,W16 Finish Implementing Database & Table Mgmt,Implement table and database deletion.
1,Adapt multi-node tests to latest version of qserv / loader,"The multi-node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately."
1,Implement query metadata skeleton,Skeleton implementation of the Query Metadata - including the APIs and core functionality (accepting long running query and saving the info about it)
2,Complete Query Metadata Implementation,Including query abort
1,Run large scale tests,NULL
3,Chile National Links Contracts Negotiation,The negotiations and conversations that have occurred with firstly Entel and finally Reuna and Telefonica for dark fiber between La Serena and Santiago
2,La Serena - Santiago Dark Fiber Acquisition,Acquiring the fiber between La Serena and Santiago and testing the segments. This will be carried out by Reuna with some collaboration with LSST personnel
3,La Serena - Santiago Link Equipment installation,Reuna will install the amplifiers and the DWDM equipment for the link between La Serena and Santiago with collaboration with LSST personnel 
2,La Serena - Santiago LInk with Live Traffic flowing,The link is established and tested with Live LSST data running over the 100G Lambda
3,Mountain to Base Implementation Plan Feasibility Check/Reevaulation,The Installation of the fiber on the AURA property from gate to Pachon via Tololo. Testing that portion of the link.  Connecting the 4 dark fibers supplied by Telefonica and testing end to end.
3,Fiber lay between Gate to Tololo,Telefonica will install the fiber cable from the Gatehouse to Tololo. This will be tested once terminated. We will oversee and monitor the installation and be present for testing.
3,Fiber Install from Tololo to Pachon ,Telefonica will install the cable from Tololo to Pachon Sahred Infrastructure building and terminate. Testing of this portion will be carried out.
2,Connection of Telefonica 4 Fibers at AURA Gate ,The connection will made with the Telefonica 4 dark fibres to the 24 fiber cable from the gate to Pachon.  Tests will then take place over the whole mountain to base link.
1,Creation of chunked views in wmgr,Current implementation of the creating chunks for views in wmgr is likely not doing right thing. Need to find an example of the partitioned views and implement correct procedure.
0,Document architecture of the data loader,Fabrice requested documentation for the overall architecture of the data loader.
3,Install 10Gbs Transceivers at the ends of the fibers and test,Purchase 2x10Gbs Transceivers and install at Pachon and La Serena.  Run tests to confirm integrity of the link  Utilize the link for AURA live traffic on the fiber backbone
3,AURA traffic utilizing 100Gbs Lambda,AURA acquires their DWDM end nodes and installs. Tests and live traffic flows.
3,Contract Negotiations for Chilean links,Defining the contracts between AURA and Reuna and Reuna and Telefonica who is supplying and installing the fibers for the La Serena to Santiago link
3,Mountain - Base Contract and Execution,Defining the contracts and execution between AURA and Reuna and Reuna and Telefonica who is supplying and installing the fibers for the mountain to base link
2,La Serena - Santiago Early Diverse Path,Reuna will provide a 4Gbps path via the legacy fiber path to Santiago
2,La Serena - Santiago Diverse Path Final Capacity,Reuna will upgrade the capacity from 4Gbps to a minimum of 40Gbps over the legacy fiber route to Santiago
3,La Serena - Santiago Fiber Tests over 3T cable,The 3T cable from La Serena to Santiago is expected to be completed September 2015 at which time Reuna can test the segments along the route.  
3,Mountain - Base AURA link upgrade to 100Gbps,AURA will obtain their DWDM equipment end nodes and install on the backbone from the summits to La Serena.
1,Implement RESTful interfaces for Database (POST),"Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""POST"" type requests only, ""GET"" will be handled separately."
3,orphan threads in archive DMCS,"The archive DMCS can experience orphaned threads if a connection is made from external processes waiting for data to arrive.   If the external process goes away, the thread that was created to handle that connection will be waiting on a data structure to be updated.   If the data doesn't arrive, the thread remains alive when it should be checking to see if the connection that created it is still viable, and die if it isn't viable."
1,LOE - Week ending 6/5/15,NULL
1,LOE - Week ending 6/12/15,NULL
1,LOE - Week ending 6/19/15,NULL
0,LOE - Week ending 6/26/15,NULL
1,Add unit tests for the new colorterms code,"The new colorterms code that we adopted from HSC may not have complete unit tests. The existing colorterms test is pretty good, but may have holes. I'm more concerned about the unit test for PhotoCalTask, which does not apply colorterms at all (likely an existing issue).    Also, be sure to test that the obs_cfht config override loads correctly (presumably with a unit test in obs_cfht) and similarly for obs_subaru."
2,Documentation and testing for Firefly Javascript and Python API,"Document, polish and test Firefly Javascript and Python APIs    - Proofread and polished all the documentation, added missing docs  - Tested all the examples and API methods. Updated test cases as needed."
0,sconsUtils should notice when SWIG python file has been modified,Currently {{scons}} will not rerun tests if a {{.i}} file has been modified if the only outcome of that modification was a change to the {{.py}} file. {{sconsUtils}} should be modified to look for changes in both SWIG output files.
1,Add support for listing async queries,"Modify mysql proxy and implement ""show processlist"" command, which should display list of currently running queries."
1,Write Qserv User Guide,"It'd be useful to write a document about Qserv geared towards users, describing what queries Qserv supports now, what will be supported in the future, what restrictions we are imposing and such. "
3,Distributed Hash Table prototyping,NULL
2,reserved,NULL
2,Near Neighbor Optimizations,"To optimize near neighbor queries we are maintaining overlap tables and subchunking. It'd be useful to revisit that. Getting rid of subchunks would simplify qserv code. This epic involves   * testing speed of in-database near neighbor queries without subchunking, including how sensitive the optimizer is for these types of queries   * exploring possibility of precalculating and storing near neighbors, perhaps per subchunk  "
3,SUI Firefly server side Python job management,"In order to support Camera team needs and L3 data production, Firefly server needs to be able to start a Python job with proper input data and get the output data as a result of running the Python job. This will make the future integration of Firefly and DM pipeline stack much easier. "
0,Tweaks to OO display interface,"When I wrote the initial version of display_firefly I found a few minor issues in the way I'd designed the Display class; at the same time, [~lauren] found some missing functions in the backward-compatibility support for ds9.    Please fix these;  note that this implies changes to afw, display_ds9, and display_firefly.  "
0,getSchemaCatalogs() breaks Task encapsulation,"The {{getSchemaCatalogs()}} method was added to {{Task}} to allow {{CmdLineTasks}} to introspect their subtasks for schemas they produce, but it requires the subtasks to report the schemas by butler dataset.  This limits subtask reusability by locking them into producing a particular Butler dataset (or, as in DM-2191, requiring additional arguments from their parent task that they wouldn't need with a better design).    Instead, we should have per-subtask-slot interfaces (i.e. an interface for all subtasks that could fill a particular role in a CmdLineTask) for how the parent tasks should retrieve their schemas.  This will require `CmdLineTask` subclasses to implement the `writeSchemas` method themselves, instead of inheriting an implementation from `CmdLineTask` itself."
1,Build the recent 10.1 release  & Gather strace logs for file system testing,"We build the recent Version 10.1 stack release in a Centos 6.6 docker container. As we do so, we also gather strace logs for candidate packages  (for example, afw) for analysis within an effort to create load simulators for file system testing/profiling.   As another product of the effort,  I will make a docker image of the latest release installed on Centos 6.6 and push to  docker hub."
0,AAA requirements document,NULL
0,Put together a few slides for NCSA-IN2P3 meeting," I put together a few slides for the NCSA-IN2P3 meeting describing previous scaling, middleware, and processing efforts of LSST DM. "
0,Fix Qserv SsiSession worker race,"The worker SsiSession implementation calls ReleaseRequestBuffer after handing the bound request to the foreman for processing. It therefore becomes possible for request processing to finish before ReleaseRequestBuffer is called by the submitting thread, resulting in a memory leak."
3,Margaret's mgmt. activities in May,"Weekly DMLT meeting  Weekly ISO meeting  Weekly NCSA local group meeting    Met with NCSA networking person (Paul) to discuss progress and plans  Attended End-to-End networking meeting    Attended remotely the CCS-DAQ-OCS-DM Workshop for SCADA presentation by ISO    Worked on/discussed AURA procurement contract amendment and to understand property management procedures at AURA and NCSA    Discussed/planned agenda, made travel arrangements, prepared slides for trip to CC-IN2P3  2-day meeting in France with CC-IN2P3 group    4-day DMLT face-to-face meeting and T/CAM day    Interview with candidate for systems lead    Cleaned up Jira tickets from April  Reviewed reporting requirements and Jira procedures with NCSA employee (Bruce) and managers (Doug, Brett)    Attended several NCSA leadership development and training courses"
1,Multi-processing capability for shear test measurements,"A suitable multi-cpu capability must be created for measurement tests.  We are hoping to just use a pipe_task, but to do so, the butler must be customized to allow it to read our cutouts and psfs from galaxies and psfs generated from GalSim and PhoSim.    This will be a relatively simple story if pipe_tasks running on 2 or 3 machines at UC Davis proves to be an adequate solution for running our shear experiments."
1,"Add support for ""ORDER BY f1, f2"" for has-chunks query","{code}   QuerySession description:    original: SELECT objectId, taiMidPoint FROM   Source ORDER BY objectId, taiMidPoint ASC;  has chunks: 1    needs merge: 1    1st parallel statement: SELECT objectId,taiMidPoint FROM LSST.Source_%CC% AS QST_1_    merge statement: SELECT objectId,taiMidPoint ORDER BY objectId,,taiMidPoint ASC    ScanTable: LSST.Source  {code}    Merge statement has syntax error"
1,"Return error for ""SELECT a FROM T ORDER BY b"" for has-chunks query",ORDER BY field has to be in result table => it has to be in select list.  Return clear error message to user if not.
0,Enquiry into MiniSub FO cable,Obtaining a quote from a company in Canada for a special clad cable for the Tololo-Pachon link
1,RFI with vendors in Vina,Open day with all interested vendors  to layout the projects for equipment on Mountain-Base and La Serena-Santiago links.
1,Validate wmgr client / server versions ,"If the client and server are on different versions, unexpected things can happen. (example: we run old version of the server, and use latest client). We need to check the version on both sides. "
1,Fix bug related to selecting rows by objectId from non-director table,"The following example illustrates the problem:    Let's select one raw from qservTest_case01_qserv    {code}  select sourceId, objectId FROM Source LIMIT 1;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    Then select it, but use ""sourceId"" in the query, all good here:  {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    But if we add ""objectId"", the row is not found:    {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250 and objectId=386942193651348;  Empty set (0.09 sec)  {code}    Similarly, even without sourceId constraint, the query fails:  {code}  select sourceId, objectId FROM Source WHERE objectId=386942193651348;  Empty set (0.09 sec)  {code}    "
0,Merge BoundedField from HSC as is,"To make headway on aperture corrections, we are bringing the HSC implementation of BoundedField over."
0,Learn about Butler,Transferring knowledge from K-T to the DB team.
0,Learn about Butler,Transferring knowledge from K-T to the DB team.
0,Learn about Butler,Transferring knowledge from K-T to the DB team.
0,Learn about Butler,Transferring knowledge from K-T to the DB team.
0,Learn about Butler,Transferring knowledge from K-T to the DB team.
2,Document butler v2 and transfer knowledge to Nate,Clean up and release prototype implementation of Butler v2.
0,Setting with CoordKey doesn't support non-IcrsCoord arguments,Something in the {{FunctorKey}} template resolution doesn't allow {{Coord}} arguments to be used when setting record values with a {{CoordKey}} (only {{IcrsCoord}} arguments work.
1,"Handle ""where objectId between""","Query in a form:    {code}  select objectId   from Object   where objectId between 386942193651347 and 386942193651349  {code}    currently fails with  {code}  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150602-20:41:43, Complete (success), 0,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150602-20:41:43, State error (unrecognized), 0,   Ref=3 Resource(/chk/qservTest_case01_qserv/6800): 20150602-20 (-1)  {code}    We already documented that such queries are not advised, but nethertheless qserv should handle it better, e.g., return a message ""not supported"" "
0,demo package should contain the same comparison script used by CI,"The lsst_dm_stack_demo package currently used in the CI system contains a {{bin/compare}} script that doesn't do all of the checks done by the numdiff script that buildbot runs.  These need to be unified, so users can anticipate buildbot results and reproduce buildbot failures locally, especially when making changes to the expected-results file.    While the {{numdiff}} script currently checks more columns than the {{compare}} script, I believe the compare script follows a much better approach and should be extended to be used in both places ({{numdiff}} converts everything to ascii then compares text files; {{compare}} works directly from the binary results and uses NumPy to do the comparisons)."
0,Add transformation tasks for new Butler dataset types,The new Butler dataset types created as part of the HSC deblender merge will need transformation tasks so they can be ingested to the database.    See also DM-2191.
3,Revisit Parser / IR,"Revisit the existing parser code   * consider reusing the code from maxscale or (antrl3) mysql parser from mysqlworkbench, or maybe reuse http://savage.net.au/SQL/sql-92.bnf.html and wrap in bison   * separate IR node productions from grammar"
3,S15 Qserv CSS v2,"Revisit Qserv Common State System. Implement mysql-based KV interface, and add support for updates. Implement ""locking"" mechanism. "
0,wcslib is unable to read PTF headers with PV1_{1..16} cards,"SCAMP writes distortion headers in form of PVi_nn (i=1..x, nn=5..16) cards, but this is rejected (correctly) by wcslib 4.14;  there is a discussion at https://github.com/astropy/astropy/issues/299    The simplest ""solution"" is to strip the values PV1_nn (nn=5..16) in makeWcs()  for CTYPEs of TAN or TAN-SIP and this certainly works.    I propose that we adopt this solution for now.  "
0,LOG() macro fails if message is a simple std::string,lsst:log LOG() macro crash with fatal error if message is a simple string.
1,Improve confusing error message,"Selecting a column that does not exist results in confusing error. Example:    {code}  SELECT badColumnName  FROM qservTest_case01_qserv.Object   WHERE objectId=386942193651348;  {code}    ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:42, Error in result data., 1, (-1)    Similarly,     {code}  select whatever   FROM qservTest_case01_qserv.Object;  {code}    prints  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:52, Error in result data., 1,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150605-16:23:52, Error merging result, 1990, Cancellation requested  Ref=3 Resource(/chk/qservTest_case01_qs (-1)    (note, sourceId does not exist in Object table)      "
1,Fix broken IN - it now takes first element only,"IN is broken - it only uses the first element from the list. Here is the proof:    {code}  select COUNT(*) AS N FROM qservTest_case01_qserv.Source   WHERE objectId=386950783579546;  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.10 sec)    mysql> select count(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId=386942193651348;  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386942193651348, 386950783579546);  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386950783579546, 386942193651348);  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.11 sec)  {code}"
0,isrTask assumes that the Exposure has a Detector,While trying to use the isrTask to interpolate over bad columns in PTF data I discovered that the code assumes that the Exposure has a Detector attached.    Please remove this restriction.  
0,Keep track of database of the director table,"An L3 child table might very well have an LSST data release Object table as its director, while almost certainly not living in the DR database. To support it, we should keep track of the database name holding director's table. Note, this is related to DM-2864 - the code touched in that ticket should be checking the director's db name.    Don't forget to add a unit test that will exercise it!"
1,Improve qproc unit testing framework,"qproc unit testing framework allow to test the whole query analysis pipeline, it has grow and should be re-organized to be easilly understandable, maintainable."
0,"treat lsst_apps, lsst_libs and lsst_thirdparty as top level products not required by lsst_distrib","Per discussion on RFC-55, it was determined that  lsst_apps and lsst_libs and lsst_thirdparty maybe be treated as separate top level products that lsst_distrib need not depend on them nor do they need to be included as part of CI builds."
2,Travel to CCIN2P3 to establish realtionship,Travel to CC IN2P3.  of establish operatal coordination between the sites
2,Prototype file system loading tools for file system studies.  ,worked on a tool chain to   -- Extract file IO patterns from program strike.  -- Represent in a flat file.  Generate codes --   -- To make many dependent copies dummy files and directories   -- To generate a python code to read and write files like the original application.  -- To generate a c-code to read and write files like the original application.    a driver program to run the pseudo codes in parallel.    The system runs on toys and needs to be show to work on traces from       
1,Host/Attend DM LT meeting ,host/attend DM LT meeting 
0,Add queries that exercise non-box spatial constraints,Qserv has code to support:   * qserv_areaspec_box   * qserv_areaspec_circle   * qserv_areaspec_ellipse   * qserv_areaspec_poly    but only the first one (box) is exercised in our integration tests. This story involves adding queries to test the other 3.
1,Clean up gitolite,"We need to clean up gitolite and cgit:  * Repositories that have moved to GitHub should be removed (or, possibly, mirrored back from GitHub).  * Empty repositories (like contrib/eups.git) and obsolete repositories (like LSST/DMS/afw_extensions_rgb.git) should be removed altogether.  * contrib/data_products.git (the Data Products Definition Document source) should be moved to GitHub in the lsst org.  * contrib/processFile.git should be moved to GitHub in the lsst-dm org unless the author adds some test cases and it can be integrated into the CI system as a top-level product (in which case it can go into the lsst org).  * The primary authors of other contrib repositories should be contacted to see if they should be moved to GitHub in the lsst-dm or another org (possibly a new lsst-contrib org).  In particular, contrib/plotz/* (Paul Lotz of the Telescope and Site subsystem) and contrib/pyreb (IN2P3 work for the Camera subsystem) contain current work that should be moved."
0,Update Scons to v2.3.4,Scons has not been updated in over a year. RFC-61 agreed that we should upgrade it now before tackling some other {{scons}} issues.
0,Add Gaussian PSF example to measurement task documentation,"I see some documentation on how to add a placeholder Gaussian Psf to an image (to work around the fact that some algorithms require a Psf) was recently added to the release notes.  I don't think that's actually appropriate, as the same algorithms also required Psfs in the framework - the failure mode was just different (previously, it'd just result in all objects being flagged and the peak position used for the centroid, so it may have been easy to miss - hence the change to a fatal error).    I propose moving the example to the documentation for SingleFrameMeasurementTask, and taking it out of the release notes.  I'll also make sure there's a link from the Measurement Framework Overhaul Release Notes page to the Doxygen for SingleFrameMeasurementTask - I'm not sure if that's sufficient to make up the visibility gap between the Doxygen docs and the release notes in Confluence, but I don't have any other short-term ideas."
0,Remove unused code from sconsUtils,The code in {{deprecated.py}} in {{sconsUtils}} is not used by anything anywhere. [~jbosch] has indicated that the file can simply be removed.
0,obs_cfht is broken with the current stack,"obs_cfht's camera mapper is missing the new packageName class variable, so it is not compatible with the current stack.    I suggest fixing obs_sdss and obs_subaru as well, if they need it."
0,Build 2015_07 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Port HSC Curve-of-Growth code,"Port content from HSC-1144, HSC-1236, HSC-1223, HSC-1219, HSC-1203, HSC-1153."
2,Improve handling of extremely large blends,Port HSC code from issues:  * [HSC-1250|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1250]  * [HSC-1245|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1245]  * [HSC-1237|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1237]  * [HSC-1268|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1268]  * [HSC-1274|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1274]  * [HSC-1265|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1265]  * [HSC-1228|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1228]
3,Port safe coadd clipping from HSC,"We have an algorithm on the HSC fork that modifies AssembleCoaddTask to clip outliers in a much safer way, based on detecting contiguous regions in the difference between a non-clipped coadd and an aggressively-clipped coadd, and only rejecting pixels that are outliers in a single epoch.    One complication for this code transfer is that some of the coadd code has been refactored on the HSC side, and there may be code in hscPipe that duplicates much of what's in pipe_tasks.  We may need help from [~price] to resolve those inconsistencies before tackling this issue.    Once that's done, HSC code can be transferred from the following issues:   - [HSC-1166|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1166]   - [HSC-1202|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1202]"
0,obs_cfht unit tests are broken,"obs_cfht has one unit test ""testButler"" that uses git://git.lsstcorp.org/contrib/price/testdata_cfht. 4 of the tests fail, as shown below.    In addition, testdata_cfht is huge, and the tests barely use any of it. It's worth considering making a new test repo that is smaller, or if the amount of data is small enough, move it into afwdata or obs_cfht itself.    {code}  localhost$ tests/testButler.py   CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  .CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  E.  ======================================================================  ERROR: testBias (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 122, in testBias      self.getDetrend(""bias"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFlat (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 117, in testFlat      self.getDetrend(""flat"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFringe (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 127, in testFringe      self.getDetrend(""fringe"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testRaw (__main__.GetRawTestCase)  Test retrieval of raw image  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 101, in testRaw      raw = self.butler.get(""raw"", self.dataId, ccd=ccd, immediate=True)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 244, in get      return callback()    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 238, in <lambda>      callback = lambda: self._read(pythonType, location)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 426, in _read      location.getCppType(), storageList, additionalData)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/persistenceLib.py"", line 1430, in unsafeRetrieve      return _persistenceLib.Persistence_unsafeRetrieve(self, *args)  FitsError:     File ""src/fits.cc"", line 1064, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int)      cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r' {0}  lsst::afw::fits::FitsError: 'cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r''      ----------------------------------------------------------------------  Ran 6 tests in 3.544s    FAILED (errors=4)  {code}"
0,PhotoCalTask mis-calling Colorterm methods,"When I implemented DM-2797 I made a few errors in pipe_tasks:  - PhotoCalTask mis-calls two methods of Colorterm by providing filterName, which is not needed  - ColortermLibrary.getColorterm mis-handles glob expressions (the two arguments to fnmatch.fnmatch are swapped).    We also need a unit test for applying colorterms, but that will require enough work that I have made a separate ticket for it: DM-2918. Meanwhile I have tested my changes by running Dominique's CFHT demo. This proves that the colorterm code runs, but does not prove that the terms are correctly applied."
1,Clean up code in afw for Approximate background estimation,"The intention is to eventually set {{useApprox=True}} (i.e. Chebychev Approximation)  as the default for background estimation.  However, in looking into the relevant code in afw/math while working on DM-2778, there is some clean-up and restructuring that needs to be done before resetting the defaults (which may also require adjusting some defaults in the calibrate stage to be more appropriate for the approximation, as opposed to inperpolation, scheme).  This issue is to clean up the code and make sure it all operates coherently.  A seperate ticket will be to actually reset the defaults and make any other config default changes required.    In particular, the config setting of approxOrderX/binSize are not being assessed properly, nor is the behavior of the given undersampleStyle being executed.  The under-sampling checks are currently only being done against the interpoation settings, which is not appropriate when useApprox=True.  A temporary check was added in {{meas.algoritms.detection.getBackground()}} in DM-2778 so that it is currently ""safe"" to run with useApprox=True and any other user overridden config setting (binSize, approxOrder, undersampleStyle) (and there currently exists similar checks in {{pipe.tasks.matchBackground}}), but these should be removed once this issue has been implemented. "
0,Set Approximation as default for background subtraction,"Once the Approximate code in {{afw.math}} has been cleaned up (see DM-2920), set the default for background subtraction to be the Chebychev Approximation (i.e. useApprox=True).  Ensure any other relevant config defaults (e.g. binSize, approxOrderX) are adjusted appropriately.  This will change the outputs of the {{lsst_dm_stack_demo}}, so the ""expected"" files will need to be replaced along with this change in default settings (see DM-2778 for some comparisons of the demo outputs using the interpolation vs. approximation background estimation schemes)."
1,Initial DC Base dseign,Prepare a Document for the Initial Design of the Base and Summit Networks
1,Port HSC-1199 to LSST (UNMASKEDNAN mask propagates to all amplifiers),Port issue HSC-1199 to LSST stack to address UNMASKEDNAN mask propagates to all amplifiers
0,Add RFD issue type to RFC project,"To support the RFD process adopted in [RFC-53], an RFD issue type in the RFC project is required.  While we could add RFD-specific fields to it, I think it's simplest if it's just generic with details provided in the Description."
0,Modernize sconsUtils code to python 2.7 standard,"As part of the work investigating DM-2839 I modernized the sconsUtils code to meet current coding standards (using {{in}} rather than {{has_key}}, using {{items()}} rather than {{iteritems}} etc). Since I'm highly doubtful that DM-2839 is going to be closed any time soon I will separate out the modernization patches into this ticket."
0,move old ingest scripts into and retire old packages,"This ticket implements RFC-57, by:   - renaming datarel to daf_ingest (there is already a daf_ingest package, but it's *completely* empty, so I'll just force-push it all away)   - removing everything from the renamed package that doesn't relate to ingest (including pruning dependencies)   - removing ap and testing_endToEnd from the CI system  "
0,Some AFW tests are not enabled with no explanation,"Running {{coverage.py}} on the AFW test suite indicated that two test classes in {{tests/wcs1.py}} are disabled. {{WCSTestCaseCFHT}} was added by [~rhl] in 2007 but disabled during a merge a long time ago by [~jbosch] in 2010 but with no indication as to why. {{WCSRotateFlip}} appeared in 2012 (added by [~krughoff]) but doesn't appear in the {{suite}} list at the end and so does not execute.    Similarly {{testSchema.py}} has two tests that are not run: {{xtestSchema}} and {{testJoin}}. I assume {{xtestSchema}} is deliberately disabled but could there at least be a comment in the test explaining why?    My feeling is that we should either run the tests or they should be removed. Having them their gives the impression they are doing something useful.    Less importantly, {{warpExposure.py}} has some support code for comparing masked images that was written in 2009 by [~rowen] but which is not used anywhere in the test."
1,Fix problem with Qserv related to restarting mysql,"I noticed some strange (reproducible!) behavior: if I run:    {code}qserv-check-integration.py --case=01{code}    then restart mysqld    {code}<runDir>/etc/init.d/mysqld restart{code}    then the query:  {code}mysql --host=127.0.0.1 --port=4040 --user=qsmaster   qservTest_case01_qserv -e   ""SELECT COUNT(*) as OBJ_COUNT   FROM Object   WHERE qserv_areaspec_box(0.1, -6, 4, 6)""{code}    consistently fails every single time.    To fix it, it is enough to restart xrootd."
0,We write truncated Wcs data to  extended HDU tables in Exposures,"When we write Wcs to extra HDUs in Exposures they are truncated if other than TAN/TAN-SIP.  Please don't write them.    A better long term solution is needed.  In particular, we shouldn't be duplicating this information unnecessarily, and we need to be able to persist e.g. TPV to the tables so as to support CoaddPsf.  These issues are not included here."
3,Test install of OCS software on CentOS VMs,Install current version of the OCS software onto two VMs
0,qserv-admin CREATE NODE fails,{noformat}  qserv > CREATE NODE worker1 type=worker host=worker-1 port=5012 runDir=1;  06/15/2015 05:59:52 QADM ERROR: Missing parameter. (mysqlConn)  ERROR:  Missing parameter. (mysqlConn)  {noformat}  
1,Refactor Histogram in edu.caltech.ipac.visualize.plot package.,"The Histogram has 6 constructors to handle 6 bitpixel data types which are byte, short integer,  integer, long integer, float and double.  Since FitsRead has now only works on float, there the  Histogram should be refactored accordingly."
0,"CalibrateTask has an unwanted ""raise"" in it","On 2014-06-30 commit 696b641 a developer added a bare ""raise"" as a debugging aid to the CalibrateTask in pipe_tasks. That change was accidentally merged to master. I confirmed it was an accident and am filing this ticket as a way to remove the raise and run buildbot before merging to master."
1,fix usage of obsolete astrometry interfaces in ProcessImageTask,"As discussed recently on HipChat (Science Pipelines Standup), there's code in {{ProocessImageTask}} that assumes an ""astrometer"" attribute on a {{CalibrateTask}} instance.  Since this is just needed to match a new set of sources against the reference catalog here, we should probably be using one of the new matcher objects, either by getting one from {{CalibrateTask}} via a documented interface, or by constructing a new one."
0,DS9 tests fail if DS9 not running in some configurations,There are a few issues with the robustness of the {{testDs9.py}} tests in AFW.    * The tests are skipped if the {{display_ds9}} package can not be loaded but they should also skip if {{ds9}} is missing or if {{ds9}} can not be loaded. The latter is especially important during builds that unset {{$DISPLAY}}.  * The launching code in {{initDS9}} can not notice the simple case of {{ds9}} immediately failing to load. It simply assumes that there are delays in launch. The reason for this is that {{os.system}} does not return bad status if the command has been started in the background. Another scheme for starting {{ds9}} should be considered. Maybe a different exception could be raised specifically for failing to start it.  * At the moment each test independently has a go at starting {{ds9}}. This makes the tests take a very long time (made worse by {{_mtv}} also trying multiple times) despite it being clear pretty quickly that {{ds9}} is never going to work.  * Currently the {{mtv}} tests must run early as they are the only tests that attempt to start {{ds9}} if it is not running. If the two tests that call {{mtv}} are disabled two other tests fail. Ideally the {{initDS9}} code should be called in all cases.
0,Wmgr refuses to serve queries from remote interface,Vaikunth discovered that wmgr returns 404 for all operations. It looks like wmgr can serve requests coming from 127.0.0.1 interface but returns 404 for queries from non-local interface.
1,Remove explicit buildbot dependency on datarel,"The buildbot scripts have an explicit dependency on the {{datarel}} package, which we'd like to remove from the stack.  It uses {{datarel}} as the top-level product when building the cross-linked HTML documentation; {{lsstDoxygen}}'s {{makeDocs}} script takes a single package, and generates the list of packages to include in the Doxygen build by finding all dependencies of that package.    So, to remove the explicit dependency on {{datarel}}, we need to either:   - find a new top-level product with a Doxygen build to pass to {{makeDocs}} (e.g. by adding a trivial Doxygen build to {{lsst_distrib}})   - modify the argument parsing in {{lsstDoxygen}} to take a list of multiple products (it *looks* like the limitation to one package is only in the argument parsing), and pass it a list of top-level products in the buildbot scripts.    This is currently a blocker for DM-2928, which itself a blocker for DM-1766, which has now been lingering for a few weeks now.  I'm going to look for other ways to remove the block on the latter, but I don't have a solution yet."
0,remove dead code and dependencies from datarel,"Removing the {{datarel}} package entirely has proved to be difficult (DM-2928, DM-2948), so instead I'm simply going to remove non-ingest code (and dead ingest code) from the package, along with its dependencies on {{ap}} and {{testing_endToEnd}}.  Other dependencies will be retained even if they aren't necessary for the code that will remain in {{datarel}}, to support {{lsstDoxygen}}'s use of {{datarel}} as a top-level package for documentation generation."
1,Refactoring the class CropAndCenter,This class contains the codes which are not used.  It needs to be simplified and refactored. 
1,Crop needs to be refactored,This class needs to be refactored to be in consist with FitsRead class which treats all data type as float.  Thus the bitpix in this class does not have to be treated based on its value.
1,Qserv code cleanup and auto_ptr --> unique_ptr migration,"Code cleanup, including migrating some parts to c++11 (in particular, auto_ptr --> unique_ptr)"
1,Add a unit test for aperture corrections in measurement task,DM-436 adds code to meas_base that allows one to run a subset of measurement algorithms based on execution order. This addition should have a unit test.    DM-436 also tasks to measure and apply aperture correction. Those tasks should have unit tests.
1,Setting up and running PhoSim for Psf Library,"Debbie Bard leaving created some new work creating the Psf Libraries we need.  While this od not a major task except for computer time, there is some setup required.  I will get an account at SLAC and learn to run the PhoSim utilities she and Michael have developed.    In the short term, Simon is going to do some runs for me.  Meanwhile, I will get into Debbie's account and run her configuration at SLAC.    The outputs then need to be checked to be sure that the Psfs are reasonable."
1,Migrate Qserv code to nullptr,NULL
1,Review ITIL V3.0 as prep for input to IT use case,"ITIL is a standard breakdown of processes used in an IT system.  While full ITIL may very well be too heavy LSST operations, it provides a useful checklist for he use cases being developed in the TOWG.   I created an ITIL type spreadsheet to check against the dump of the workflows in the EA  tool  "
1,add metric to  application specific IO benchmarking tool,"iosim is an application-specific benchmarking tool that is developed to be responsive to the request from LSST to select and investigate file systems ahead of actual benchmarks, and in advance of having workflow and other infrastructures needed to investigate file systems under realistic load.  The week the software was    -- Optimized to allow for faster test cycles.   -- Threads were supported by squashing all thread IO into a single simulation process.  -- Information from the original ""model"" program is propagated to the simulation program, allowing for comparison to the model.  -- initial matplotlib plots allowing a degree of visualization of the performance of a simulated run was added    The goals to deliver this capability in a few week when common systems at NCSA are available for testing."
1,Management for Don for week of June 15,On boarded Mattais Carrasco-Kind to work in the process execution in the context of Level 3 processing.   Misc.
2,Port the psfextractor external library from HSC to LSST,NULL
1,Prototype iRODS tiered resource with NERSC HPSS,"iRODS can support access to a tape archive with the use of a ""tiered resource"" where one resource has the role of the cache, and a second has the role of archive.    Use of such a tiered resource construct could be valuable to data management.   Because a iRODS plugin for HPSS is readily available, we examine the set up and use of the tiered resource testing against NERSC HPSS."
0,Read through and comment on latest version of LSE-78,NULL
0,Design CSS that supports updates,"Design how to redesign CSS, we currently take a snapshot when char starts. It is too static. "
0,Fix to DM-2883 isn't quite right,"The fix to DM-2883 (remove illegal PVi_j cards) isn't quite right, and the error was masked by a piece of code elsewhere that duplicated the functionality.    The issues is that while PV1_[1-4] cards are indeed valid, the ones that SCAMP writes are not.  So we should remove them too, if there are any other SCAMP TPV coefficients.    The masking code was a unilateral removal of PVi_j cards dating back years.  "
1,Discourse evaluation (Part 1),Work in support of evaluation Discourse as a DM platform for internal and external interactions.
0,Quantify how much objects are blended,It would be useful to have a parameter that indicates how much any given galaxy is blended. This will be useful for testing how photometry or shears are affected by blending effects.    Ports code from HSC-1260.
0,SourceCatalog.getChildren requires preconditions but does not check them,This is a code transfer from HSC-1247.
0,Miscellaneous CModel improvements from HSC,"This improves handling of several edge case failure modes, tweaks the configuration to improve performance, and adds some introspection useful for Jose Garmilla's tests.    Includes HSC-1288, HSC-1284, HSC-1228, HSC-1250, HSC-1264, HSC-1273, HSC-1240, HSC-1249, HSC-1238, HSC-990, HSC-1155, HSC-1191"
0,FootprintMerge: fix bug when identifying existing peaks in a merge.,"If two separate footprints from the same catalog happen to be merged because an existing merged object overlaps both of them, the flags of which peaks are being detected in which bands is not being propagated. This is causing the apparent dropout of some sources in a merged catalog which were detected in single frame processing.    Taken from ticket HSC-1270"
1,refactor coaddition code,"The HSC fork has coaddition code in two places: pipe_tasks and hscPipe.  The code in hscPipe is what we use (though that depends on the code in pipe_tasks in places), while the code in pipe_tasks is more similar to what's currently on the LSST side.    We want to bring the refactored version in hscPipe back to LSST, but we want to put it directly in pipe_tasks to remove the code duplication that currently exists on the HSC side.    Work on this issue should begin with an RFC that details the proposed changes.    Note that this should not bring over the ""safe coadd clipping"" code, which is DM-2915."
1,polygon masking in CoaddPsf,"We need to create polygon-based masks of the usable area of the focal plane, persist them with exposure, and include them in coaddition of PSFs and aperture corrections.    This includes HSC issues HSC-972, HSC-973, HSC-974, HSC-975, HSC-976.    At least some of this will be blocked by DM-833, which is the port issue for coaddition of aperture corrections."
0,Updating node status in qserv-admin to INACTIVE fails,"In qserv-admin.py when attempting to update a node status from ACTIVE to INACTIVE the following error is produced:    {code}  > update node worker2 state=INACTIVE;  Traceback (most recent call last):  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 650, in <module>  main()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 645, in main  parser.receiveCommands()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 163, in receiveCommands  self.parse(cmd[:pos])  File ""/usr/l  ocal/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 180, in parse      self._funcMap[t](tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 380, in _parseUpdate      self._parseUpdateNode(tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 405, in _parseUpdateNode      self._impl.setNodeState(**options)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/qservAdmin.py"", line 660, in setNodeState      self._kvI.set(nodeKey, state)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/css/kvInterface.py"", line 415, in set      self._zk.set(k, v)    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1170, in set      return self.set_async(path, value, version).get()    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1182, in set_async      raise TypeError(""value must be a byte string"")  {code}"
1,Backport HSC parallelization code,"Assuming RFC-68 is approved, transfer the HSC code to LSST as described there."
2,Compare LSST and HSC pipelines through ISR,"Run both the HSC and LSST ISR routines on two to three visits worth of HSC engineering data. Compare the results. Where differences exist, either:    * Create and work other tickets to resolve them;  * Explain their origin and why we don't think they are a problem."
0,Integrate javascript build with gradle,Integrate javascript build tools webpack with gradle.
1,Conversion of FITS binary table extension to IPAC table format. ,FITS binary table contains data types and structures that cannot map directly to IPAC table.  We need to define ways to handle these cases.
0,Modify IpacTableParser to support extra wide table.,IpacTableParser fail to load IPAC table with extra wide headers and columns.  Replace the logic for reading headers and columns information so that it will support any file/size.
0,XY plot need to be able to handle multiple tables with the same name,"XY plot was relying on a table request object to cache previously loaded tables.  This was done for performance reason.  However, table request is not reliable since the same request may be submitted multiple times."
1,Add XYPlot to Python interface,Make it possible to add plots (not connected to a displayed table) to StandaloneUI.  Add showXYPlot to python API.
2,Firefly External Task Launcher,"Implement and external task launcher, which forks a [python] process and gets back the results. The results can be a table, an image, or a JSON.    "
1,"Search processors to get image, table, or json from an external task","Implement three search processors, which use the External Task Launcher (DM-2991):    - to get a table (possibly in binary FITS format)  - to get an image  - to get JSON"
0,Products must not depend on anaconda,"{{setupRequired(anaconda)}} should be removed from webservcommon.table.    We want to keep the stack buildable with any python 2.7, and should not explicitly depend on anaconda."
1,Understand and improve error code management,"It seems there is several constants to store Qserv error code (for example see msgCode.h and util::ErrorCode, or MsgState::RESULT_ERR,JobStatus::RESULT_ERROR). This could certainly be simplified and clarified?    Furthermore in util::Error it seems there's a confusion between code and statuses"
0,Bump eups anaconda package to 2.2,By popular request. 
1,Begin to write a note for the TOWG using ITIL as a checklist,"Began to work use cases, and found that they were long, considering the number r of aspects that need to be considered,  Presented to the TOWG,  got guidance to think in terms of processes, but to take that the effort estimates would  have a reasonable basis.  Got guidance to think about the sites's needs but to congress a central approach.    Agree that this would be the struggle for the week."
1,Management work for Don in the week of June 22,Internal and external recruiting.   Input on NCSA re-organizaiton to ensure proper placement of LSST activities in the NCSA organization.  Meetings.
0,Whitepaper submission to NSF Cyber Summit,Working on drafting whitepaper and abstract for SCADA security challenges faced by LSST.
0,ISO presentation to all-hands meeting,"Presentation giving overview of ISO work, esp. w.r.t AUP and master security plan."
3,Extend the Process Execution Framework to accomodate changes needed by SUI and others,Extend the Process Execution Framework to accomodate changes needed by SUI and others by changing the task and configuration classes.    Covers effort from July 1st - August 31st at 0.25 FTE.
1,Preliminary Process Execution Framework work,"Preliminary work to extend the Process Execution Framework to accomodate changes needed by SUI and others.    This story captures work done in June, prior to incorporating the activity into the baseline plan. Work will be logged under DM-3003 starting July 1st."
2,"prepare jenkins ""demo"" for usage as an interim CI system",We have a working plan of putting the buildbot-scripts under jenkins demo into usage as a production CI system as an intermediate step towards a fully decomposed build.
3,complete puppet jenkins native type implimentation and merge upstream,NULL
0,Meeting with CTSC at CLHS Portland OR,"Discussed LSST security plan going forward.  Specifically work on SCADA security plan.  Meeting held at conference in Portland OR, June 14th, ACM CLHS."
3,Do more research into Flux modules and bring one in ,NULL
0,expose stretch to python API,NULL
1,Improve region support,Some parts of the region support has been more testing because of the python interface.  It is now clear what we should do.
1,Convert Color Stretch dialog to React/flux/JavaScript,NULL
0,Review LSE-78,Review either the current version #26 and/or the newest version when it become available.
0,Discuss US WAN options with NCSA ESnet representative,NULL
0,Provide network support for ceph and openstack lsst storage server efforts,"work done to provide network connectivity, troubleshoot and monitor connections for the above efforts"
1,DLP/LDM-240 support chages,"  - JIRA changes to create DLP project    - lsst-sqre/sqre-jirakit to generate LDM-240-like display     - iterate with T/CAMs, Kevin, Jeff"
0,Early access user onboarding and feedback ,"Getting comments, testing, hipchat/JIRA changes"
0,Display stories in JIRA epic table display,"  Solved with Issue Matrix plugin; unfortunately this removed the ""create issue in this epic"" functionality, so that needs to be a new ticket."
0,Set up Slack for evaluation,  Free account procured and tested by various volunteers; next step is to apply for non-profit status which gives us the first paid tier free to 100 users. 
0,Set up Discourse for evaluation.,  Server up on DO at community.lsst.org. Email needs fixing before volunteer users can be invited. 
0,Addressing File corruption in iRODS 4.1.x,We examine solutions for repairing corrupt files within an iRODS 4.1.x zone.
1,Read revised LSE-209 and LSE-70,Read over the revised LSE-209 and LSE-70 documents
1,"Add Sdss3Mapper to ingest, convert and map SDSS-III ""frame"" files","SDSS-III does not use the fpC file format for science images.  Science images are now released as [""frame"" files. | http://data.sdss3.org/datamodel/files/BOSS_PHOTOOBJ/frames/RERUN/RUN/CAMCOL/frame.html]  The primary science image (hdu0) comes background subtracted and calibrated to units of nanomaggies, with the backgrounds and flat-field conversions included as extensions. The astrometric information is in hdu3 instead of a separate asTrans file.     obs_sdss should be able to ingest frame files and map them to load as dataset ""raw."" It should also optionally replace the backgrounds and de-calibrate to convert the units back from nanomaggies to counts.     This will be implemented as lsst.obs.sdss.sdssMapper.Sdss3Mapper."
2,Margaret's mgmt. activities in June,NULL
1,Check czar->proxy messages size,"These messages are stored in VARCHAR(255) (FYI, MEMORY tables can't contain TEXT). We just need to make sure we have a reasonable fixed size CHAR (and maybe check whether we are hitting the limit, and log it somewhere)"
1,Move Qserv code comment to LSST documentation standards,LSST documentation standards: https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards#DocumentationStandards-RequiredDocumentationStyle  is different from the previous standards used by Qserv (i.e. /// text).     We should convert everything to LSST documentation standards.
0,remove lsst/log wrapper from Qserv,"lsst/log API looks stable now, so removing the wrapper would simplify the code."
3,Debug problems with Qserv at scale,NULL
0,Implement test suite for new class SqlTransaction,Some test that shows that transactions are properly committed/aborted would be nice to have.
0,Remove unused function populateState() ,"Qserv doesn't seem to relaunch no more chunk query in case it fails (see DM-2643)    And this function is now unused:  {code:bash}  qserv@clrinfopc04:~/src/qserv (master)$ grep -r populateState core/  core/modules/qdisp/Executive.cc:void populateState(lsst::qserv::qdisp::ExecStatus& es,  {code}  "
0,LOE - Week ending 7/10/15,NULL
0,LOE - Week ending 7/17/15,NULL
0,LOE - Week ending 7/24/15,NULL
0,LOE - Week ending 7/31/15,NULL
0,Bi-weekly meeting with Victor and Iain.,NULL
0,Incident response report template,NULL
0,Incident response security work plan document,NULL
1,Creation of XML descriptions of messages sent to OCS,Create XML descriptions of messages sent to the OCS. Upload these to a new github repository.
1,Resolve segmentation fault in LoggingEvent destructor,"There seems to be a possible race condition in log4cxx::spi::LoggingEvent::~LoggingEvent. I've had multiple segmentation faults in that function. In all cases, another thread was involved in writing. In at least 2 cases, the second thread was in XrdCl::LogOutFile::Write.  "
2,RFI with prospective DWDM vendors for Chile National Networks,Hold Request for Information meetings in Chile with equipment vendors. 
1,"Add ""ORDER BY"" clause to lua SQL query on result table","If user query has ""ORDER BY"", then lua  can't just execute ""SELECT * FROM result"" because the order for such query is not guaranteed. To fix that, we need to add ""ORDER BY"" clause to the ""SELECT * FROM result"" query on the lua side.    Once we have the above, we might want to remove ""ORDER BY"" from the query class which runs a merge step on the czar (this has to be done in query analysis step)."
1,Add assertXNearlyEqual methods for image-like classes,"Presently one can compare two image-like objects using free functions imagesDiffer, masksDiffer and maskedImagesDiffer in lsst.afw.image.testUtils. These should be replaced by assertXNearlyEqual methods that afw adds to lsst.utils.tests, as per DM-2193.    If necessary, we could leave the old functions around for awhile. But I would prefer to simply get rid of them if we can.    One subtlety is that the current functions take numpy arrays, not afw image-like class instances. Examine the existing users of the code to determine how best to deal with that."
0,Add slot for calibration flux,This is a port of [HSC-1005|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1005].
0,Use aperture flux for photometric calibration,This is a port of work performed on HSC but without a ticket. Relevant commits are:    * [05bef6|https://github.com/HyperSuprime-Cam/meas_astrom/commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0]  * [4a6be5|https://github.com/HyperSuprime-Cam/meas_astrom/commit/4a6be51c53f61e70f151de7f29863cb723197a99]  * [69d35a|https://github.com/HyperSuprime-Cam/obs_subaru/commit/69d35a890234e37c1142ddbeff43e62fe36e6c45]  * [9c996d|https://github.com/HyperSuprime-Cam/obs_subaru/commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99]
1,Add support for accessing schema from QueryContext,"When we are analyzing a query, sometimes there are situations where we need to know the schema of tables involved in a query. It will also be useful for checking if user is authorized to run query, and for queries like ""SHOW CREATE TABLE"". This story involves writing code that will provide access to schema."
0,qserv code cleanup,"I made some random cleanup of the qserv code while playing with css v2. I want to push these changes to master, thus I am creating this story for this. It involves improvements to logging in UserQueryFactory and Facade (both are now per-module), removing unnecessary namespace qualifiers, and whitspace cleanup."
3,Convert major portion of GWT in Firefly to pure JavaScript (W16),Continue to convert GWT portion of Firefly to pure JavaScript
1,Enable aperture correction in the integration test,"The present integration test does not enable aperture correction. This should be enabled and the results sanity-checked.    This is a separate ticket rather than DM-436 at Jim Bosch's suggestion, to avoid ticket bloat.    It requires two separate changes:  - update obs_sdss's SdssCalibrateTask to measure and apply aperture correction  - update the expected results from the integration test lsst_dm_stack_demo"
1,Attend CCS-DAQ-OCS-DM Workshop IV,NULL
0,whitepaper CFP for nsf cyber summit,NULL
1,Execution Framework prototype,NULL
0,Graphical communication interface,Creating a graphical representation of execution framework
1,basic monitoring of jenkins nodes with notification,"This last weekend, the build slaves el6-2 and el7-2 ran out of disk space and were causing stack-os-matrix build failures.  We should have an active monitoring system that sends notifications via at least one of hipchat/email/pagerduty.  There is disk utilitization information present in jenkins itself, aws cloudwatch, and ganglia (as of v0.2.x of the demo).  However, it may make be more convenient (read: expedient) to use a dedicated monitoring system such as sensu instead of mining existing data sources.    We should also investigate if we can configure jenkins to not schedule jobs on a slaves with low disk space."
0,gcc 4.8 package does not create a symlink bin/cc,"I created a new lsst package named ""gcc"" that contains Mario's gcc 4.8 package. I used it to build lsst_distrib on lsst-dev and it worked just fine. Unfortunately the package does not include bin/cc (which should be a symlink to bin/gcc), and this is wanted because the LSST build system uses cc to build C code.    The desired fix is to modify the installer to make a symlink bin/cc that points to bin/gcc."
0,"add ""dax_"" prefix to data access related packages","As agreed at [Data Access Mtg 2015/07/13|https://confluence.lsstcorp.org/display/DM/Data+Access+Meeting+2015-07-13], add dax_ prefix towebserv, webservcommon, webserv_client, dbserv, imgserv, metaserv"
3,Implement Result Streaming in Qserv,Qserv supporting streaming results while the query is running to client application.
3,FY19 Handling unexpected conditions during query execution,"Detect and handle unexpected conditions during query execution (e.g. bad chunk, hit per user resource limit)"
0,Add & use new mask plane for out-of-bounds regions,"Add a new mask plane for regions with no data - fully vignetted, edge patches in coadd.    This is a port of [HSC-669|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-669]."
0,Handle bad pixels in image stacker,"We currently OR together all mask bits, but we need to be cleverer about how we handle pixels that are bad in some but not all inputs.    This is a port of work carried out on [HSC-152|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-152]."
1,Open day for RFP from Equipment vendors,Open day with vendors to describe the needs and requirements of the mountain to base and La Serena to Santiago networks. This is the first formal meeting in the procurement process. Vendors will be invited to propose their solution along with cost.
0,"HSC backport: extra ""refColumn"" class attributes in multiband",This is a transfer for changesets for [HSC-1283|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1283].  
0,add gcc to list of packages in lsstsw,Add gcc to the list of packages in etc/repos.yaml in lsstsw
0,Reduce verbosity of astrometry,"The astrometry.net solver that runs by default in meas_astrom 10.1 is very verbose.  Here's an example running HSC data with an SDSS reference catalog:  {code}  $ processCcd.py /tigress/HSC/HSC --output /tigress/pprice/lsst --id visit=904020 ccd=49 --clobber-config  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  hscAstrom is not setup; using LSST's meas_astrom instead  Cannot import lsst.meas.multifit: disabling CModel measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/pprice/lsst  CameraMapper: Loading registry registry from /tigress/pprice/lsst/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  processCcd.isr: Applying linearity corrections to Ccd 49  processCcd.isr.crosstalk: Applying crosstalk correction  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  : Empty WCS extension, using FITS header  processCcd.isr: Set 0 BAD pixels to 647.04  processCcd.isr WARNING: There were 6192 unmasked NaNs  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  processCcd.isr: Flattened sky level: 647.130493 +/- 12.733898  processCcd.isr: Measuring sky levels in 8x16 grids: 648.106765  processCcd.isr: Sky flatness in 8x16 grids - pp: 0.024087 rms: 0.006057  processCcd.calibrate: installInitialPsf fwhm=5.88235294312 pixels; size=15 pixels  processCcd.calibrate.repair: Identified 80 cosmic rays.  processCcd.calibrate.detection: Detected 303 positive sources to 5 sigma.  processCcd.calibrate.detection: Resubtracting the background after object detection  processCcd.calibrate.initialMeasurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  LoadReferenceObjects: read index files  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.047945 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.measurePsf: Measuring PSF  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  /home/pprice/LSST/meas/algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:143: RuntimeWarning: invalid value encountered in less    update = dist < minDist  processCcd.calibrate.measurePsf: PSF star selector found 163 candidates  processCcd.calibrate.measurePsf: PSF determination using 114/163 stars.  processCcd.calibrate.repair: Identified 92 cosmic rays.  processCcd.calibrate: Fit and subtracted background  processCcd.calibrate.measurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  Solver:    Arcsec per pix range: 0.153025, 0.18516    Image size: 2054 x 4186    Quad size range: 205.4, 4662.78    Objs: 0, 50    Parity: 0, normal    Use_radec? yes, (320.343, 0.500178), radius 1 deg    Verify_pix: 1    Code tol: 0.01    Dist from quad bonus: yes    Distractor ratio: 0.25    Log tune-up threshold: inf    Log bail threshold: -230.259    Log stoplooking threshold: inf    Maxquads 0    Maxmatches 0    Set CRPIX? no    Tweak? no    Indexes: 3      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_0.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_1.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_2.fits    Field: 258 stars  Quad scale range: [641.674, 2208.56] pixels  object 1 of 50: 0 quads tried, 0 matched.  object 2 of 50: 0 quads tried, 0 matched.  object 3 of 50: 0 quads tried, 0 matched.  object 4 of 50: 0 quads tried, 0 matched.  object 5 of 50: 0 quads tried, 0 matched.  object 6 of 50: 0 quads tried, 0 matched.  Got a new best match: logodds 787.099.    log-odds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index.    RA,Dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix.    Hit/miss:   Hit/miss: ++-+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++  Pixel scale: 0.167612 arcsec/pix.  Parity: pos.  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.048271 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.photocal: Not applying color terms because config.applyColorTerms is False  processCcd.calibrate.photocal: Magnitude zero point: 30.685281 +/- 0.058711 from 173 stars  processCcd.calibrate: Photometric zero-point: 30.685281  processCcd.detection: Detected 1194 positive sources to 5 sigma.  processCcd.detection: Resubtracting the background after object detection  processCcd.deblend: Deblending 1194 sources  processCcd.deblend: Deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources  processCcd.measurement: Measuring 1552 sources (1194 parents, 358 children)   processCcd WARNING: Persisting background models  processCcd: Matching icSource and Source catalogs to propagate flags.  processCcd: Matching src to reference catalogue  LoadReferenceObjects: getting reference objects using center (1023.5, 2087.5) pix = Fk5Coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg  LoadReferenceObjects: found 499 objects  LoadReferenceObjects: trimmed 261 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels  {code}    The verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing.    This is a pull request for fixes I have made."
1,Port HSC optimisations for reading astrometry.net catalog,"Some astrometry.net catalogs used in production can be quite large, and currently all of the catalog must be read in order to determine bounds for each component.  This can make the loading of the catalog quite slow (e.g., 144 sec out of 177 sec to process an HSC image, using an SDSS DR9 catalog).  We have HSC code that caches the required information, making the catalog load much faster.  The code is from the following HSC issues:    * [HSC-1087: Make astrometry faster|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1087]  * [HSC-1143: Floating point exception in astrometry|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1143]  * [HSC-1178: Faster construction of Astrometry.net catalog|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]  * [HSC-1179: Assertion failure in astrometry.net|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]    While there have been some changes to the LSST astrometry code that will mean we can't directly cherry-pick the HSC code, yet I think the main structure remains, so the approach can be copied without much effort."
1,Audit and improve warm-start configuration options,"Many of our command-line tasks - particularly the high-level MPI-based drivers we're moving over from the HSC side - typically reuse intermediate data products they find on disk rather than regenerate them by default, and have a suite of configuration options to control this behavior.    While this aids in faster reprocessing of aborted or failed jobs, it can produce results users would consider surprising (""I re-ran with a new version of the pipeline and nothing changed""), and (IMO) should not be the default behavior for any task.    We also need to guarantee that any warm-start reprocessing always produces the exact same results as a single consistent run.  I do not believe this is currently the case for some of the options in ProcessCcdTask.    Finally, we should put these configuration options somewhere other than the main task config, because they control how the processing is done, not what the results are, and hence should not be checked against existing config files in an output data repo before running.  This will probably require a new mechanism in pipe_base; perhaps a second config class associated with each Task, containing only options that affect the ""how"" of processing without affecting the ""what"".    The first step of implementing this issue should be an RFC."
1,Purchase transceivers for use by AURA,Cisco Xenpack for extra long distance 10Gbs
1,Install switches and transceivers on Pachon and Tololo,Buy switches to deploy MPLS over the fibers. 
1,Configure switches for AURA tenants,Switch configuration for use by individual tenants of AURA
0,CI validation of lsstsw's repos.yaml,"Having some sort of automatic ""lint check"" of the repos.yaml file is desirable due to the length of time required to do a full up test of lsstsw.  It should be possible to cobble a sanity checker together that can be run from travis-ci."
0,meas_base still uses eups in tests,{{tests/centroid.py}} uses EUPS to determine the location of the data file used by the test. This needs to be fixed to use a location relative to the test file.
0,meas_astrom still using eups in tests,"In DM-2636 we modified the tests to be skipped if EUPS is not available. I've had a closer look and all the ones I have glanced at seem to be easily fixable to run without EUPS. The tests seem to be using EUPS to locate the {{meas_astrom}} (effectively asking EUPS for the location of the test file), then a path to the astrometry.net test data within the {{tests/}} directory is located and then EUPS is asked to setup {{astrometry_net_data}} using that path. Since the table files are all empty this is the equivalent to simply assigning the {{ASTROMETRY_NET_DATA_DIR}} environment variable directly to the path in the tests sub-directory.    Making this change to one of the tests seems to work so I will change the rest."
3,W16 Qserv Release and Testing,"This epic captures stories related to building, testing and maintaining Qserv releases, along with related documentation. Note that testing involve running larger scale tests more or less monthly to ensure we haven't broken anything. 3 SPs per month."
3,Data Distribution + Qserv,NULL
0,Improve name and default value of MeasureApCorrConfig.refFluxAlg,The config name refFluxAlg should be refFluxField (since it is a flux field name prefix) and the default should be  base_CircularApertureFlux_5 instead of base_CircularApertureFlux_0 (thus giving a reasonable radius instead of one that is ridiculously too small).    I should have handled it on DM-436 but it slipped through.
2,Implement MySQL-based KVInterface,"This story covers adding mysql-based implementation of KVInterface. The implementation will be done in C++, and it will be exposed to the python layer."
1,Extend KVInterface - add support for updates,The CSS Facade and KVInterface currently do not support updates. This story covers adding support for basic updates.
2,CSS/QMeta interaction in czar,"CSS currently does not have any notion of locks. The snapshots of CSS should be taken per query, for each query and they should be done in coordination with Query Metadata. This will ensure tables used by a running query never gets altered or deleted while the query is running."
2,S17 Fine-tune Data Access Interfaces,NULL
1,Install the LSST Stack on loaned laptop ,NULL
2,Qserv Release and Testing,"This epic captures stories related to building, testing and maintaining Qserv releases, along with related documentation. Note that testing involve running larger scale tests more or less monthly to ensure we haven't broken anything. 3 SPs per month."
3,X16 Qserv Refactoring,NULL
2,Accessing the current obs_decam package ,Installing the non-official obs_decam package from Simon Krughoff's Github   and processing some DECam data blindly     
0,In CalibrateTask if one disables psf determination then aperture correction will fail,"In pipe_tasks CalibrateTask, by default aperture correction uses source flag ""calib_psfUsed"" to decide if a source is acceptable to use for measuring aperture correction. If PSF determination is disabled then this flag is never set and aperture correction will fail with a complaint that there are 0 sources.  "
0,"CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the wrong schema","CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the initial schema ""schema1"" instead of the final schema. Normally this would not matter since most of the fields are shared, but aperture correction wants aperture flux at a larger radius than the narrowest option, and schema1 may only provide the narrowest option.    In any case it is safer to instantiate those three subtasks using the final schema, since they are only ever run on the final schema. (Several other subtasks are run on both the initial and final schema, and should continue to be instantiated using schema1)."
0,Build 2015_08 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Build and Test 2015_09 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Build and Test 2015_10 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Build and Test 2015_11 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Build and Test 2015_12 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Build and Test 2016_01 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
0,Build and Test 2016_02 Qserv Release,See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
1,Aperture correction not applied for some measurements,"Aperture correction needs to be applied every time a measurement is run after it is first measured in CalibrateTask. As of DM-436 aperture correction is only being applied in CalibrateTask, which for example means the information is overwritten during the final measurement of ProcessImageTask.run.    This is probably best done by adding code to apply aperture correction to BaseMeasurementTask, so it is inherited by SingleFrameMeasurementTask and ForcedMeasurementTask."
0,CalibrateTask instantiates some subtasks with the wrong schema,"CalibrateTask instantiates some subtasks with the wrong schema, in particular:  - astrometry is instantiated with the final schema but run on schema1  - measureApCorr, applyApCorr and photocal are instantiated with schema1 but run on the final schema    One way this can cause problems is that schema1 may not have the data needed to measure aperture correction (e.g. it may contain only one tiny radius of aperture flux), as came to light when running the lsst stack demo."
2,Investigate workflow for OpenStack via Python scripts,"We investigate the use of Python scripts that work against OpenStack APIs to start up VMs and configure them for use, for example, in processing, build & test scenarios, etc.   We are initially working against  the ISL OpenStack, and intend to test against the ""nebula"" system when it becomes available.  (This type if work was initiated in issues DM-1787, DM-1788 in a previous Epic, and we continue within the context of DM-1273, )    "
1,Add PT.12 Filter/Science_Ccd_Exposure tables to extend test query coverage,"Filter table is missing from case02, case05 data, so next query can't be tested:  {code:sql}  -- datasets/case02/queries/3023_joinObjectSourceFilter.sql.FIXME  --- Join on Source and Filter and select specific filter in region  --- https://dev.lsstcorp.org/trac/wiki/db/Qserv/IN2P3/BenchmarkMarch2013  --- https://dev.lsstcorp.org/trac/wiki/db/queries/007  -  SELECT objectId, taiMidPoint, fluxToAbMag(psfMag)  FROM   Source  JOIN   Object USING(objectId)  JOIN   Filter USING(filterId)  -WHERE   ra_PS BETWEEN 1 AND 2 -- noQserv  AND decl_PS BETWEEN 3 AND 4 -- noQserv  --- withQserv  qserv_areaspec_box(1,3,2,4)  AND  filterName = 'u'  AND  variability BETWEEN 0 AND 2    {code}    Same thing for case02:1011_objectsForExposure and case02:1030_timeSeries.sql"
1,Fix UDF for case01 query: 3005_orderByRA.sql,"query    {code:bash}  mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv -e ""SELECT * FROM Object WHERE qserv_areaspec_box(0.,1.,0.,1.)""  {code}  returns nothing whereas   {code}  SELECT *   FROM Object  WHERE ra_PS BETWEEN 0. AND 1.   -- noQserv  AND decl_PS BETWEEN 0. AND 1.  {code} does (but doesn't use geom index)"
1,"Remove _chunkId, _subChunkId column from case02:Object table","This columns are Qserv internal and shouldn't be in input data. For example, this prevents case02:3021_selectObjectSortedByRA to work.    Check also that these columns aren't in other test data set and remove FIXME suffix from related broken query."
0,Document deprecation of DecoratedImage,"According to discussion on Hipchat (20 July 2015)    {quote}  Jim Bosch: [...] DecoratedImage is strongly deprecated, though  {quote}    This was news to me, and certainly isn't reflected [in (at least the obvious place) in Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/afw_sec_image.html]. It should be."
1,Re-implement watcher based on new CSS implementation,"Current watcher implementation (in {{admin/bin/watcher.py}}) is based on direct watching of zookeeper updates via kazoo. If we are to re-implement CSS based on mysql then watcher needs to be updated to support it. Mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed."
1,Audit existing test and development system,Document in the wiki how the test and development systems are connected and configured.
0,Fix cluster install procedure and improve docker support,Document how-to update cluster from Qserv release:    See  http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/cluster-deployment.html
0,makeWcs() chokes on decam images in 10.1,"In 10.0, processCcdDecam.py could process decam images to completion (whether the WCS was read correctly is a different question). Now it fails on makeWcs() (see traceback below), and I suspect this change in behavior is related to DM-2883 and DM-2967.    Repository with both data and code to reproduce:  http://www.astro.washington.edu/users/yusra/reproduce/reproduceMakeWcsErr.tar.gz  (apologies for the size)    The attachment is a document describing the WCS representation in the images from the community pipeline, courtesy of Francisco Forster.    Please advise. This ticket captures any changes made to afw.     {code}  D-108-179-166-118:decam yusra$ processCcdDecam.py newTestRepo/ --id visit=0232847 ccdnum=10 --config calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment"" doWriteCalibrateMatches=False --clobber-config  : Loading config overrride file '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/processCcdDecam.py'  : Config override file does not exist: '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/decam/processCcdDecam.py'  : input=/Users/yusra/decam/newTestRepo  : calib=None  : output=None  CameraMapper: Loading registry registry from /Users/yusra/decam/newTestRepo/registry.sqlite3  processCcdDecam: Processing {'visit': 232847, 'ccdnum': 10}  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV/DEC--TPV  processCcdDecam FATAL: Failed on dataId={'visit': 232847, 'ccdnum': 10}:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'    Traceback (most recent call last):    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/processCcdDecam.py"", line 77, in run      mi = exp.getMaskedImage()    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__      subject = oga(self, '__subject__')    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__      set_cache(self, get_callback(self)())    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 236, in <lambda>      location, dataId)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/decamMapper.py"", line 118, in bypass_instcal      wcs         = afwImage.makeWcs(md)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/afw/10.1-26-g9124caf+1/python/lsst/afw/image/imageLib.py"", line 8706, in makeWcs      return _imageLib.makeWcs(*args)  RuntimeError:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'  {code}    "
1,Standardize Qserv install procedure: step 1 build docker container for master/worker instance and development version ,"- shmux could be used for parallel ssh (remove Qserv builtin one)  - look at ""serf and consul"" (See Confluence pages)  - improve doc: http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/index.html  - run multiple instances/versions of Qserv using different run dir/ports and the same data"
1,Include reference magnitude errors in PhotoCal,"PhotoCal task currently ignores the uncertainties on reference sources, which can lead to problems when the reference and measured catalogs have relatively little overlap or otherwise disagree on how trustworthy a source is."
1,W16 Data Access and Db Release Documentation,Write Release documentation covering Data Access and Database work.
1,Revisit cost of replicating non-partitioned tables on all nodes,"Revisit size of all non-partitioned tables, and cost of replicating them on all worker nodes."
1,Estimate I/O load for non-partitioned tables,"Estimate realistic IO load from user queries on non-partitioned tables. Consider whether there might be hot spots (eg., maybe a small subset of columns from exposure is used very often. If it is it, maybe it'd be worth replicating only these columns across all worker nodes and serve the rest from one shared file system)."
2,Experiment with CONNECT engine for non-partitioned tables,"Idea: store non-partitioned tables in a dedicated mysql server, and bring them to the worker nodes using connect engine.    This story involves exploring if that would work, and uncovering potential pitfalls."
0,Add debugging for astrometry.net solver,"To be able to debug astrometric matching, it helps to be able to visualise the source positions, the distorted source positions, and the reference positions.  This is a pull request to add these."
3,Query Coverage,NULL
2,Get ImageDifferenceTask running again,ImageDifferenceTask doesn't run. The issues I've seen so far are related to the measurement overhaul. This ticket will capture the one-off updates needed to get this running again.     Appropriate Bugs and Papercuts epic?
0,ChebyshevBoundedField should use _ not . as field separators for persistence,"ChebyshevBoundedField uses ""."" instead of ""\_"" as field separators in its afw table persistence. This is the old way of doing things, and unfortunately causes errors when reading in older versions of tables, becaus afw converts ""."" to ""_"" in that situation.    This shows up as a unit test failure in DM-2981 (brought over from HSC) when an older version table is read in.    It is an open question whether to fix this as part of DM-2981 (which conveniently has a test that shows the problem, though not intentionally so) or separately, in which case a new test is wanted. In the former case I'm happy to do the work so I can finish DM-2981.    Many thanks to Jim Bosch for diagnosing the problem."
3,Begin drafting specification document for the Level 1 System,Don Petravick (.5 FTE) + Jason Alt (.8 FTE) + Paul Wefel (.125 FTE)  July 2015 - August 2015
3,Initial work to process DECam data with LSST stack,Hsin-Fang Chiang (1 FTE)  July 2015 - August 2015
1,Extrapolate to the current document ,"Discussed use  cases in the context of the TOWG, and also with the site and telescope group.  A picture of the  structure of IT operations has (I believe consensus emerged) that is the  four layers ITIL cake    Service Design (cataloged, budget, availability, etc).  Service Transition (The work of inserting Charge into the system).  Service Delivery  (the work of running the as-is system of servinces)  ITC -- The work of providing the Facility, Hardware and networking.     I also obtained the ability to interact with EA. (but still working to master it and its concepts)"
0,unable to create public images,Errors are returned when attempting to upload an image marked as public.
0,Improve czar-worker communication debugging,"Add features to make it easier to debug communication problems. Particularly, record the source of a message, and remove extraneous messages."
1,Document setting up multi-node Qserv and running integration test,NULL
0,openstack API endpoint is broken,"Similar to what was observed in DM-3226, the referral endspoint returned by   {code:java}  https://nebulous.ncsa.illinois.edu:5000  {code}  are not FQDNs.  This fundamentally breaks any attempt to use the API one step past authenticating with keystone.    This is an example HTTP response:    {code:java}  HTTP/1.1 200 OK  Date: Mon, 27 Jul 2015 23:11:02 GMT  Server: Apache/2.4.10 (Ubuntu)  Vary: X-Auth-Token  X-Distribution: Ubuntu  x-openstack-request-id: req-ac7bb613-86ef-43ab-a663-75c2ed3fb124  Content-Length: 1656  Content-Type: application/json    {""access"": {""token"": {""issued_at"": ""2015-07-27T23:11:02.342216"", ""expires"": ""2015-07-28T00:11:02Z"", ""id"": ""99b843d4baf94569a0d34ca4fecb470c"", ""tenant"": {""description"": null, ""enabled"": true, ""id"": ""d1f16653856540d386224fb057b5b00c"", ""name"": ""LSST""}, ""audit_ids"": [""fAP8851vTQi1n5pYmNoIjw""]}, ""serviceCatalog"": [{""endpoints"": [{""adminURL"": ""http://nebula:9292"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9292"", ""id"": ""49365a8e8fe743af9d517e84a98e3ee9"", ""publicURL"": ""http://nebula:9292""}], ""endpoints_links"": [], ""type"": ""image"", ""name"": ""glance""}, {""endpoints"": [{""adminURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""id"": ""c1e31df3656042ef9c5502efd7d574f2"", ""publicURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c""}], ""endpoints_links"": [], ""type"": ""compute"", ""name"": ""nova""}, {""endpoints"": [{""adminURL"": ""http://nebula:9696"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9696"", ""id"": ""266c9dc8e0344f8fa3f078652e868443"", ""publicURL"": ""http://nebula:9696""}], ""endpoints_links"": [], ""type"": ""network"", ""name"": ""neutron""}, {""endpoints"": [{""adminURL"": ""http://nebula:35357/v2.0"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:5000/v2.0"", ""id"": ""5d474008bcee4f44800546e3f3302404"", ""publicURL"": ""http://nebula:5000/v2.0""}], ""endpoints_links"": [], ""type"": ""identity"", ""name"": ""keystone""}], ""user"": {""username"": ""jhoblitt"", ""roles_links"": [], ""id"": ""6ea0c8e153b04ae29572c5fd877b6ac3"", ""roles"": [{""name"": ""user""}], ""name"": ""jhoblitt""}, ""metadata"": {""is_admin"": 0, ""roles"": [""142761bd922e453294e9b7086a227cbc""]}}}    {code}  "
0,evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 1,See also https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LDMDG&title=NCSA+Nebula+OpenStack+Issues
2,"Refinement, restatement of DM facilites and functions ","Began detailed refinement.  The initial version was a word document with omnigraffle figures.  Began moving this to LSST Confluence to make the model more accessible for scrutiny.    The top level  functional  - physical  breakdown diagrams now exist and transferring the functional breakdown from word to confluence is in place.   I have request that the simple citation package be installed in  the LSST lira (not perfect, but helps).   "
0,Potential talk for All-Hands,NULL
0,Potential talk for nsf cyber summit,NULL
0,Collab. with Ron Lambert and Oliver W.,NULL
0,Fix problems with no-result queries on multi-node setup,"For queries like:        select * from Object where id = <non existent id>    qserv can't map it to any chunk, and it ends up executing      SELECT *     FROM qservTest_case01_qserv.Object_1234567890 AS QST_1_     WHERE objectId=<non existent id>    the chunk 1234567890 is a special chunk and it exists on all nodes.    And that fails with:    (build/qdisp/QueryResource.cc:61) - Error provisioning, msg=Unable to  write  file; multiple files exist. code=2 "
2,Build a demo system for camera team to use the Firefly external task launcher,"We have the code to launch ab external task from Firefly server. ( see DM-2991)  IN order to facilitate the development by the UIUC group for Camera team, we need to have a simple example to show how to connect the front extension to the external task at the server side.  "
2,Support to the camera team development,We need someone to attend the weekly meeting at UIUC  group for camera team. To discover issues and answer questions. This is an on-going effort.   
3,Create images for the mask bits at server side,"LSST FITS images will have a extension that indicate the mask bits. In order to overlay the masks on the primary image, we need to turn the mask bits into a set of images. This task is to take the requested bits and FITS as input, output a set of images for each requested bit. Each bit will have different color. "
2,a simple demo version to use Firefly in iPython notebook,We want to build a simple demo version of Firefly  that works in the iPython notebook.  It will make it easier for users to try out Firefly visualization capabilities  with Python APIs. 
0,Include polygon bounds in CoaddPsf logic,This is a port of [HSC-974|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-974]. Original description:    The {{CoaddPsf}} class should use the polygon bounding areas that were added to {{Exposure}} and {{ExposureRecord}} in DM-2981 (was: HSC-973) when determining which PSF images to coadd.
1,Add support for passing query classification info from user to czar,We need to be able to pass information from user about query type (sync/async). This will require tweaking the parser.  
2,Add support for async query results,"Modify Qserv to support async queries: send query results to the right place instead of to mysql proxy. In this story, we can simply use some reasonable default location and send the results there. Later on we will extend qserv to make it configurable."
1,Add support for configuring async queries,"Extend Qserv configuration to allow a DBA to specify (a) where results from async queries should be stored and (b) what rules to apply when purging old results.    Note that we need to think about the purging rules, it is not immediately obvious what would make most sense."
1,Revisit and document user-facing aspects of async queries,"Outline all aspects of async queries that are affecting users, discuss with the DM team, and document. This includes things like:   * managing async queries (checking status, terminating)   * retrieving results from async queries   * managing query results (purging policies etc)   * probably more, need to think about it..."
1,Unify KVInterface python and c++ interfaces,Swig the C++ mysql-based KvInterface implementation.   
3,FY18 Qserv Health Verification Tool,"Need a tool for verifying if all the services are up and running, including things like whether udfs are loaded"
2,Prepare and implement RFP for DWDM devices,Prepare and implement RFP
1,Fiber path Gate to pachon,Discussion on fiber path
0,Port flux.scaled from HSC,"[HSC-1295|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1295] introduces {{flux.scaled}}, which measures the flux within a circular aperture that is set from the size of the PSF, scaled by some factor.  Stephen Gwyn recommends using this as our fiducial calibration flux."
0,CoaddPsf.getAveragePosition() is not a valid position,This is a port of [HSC-1138|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1138] to LSST. That is an aggregate of two related minor fixes:    * {{CoaddInputRecorder}} should default to {{saveVisitGoodPix=True}} so that average positions in the {{CoaddPsf}} can be properly weighted;  * {{computeAveragePosition}} and {{doComputeKernelImage}} should be consistent about the data included when determining whether a source is off image.
0,Define polygon bounds for CCDs based on vignetted regions,"This is a port of [HSC-976|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-976] to LSST. The original issue description was:    We should set the polygon bounds (added in DM-2981 [was HSC-973]) for HSC CCD exposures to cover the non-vignetted regions. This should probably be done in ISR or some other camera-specific location.    Note that, contrary to the description in DM-2981, this functionality was not included there."
2,Fix problems in xrootd discovered in multi-node qserv tests,NULL
1,Explore how to run multi-node tests,Not testing qserv code often enough in multi-node environment led to introducing many problems over the past two years since we last run large scale test. It should be simple for developer to run a multi-node test. This story covers work related to understanding how to run integration test on multi-node.
3,W16 Make Query Cancellation Robust,"It is clear that the code responsible for query cancellation needs some more thoughts and refactoring (it was prematurely rushed when Daniel was about to leave slac). In particular, the code seems to have some subtle problems. We need to debug these problems and solve them."
0,Audit & cherry-pick HSC-1126 fixes,"[HSC-1126|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1126] contains a number of unrelated bug fixes. Given the nature of that ticket, it's not immediately clear which might already have been ported to LSST, which don't apply, and, of the others, what dependencies they have on code which might still be in the queue for merger.    We need to dig through that ticket and ensure that everything is properly merged."
0,add task to meas_astrom to fit an aribtrary WCS with a TAN-SIP WCS,"Sometime in the past, Russell Owen wrote a method to take an arbitrary WCS and approximate it as a TanWcs (our implementation of FITS' TAN-SIP WCS formalism).  That method is currently just a utility function in the unit test testFitTanSipWcsHighOrder.py.  This issue will promote that method to a full-fledge task in meas_astrom."
3,Reconfigure Openstack systems,1) Configured the IPMI on 13 systems.
0,Administrative - 7-2015,NULL
0,Networking support of Openstack efforts,NULL
0,assertWcsNearlyEqualOverBBox and friends is too hard to use as a free function,"assertWcsNearlyEqualOverBBox and similar functions elsewhere in afw were written to be methods of lsst.utils.tests.TestCase, so their first argument is a testCase. This is fine for use in unit tests, but a hassle to use as free functions because the user must provide a testCase argument (though it need only be a trivial class with a fail(self, msgStr) method). Worse, that minimal requirement is not documented, so technically providing a simple mock test case is unsafe.    I have two proposals:  - Document the fact that testCase need only support fail(self, msgStr). This makes it clear how to safely use these functions as free functions.  - Allow testCase to be None, in which case RuntimeError is raised. That makes these functions even easier to use as free functions.  "
1,Include translation of aliases in measurement calibration,"Tasks that calibrate measurement outputs should include transferring (and translating, as needed) the aliases in the original measurement catalog.  This should include transferring slots, which may involve ""renaming"" the original slots, as those names refer explicitly to raw measurements (i.e. ""slot_PsfFlux"" might become ""slot_PsfMag"")."
0,Add test case for ExposureRecord::contains,In DM-3243 we ported from HSC the ability to take account of the associated {{validPolygon}} when checking whether a point falls within an {{Exposure}}. This functionality was not accompanied by an adequate unit test.
0,Reproducing errors of the current obs_decam package,"Learning the stack and development worlflow by reproducing errors in obs_decam as DM-3196.  Changes from the stack need to be incorporated into obs_decam package to keep the package up to date, hence the errors. As a learning process I reproduced the errors and used her fix (afw branch u/yusra/DM-3196) to move on.  "
1,Study basic afw ,"Learn the basic operation of the aft package about handling images, tables, etc.  "
2,Management until end july 215,"Finsh system development lead and on Onboard Jason Alt.  Deal with management of group providing NCSA openstack.  Review and re-review procurement contract prose and directions.  Coordinate  TOWG input with NCSA management.    Gain acume about exiting design documents, decide and prototype details refinement/ analysis /restatement needed to manage project at NCSA.  Management of people."
0,Read in the FITS cube that Herschel project produced,IRSA needs to be able to read in the FITS cube generated by Herschel project. We need to support and guide the effort so the code is generic enough for non-Herschel data. 
0,Support the FITS cube reader,RSA needs to be able to read in the FITS cube generated by Herschel project. We need to guide the effort so the code is generic enough for non-Herschel data.
0,Fix Firefly build script so it'll work with latest version of gradle,Firefly build was failing when using gradle version 2.5.  Minor changes to the dependencies declaration fixed it.
3,Margaret's mgmt. activities in July,NULL
1,Add mysql-based test to multi-node integration test,"At the moment multi-node integration test runs only on multi-node using Qserv, it does not run on plain mysql, and thus we can't validate results. The story involves tweaking qserv_testdata such that we can run mysql test on the czar, and compare results from mysql and qserv."
0,Investigate jenkins creating a container per job,NULL
1,Run large scale tests,NULL
1,Debug problem with joins in multi-node tests,"We seen to have problems with joins:  {quote}  SELECT o.deepSourceId, s.objectId, s.id, o.ra, o.decl FROM Object o, Source s WHERE o.deepSourceId=s.objectId;  {quote}    cluster seems to have hung. I can send new queries to the czar, and they show up in the czar's log, but they don't get answered (they can be cancelled).  Cancelling the join works(at least for the czar) but no further queries work."
1,Analyze qserv performance / KPIs,NULL
1,Explore Qserv authentication and authorization,NULL
1,Produce Data Access & DB team S15 Release docs,Complete these documents:  * https://confluence.lsstcorp.org/display/DM/Summer+2015+Qserv+Release  * https://confluence.lsstcorp.org/display/DM/Summer+2015+WebServ+Release
0,Add multi-process python runner script for Galaxy Shear Experiments,"The current runner scripts are in tcsh and bash.  There is no good excuse for this, except that it was easy to implement.  Since we need both multi-threading and better parameter parsing, this will be replaced with a python script."
1,Port HSC MPI driver for single-visit processing,"Transfer the {{reduceFrames.py}} script and the {{ProcessExposureTask}} it utilizes from hscPipe to a new package in the LSST stack (RFC-68 proposes calling this new package {{pool_tasks}}, but this isn't set in stone).    We should probably rename either the driver script or the task (or both), so they agree; the lack of consistency is a historical artifact on the HSC side, and I think it's time to change that."
1,Port HSC MPI driver for coaddition,"Port the HSC driver for coaddition, {{stack.py}} from hscPipe to a new LSST package (the same as DM-3368).    In the process, we should remove the inclusion of {{ProcessCoaddTask}}, and instead run detection and background subtraction only.    I think it might be time to consider renaming this task as well; I find it a little unfortunate we use ""coadd"" everywhere else but ""stack"" here."
1,Port HSC MPI driver for multi-band coadd processing,"Port the HSC MPI driver of multi-band coadd processing, multiBand.py, from hscPipe to a new LSST package (the same as in DM-3368)."
1,Port HSC --rerun option for CmdLineTask,"Port the HSC side's {{--rerun}} option for specifying processing inputs and outputs.    This work should be preceded by an RFC; we've proposed implementing this option on the LSST side in the past, and it was met with some resistance as it isn't strictly necessary.  We've since found it extremely convenience on the HSC side, and I think it's very much worth porting."
1,"Port, replace, or defer HSC-side provenance of EUPS products","The HSC pipeline checks that setup EUPS products are identical between runs with the same output directory, in the same way configuration is checked in both the LSST and HSC pipelines.    The implementation is a bit messy, and it's not strictly necessary, so it's not clear we should port this over as-is, or just wait for a better implementation to be provided by the Process Middleware team.  We should at least RFC this question now."
1,Port HSC code for generation of calibration products,"Port HSC code for building calibration products (flats, bias frames, etc.)."
1,add realistic Footprints to measurement code,The current measurement code for the galaxy shear simulations uses the full postage stamp bounding box for the Footprint.  We need to use more realistic Footprints for some of the tests we want to run.  That probably involves running {{SourceDetectionTask}} and somehow combining that with the input-catalog based iterating already in the measurement code.
1,Test shear bias vs. CModel region.nGrowFootprint,"One piece of how the CModel code chooses its fit region size is via nGrowFootprint, which is used to grow the original detection Footprint.  We should test how changing this parameter affects the _m_ and _c_ shear biases between input and recovered shear.  They should decrease for larger nGrowFootprint values, and eventually plateau.  We want to find the point where this happens, and see how the parameter affects both the fit region area and the shear biases before this threshold.    It may be necessary to make a small modification to the CModel code to output the fit region area to complete this test."
1,Test shear bias vs. CModel region.nInitialRadii,"Like DM-3375, but testing the region.nInitialRadii parameter instead.  This parameter sets the fit region using a multiple of the half-light ellipse from an initial approximate fit.  The full fit region is formed as the union of this with the grown detection Footprint, so it may be necessary to set nGrowFootprint to a negative number to see any affect from this parameter."
3,Initial issue investigation for the nebula openstack,"    The nebula openstack system at NSCA first became available ~Fri Jul 24 and  the week of Jul 27 -- 31 was spent testing and debugging issues that the                 LSST team identified within, for example, DM-3225, DM-3219, DM-3227 and others.  "
1,NSF Cyber Summit talk,"NSF Cyber Security Summit talk:  a case study of LSST cyber security.  Talk goes over challenges and successes with LSST's security program.  Talk is divided into four sections:  security plan, data security, user access, and security for the observation site."
1,Port HSC hooks for simulated source injection,Port HSC hooks injecting simulated sources into real images to test processing.    This includes the code in {{fakes.py}} in pipe_tasks and its callers.  The pipeline does not include code for actually adding the fake sources; it just provides a callback interface that is implemented by third-party plugins such as https://github.com/clackner2007/fake-sources.
0,Add test cases for thresholding,"In DM-3136 changes were made to the way thresholds are handled in detection ([{{a4b011d}}|https://github.com/lsst/meas_algorithms/commit/a4b011dd0775908c925ad9f40f802f9ed8723ef9] and [{{74c2ed0}}|https://github.com/lsst/meas_algorithms/commit/74c2ed0b79afce4c94b0db5f1e168c28ba1aa15b]). These were not accompanied by test cases, but they should be."
0,security playbook,Practical document for handling and responding to incidents.
0,Meeting with on HTCondor,Attended meeting with Miron Livny.
1,Port HSC improvements to HSM moments code,"The HSM shear estimation has received several improvements and important bugfixes on the HSC side that need to be ported to LSST.  This is complicated by the fact that much of the code has been entirely rewritten on the LSST side to work within the new measurement framework, but we've also synchronized this package with the HSC side much more frequently than with other packages."
0,Administrative - 8-2015,NULL
0,Parallelism Framework migration,NULL
0,Make use of good pixel count when building CoaddPsfs,"When building a CoaddPsf we have the ability to take account of the number of pixels contributed by the inputs (see http://ls.st/paj and DM-3258). However, the {{CoaddPsf}} constructor fails to use this information. It should copy this field when copying the provided {{ExposureCatalog}}, so that {{computeAveragePosition}} can use it."
0,Edit end-to-end test plan to reflect current DM plans,NULL
1,Re-generate data for large scale tests at in2p3,"Sources were incorrectly duplicated, need to be redone"
0,Refactor Zscale.java class ,"In early this year, the decision all data types would be converted to float in FitsRead.  Thus,the bitpixel is not relevant.  In Zscale, it still uses bitpixel to test the data type.  It should be refactored in the same manner as FitsRead etc. "
1,Fix precision related problem in UDFs,"SciSQL udfs seem to have a subtle precision problem. The following query that is not relying on scisql returns one row:    {quote}  select ra, decl, deepSourceId   FROM Object o   WHERE decl between 0.992 and 0.993 and ra between 19.171 and 19.172;  +------------------+-------------------+------------------+  | ra               | decl              | deepSourceId     |  +------------------+-------------------+------------------+  | 19.1719166801441 | 0.992087616433663 | 4368217963236477 |  +------------------+-------------------+------------------+  1 row in set (2 min 9.49 sec)  {quote}    But equivalent scisql-based query:    {quote}  select ra, decl, deepSourceId   FROM Object o   WHERE qserv_areaspec_box(0.992, 19.171, 0.993, 19.172);  {quote}    will fail to find that row.    If we relax the search criteria just a little bit, it finds some other row, but still not the one with decl = 0.992087616433663    {quote}  select ra, decl, deepSourceId FROM Object o WHERE qserv_areaspec_box(0.99, 19.171, 0.999, 19.172);  +-------------------+-----------------+------------------+  | ra                | decl            | deepSourceId     |  +-------------------+-----------------+------------------+  | 0.994098536926311 | 19.171425377618 | 4372684729224984 |  +-------------------+-----------------+------------------+  {quote}  "
1,Fix column names in query result,"The following shows the problem (See the column names in the results, they are not what user will expect). It happens for all aggregates: min, max, avg, count etc    {code}  select min(ra_PS), min(decl_PS), max(ra_PS), max(decl_PS), avg(ra_PS) from Object;  +----------------+---------------+---------------+---------------+-------------------------------+  | MIN(QS1_MIN)   | MIN(QS2_MIN)  | MAX(QS3_MAX)  | MAX(QS4_MAX)  | (SUM(QS6_SUM)/SUM(QS5_COUNT)) |  +-----------------+--------------+---------------+---------------+-------------------------------+  | 0.041714119635 | -6.1011707745 | 359.938579891 | 3.89870649736 |                 112.537203939 |  +----------------+---------------+---------------+---------------+-------------------------------+  {code}"
2,Cost Model Discovery,Learn about the cost model (LDM-144) and related documents in preparation for updating it per the contract
1,Discourse evaluation (Part 2),Work in support of evaluation Discourse as a DM platform for internal and external interactions.
0,Find and evaluate multi-user password wallet for SQuaRE,Work to find and evaluate an off-the-shelf solution for sharing web services passwords between the SQuaRE group.
0,Fix problem with default_engine,"Fix the problem:    {quote}  08/04/2015 05:39:47 werkzeug INFO: 141.142.237.30 - - [04/Aug/2015 17:39:47] ""GET /meta/v0/ HTTP/1.1"" 200 -  08/04/2015 05:39:49 __main__ ERROR: Exception on /meta/v0/db [GET]  Traceback (most recent call last):    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app      response = self.full_dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request      rv = self.dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 59, in getDb      return _resultsOf(text(query), scalar=True)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 122, in _resultsOf      engine = current_app.config[""default_engine""]  KeyError: 'default_engine'    {quote}"
1,Research and Documenting the L1 System,NULL
0,Eliminate circular aliases in slot centroid definition,"[~smonkewitz] has discovered that our schema aliases for even the default configuration of measurement algorithms involve cycles, because the slot centroid algorithm contains a reference to its own flag.  Fixing this should just involve an extra check in {{SafeCentroidExtractor}}."
1,Explicitly disallow alias cycles in Schemas,"The current guard against cycles is lazy and incomplete, as it seemed unlikely we'd ever have them.  That's already been disproven (DM-3400), so it seems prudent to fix the guard code now."
0,Port HSC updates to ingestImages.py,ingestImages.py provides a camera-agnostic manner of creating a data repository (including a registry).  The HSC fork contains multiple improvements not present on the LSST side.  We need these in order to ingest the HSC data.
1,organize workspace functions discussion ,NULL
1,workspace functions discussion,NULL
3,workspace functions specification document,The first version of the document is here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41783931
0,Review and edit import of Level 2 ICD milestones into DLP,NULL
0,Clarify status of LSE-77,"Work on LSE-77 _per se_ appears not to have kept up with the status of the substance of the interface requirements, as represented in, for instance, LSE-239.  The action here is to see what change request actions may be appropriate for LSE-77 at this point."
1,Review all DM ICDs for open issues and work with partner subsystems to clarify schedules for completion,NULL
0, workspace functions discussion,NULL
1,Fix overestimation of aperture correction error,"We're overestimating the aperture correction errors by including photon noise twice: both in the aperture correction errors and the original measurement errors.    This is a migration of [HSC-1277|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1277] to LSST, which is where it should be fixed."
1,Evaluate changes to LSE-209 and LSE-70,Action item from    https://confluence.lsstcorp.org/display/SYSENG/2015+July+08-10+CCS-DAQ-OCS-DM+Workshop+IV    Send initial feedback on LSE-70 and LSE-209
0,obs_decam unit test for reading data ,The unit test wasn't working before and I edited the unit test of reading raw data. This got included with DM-3462.   This unit test needs testdata_decam to be setup.      The test fails with the stack b1597 at makeWcs (DM-3196).   The afw branch u/yusra/DM-3196 is a temporary fix before DM-3196 is resolved.      
0,Create testdata_decam ,Create a new testdata_decam repo with public instrument calibrated data.        The 435MB file can be downloaded from http://uofi.box.com/testdata-decam
2,S15 Qserv Release and Testing,"This epic captures stories related to building, testing and maintaining Qserv releases, along with related documentation."
3," CI, Deploy and Distribution Improvements Part II","[Retitled epic to better capture current plan]    This epic is an umbrella for Jenkins improvements, OpenStack and AWS  automatic deployment, binary distribution, developer requests.    Docker items in particular should go to a different epic. DM-2053    [JH 100%]     "
0,DLP/LDM-240 support chages - Part II,"Service feature requests and bugfixes from JK, T/CAMs including format changes on sqre-jirakit."
2,Meetings - Aug 2015 ,8/3: local group meeting  8/3: UIUC postdoc orientation  8/5: LSST2015 NCSA coordination meeting  8/10: local group meeting  8/17-22: LSST2015 Bremerton All Hands Meeting  8/24: local group meeting  8/28: Astro postdoc meeting
0,Set up new desktop and install the stack,#NAME?
1,Learn the development workflow and obs_decam status update ,#NAME?
1,Debug problem with timeout,"If I run 4 simultaneous large queries: 3 object scans and 1 source scan, Xrootd silently died on 2 machines. Below I pasted the tail of the log files      ccqserv108    0808 00:22:33.824 [0x7fb739583700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:33.824 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (256, (more))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (62, (last))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:34.468 [0x7fb739583700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=8  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0    -----    ccqserv124    0808 00:22:25.329 [0x7f25a67bf700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:234) - ChunkDisk registering for 2044 : SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_ p=0x7f257003d0b8  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=7 chunk=2044 db=LSST entry time=Fri Aug  7 23:52:06 2015   frag: q=SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_, sc= rt=r_7bff268d0e369dda8fa314132538a96ad_2044_0  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_b762c96f418726ae3457c74c0350d0c4  0808 00:22:25.329 [0x7f25a67bf700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (44, (last))  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=81  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0 "
1,Fix socket timeout problem in xrootd framework,"When we run a query that take long time, client times out, it closes the socket, which triggers cancellation on the server side."
3,W16 End-to-End Integration (Data Access portion),End to end system integration. This epic covers work related to  # loading data generated by pipelines into Qserv  # fully integrating Qserv with SUI
0,Qserv - webserv integration,"Setup Qserv and configure webserv to talk to Qserv. Verify all works, and fix discovered problems."
0,Add column names metadata to db query results,"Per discussion at data access meeting Aug 10, it'd be good to send column names with the query results."
0,Revisit KPIs for Qserv,Need to come up with KPIs for Qserv  
0,Rename ingest.py to reduce confusion with database,"The {{ingestImages.py}} bin script provides a camera-agnostic manner of creating a data repository (including a registry).  The back-end code resides in pipe_tasks under the name {{ingest.py}}, and the {{IngestTask._DefaultName = ""ingest""}}, which means that configuration files in obs packages are also named {{ingest.py}}.  This choice of name was unfortunate, as it may be confused with ingest of sources into the database.  We should change the name to reduce this confusion, perhaps {{ingestImages.py}} like the bin script."
0,"add meas_extensions_photometryKron to lsstsw, lsst_distrib","meas_extensions_photometryKron should be added to the CI system, since we are trying to keep it updated.    This is blocked by DM-2429 because that includes a fix for a unit test (which the CI system would have caught)."
0,Processing y-band HSC data fails in loading reference sources,"{code}  processCcd.py /lsst3/HSC/data/ --output /raid/price/test --id visit=904400 ccd=50  [...]  processCcd.calibrate.astrometry.solver.loadAN: Loading reference objects using center (1023.5, 2091) pix = Fk5Coord(319.8934727, -0.0006943, 2000.00) sky and radius 0.111920792477 deg  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}: Could not find flux field(s) y_camFlux, y_flux  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processImage.py"", line 160, in process      calib = self.calibrate.run(inputExposure, idFactory=idFactory)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/calibrate.py"", line 457, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 177, in run      results = self.astrometry(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 292, in astrometry      astrom = self.solver.determineWcs(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 409, in determineWcs      return self.determineWcs2(sourceCat=sourceCat, **margs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 437, in determineWcs2      astrom = self.useKnownWcs(sourceCat, wcs=wcs, **kw)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 308, in useKnownWcs      calib = None,    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 173, in loadPixelBox      loadRes = self.loadSkyCircle(ctrCoord, maxRadius, filterName)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/loadAstrometryNetObjects.py"", line 141, in loadSkyCircle      fluxField = getRefFluxField(schema=refCat.schema, filterName=filterName)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 40, in getRefFluxField      raise RuntimeError(""Could not find flux field(s) %s"" % ("", "".join(fluxFieldList)))  RuntimeError: Could not find flux field(s) y_camFlux, y_flux  {code}    We should be able to fix this by setting config parameters (e.g., {{calibrate.astrometry.solver.defaultFilter}} or {{calibrate.astrometry.solver.filterMap}}), but how do we keep that synched with the choice of reference catalog?  And once we get past astrometry, we also have the same problem in photocal."
1,Add support for parsing user log files,"In order to get timings for jobs executed, add support for non-dagman generated log files.  These are the user log files that HTCondor writes out for each individual job."
2,Improve packaging of shared libraries in scons,"As discovered through DM-3161, our swig-generated libraries are messy. Specifically, we are dumping everything we might need into the czarLib library. That includes mysql and mysql-client related things. CssLib needs mysql functions too. Given that czar imports both these libraries, we ended up with duplicate symbols. That is being patched in DM-3161, but it needs a further look / cleanup. We need to break things into smaller libraries. Difficulty: understanding dependencies and avoiding circular dependencies."
3,S17 Long-running Query Optimizations,"As discovered through DM-3432 when a query runs for long time, czar does not get response from worker for a long time, and times out. A quick patch we did during Summer 2015 tests was to increase the timeout to a large number.    The issue was discussed at [Qserv mtg Aug 12 2015|https://confluence.lsstcorp.org/display/DM/Database+Meeting+2015-08-12]. If we knew how long a query was going to take, we could set the timeout to appropriate value, but we can't always estimate well how long the query is going to take. The best solution we came up with: periodically check with the worker asynchronously what the status of the query is. Worker should respond with something like: queued, scheduled, working / started x sec ago. Note, this might require changes to the xrdssi API."
1,Handle problems with connecting to mysql in czar,"We observed in S15 tests that if we run too many queries, czar is running out of connections to mysql and as a result through exception that is uncaught (and dies). We triggered this by starting 110 queries. To ""fix"" this problem we increased the max_connections in etc/my.cnf from 256 to 512. So, most likely the easiest way to reproduce it would be to set the max_connections to a very small number.     This story involves handling the ""uncaught exception"" gracefully."
0,Tweaks to configurations discovered during S15 tests,"Apply tweaks we found useful when running large scale tests. This includes:  # etc/my.cnf: change max_connections to 512  # add"":  {quote}export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000{quote}  to init.d/qserv-czar  # add :  {quote}ulimit -c unlimited{quote}  to all startup scripts in init.d. This will make sure core file is always dumped when we have problems."
0,Resolve problem with running many simultaneous queries,"When we run with 110 simultaneous queries, czar fails with ""uncaught exception"""
1,Integrate pipelines with MySQL and Qserv,"Load data produced by pipelines into MySQL (on lsst10), and Qserv"
0,AstrometryTask.run return not consistent with ANetAstrometryTask,"ANetAstrometryTask.run returns matchMetadata but AstrometryTask.run returns matchMeta. The two must agree. It turns out that matchMeta is more widely used, so I'll standardize on that."
0,ProcessImageTask.matchSources fails if using ANetAstrometryTask,"ProcessImageTask.matchSources fails when using ANetAstrometryTask with the following error:  {code}  processCcd.calibrate.astrometry: Applying distortion correction  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'    Traceback (most recent call last):    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 219, in process      srcMatches, srcMatchMeta = self.matchSources(calExposure, sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 250, in matchSources      astromRet = astrometry.loadAndMatch(exposure=exposure, sourceCat=sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 321, in loadAndMatch      with self.distortionContext(sourceCat=sourceCat, exposure=exposure) as bbox:    File ""/ssd/rowen/lsstsw/anaconda/lib/python2.7/contextlib.py"", line 17, in __enter__      return self.gen.next()    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 295, in distortionContext      sourceCat.table.defineCentroid(self.distortedName)    File ""/ssd/rowen/lsstsw/stack/Linux64/afw/10.1-37-gaedf466/python/lsst/afw/table/tableLib.py"", line 8887, in defineCentroid      return _tableLib.SourceTable_defineCentroid(self, *args)  NotFoundError:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'  {code}  This is probably a result of DM-2939. The basic problem is that the distortion context in ANetAstrometryTask should not be run at that point in processing. [~price] suggests that a simple clean fix is to make the distortion context a no-op if the WCS already contains distortion, if that works. This is what I will try first."
0,Fix problems with talking from webserv to qserv,"Flask or sqlalchemy which are part of webserv are producing some extra queries that are confusing qserv. So basically, at the moment even the simplest query run via webserv that is directed to qserv fails."
1,Implement prototype stack documentation with Sphinx,Implement a minimally-viable Sphinx documentation repository for the LSST stack. The code is available at https://github.com/lsst-sqre/lsst_stack_docs
0,Research existing sphinx doc implementations,Examine how python packages such as astropy structure and implement their sphinx docs.
0,make forced and SFM interfaces more consistent,"From [~rowen]:  {quote}  SimpleMeasurementTask.run and ForcedMeasurementTask.run now both take a source catalog, but the two use the opposite order for the first two arguments (one has the catalog first, the other has the exposure first)  {quote}"
0,applyApCorr mis-handles missing data,"In ApplyApCorrTask.run the following lines do not behave as expected because get returns None if the data is missing, rather than raising an exception:  {code}              try:                  apCorrModel = apCorrMap.get(apCorrInfo.fluxName)                  apCorrSigmaModel = apCorrMap.get(apCorrInfo.fluxSigmaName)              except Exception:  {code}"
2,Make obs_decam handle raw data ,"The current obs_decam expects instrument calibrated data from the community pipeline, i.e. it  requires matching instcal (Instrument Calibrated), dqmask (the associated mask file), and wtmap (weight map) data from the same visit.  This issue is to add functionality so that raw DECam images can be ingested into registry and retrieved by the data butler.     Practically, this will create new or expand existing sub-classes of CameraMapper and IngestTask.        A brief summary of changes:  - The unit test getRaw.py is updated and should pass, with DM-3196     - Working testdata_decam for the unit test is currently at lsst-dev /lsst8/testdata_decam and https://uofi.box.com/testdata-decam  - DecamInstcalMapper is renamed to DecamMapper, to reflect that Butler can also get ""raw"" now besides ""instcal"". Please update _mapper in your data repositories.  - To create a registry for raw data, run  {code:java}     ingestImagesDecam.py /path/to/repo --mode=link --filetype=""raw"" /path/*.fits.fz  {code}  - The default filetype is ""instcal"" for ingestImagesDecam.py, so previous use for instcal stays.  "
0,psfex lapack symbols may collide with built in lapack,"On my Mac meas_extensions_psfex fails to build due to the numpy config test failing. ""import numpy"" fails with:  {code}  dlopen(/Users/rowen/LSST/lsstsw/anaconda/lib/python2.7/site-packages/numpy/linalg/lapack_lite.so, 2): can't resolve symbol __NSConcreteStackBlock in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib because dependent dylib #1 could not be loaded in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib  {code}  Our best guess (see discussion in Data Management 2015-08-14 at approx. 1:57 PM Pacific time) is that the special lapack functions in psfex are colliding with the lapack that anaconda uses.    In case it helps I see this on OS X 10.9.5. I do not see it on lsst-dev."
0,lsstswBuild.sh --print-fail should report config.log,NULL
1,Make async cancellation more flexible,NULL
0,drawing text to ds9 fails if size or the font family is set,"Commands like  {code}  ds9.dot('xxxx', 100, 100, size=3)  ds9.dot('xxxx', 100, 120, fontFamily=""times"")  {code}  silently fail.  The problem is that commands like  {code}  xpaset -p ds9 regions command '{text 100 100 # text=xxxx color=red font=""times 12""}'  {code}  fail; you need to say {{font=times 12 normal}}"
2,Prepare for S15 end-to-end system and Firefly relesase,"For S15, DM will try to have an end-to-end system in place, and work out all the issues during the process. SUI will try to a web application to  access some of dada access APIs (dax-) on a system at NCSA.  SUI will have a simple Firefly release to the community."
1,Install/deploy SUI web application at NCSA,"For summer 15 release,  we will deploya SUI web app on NCSA accessible to DM team.    - work with NCSA to have a server setup  - install necessary software packages  - install SUI software  - deploy the system and test "
2,A simple Firefly release for S15,"TO install and deploy a Firefly application from scratch will take a few hours, including getting all the necessary  software packages and install them all.  In order for users to get the system up and running in as little time as possible, we will provide a war file with embedded tomcat server. Users don;t need any third party software except Java1.8.  The time to get the system up and running after installing Java1.8 will be a minute after the download of war file."
2,Implement Basic spatial lookups for the Butler ,"Based on [this |https://confluence.lsstcorp.org/display/DM/Dependencies+on+02C.06+Science+Data+Archive+and+Application+Services] pipeline software will require spatial lookups via butler.    Current plan is:  Provide an ""ingest"" script that looks at a butler exposure dataset, pulls spatial information and data ID out of it, then shoves all of that into an sqlite3 table that can be used for accelerated spatial lookups.  Then provide a task that takes skymap parameters, and maps those to potentially overlapping exposures: the sqlite3 table is outside butler; and the will use a spatial location to get a dataId, and then pass that dataId into butler to get that file. "
1,Debug problems with near neighbor queries,"The following near neighbor query:    {quote}  select o1.ra as ra1, o2.ra as ra2, o1.decl as decl1, o2.decl as decl2,   scisql_angSep(o1.ra, o1.decl,o2.ra, o2.decl) AS theDistance   from Object o1, Object o2   where qserv_areaspec_box(90.299197, -66.468216, 98.762526, -56.412851)   and scisql_angSep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015  {quote}    fails on the IN2P3 cluster."
0,Design SQL API for getting query type,"When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the SQL API will look like."
1,Design API for passing query type,"User should be able to pass hint with a query indicating what query type it is. Based on that the return result will either be the query result, or queryId. This story involves designing the API  (sql and RESTful)."
0,Design RESTful APIs for async queries in WebServ,Need RESTul API for:   * retrieving partial results while query is running   * killing async queries
1,Design SQL APIs for async queries,"Need SQL API for:   * submitting async query, note that we should be able to specify where the results are going / what is the format of the results   * retrieving status of async query   * retrieving results of async query   * retrieving partial results of async query while it is running  "
0,adapt sandbox-jenkins-demo to changes in jfryman/nginx 0.2.7,Changes in the way  jfryman/nginx 0.2.7 handles tls cert files since 0.2.6 have run awful of selinux permissions issues.
1,Attending Cyber Security Summit,Attending NSF Cyber Security Summit in my capacity as LSST ISO.
0,Calibration transformation should not fail on negative flux,"Before database ingest, measured source fluxes are converted to magnitudes as per DM-2305. The default behaviour of {{afw::image::Calib}} is to throw when a negative flux is encountered, which derails the whole transformation procedure. Better is to return a NaN."
0,Design RESTful API for getting query type,"When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the RESTful API will look like."
0,Debug problem with large results set,Query returning 2 billion rows causes problems for czar - czar is using nearly 16 GB or memory. Need to understand why RAM usage in czar is correlated with result size.
0,replace Calib negative flux behavior methods with context manager,"DM-3483 adds a nice context manager for handling the behavior of Calib objects when encountering negative fluxes.  This could be even nicer if we moved it into afw and integrated it with Calib itself, replacing the existing static methods (which are bad because they use global variables).    This will be an API change, and will require an RFC."
0,Quick-and-dirty n-way spatial matching,"This issue will add limited N-way spatial matching of multiple catalogs with identical schemas, sufficient for measuring FY15 KPMs.  It will be a simple wrapper on our existing 2-way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface)."
0,Update ip_diffim to use the new NO_DATA flag instead of EDGE,"In ip_diffImm some uses of EDGE were converted to or supplemented with NO_DATA, but others were not. This ticket handles the missing instances."
1,Correct for distortion in matchOptimisticB astrometry matcher,"matchOptimisticB does not correct for distortion, although an estimate of the distortion is available.  We suspect that doing the matching on the celestial sphere might be ideal, but matching on a tangent plane has worked for HSC."
0,Fix crosstalk following ds9 interface changes,"crosstalk.py in obs_subaru uses ds9 without actually displaying anything, which causes trouble if display_ds9 is not setup."
0,lsst.afw.display.setMaskTransparency doc doesn't match code,"The docstring for {{setMaskTransparency}} says ""Specify display's mask transparency (percent); or *None to not set it when loading masks*"", but (from inspection of the code), it doesn't do anything if {{transparency}} is {{None}}.  "
3,X16 Large Results,"As described in [LDM-135|http://ldm-135.readthedocs.org/en/master/#query-access-related-requirements] high volume queries can return large results (~6 GB per query). Current version of Qserv poorly supports large results. We have observed that a single query that returns ~40 million or two-column rows (~0.5 GB) requires nearly 16 GB of RAM in czar, and czar uses 100% of CPU for extended period of time. This is most likely related to aggregating results - results are currently cached in memory before they are returned to the proxy. This epic involves redesigning this part of Qserv. One reasonable approach would be to write to separate tables from each worker, and deliver the final result as UNION of these tables. Or, perhaps we could use a shared files system and each worker would write results directly there, without burdening czar.    Deliverable: demonstration involving 8 queries each returning 2 GB result handled through a single czar. Czar should not use excessive amount of RAM or CPU."
1,Add support for images produced by pipelines in end-to-end integration test,We need to add support for images that are produced by pipelines which are run as part of the end-to-end integration tests - ImgServ should be able to serve these images.
2,Add support for executing async queries through czar,NULL
2,S17 Async Queries in WebServ,"Add support for async queries in webserv. Deliverable: webserv that allows to manage async queries (start, kill, retrieve results). Partial results are not covered here."
2,Implement RESTful API for async queries in WebServ,NULL
2,Research mysql proxy alternatives,"Research alternatives to mysql proxy, including things like maxScale."
2,Improve spatial image search for butler,NULL
1,Modify czar to use per query CSS metadata,"Czar is currently caching CSS information, the snapshot is taken when czar starts. Once we start dealing with more dynamic system where databases and tables can come and go anytime while the system is up (in particular, L3 databases and tables), the metadata in CSS would need to be refreshed per query to stay up to date"
3,W16 Support Dynamic CSS Metadata in Czar,Czar needs to support dynamic CSS metadata. This epic involves reworking Facade and related code so that czar can have up to date CSS metadata (per query) instead of relying on static snapshots
1,Revisit Facade API,NULL
1,Adopt Webserv to work with reworked db module,NULL
1,Expose column metadata via metaserv,"We need to expose via RESTful APIs information about columns such as units, ucds, column description etc. It will require querying information_schema with information from our DDT tables. This should include exposing information which columns are the ra/decl columns used for indexing."
1,Add support for row counts in Metaserv,"SUI software would like to frequently check what the row counts for our tables. This story involves caching the information about the row count information in the metaserv.     Note that for L3 tables, that might change, we will need to intercept queries that alter these tables, and update the cache.This is beyond the scope of this story."
1,Add flag to MetaServ showing if qserv_area_spec is available,SUI software needs to know which tables support spatial queries. This story involves exposing this information via Metaserv (the information is available via CSS) 
1,Refactor DipoleMeasurement: Dipole classification to plugin,"DipoleMeasurementTask currently runs dipole measurement plugins and runs its own implemented dipole classification method. This ticket translates dipole classification to a plugin itself to simplify DipoleMeasurementTask.  Instead of inheriting from SingleFrameMeasurementTask, DipoleMeasurementTask will run SingleFrameMeasurementTask setting the appropriate default slots and plugins (including dipole classification plugin).      "
3,Base DC network design,Design proposal for Base DC network
1,prune stale obs_subaru dependencies,"obs_subaru has some Eups dependencies that should be removed:   - meas_extensions_multiShapelet (removed from the LSST stack, content moved to meas_modelfit)   - meas_multifit (renamed to meas_modelfit)   - fitsthumb (need to cherry-pick code from HSC to replace it)   - pyfits (investigate what we use it for; it might not be needed)   - psycopg2 (I don't think we'll ever use this on the LSST side; we should add it back to the HSC side after we re-fork)"
3,Create flux environment for input panels,NULL
2,Organize React component to use new flux environment,NULL
1,Releasing un-acquired resources bug,"Running a mix of queries: 75 low volume and 10 high volume that include near neighbor failed at some point with    {quote}  terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}    Stack trace:    {quote}  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  Missing separate debuginfos, use: debuginfo-install expat-2.1.0-8.el7.x86_64 glibc-2.17-78.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.12.2-14.el7.x86_64 libcom_err-1.42.9-7.el7.x86_64 libgcc-4.8.3-9.el7.x86_64 libicu-50.1.2-11.el7.x86_64 libselinux-2.2.2-6.el7.x86_64 libstdc++-4.8.3-9.el7.x86_64 nss-softokn-freebl-3.16.2.3-9.el7.x86_64 openssl-libs-1.0.1e-42.el7_1.9.x86_64 pcre-8.32-14.el7.x86_64 xz-libs-5.1.2-9alpha.el7.x86_64 zlib-1.2.7-13.el7.x86_64  (gdb) where  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  #1  0x00007fb6b0ccfcf8 in abort () from /lib64/libc.so.6  #2  0x00007fb6b15d29b5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6  #3  0x00007fb6b15d0926 in ?? () from /lib64/libstdc++.so.6  #4  0x00007fb6b15cf8e9 in ?? () from /lib64/libstdc++.so.6  #5  0x00007fb6b15d0554 in __gxx_personality_v0 () from /lib64/libstdc++.so.6  #6  0x00007fb6b1069913 in ?? () from /lib64/libgcc_s.so.1  #7  0x00007fb6b1069e47 in _Unwind_Resume () from /lib64/libgcc_s.so.1  #8  0x00007fb6ab554247 in lsst::qserv::wdb::ChunkResourceMgr::Impl::release (this=0x21d1cc0, i=...) at build/wdb/ChunkResource.cc:398  #9  0x00007fb6ab552696 in lsst::qserv::wdb::ChunkResource::~ChunkResource (this=0x7fb68a5f9b70, __in_chrg=<optimized out>)      at build/wdb/ChunkResource.cc:131  #10 0x00007fb6ab560f0f in lsst::qserv::wdb::QueryAction::Impl::_dispatchChannel (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:392  #11 0x00007fb6ab55f5ab in lsst::qserv::wdb::QueryAction::Impl::act (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:187  #12 0x00007fb6ab562084 in lsst::qserv::wdb::QueryAction::operator() (this=0x7fb658050548) at build/wdb/QueryAction.cc:450  #13 0x00007fb6ab544f46 in lsst::qserv::wcontrol::ForemanImpl::Runner::operator() (this=0x7fb67400fa20) at build/wcontrol/Foreman.cc:302  #14 0x00007fb6ab551cf0 in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::_M_invoke<>(std::_Index_tuple<>) (      this=0x7fb67400fa20) at /usr/include/c++/4.8.2/functional:1732  #15 0x00007fb6ab551a8b in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::operator()() (this=0x7fb67400fa20)      at /usr/include/c++/4.8.2/functional:1720  {quote}    Tail of log file from xrootd log:    {quote}  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:139) - _getNextTasks(1)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:151) - Returning 1 to launch  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:154) - _getNextTasks <<<<<  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:172) - _getNextTasks(29)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:199) - ChunkDisk busyness: yes  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:171) - ChunkDisk getNext: current= (scan=10436,  cached=8360,8259,) candidate=10301  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:184) - ChunkDisk denying task  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:196) - _getNextTasks <<<<<  0821 19:08:58.531 [0x7fb68a6fb700] INFO  root (build/xrdsvc/SsiSession.cc:120) - Enqueued TaskMsg for Resource(/chk/LSST/2732) in 0.001016 seconds  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:175) - Registered runner 0x7fb66c141ab0  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:209) - Started task Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_fd51ad249f62fb765e173d7b3cae5d94  0821 19:08:58.531 [0x7fb6895f8700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=106  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=210  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=316    ...(thousands of _fillRows lines)    terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}  "
0,data center requirements,NULL
2,Revise LDM-240,Revising LDM-240.
0,Add ITIL model use cases to Enterprise Architect,NULL
0,Prepare use case diagrams for LSST 2015 Operations Concepts breakout session,NULL
3,FY19 Query Result Caching,"Implement Query caching. Note that most likely we can't ""just"" rely on mysql caching, and we will need to do some custom tweaks, in particular for async queries.    I'd be useful to allow users to ""pin"" results from interesting queries. Typical usecase: user runs a query (it can be interactive) and then decides to keep the results for longer time. "
1,LOE - Week ending 8/21/15,NULL
1,LOE - Week ending 8/28/15,NULL
1,Information categorization for docushare,Project started to categorize documents on docushare as per LSST's information categorization policy.
0,Meet with Scott K for IdM project,Scott K will come to NCSA to discuss initial IdM requirements for LSST.
0,NIST SP 800.82 investigation,NIST SP 800.82 might contain useful information for securing the scada enclave at the observatory site.  Or it might even be the model we should use in full.
3,Czar Failover in xrdssi,In production we will need to recover from czar failures by automatically failing over to a different czar. This epic involves designing and implementing the interfaces in xrootd xrdssi that will be need to support czar fail over.
0,Translate more mask pixel bits from instcal data quality mask of DECam data ,"The mask pixel bits are defined differently in DECam instcal data from different pipelines, see http://community.lsst.org/t/decam-data-quality-masks/133  for a summary.      The current mapping in DecamInstcalMapper is for the community pipeline Pre-V3.5.0.  More mask bits were defined in DESDM y1 products. This issue provides full mapping for DESDM y1 products so all questionable pixels are translated into the MaskedImage.       "
1,Fix bugs found in FitsRead ,"There was a bug found in writeFitsFile(OutputStream stream, FitsRead[] fitsReadAry, Fits refFits) method where it saved the data from refFits into the output Fits file instead of the data from the FitsRead object.    There is another possible bug under investigation.  The output Fits file created by FitsRead.createNewFits() can not be opened as an image.   "
1,Refactor ChunkResource for testability,"While the wdb/testChunkResource.cc unit test appears to exercise the chunk resource management code, it does not seem to actually test it.    We should refactor the classes in the ChunkResource header and implementation files to make the sanity of the implementation checkable. In particular, I think that Backend from ChunkResource.cc should be a public interface (with a less generic name) that the unit test can implement, and that ChunkResourceMgr should be a concrete class implemented in terms of a user specifiable Backend.    This way the unit test can inject the dependency it wants (namely a mock backend that tracks sub-chunk tables as they are acquired and released), we don't pollute the actual implementation classes with ""I'm fake"" flags and alternate code paths for fake objects, and we can make the unit test actually perform validity tests.    The first cut at this should include a check for the problem described in DM-3522."
3,Emergent Uncategorized Work,Epic to capture work that is not easy to categorize in other WBSs.
3,Cleanup of initial astrometry improvements,"The astrometry improvements are working, but some cleanup would be good to remove dependencies on A.net and to provide default reference catalog loaders."
2,Improve aperture correction implemented in HSC,There is technical debt left over from the HSC-LSST merge of the aperture corrections.  This epic will take care of the debt noted during the port.
0,Move LDM-151 to Sphinx/Read the Docs,Move the LDM-151 (DM applications design document) to restructuredText (built with Sphinx) and published automatically via readthedocs.org.    See discussion at http://community.lsst.org/t/requesting-comments-for-design-documentation-format-for-dm/132?u=jsick    This is an experiment.
0,SsiService not being destroyed,SsiService::~SsiService is not being called. 
2,Learn about IPython / Jupyter internals,"In order to evaluate the suitability of IPython as a framework for Level 3 work, and its ability to be integrated with the SUI Tools, the internal structure of IPython and its communication protocols need to be understood.    Work on this story will include reading about IPython, experimenting with it, and reading the IPython code as needed."
2,Further improve the TAN-SIP WCS fitter,"lsst.meas.astrom.sip.makeCreateWcsWithSip, our C++ implementation of a TAN-SIP WCS fitter, fits for X and then Y. Thus it must be called several times (e.g. by FitTanSipWcsTask) in order to converge. Please make makeCreateWcsWithSip fit for X and Y simultaneously. This would simplify its use and speed up fitting.    In addition, please investigate whether outlier rejection can be added to makeCreateWcsWithSip. As of DM-3492 outlier rejection is implemented in FitTanSipWcsTask, but it would make makeCreateWcsWithSip easier to use and speed up fitting if makeCreateWcsWithSip did the outlier rejection itself. On the other hand, the current solution may be sufficient.    Once the above is implemented, please simplify FitTanSipWcsTask by removing the unneeded extra iterations used to work around these problems."
3,Future measurement algorithm enhancements,NULL
1,Attend SciPy 2015 tutorials,"Attend the tutorials at SciPy 2015 (July 6-7, 2015) in order to get hands-on experience with current scientific data analysis tools in the Python environment.    Planned attendance:   * Introduction to NumPy  * Building Python Data Applications with Blaze and Bokeh  * Efficient Python for High-Performance Parallel Computing  * Jupyter Advanced Topics Tutorial  "
3,Discuss the QA visualization needs,Meet with SQAURE lead to discuss the QA needs of visualization tools.     The whole SUIT team of 8 people met with SQUARE lead Frossie for an all day discussion to outline the visualization needs that SQUARE team may need in support of the pipeline stack data processing verification. We agreed that the image visualization and XY 2D plot  components should be separately accessible through JavaScript API and Python API. SQUARE team could build its own web portal using the JavaScript API and build its own analysis tool in iPython notebook using the Python API. The resulting work work was captured in other Epic and stories.   
2,Access predefined catalogs via Data Access API,"As a part of end-to-end exercise access predefined catalogs and their definitions from the new Data Access API.     We have been doing it via JDBC calls to QServ and queries to http://lsst-web.ncsa.illinois.edu/schema/index.php?sVer=S12_sdss    Even though we can not access QServ via Data Access API at the moment, it should be transparent to us in the future.    As for data definitions, for now we can only access column names and types. In future, more information (like units and field descriptions) should be available."
1,Binary FITS table catalog upload,"We need to be able to upload catalogs in binary FITS table format.    We'll do it by converting the first table in the provided FITS into an IPAC table. Our upload should be handling the conversion.    Later, we should figure out how to handle multiple tables in one FITS."
0,"Ignore ""SELECT @@tx_isolation"" queries",Looks like one of the queries we registered in webserv is: cursor.execute('SELECT @@tx_isolation') and that is bound to confuse Qserv. Need to suppress it at mysql proxy level.
1,Attend SciPy 2015 conference,"Attend the SciPy 2015 conference, July 8-10, 2015, to learn about current trends in this community.    Notably, the conference covers a variety of interactive data analysis tools, including Jupyter/IPython, interactive graphics packages such as VisPy and Bokeh, and astronomical data handling tools such as astropy."
1,Document SciPy 2015 takeaways,"Write up what was learned, and recommendations, from SciPy 2015."
1,Experiment with Jupyter widget technology and Firefly Tools,"Based on DM-2047 work to date, investigate the feasibility of using the Jupyter widget interface to wrap up Firefly tools."
3,Implement improved Footprints,In S15/DM-1904 we began a redesign of the Footprint API. We need to follow through and convert that to code on disk.
3,Complete HSC port: object characterization,Merge all functionality from HSC to LSST that is required so that the LSST stack can be used for all standard HSC processing functionality and the HSC fork is removed.
3,Continued galaxy shear fitting experiments,Build on the test framework delivered in DM-1108 to arrive at conclusions regarding the number of pixels which must be included in galaxy fitting and the number of shapelet terms needed in the PSF.
3,Refactor executor code,NULL
2,Add unit tests to exercise new scheduler,Add tests (unit test and/or extend the integration test) to test the new scheduler.
1,Integrate Qserv code with cancellation-friendly xrdssi,NULL
1,Add support for type aliases,NULL
3,Complete HSC port: framework,"Merge all framework functionality (afw, middleware, etc) from HSC to LSST that is required so that the LSST stack can be used for all standard HSC processing functionality and the HSC fork can be removed."
1,Research/reading for PAST prototype C++,NULL
1,Make location of images more flexible,It'd be useful to be able to point imgserv to any location containing images (in particular for various random tests that we will be running over the next few years). Right now this requires changing imgserv code. An idea tossed around: pass the location via URI (optional  parameter)
1,Collect requirements for pipeline developer visualization tools,"This story covers a series of conversations with Robert Lupton, Jim Bosch, Paul Price, K-T Lim, and others going into details about how to make the visualization environment (based on Firefly tools) more useful for developers."
2,Document AP simulator,"Write up how the AP simulator current works, i.e. start the processes doing the AP simulator, sending messages to the base DMCS, how messages are sent from archive to jobs on the worker cluster, etc."
1,Replace qservAdmin.py use with CssAccess,WE have new CSS interface which unifies C++ and Python and it is time to replace qservAdmin.py with CssAcces in places like qserv-admin.py and data loader.  
3,Research requirements for chromaticity,NULL
2,Redesign CalibrateTask,This covers design work only; an accompanying epic (perhaps in 02C.03?) will handle implementation.
3,Refactor sub-task interfaces,NULL
3,"Audit, update & integrate top level tasks","As part of the HSC merge, we've pulled in a number of high-level pipeline tasks, which may not be consistent with existing LSST tasks. Audit the overall pipeline flow, ensuring consistency. To include:    * Rewrite top-level pipeline tasks and Butler datasets.  * Remove ProcessCoaddTask.  * Ensure consistency between MPI drivers and individual CmdLineTasks."
3,Investigate options for physically motivated PSF models,"Rather than developing PSF models based on the optics and the atmosphere, this epic is focused on developing requirements and establishing what resources & expertise are available in other groups (e.g. DESC) which we can make use of. If necessary, it will flow down to another epic in which we actually produce working code."
3,Develop improved galaxy model flux measurements,"Hard thinking, cleaning up & optimizing the existing galaxy model flux measurement code."
3,DRP W16 emergent work: object characterization,Catch all epic for emergent work in 02C.04.06 during W16.
3,DRP W16 emergent work: framework,Catch all epic for emergent work in 02C.04.01 during W16.
3,Firefly infrastructure improvement to support new functions (W16),This epic will capture the necessary changes of Firefly infrastructure to support new functions needed. It does not include the changes caused by the the conversion from GWT infrastructure to pure JavaScript based system using React and FLUX platform. 
3,Refactor the Firefly Java code (W16),This epic will capture all the refactoring work related to Java code. We are converting portions of GWT code to pure JavaScript React based code. We will only refactor the Java code if it is not going to be converted. 
1,Catalog transformation should pass through all non-measurement fields,"We need to include fields added by other tasks (e.g. the deblender) or the source minimal schema (e.g. parent) in transformed catalogs.  I believe it should be safe to assume any such fields can simply be copied (i.e. they do not require any actual transformation - they're mostly flags), but we do need to make the list of fields to be copied by this mechanism dynamic."
2,Add support for registry-free repository,Existing butler unit tests should run without an sqlite database registry.
2,Refactor Backend to improve visibility,Backend is a class that connects to mysql and can cause the worker to terminate. It is buried in the Foreman and should be renamed and have the class defined in a header file so that it is more visible. The class should also be available to use for any other in memory tables. 
3,Firefly support for pipeline visualization needs  (W16),Data products pipeline needs visualization capabilities for display. Firefly needs to have new capabilities to support it. 
3,New functions for XY 2D plot (W16),Firefly should support histogram plot
3,new algorithm and functions for 2D XY plot (F17),NULL
0,use devtoolset-3 for Jenkins CI builds on EL6,NULL
3,Firefly support for Camera team visualization needs (W16),Camera team plans to use Firefly visualization capabilities in camera test.  SUI/T team will continue to support this effort. 
3,Workspace preliminary functional design  (W16),Produce a preliminary functional design of user workspace. 
2,Authentication and authorization system API requirement,"Produce a requirement document on the authentication and authorization  API needed for the full SUIT. Work with other DM subsystems (DB, SQUARE, NCSA) closely to achieve this. "
2,Prepare for system setup in NCSA for LSST web UI (X16),"We want to start preparing for the SUI system setup in Fall 2016 at NCSA to be able to do the following:    Set up a web UI for DM or even general public to access, so we can have a constant end-to-end system. It will be very useful for DM testing. SUIT will use it for simple test of accessing the DAX APIs.  Pipeline team can use it to see the processed data, search source table etc. The goal is to be able to access SDSS strip 82 data through SUIT web portal"
2,Expose more Firefly visualization functions through JavaScript API (W16),Expose more Firefly visualization functions through JavaScript API to users so they can have more control in building their own web page. 
3,Expose more Firefly visualization functions through Python API (F16),Expose more Firefly visualization functions through Python API as needed.
2,Data access (DAX) API design support (W16),Continue to work with SLAC  in data access (DAX) API design and test. Participating the discussion and exercising the API as needed to help test and expose potential issues. 
1,"Submit change request for LSE-68, mid-phase-3 update","Collect changes and submit a change request to LSE-68 for a mid-phase-3 update to the document, covering clarifications on guider data and image identification, among others.    Includes preliminary work to prepare for change request."
1,Prepare document for CCB review of LCR-357,"LCR-357 was an outline of work to be done, based on discussions with the Camera DAQ staff.  This task is to generate an actual proposed document change for the CCB."
0,provide detailed information needed to DAX meta API,"SUIT needs certain specific information through DAX meta service when searching for meta data. For Example, what kind of table it is, does it have spatial index to search by position, which set of (ra, dec) columns is the primary one, etc?"
1,The Alert subscription system requirement gathering (F16),Solidify the requirement for the alert subscription system. 
0,CCB review and posting of final updated document,"Carry out the CCB review, respond to questions, support final implementation of updated document."
1,Prepare for Winter 2016 work on LSE-68,Use a session at the LSST 2015 all-hands meeting to prepare for LSE-68 work in the Winter 2016 cycle.
1,option to plot error bars on XY plot,"When there are error or uncertainty for a data point, there should be option to plot the error bars in the XY plot. "
2,Time series plot,Time series plot
2,capability to reverse the axis,"Provide option to reverse the axis for XY plot, one example is the magnitude value convention. "
1,expose region overlay on image function through JavaScript API,expose region overlay on image function through JavaScript API
0,Expose image XY readout at cursor point function in JavaScript API,Expose image XY readout at cursor point function in JavaScript API
1,Review risk register status,NULL
0,Fix bug related to restarting xrootd in wmgr,Changes from DM-2930 are failing integration tests because wmgr is restarting xrootd and now we need to also restart mysqld if xrootd pid changes.
0,Move env variables related to xrootd/czar and unlimit in etc/sysconfig/qserv,NULL
1,OCS-DM-CCS-DAQ workshop,Prepare for and attend the OCS-DAQ-CCS-DM workshop in November 2014.  This is primarily intended to review the status of LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.
1,OCS-DM-CCS-DAQ workshop II,Prepare for and attend the OCS-DAQ-CCS-DM workshop in February 2015. This is primarily intended to continue to make progress on LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.
3,Overhaul deblender,Re-write the existing deblender code to use the refactored Footprints (DM-3559). Take the opportunity to resolve known issues and add new functionality from SDSS and elsewhere to make the deblender equal to the state of the art.
1,LDM-144 costing model update,NULL
0,Investigate disk-only (no tape) data releases,"The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing  model, observing the results as they propagate through LDM-144, the cost model,  and checking to make sure that everything makes sense and nothing has been overlooked. It looks like an answer to this question will be needed somewhere between a month and three months from now."
2,Continue to refine the SUIT requirements (X16),We did lots of work in FY15 to redefine the SUIT requirements. The picture is getting clear as we talk to others in DM system and science community.  We would like to have a better definition and provide a document for this. 
2,a catch all epic for unexpected bug fixes  (W16),This is an epic for unexpected bugs found and need to be fixed in this cycle. 
1,implement jenkins support for running builds in docker containers,"DM-3359 demonstrated the feasibility of running jenkins builds in one-off docker containers but did not cover investigation of user creation of containers nor did it include a puppetized deployment.    Per build containers are desirable for a number of reasons but the largest motivation is allowing DM developers to define their own jobs without disrupting the ""sanctity"" of the lsstsw build slaves."
3,HSC port: verification,"Compare the LSST and (old) HSC stacks for compatibility; identify and resolve any differences. Provide basic integration tests to SQuaRE to ensure the reliable and consistent operation of the LSST stack for processing HSC data. Cooperate with the SQuaRE team to develop more elaborate QA testing.    This work is timeboxed to consume the available effort in W16, and will be continued in the subsequent cycle."
0,Change root to config in config override files,Implement RFC-62 by using {{config}} rather than {{root}} in config override files for the root of the config.    Note that I propose not modifying astrometry_net_data configs because those are numerous and hidden. They have their own special loader in LoadAstrometryNetObjectsTask._readIndexFiles which could easily be updated later. if desired. An obvious time to make such a transition would be when overhauling the way this data is unpersisted.
1,Management for Aug 23-29,"Hiring -- make offer to candidate, and also review candidates and strategy.  Review proposal for FY 2015 spend  Management meetings.  Openstack meeting."
2,"Study JavaScript, read up on React ",Spend about one hour per day to read up on JavaScript and React framework. Get ready to work on some of the JavaScript programming in Firefly
1,Add readMatches back to meas_astrom,"Recent changes to meas_astrom accidentally removed a function readMatches (copied below). Please restore it, preferably in its own module (though if someday we have more small python functions we may want a utils.py module).    Also please include a unit test.    {code}  def readMatches(butler, dataId, sourcesName='icSrc', matchesName='icMatch', config=MeasAstromConfig(), sourcesFlags=afwTable.SOURCE_IO_NO_FOOTPRINTS):      """"""Read matches, sources and catalogue; combine.      @param butler Data butler      @param dataId Data identifier for butler      @param sourcesName Name for sources from butler      @param matchesName Name for matches from butler      @param sourcesFlags Flags to pass for source retrieval      @returns Matches      """"""      sources = butler.get(sourcesName, dataId, flags=sourcesFlags)      packedMatches = butler.get(matchesName, dataId)      astrom = Astrometry(config)      return astrom.joinMatchListWithCatalog(packedMatches, sources)  {code}  "
2,Preliminary design of Firefly core  using React and FLUX framework,Propose a preliminary design for Firefly core using React and FLUX framework. 
0,configDictField.py has code that relies on an undefined variable,"While taking a linter pass on {{pex_config}} I found that {{ConfigDict.\_\_setitem\_\_}} in {{configDictField.py}} has some code that uses an undefined variable {{value}}. See the else clause in:    {code}          if oldValue is None:                          if x == dtype:                  self._dict[k] = dtype(__name=name, __at=at, __label=label)              else:                  self._dict[k] = dtype(__name=name, __at=at, __label=label, **x._storage)              if setHistory:                  self.history.append((""Added item at key %s""%k, at, label))          else:              if value == dtype:                  value = dtype()              oldValue.update(__at=at, __label=label, **value._storage)              if setHistory:                  self.history.append((""Modified item at key %s""%k, at, label))  {code}"
1,continue L1 refined specifications ,"Re-synchronize with prior work after vacation.    Write up page both engineering and facility database ingest  and use, incorporate suggestion and comments from KT and GDF.    Clean up (better name entities, and move for better narrative flow) pages about the general thing that is called ""level 1"",  -- the context diagram and supporting prose for further descriptions,  Review internally with Jason, Steve and Margaret.    Create a revised  diagram that would change LDM-230 specifications. (all non crosstalk corrected data flow into a disk buffer, and and an new archive tasks empties it)  to make specifications consigned with the refined specifications on the page above.  Yet to write the prose....    These pages are in my personal pages in the LSST confluence area.        "
1,preliminary detailed content required for Authorization and Authentication system for SUIT,Provide the first draft  of detailed content required for authorization and authentication system from SUIT point of view to NCSA. 
0,RangeField mis-handles max < min,"RangeField contains the following bit of code to handle the case that max < min:  {code}           if min is not None and max is not None and min > max:              swap(min, max)  {code}    This is broken because there is no swap function and if there was it could not work in-place like this. However, rather than replace this with the standard {{min, max = max, min}} I suggest we raise an exception. If max < min then this probably indicates some kind of error or sloppiness that should not be silently ignored. If we insist on swapping the values then at least we should print a warning.    The fact that this bug has never been reported strongly suggests that we never do set min > max and thus that an exception will be fine."
0,"OCS-CCS-DAQ-DM teleconference, April 2015",Prepare for and attend a half-day teleconference on OCS issues.
1,"OCS-CCS-DAQ-DM workshop III, May 2015","Prepare for and attend an OCS-subsystems workshop at SLAC May 6-8, 2015."
3,Firefly server side extensions using DM stack (F16),Design and implement a control system to extend Firefly server side capabilities using task in DM stack.  This will make it easier to use DM stack for customized data processing. 
3,"Support OCS revision of LSE-70, LSE-209","Support the OCS efforts to update LSE-70 and create a new associated document, LSE-209.  Getting current versions of these under change control will allow us to complete a round of work on LSE-72."
1,"install DM stack, get familiar with the current DM task concept ","install DM stack, get familiar with the current DM task concept.   This is for getting ready to use task with Firefly server side extension capability.  Here is the link to the tutorial.  https://confluence.lsstcorp.org/display/DM/Getting+started+with+stack+development"
1,Support the design of Firefly core system using React and FLUX,Working with Loi on the design of Firefly core system based React and FLUX frameowrk
1,Support the design of Firefly core system using React and FLUX ,Working with Loi on the design of Firefly core system based React and FLUX frameowork  
0,"LSE-72: OCS-CCS-DAQ-DM workshop, July 2015","Work associated with Workshop IV in the series, held at NCSA July 8-10, 2015."
3,Preliminary SUIT design ,Working with the suit-wg to produce a preliminary design of SUIT.
0,SUIT design document outline,SUI/T design document outline.  
0,on-going support to Camera team in UIUC,Attend UIUC weekly meeting and give support as needed. 
0,MakeDiscreteSkyMapRunner.__call__ mis-handled returning results,"{{MakeDiscreteSkyMapRunner.\_\_call\_\_}} will fail if {{self.doReturnResults}} is {{True}} due to trying to reference undefined variables. This is at least approximately a copy of a problem that was fixed in pipe_base {{TaskRunner}}.    {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} should be fixed in a similar way, and (like {{TaskRunner}}) changed to return a pipe_base {{Struct}}.  "
0,SUIT design document outline,"Work with Gregory on the SUIT design document outline    1.  Requirements flow down, making sure that we design the system satisfying the current requirements.  2.  Use cases collection. at least one typical use case in each major science theme  3.  Levels of different users  ** novice: treat the web portal as a archive to get some information, don't know much about LSST  ** novice expert: has some ideas of what special functions they would like, has some knowledge of LSST data  ** domain expert: knows LSST data very well and want some special functions ready to use  ** savvy expert: knows LSST data very well and like to use API to their own programming    4. functions for all different levels of users  5 system design   ** system diagram  ** details of the different parts  *** Firefly server  *** Firefly client  *** Firefly server extension  *** Firefly JavaScript API  *** Firefly Python API  *** Firefly Python API, Jupyter notebook, and other Python applications  *** workspace and level3 data  *** SUI web portal sketch, workflow  ** dependency on other capabilities of other institutes    6. development and test plan,  timeline  7. deployment plan                "
0,SUIT design document outline,work with John Rector on SUIT design document outline
1,Summarize current LSE-75 status as intro for new T&S personnel,"With the arrival of new Telescope & Site personnel, especially the Telescope Scientist, [~sthomas], prepare a summary of the current state of LSE-75 and its open issues."
0,Data loader doesn't work for match tables,qserv-data-loader.py fails to load match tables:   - it does not invoke the correct partitioner executable for them   - not all CSS parameters required for match tables are passed down to the CSS update code
0,Create change request for LSE-75,"Create a change request for LSE-75, the TCS - to - DM ICD."
1,Discussions on LSE-75 with Telescope & Site personnel,"Pursue interactions with Telescope and Site personnel regarding LSE-75, and in particular the issues surrounding calibration data products for the wavefront and guider data analysis pipelines.    Covers work through the end of August 2015."
0,Initial discussions with Patrick Ingraham,"This story is a catch-all for preliminary conversations about LSE-140 with the new Calibration Instrumentation Scientist, Patrick Ingraham."
0,"Review pending work, clean up related JIRA DM- and LIT- issues",NULL
1,Configure VMs to provide additional slots for task switching,"We have to set up a new set of slots that will be used to execute the overlapping thread of execution for alert production.   Because of limited resources at this time, this means reconfiguring the worker nodes to provide additional slots, and to split the worker nodes into two sets.    "
0,Add support for clang and OS X to qserv scons,NULL
1,Basis for HSC integration test,"We need to assemble the basis for an integration test using HSC data, to protect HSC processing and obs_subaru from upstream changes.    This includes the assembly of the required data, a basic mechanism to test the mechanics of data release production, sufficient for the SQuaRE team to take it and incorporate it into the Jenkins system.  Addition of any scientific validation is deferred for now."
0,rename parameter vector methods in afw.geom.ellipses,"[~nlust] notes that the {{writeParameters}} and {{readParameters}} methods on the ellipse classes are confusingly named, especially when compared to similar methods on {{meas.modelfit.Model}}."
1,improve test coverage of CModel failure modes,"The CModel has a large number of failure modes, largely dealing with different kinds of problems in the inputs, and a correspondingly large number of flags.  It also has some fairly complex logic determining which flags can be set simultaneously.  All of these combinations need to be tested.    DM-1574 may be useful in capturing these conditions from runs on real data."
1,Revisit Footprint design,NULL
0,PSFEX does not build if PLplot is installed,"During the configure phase PSFEX checks for the presence of PLplot. If PLplot is found then the build fails (at least on a Mac using homebrew):  {code}  /bin/sh ../libtool  --tag=CC   --mode=link clang  -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include   -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib -lfftw3f -lm  -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  libtool: link: clang -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o  ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib /Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib/libfftw3f.dylib -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  Undefined symbols for architecture x86_64:    ""_plwid"", referenced from:        _cplot_drawloccoordgrid in cplot.o        _cplot_fwhm in cplot.o        _cplot_ellipticity in cplot.o        _cplot_moffatresi in cplot.o        _cplot_asymresi in cplot.o        _cplot_counts in cplot.o        _cplot_countfrac in cplot.o        ...  ld: symbol(s) not found for architecture x86_64  {code}    This particular error is caused by PSFEX using a deprecated PLplot API ({{plwid}}) that is not enabled by default and whose name is not translated to {{c_plwid}}. This PLplot change occurred in version 5.9.10 released in 2013. I assume upstream PSFEX has a fix for this.    Given that LSST does not need the PLplot functionality I think the simplest fix may well be to disable the test for PLplot in our version.    It seems likely that there will be a reasonable number of systems ""in the wild"" who will have PLplot installed so I'm inclined to think that this should be a blocker for the v11.0 release.    If we are lucky people will have all upgraded their PLplot installs to v5.11.0 because in that version PLplot change the name of the library from {{libplplotd}} to {{libplplot}} and PSFEX has hard-wired the former rather than using pkg-config. This results in configure not finding PLplot. I don't think this eventuality is likely though.  "
0,obs_test needs to override map_camera and std_camera,"The Butler can't get a camera unless the map_camera and std_camera are defined correctly.  In most cases the camera can be built by the map_camera method.  In the case of obs_test, the camera is built in the constructor of the Mapper, so std_camera should just return the camera attribute."
2,W16 Webserv Unit Tests,"Implement unit testing across the webserv components. Use either a SQLite backend, or mock objects and mock results to emulate database interaction."
2,Gather candidates for Verification Datasets and identify collaborators,"  Identify candidate Verification sets (in the first instance, datasets with extant processed data used in one-off reductions with lsst_apps to allow a preliminary assessment of current quality from a science QA perspective).     Identify people within the project with effort, expertise and interest available to contribute to this effort. "
2,Present Verification approach at AHM,  Present talk at AHM to seed Verification effort and seek feedback. 
0,Resourcing Verification runs,  Identify required resources for Verification runs and communicate them to NCSA.   
1,HSC backport: Cleanup interpolation tasks and implement useFallbackValueAtEdge,This is a port of the changesets from:  [HSC-756|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-756]  
0,HSC backport: Standalone updates to star object selection,"This involves pulling over the following standalone (i.e. non-ticket) HSC commits:  [Updated star selection algorithm.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/071fcadc016908a10583c746f0a8e79df2a45ead]    [Appropriate config parameter for a unit test of testPsfDetermination.py.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/e73c5e447ac0b8a71926d3e78fec30aad4beee91]    [Remove HSC specific codes.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/15bb812578531766199e9a1ee41cc707fb3d9873]  (Note, the above reverts some unwanted camera-specific clauses added in the first commit.  May just squash them to only add the desired features)    [ObjectSizeStarSelector: push non-fatal errors to DEBUG level|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4]    [We don't use focal plane coordinates anywhere, and detector may be None|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/4413db4610e4793727e591f395f5ad8cd0cb6030]    [Fixed axis labels|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/67efacaccf8346fdfa1b450617aebabddb2b7ec0]    [Improved PSF debugging plots|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/b1bc91ed1538607eb90e070881a82498fd551909]    [Worked on star selector|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/6b36f4d757187d30142a7e026754a07ffeb8dea2]"
0,Allow building/publishing components off branches other than master,"Support of xrootd within the stack is currently complicated by the fact that qserv depends on features that are not available on upstream master (only available on an upstream non-master branch).  Since we can currently only publish packages from master, this means that our lsst fork of xrootd cannot be a ""pure"" fork -- we end up merging/rebasing from an upstream branch, then force-pushing the downstream master.  Upstream and downstream xrootd repos thus have completely different branch topologies, labels, etc., and history of master in the lsst fork is being continually rewritten to carry local patches forward.  The processes of both adopting upstream changes into the lsst fork and the pushing lsst changes back upstream are cumbersome, confusing, and labor intensive.    It is proposed that we extend our tools to allow publishing components from branches other than master.  This would allow us to have xrootd for example be a ""pure"" fork of upstream -- we could then create our own branch based off any upstream branch, carry our downstream patches there, and release off of that.    This functionality could be used similarly for any of our current ""t&p"" components where it would be convenient to track the upstream repo directly and/or carry changes in git instead of in an agglomerated patch file (e.g. when we might want to update frequently and/or contribute general purpose changes back upstream regularly with pr's, etc.)"
3,Jira PMCS EVM integration,This epic captures management support requests and non-SPed activities in FY15 covered by WBS 02C0102 that were not included in other Epics. 
3,LDM-240 Long range planning,NULL
3,Jira Data Management long range planning project,NULL
3,Release engineering Part Two,"This epic covers testing and co-ordination work associated with making  engineering and official releases, and code to support them.      [FE at 70%, JH at 20%, JS at 10%]"
3,Evaluate GitLFS,"We want to try out GitLFS and evaluate its suitability as a solution to storing binary data in repos for CI. We want to know how it compares in usability terms to git-annex, git-fat etc. "
0,Fix PATH and compiler version detection in qserv scons,"In recently merged DM-3662 compiler version testing was done using OS tools with regular $PATH. This is inconsistent with other scons tools which reset PATH when executing actions.   We want to do two things:  - propagate PATH to the command execution  - Use scons tools to run ""$CXX --version"" instead of OS tools to keep things consistent"
1,Revisit KPIs for Image Access,Need to come up with KPIs for Image Query Access
0,lsst_dm_stack_demo failure,Viz:    {code}  $ ./bin/demo.sh  [...]  $ bin/compare detected-sources.txt.expected detected-sources.txt  Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_flux.  Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_fluxSigma.  Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_flux.  Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_fluxSigma.  {code}    This is on OS X with {{lsst_apps}} {{w_2015_36}}.    
1,Forward community.lsst.org (Discourse) notifications to existing mailman lists,Setup a system to forward new post notifications from http://community.lsst.org categories to their appropriate legacy Mailman email list counterparts.    ||Discourse Source Category||Mailmain Forward List||  |DM Team||dm-staff|  |Announcements||dm-announce|  |DM Notifications||dm-devel|    Once this is implemented I will send a notice that all communications and replies should occur on discourse (these mailing lists should be read-only).    I will also send a notice that dm-users is deprecated.  
0,"CalibrateTask has outdated, incorrect code for handling aperture corrections","The CFHT-specific CalibrateTask tries to apply aperture correction once just after measuring it (which is too early) and again later, at the right time. The error probably has no effect on the final results, but it is confusing and needlessly divergent from the standard CalibrateTask. The required changes are small. I plan to test by running [~boutigny]'s CFHT demo."
0,HSC backport: Allow for some fraction of PSF Candidates to be reserved from the fitting,This is a port of the changesets from [HSC-966|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-966].    It provides the ability to reserve some fraction of PSF candidates from the PSF fitting in order to check for overfitting and do cross validation.
0,HSC backport: allow photometric and astrometric calibrations to be required,"This is a port of the standalone changesets:  [calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/e9db5c0dcdca20e8f7ba71f24f8b797e71699352]  [fixup! calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/c2d89396923f9d589822c043ed8753647e70f3f6]  (the above is a fixup, so will likely be squashed)  [make failure to match sources non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/cf5724b852937cfcef1b71b7a372552011fda670]  [calibrate: restore original Wcs after initial astrometry solution|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/ab6cb9e206d0456dc764c5ef78ac80ece937c610]  [move CalibrateTask from ProcessImageTask into ProcessCcdTask|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/08a8ec029dd52ac55e47b707a6905df061a40506]  [processCoadd: set detection to use the declared variances|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/9e8563fd8d630dad967786387b1f27b6bc7ee039]  [adapt to removal of CalibrateTask from ProcessImageTask in pipe_tasks|https://github.com/HyperSuprime-Cam/obs_subaru/commit/52733a7ab1731a15cbb93151851f57cec276f928]  and HSC tickets:  [HSC-1085: background not saved in processCcd|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1085] and  [HSC-1086: psf - catalog scatter is very large in some coadds|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1086]"
0,Decrease buildbot  frequency,"Buildbot frequency is now down to two builds, one at 19:42 machine time (NCSA) and one at 1:42. This is to stop people needing buildbot runs to eups publish to have to wait before a CI build, since they are now done on[ https://ci.lsst.codes ]/ Jenkins.     "
1,Add unit tests for secondary index,"qproc::testIndexMap.cc is very sketchy and doesn't perform any test for now (i.e. no BOOST_CHECK). It should be improved to really cover code related to secondary index. A mock secondary index is required here, i.e. qproc::FakeBacken should be strengthened."
1,Install squid proxy on cc-in2p3 build node,"Can we add this one to current sprint? It is required to access docker hub on in2p3 cluster.    I also need to automate/document it, test it on build nodes, and be reviewed by in2p3 sysadmins.    Cheers"
0,Replace --trace with --loglevel in pipe_base ArgumentParser,"Replace the --trace argument with an enhanced version of --loglevel that supports named values and numeric log levels (which are the negative of trace levels). This simplifies the interface for users and potentially reduces the log level/trace level confusion, though that won't fully happen until we finish replacing use of pex_logging Trace and Debug with Log.    This work was already done as part of DM-3532; it just needs to be copied with minor changes (since there are no named trace levels in pex_logging)."
2,Literature search for DCR -- Sullivan,Go through the literature to find relevant seminal papers on DCR.    The outcome will be a bibliography and executive summary.  This should be posted on Discourse.
1,Setup and conduct a conversation about DCR in the project,Advertise and conduct a broadly advertised videocon on DCR in the context of diffim.  The result of this should be minutes.  Ideally we would come out of this meeting with a list of possible techniques for dealing with DCR (preferentially sorted by priority).
3,Assemble the report on DCR,"Everything learned through the literature search and project wide meeting should be synthesized into a single readable report that details the expected effects of DCR on difference imaging as well as possible mitigation techniques.    This may involve some preliminary analysis work to measure effectiveness of various techniques.    Note that I expect this to be two weeks of work for two people, thus the 40 story points.  I don't know how to assign a story to two people."
1,Port suspect pixel flags to meas_base,"Pull HSC pixelFlags for suspect and suspect center over from {{meas_algorithms}} to {{meas_base}}. Additionally there are a few places in {{meas_base}} (& possibly in {{_algorithms}} as well) which set flags that have comments such as ""Set suspect flag if it was available"". Each of these places should be updated to use the ported bit. The relevant commit can be found at https://github.com/HyperSuprime-Cam/meas_algorithms/commit/21be65187c30302abb430d59fc5f67730ca7e0a1 and is discussed at https://dev.lsstcorp.org/trac/ticket/2838    It may also be necessary to add or update a unit test to make sure the flag is set appropriately."
2,Refactor ImageDifferenceTask: Split into two tasks,"This issue is to modernize the diffim task.  Once it is running it should be refactored to use the new measurement tasks and reference loading wherever possible.  Also remove any one off code used in past reports.    Measurement should be split into its own task.    The resulting task should be general, but not so general as to make it hard to run.    This issue is being split into three issues:  * DM-3704: split monolithic task into:  ** ImageDifferenceTask - creates image difference  ** ProcessDiffim - perform detection and measurement on image differences (and calexps)  * DM-5294 Refactor/Clean up new ImageDifferenceTask  * DM-5295 Refactor/Clean up new processDiffimTask"
1,fix EventLog references in ctrl_orca,"There are a couple references to EventLog in ctrl_orca, which is an object that no longer exists."
0,qserv scons - do not copy files to variant_dir,"Some people are not happy with our current scons setup which copies source files from source directories to variant_dir, it makes it harder to trace errors using tools like eclipse or debug code. Would be nice to get rid of the extra copy, but we still want to have separate build directory (variant_dir). It should be simple enough, I think, but will need some testing of course."
0,Scons build of lapack_functions in PSFex fails if SCONSFLAGS are set,"The scons build system is unaware of extra flags which may be set in SCONSFLAGS environment variable, which are used from scons utils. This will cause the build to fail. The package needs to behave properly and build in the presence of these flags"
3,Prototype DRP sequence using DECam data,"Learn the LSST stack and prototype a sequence for Data Release Production using DECam/DES data as inputs.    Assignees: Hsin-Fang Chiang, Robert Gruendl  Duration: September 2015 - February 2016"
3,Refine design specification and requirements analysis of Level 1 and Level 2 systems,"Review of existing design documentation and gathering of requirements of all Level 1 & 2 (and 3) systems. Specify both a functional and physical breakdown of the systems for long-term planning and for building the production infrastructure needed at NCSA.    Assignees: Don Petravick, Jason Alt, Paul Wefel, Steve Pietrowicz, Margaret Gelman  Duration: September - December 2015"
3,Margaret's mgmt. activities in August,Local coordination meetings  DMLT meetings  LSST2015 workshop and conference  Hiring/interviewing  Budget review and FY16 prep  Invoice re-breakouts  July TPR  etc.
2,Revisit shared scans design,NULL
3,Implement feature sets requested by SUI and DRP processing in Process Execution Framework,Extend the Process Execution Framework with feature sets requested by SUI and DRP processing.    Assignees: Matias Carrasco Kind
3,Operations planning for Archive and US DAC in TOWG/TPWG,"Develop use cases and plan for operations in the Technical Operations Working Group and Technical Proposal Working Group.    Assignees: Don Petravick, Margaret Gelman, Chris Pond, Robert Gruendl  Duration: September 2015 - January 2015"
1,Expose table metadata via metaserv,"SUI team would find it useful to get counts of columns for a table, ideally, all counts of columns for all tables in a given database in one request. They'd also find it useful if we could send column description."
3,Analyze Qserv KPIs,"Improve script for analyzing KPIs, measure and document KPIs."
3,Support operation of development infrastructure (lsst-dev and other),"Administrative support to operate the LSST development cluster and LSST's use of the NCSA OpenStack nebula. Includes planned maintenance activities and LOE/emergent work. Expect ~ 1FTE day per week.    Assignees: Bill Glick, Matt Elliot, Bruce Mather, Paul Wefel  Duration: September 2015 - February 2016"
1,Sizing model storage costing update,"The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing model, observing the results as they propagate through LDM-144, the cost model, and checking to make sure that everything makes sense and nothing has been overlooked.    Assignees: Jason Alt  Duration: September 2015"
2,Refine file system policies and services,"Refine data management policies and services (e.g., data retention, backup policies). Ideally we would have a draft of this by the November DMLT meeting.    Assignees: Jason Alt, James Parsons  Duration: October - November 2015"
1,Czar dies when parser throws exception,"Running a query that mistakenly uses scisql_s2PtInBox instead of qserv_s2PtInBox    {code}select objectId, coord_ra, coord_dec   from smm_bremerton.deepCoadd_forced_src   where scisql_s2PtInBox(coord_ra, coord_dec, 320.05, 0.457, 320.06, 0.46){code}    kills czar, the error message in czar log file is:    {code}ERROR ccontrol.UserQueryFactory (build/ccontrol/UserQueryFactory.cc:117)   - Invalid query: ParseException:Parse error(ANTLR):unexpected token:  scisql_s2PtInBox:{code}    We need to change the code so that a random query does not kill czar. This story involves fixing czar so that it does not die when parser chokes on the syntax.  "
2,"For registry-free butler, look up information in related data type.","Butler reading information (in particular the observation time and length) out of an input dataset's file representation in order to provide rendezvous with calibration data in another repository (that does have a registry). Today, that read is handled by genInputRegistry.py so that the butler doesn't need to look into the dataset itself. If there's no registry, such a read will be necessary.    The ultimate test is to run processCcd.py (a CmdLineTask that runs Instrument Signature Removal) on a repository that contains raw frames that require more than one set of calibrations and have it pick up the correct ones.  That would have to be condensed down into a unit test."
3,Add Butler access to calibration data in obs_decam,"Goals:  - Have something to ingest calibration data and create a calibration registry (calibRegistry.sqlite3)  - Add mapping class calibration into DecamMapper and make butler able to get bias/flat/fringe.  - Include calib in testdata_decam and add to the unit test retrieving calib data.    Summary:  - A new task {{ingestCalibs.py}} is added to {{pipe_tasks}} for parsing through calibration data and creating a calibration registry.  The DECam customized configuration is added in {{DecamCalibsParseTask}} in {{obs_decam}}.   {{pipe_tasks/ingest.py}} is modified slightly for more general use while its original behaviors are kept by default.   - The calibration data are DECam Community-Pipeline products, including nightly-MasterCal bias/flat downloadable from NOAO Science Archive, and fringe of the latest version 56876 from  http://www.ctio.noao.edu/noao/content/decam-calibration-files or also on /lsst8/decam/cal/DECamMasterCal_56876/  Note that files from the two sources have different formats (MEF one HDU for each detector, or one single-HDU fits for each detector).  - New Butler dataset types bias/flat/fringe are added along with functions to standardize them to Exposures in {{DecamMapper}}.  - Tests of retrieving bias/flat/fringe by Butler are added.    - testdata_decam used by the unit tests is at lsst-dev:/lsst8/testdata_decam/    Known caveats for future users  - The task could be a bit noisy when ingesting DECam calibration data products.  But given the variety of data products on hand I might rather have those reminders than letting all pass silently.      - I wouldn't be too surprised if future DECam Community-Pipeline products appear in different formats. Depending on how different they become, {{DecamCalibsParseTask}} might or might not need future modifications.   "
1,Resolve the issues found in the S15 end-to-end system exercise,There are a few items we need to take care to finish the end-to-end system for S15. 
0,access the database created and populated for Bremerton end-to-end system,Collect the information for the tables populated for Bremerton end-to-end exercise. Use them in SUI/T so we can access them using the DAX API. 
0,build the SUI system on NCSA to use the right database and tables,"Due to the changes of the database and tables, the system has to be rebuilt."
1,Resolve the issues accessing the newly populated tables,There are several issues need to be resolved for the system to work properly. 
0,Fix compiler detection for non-default gcc/g++ compiler,{{scons CXX=g+\+-4.4}} launches {{g\+\+-4.4 --version}} which returns {{g++-4.4 (Debian 4.4.7-2) 4.4.7}}. Nevertheless the {{-4.4}} is not supported by Qserv compiler detection tool. Support will be added here
1,add RUNID option to EventAppender,A RUNID needs to be added as an option to EventAppender to allow event logging selectors to receive only events for a particular run.
0,lsst_build's default ref from repos.yaml support is broken when building multiple packages,"A problem with the default ref in {{repos.yaml}} support implemented in DM-3679 was discovered last Friday, shortly after deploying this feature to the production CI systems.    The default ref for {{xrootd}} was changed/overridden in {{repos.yaml}} to {{legacy/master}}.  This worked as expected (and as was tested) when setting {{xrootd}} as the sole {{lsstswBuild.sh}} product or when running {{rebuild}} by hand.  However, when building any package that pulled in {{xrootd}} as a recursive dependency, the {{master}} branch was being used (this case had not been manually tested)."
0,HSC backport: updates to tract and patch finding,This is a port of the following HSC updates to how tracts and patches are found and listed given a set of coordinates.  These are all standalone commits (i.e. not associated with a ticket):  [Add findTract() and findTractPatchList() in ringsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/761e915dde25ce8ed5622c2d84b83793e9580fd7]  [move RingsSkyMap.findTractPatchList to BaseSkyMap.findClosestTractPatchlist|https://github.com/HyperSuprime-Cam/skymap/commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b]  [Small bug fix for RingsSkyMap.findTract().|https://github.com/HyperSuprime-Cam/skymap/commit/f202a7780ebb89166f03479d7447ace1555027c1]  [Add fast findTractPatchList() in RingsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22]  [Fixed the problems regarding poles and RA wrap.|https://github.com/HyperSuprime-Cam/skymap/commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe]  [Add spaces around '+' and '-' to match LSST standard coding style.|https://github.com/HyperSuprime-Cam/skymap/commit/f7e2f036494afe382e653194c82bb15728c60fc3]
1,LDM-144 Consistency Update,"Due to the 2 year gap between the original authoring date of LDM-144 and the recent update,  the 'costing only' update caused the document to be less self consistent than desired. This work is to do more than 'costing' updates such that the document is useful and on target to be more than a costing umbrella."
2,server side preparation for  histogram plot (2),"1. On the initialization, server needs to return the following summary table for all numeric columns:       a. column name / description / unit      b. min and max values      c. number of points    2. For a given column and binning options, return the table of bins:     a. first column - numInBin - number of points in a bin     b. second column - binMin - bin's lower bound     c. third column - binMax - bin's upper bound     [10-13-15] Added by LZ  Here is the detailed requirement from Tatiana:  The implementation of this ticket are two search processors (similar to IpacTableFromSource search processor). Both search processors should be located in edu.caltech.ipac.firefly.server.query package and produce an IPAC Table.     1.  Getting table statistics    INPUT:  The input to the first search processor should be TableServerRequest (treq) with ""searchRequest"" parameter set. The searchRequest parameter will be a serialized JSON string, which can represent a search request to any other search processor. This  parameter determines how the input IPAC table is  obtained. (The input IPAC table is produced similarly to how IpacTableFromSource produces the result with ""processor"" parameter set.)    To parse SearchRequest parameter into key-value pairs - the parameters for the search request - you can use json.simple library:    JSONObject searchRequestJSON = (JSONObject)JSONValue.parse(treq.get(""searchRequest""));  if (searchRequestJSON != null) {      for (Object param : searchRequestJSON.keySet()) {          String name = (String)param;          String value = (String)searchRequestJSON.get(param);          }     }  One of the parameters should be the ""id"" (ServerReuqest.ID_KEY) - which tells which search processor to use to obtain input IPAC table.    OUTPUT:  The output IPAC table produced by the first search processor should contain 5 columns: columnName, description, unit (unit or empty string), min, max, numPoints (the number of non-null values).    2.  Getting histogram data    INPUT:  The input to the second processor should be TableServerRequest (treq) with ""searchRequest"" parameter set (exactly the same as ""searchRequest"" parameter in item 1) plus binningOptions. The binningOptions parameter will be serialized JSON object with the following keys (object properties):  {  ""algorithm"" : ""fixedSizeBins"",          ""binSize"" : 20,          ""column"" : ""wavelength"",          ""scale"" : ""linear"",          ""min"" : 0,          ""max"" :100  }    Notes:   -the algorithm is simple fixed size binning for now, in future we'll need to add variable size bins algorithm described here: http://www.astroml.org/examples/algorithms/plot_bayesian_blocks.html  - at the beginning, you can assume that column is column name, but it can be an expression based on several columns.   - scale can be ""linear"" or ""log""  - min and max define filter on the points included into histogram calculation, any of them or both can be absent. If both are absent, no filter needs to be applied.    OUTPUT:  The output IPAC table produced by the second search processor should contain 3 columns:  numInBin, binMin, binMax in that order. The table should be sorted by binMin. The binMax of row i should be always less or equal to binMin of row i+1. If number of points in the bin is 0, it's OK to skip this bin.    "
0,Fix compiler warns in protobuf clients,Google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages.    Proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.
0,clean up gcc and eclipse code analyzer warns,"We've been ignoring some accumulating warns in the qserv build for some time now.  Now that it is possible to develop qserv in eclipse, it would be useful to address warns and analyzer issues so that we can start to notice when new ones pop up."
0,Rationalize lsst/xrootd repo and maintenance procedures,"The procedure for pulling/pushing xrootd changes from/to the upstream official xrootd repo is cumbersome, confusing, and error-prone.    Buildbot now has support for releasing packages from branches other than master.  Given this, we can now reasonably replace our lsst/xrootd repo with a fresh genuine fork (shared history) of upstream, then carry our lsst-specific work forward on a dev-branch.  This will make it much easier to track and contribute to the xrootd project moving forward.    Existing legacy branches and tags are to be migrated to the fresh fork, so historical builds will not be broken."
1,"Review, plan, procure development infrastructure (FY15)",NULL
0,Refining file system policies,NULL
3,W16 Operation of Joint Coordination Council,"Activities associated with implementation of the MOA establishing CC-IN2P3 as a satellite processing center during operations. Includes preparation and execution of monthly JCC meetings, as well as a face-to-face meeting for intense coordination scheduled for early November.    Assignees: Don Petravick, Margaret Gelman, Jason Alt, Robert Gruendl  Duration: September 2015 - February 2016"
3,W16 processing control emergent work,"Bucket epic for bug fixes and *minor* work that falls outside planned epics.    Assignees: Steve Pietrowicz, Greg Daues, Matias Carrasco Kind, Hsin-Fang Chiang, James Parsons  Duration: September 2015 - February 2016"
3,Update sizing model for February 2016 refresh - technology and costing,"Biannual refresh of Sizing Model (LDM-144 et al.), including updates to both costing and technology. As it will be the first technology refresh in at least 2 years, we anticipate it will take a considerable amount of time. To prepare and gain insight on technology and costing trends, this activity includes attending SC2015 in mid-Nov.    Assignees: Jason Alt  Duration: November 2015 - January 2015"
3,Liaise all groups to commission OpenStack for LSST,Greg has been appointed the service manager of the NCSA nebula for LSST. Work with LSST developers and NCSA system engineers to commission the OpenStack for LSST's use.    Assignees: Greg Daues  Duration: September 2015 - February 2016
2,FY16 Hardware Purchasing Plan,"The Annual Acquisition Strategy Document describes the capabilities (hardware, compute cycles, software, licenses, etc.) planned for procurement during the fiscal year. We consider systems that will satisfy the needs of developers, systems for prototyping the production infrastructure, etc.    Assignees: Jason Alt, Bill Glick  Duration: September - October 2015"
1,Evaluate PASTRY DHT implementation,The David Keller kademlia implementation used in the earlier prototype has some bugs/limitations.  Try to find a better off-the-shelf DHT and integrate with prototype framework.
0,obs_test data mis-assembled,obs_test images are mis-assembled and need to be regenerated. This may affect some existing unit tests that rely on the data.
0,(FY16) Initial discussions and requirement consolidation,NULL
0,remove install_name_tool fix to libpython2.7.dylib from anaconda package,"Now that SIM-1314 has been merged, we should be able to remove the    {code}  	if [[ $(uname -s) = Darwin* ]]; then  		#run install_name_tool on all of the libpythonX.X.dylib dynamic  		#libraries in anaconda  		for entry in $PREFIX/lib/libpython*.dylib  		do  			install_name_tool -id $entry $entry  		done  	fi  {code}    from eupspkg.cfg.sh in the EUPS anaconda package, and still have GalSim build correctly"
0,Enable SSL to community.lsst.org,Enable SSL (https) for the Discourse site at community.lsst.org
0,Update flag names and config override files to current conventions,The {{deblend.masked}} and {{deblend.blendedness}} flag names in {{meas_deblender}} need to be updated to use underscores instead of periods.  Various flag names in the {{examples}} scripts also need updating to the underscore and camelCase format.    A search for these flags throughout the database revealed a number of config files that need updating to current conventions.  These are also included here.
0,testProcessCcd.py computes values that are too different between MacOS and linux,"tests/testProcessCcd.py runs processCcd on visit 1 of obs_test's data repository. The result on MacOS is surprisingly different than on linux in at least one case: psfShape.getIxx() computes 2.71 on MacOS X and 2.65 on linux. Iyy and Ixy are likely different. It's worth checking all other computed values, as well. These differences likely indicate that something is wrong, e.g. in obs_test, processCcd, or the way the test runs processCcd.    This showed up as part of fixing DM-3792, but it is not clear if the changes on DM-3792 actually caused or increased the difference between MacOS and linux, or if the difference was always too large, but was masked by an intentionally generous tolerance in the unit test."
1,The gains in obs_test's amplifier table appear to be incorrect,"As of DM-3792 the gains in obs_test's camera's amplifier table were set to the values reported in the headers of the lsstSim raw data used to generate obs_test's raw data. (Before that one nominal gain was used for all amplifiers).    However, [~rhl] reports that these gains are incorrect. He measured the following gains by scaling the nominal gains by the median values in the bias-subtracted data:  {code}  amp   meas      curr  name  gain      gain  00    1.7741    1.7741  01    1.8998    1.65881  10    1.8130    1.74151  11    1.8903    1.67073  {code}    We could simply adopt these values, but I would like to understand why the gains are so far off from those reported by phoSim in the raw data."
0,"The obs_test's sensor is shown 90 degrees rotated from that desired, in camera coords","When plotting the obs_test sensor, e.g. using lsst.afw.cameraGeom.utils.plotFocalPlane, the image is a short, wide rectangle. This suggests that the camera coordinate frame is rotated 90 degrees from the CCD coordinate frame (which has 1018 pixels in X and 2000 pixels in Y).    We would prefer that the camera frame and CCD frame have the same orientation."
0,Fix Qserv compiler warnings with clang,"Qserv triggers numerous warnings with clang on OS X. Full details are in the attached ticket, here we summarize the distinct warnings classes:    h5. Protobuf    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/protobuf/2.6.1+fbf04ba888/include/google/protobuf/unknown_field_set.h:214:13: warning: anonymous types declared in an anonymous union        are an extension [-Wnested-anon-types]      mutable union {              ^  {code}    h5. Qserv    {code}  In file included from core/modules/sql/statement.cc:32:  core/modules/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Schema {  ^  core/modules/sql/statement.h:35:1: note: did you mean struct here?  class Schema; // Forward  ^~~~~  struct  {code}    {code}  core/modules/proto/WorkerResponse.h:34:1: warning: 'WorkerResponse' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct WorkerResponse {  ^  core/modules/ccontrol/MergingRequester.h:38:3: note: did you mean struct here?    class WorkerResponse;    ^~~~~    struct  {code}    {code}  In file included from core/modules/qana/QueryMapping.cc:46:  core/modules/qproc/ChunkSpec.h:51:1: warning: 'ChunkSpec' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChunkSpec {  ^  core/modules/qana/QueryMapping.h:44:5: note: did you mean struct here?      class ChunkSpec;      ^~~~~      struct  {code}    {code}  core/modules/qana/TableInfo.h:186:1: warning: 'DirTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct DirTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:86:1: note: did you mean struct here?  class DirTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:221:1: warning: 'ChildTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChildTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:87:1: note: did you mean struct here?  class ChildTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:260:1: warning: 'MatchTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct MatchTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:88:1: note: did you mean struct here?  class MatchTableInfo;  ^~~~~  struct  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:513:1: warning: struct 'Vertex' was previously declared as a class [-Wmismatched-tags]  struct Vertex;  ^  core/modules/qana/ColumnVertexMap.h:44:7: note: previous use is here  class Vertex;        ^  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:547:1: warning: 'Vertex' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Vertex {  ^  core/modules/qana/ColumnVertexMap.h:44:1: note: did you mean struct here?  class Vertex;  ^~~~~  struct  {code}    {code}  core/modules/wbase/Base.h:72:1: warning: 'ScriptMeta' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ScriptMeta {  ^  core/modules/wbase/Task.h:41:5: note: did you mean struct here?      class ScriptMeta;      ^~~~~      struct  {code}    {code}  In file included from core/modules/parser/BoolTermFactory.cc:46:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/FromFactory.cc:62:15: warning: unused function 'walkToSiblingBefore' [-Wunused-function]  inline RefAST walkToSiblingBefore(RefAST node, int typeId) {                ^  core/modules/parser/FromFactory.cc:72:1: warning: unused function 'getSiblingStringBounded' [-Wunused-function]  getSiblingStringBounded(RefAST left, RefAST right) {  ^  {code}    {code}  In file included from core/modules/wsched/ChunkDisk.cc:25:  core/modules/wsched/ChunkDisk.h:130:10: warning: private field '_completed' is not used [-Wunused-private-field]      bool _completed;           ^  {code}    {code}  In file included from core/modules/parser/PredicateFactory.cc:45:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/WhereFactory.cc:265:31: warning: binding reference member 'c' to stack allocated parameter 'c_' [-Wdangling-field]      PrintExcept(Check c_) : c(c_) {}                                ^~  core/modules/parser/WhereFactory.cc:291:28: note: in instantiation of member function 'lsst::qserv::parser::PrintExcept<lsst::qserv::parser::MetaCheck>::PrintExcept' requested        here      PrintExcept<MetaCheck> p(mc);                             ^  core/modules/parser/WhereFactory.cc:269:12: note: reference member declared here      Check& c;             ^  {code}    {code}  core/modules/rproc/ProtoRowBuffer.cc:44:11: warning: unused variable 'largeRowThreshold' [-Wunused-const-variable]  int const largeRowThreshold = 500*1024;            ^  {code}    {code}  core/modules/util/testIterableFormatter.cc:85:43: warning: suggest braces around initialization of subobject [-Wmissing-braces]      std::array<std::string, 6> iterable { ""1"", ""2"", ""3"", ""4"", ""5"", ""6""};                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                                            {                           }  {code}    {code}  In file included from core/modules/qdisp/XrdSsiMocks.cc:37:  core/modules/qdisp/XrdSsiMocks.h:64:16: warning: private field '_executive' is not used [-Wunused-private-field]      Executive *_executive;                 ^  {code}    {code}  core/modules/xrdoss/QservOss.cc:77:1: warning: unused function 'print' [-Wunused-function]  print(std::ostream& os, lsst::qserv::xrdoss::QservOss::StringSet const& h) {  ^  {code}    h5. OS X    {code}  core/modules/qdisp/QueryRequest.h:54:25: warning: 'lsst::qserv::qdisp::BadResponseError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:37:  core/modules/qdisp/QueryRequest.h:67:25: warning: 'lsst::qserv::qdisp::RequestError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  {code}    {code}  core/modules/proto/TaskMsgDigest.cc:55:5: warning: 'MD5' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      MD5(reinterpret_cast<unsigned char const*>(str.data()),      ^  /usr/include/openssl/md5.h:116:16: note: 'MD5' has been explicitly marked deprecated here  unsigned char *MD5(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    {code}  core/modules/util/StringHash.cc:78:24: warning: 'SHA1' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA1, SHA_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:124:16: note: 'SHA1' has been explicitly marked deprecated here  unsigned char *SHA1(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  core/modules/util/StringHash.cc:83:24: warning: 'SHA256' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA256, SHA256_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:150:16: note: 'SHA256' has been explicitly marked deprecated here  unsigned char *SHA256(const unsigned char *d, size_t n,unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}    h5. boost    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.55.0.1.lsst2+fbf04ba888/include/boost/regex/v4/regex_raw_buffer.hpp:132:7: warning: 'register' storage class specifier is        deprecated [-Wdeprecated-register]        register pointer result = end;        ^~~~~~~~~  {code}"
0,Fix order of arguments change in meas_base SingleFrameMeasurement,"In sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from meas_algorithms to meas_base is complete. This work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. See https://github.com/lsst/meas_base/blob/tickets/DM-2915/python/lsst/meas/base/sfm.py#L271"
0,convert newinstall.sh to use miniconda instead of anaconda,To match the conversion of lsstsw from anaconda -> miniconda to reduce the disk footprint and improve install times.
0,Setup lsst_sphinx_kit package structure,"Setup the lsst_sphinx_kit package, including    * setup.py  * unit tests, tox and Travis CI  * README stub  * Sphinx stub and readthedocs"
1,HSC backport: Include documentation strings for config parameters when they are dumped,This is a port of the following HSC tickets:  [HSC-1072|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1072]  and  [HSC-1175|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1175]
0,Migrate LDM-152 to reST Design Doc Platform,NULL
1,Intermittent build failures on v11 candidate with eups distrib,"We are seeing frequent intermittent failures on a variety of platforms when installing the v11 candidate with eups distrib install. Repeating the command works. It doesn't seem to be evenly distributed between packages: cfitsio, -meas_astrom-(?), meas_algorithms, and obs_lsstSim have been seen multiple times. It's been seen in the release verification CI (that uses the documented user facing process of eups distrib install instead of the factory CI which uses lsstsw) and in individual user ""manual"" builds on lsst-dev.     Test build key:  ||buildbot build # || eups tag || refs || comment ||  ||b1688 || t20150914-b1688 || tickets/DM-3829 v11_0_rc2 || ||  ||b1689 || t20150914-b1689 || tickets/DM-3829 tickets/DM-3815-intermittent-build-failure v11_0_rc2 || ||  ||b1690 || t20150914-b1690 || tickets/DM-3829 v11_0_rc2 master || modifications to tickets/DM-3829 ||  ||b1692 || t20150915-b1692 || tickets/DM-3829 ticket/DM-2752-egg-error v11_0_rc2 || ||"
0,levels in DecamMapper.paf is not quite right,"When ccdnum is not given as part of the dataId, instead of iterating over it, an error like this happens    {code:java}    RuntimeError: No unique lookup for ['ccdnum'] from {'visit': 205344}: 61 matches  {code}    Likely a problem in policy/DecamMapper.paf"
1,Deploy docker images on ccqserv124/149,"Creation of configured master and worker image will be improved here, and a deploymen tool (like swarm, of hand-made) will be used to deploy images over in2p3 cluster."
0,Bi-weekly PO security meeting,Bi-weekly meeting with PO on cyber security.
0,IdM work,Work for LSST Identity management and authentication.
0,NIST SP 800.82 investigation,NIST SP 800.82 investigation for a more cohesive SCADA enclave security plan.
0,Recent CModel bugfixes from HSC,"I've just fixed two rather critical bugs in the CModel code on the HSC side (they would have been introduced on the LSST side in the last transfer, DM-2977):   - The {{minInitialRadius}} configuration parameter had a default that is too small, causing many galaxies to be fit with point source models, leading to bad star/galaxy classifications.  This is HSC-1306.   - There was a simple but important algebra error in the uncertainty calculation, making the uncertainty a strong function of magnitude.  This is HSC-1313.    On the LSST side, the transfer should be quite simple; we'll have to rewrite a bit of code due to the difference in measurement frameworks, but there was very little to begin with (most of the effort in the HSC issues was in debugging)."
3,Orchestration work to support verification dataset processing,"Upgrades, modifications, fixes, etc. to orchestration framework for use in SQuaRE's verification dataset processing tests.    Assignees: Steve Pietrowicz, Greg Daues  Duration: September 2015 - February 2016"
3,W16 General Management Activities,NULL
1,meas_astrom bugs exposed by new Eigen,"Trying a newer Eigen has exposed several issues in meas_astrom:  - tests/createWcsWithSip.py blindly uses sipWcs in the result returned by ANetBasicAstrometryTask.determineWcs2, but that attribute may be None  - ANetBasicAstrometryTask.determineWcs2 terminates iteration early if the # of matches goes down, even though the result may be improved. In the case in question the first fit iteration results in significantly better RMS error, but has one fewer matches, so the SIP fit is rejected, triggering the first bug mentioned."
0,"write meeting agenda, for Sept 14 meeting ",NULL
2,Further refinement ,added the image and engineering facility database.  and Observatory Operations Server.
0,"email discussion w.r.t Service separation for L1, and also some work on ITIL roles","email to German about the all the L1 stuff and clarified that the L1 system were a  derive provided to the Telescopes site (important for fitting this into the proper place in the who is worrying about what hierarchy.  Also, revised SA to see fi the EPO changes discusses affected the IT roles in th model (not apparently)  lastly attitude the TOWC "
0,Management for week ending sept 11,"Deal with Hiring James Parsons, and interfacing with the new NCSA organizations that will support LSST at NCSA, general group management issues"
3,Deploy FY16 Storage Expansion,"Install, and deploy a prototype production GPFS cluster."
1,Preparation work to process raw DECam data,Try to run processCcd.py with raw DECam data and see what are yet to be solved for it to run. 
3,Deploy FY16 Nebula Expansion,"* Install, test, and deploy additional capacity for the NCSA nebula.    (Note: this work occurs outside of LSST so we can only track it at a less granular detail than other deployments. We can include the final story for blockers however.)"
3,Decommission old development infrastructure,"Decommission old hardware currently in the LSST development cluster when replacement equipment arrives and is provisioned.    Assignees: Bill Glick, Matt Elliot, Bruce Mather  Duration: November - December 2015"
0,Migrate LDM-230 to new docs platform,Convert LDM-230 from Word to restructuredText and deploy on readthedocs.org
1,Migrate LDM-135 to new design docs platform,Convert LDM-135 from Word to restructuredText and deploy on readthedocs.org
0,Migrate LDM-129 to new design docs platform,Convert LDM-129 from Word to restructuredText and deploy onto readthedocs.org
2,SuperTask Redesign,Redesign pipe_base to allow the creation of supertasks which will be more flexible for  different execution applications
3,Evaluate authentication and authorization services for user workspace,"Develop an Identity and Access Management (IAM) program.    Deliverables (from SOW):  - LSST IAM Design Document: describe LSST's current and expected IAM needs and specify technical recommendations for the LSST IAM system architecture, including interface standards (e.g., LDAP, OAuth) and system components (with functional descriptions and implementation recommendations).  - LSST IAM Program Plan: specify future and ongoing IAM development and operational activities required to meet the LSST project mission.  - LSST IAM Technical Demonstration: provide an initial implementation of the LSST IAM design according to the philosophy of ""rough consensus and running code."" The project will prioritize technical implementation work based on the immediate needs identified by LSST developers for functional implementations of IAM system interfaces to ""fill the gaps"" needed for LSST development and integration work to proceed on schedule.    Assignees: Jim Basney, Terry Fleurry, Daniel Thayer, Alex Withers (ISO), Don Petravick, Jason Alt, Margaret Gelman  Duration: October 2015 - March 2016"
0,Graphical representation Example demo,NULL
1,LSE-72: Phase 3 in X16,Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during F16.
1,LSE-75: Refine WCS and PSF requirements in W16,Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.    Depends on the ability of the T&S group to engage with this subject.    Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.
1,LSE-68: ICD Details in X16,Bring ICD to phase 3 level of detail
1,LSE-74: ICD Details in W16,"""Bring ICD to phase 3 level of detail"" was the original specification, but actual work by the OCS group in the Winter 2016 period didn't quite reach Phase 3.  Nonetheless, a very useful revision was submitted to and recommended for approval by the CCB at the March meeting."
1,Add tutorial-level documentation for ctrl_pool,"The new ctrl_pool package (port of hscPipeBase) has no tutorial-level documentation, making it hard to figure out how to start using the package.    Unfortunately, I think only [~price] is qualified to write it directly, though it may make sense to have someone unfamiliar write it while bugging Paul a lot, both to transfer the knowledge and target the documentation better."
1,Add unit tests for ctrl_pool,"ctrl_pool (formerly hscPipeBase) is being ported with no unit tests - the only testing is an example script that can be run by hand to demonstrate a piece of the functionality.    Some functionality may simple not be amenable to tests (such as batch submission).  Other parts may be tricky to run via SCons because they're intrinsically parallel, and SCons naively wants to be able to run each test in a separate process.  Overcoming those problems is the reason this is challenging - there isn't really that much functionality to test."
0,Read over LSE-70 and LSE-209 and discuss for meeting,Read over LSE-70 and LSE-209 for meeting on Friday 9/11.
1,Grid overly bug,"when using the grid overlay in galatic coordinate over an image that is around longitude = 0, the overlay doesn't work properly. In BOLOCAM, on fits image works but not the other one.    Reproducible:	  Steps to Reproduce:	  Go to Atlas search, and select BOLOCAM galactic plane survey (GPS).  Then enter single coordinate search on ""0 0 gal"" to find images taken around that.     See column ""FITS filename"" and search for the sharc-ii instrument images such as  images/sharc2/L000.15+0.00.fits [bad]   and   images/sharc2/L000.00+0.00.fits [good]     Then open it on irsaviewer by clicking on the icon link (first column).  On the image viewer, enable the grid overlay and click on the icon 'layer' on the toolbar.  Change the coordinate system to galactic to see the problem.     On the overlay for the associated Bolocam map there are three horizontal lines which exist only on the left hand side of the image. When the image coordinate system is set to ""Gal"", the reported GLON values range from -355.4 to -1.6 (from left to right), passing through -0.0 in the center.     Same problem can be seen for   images/v1/INNER_GALAXY/map/v1.0.2_super_gc_13pca_map50_crop.fits [bad]  "
0,evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 2,NULL
0,Nebula metadata service is intermittent,"Upon restarting one of my nebula instances (ktl-test), I noticed a failure in the logs:  {quote}  Sep 14 18:07:29 ktl-test cloud-init: 2015-09-14 18:07:29,157 - util.py[WARNING]: Failed fetching metadata from url http://169.254.169.254/latest/meta-data  {quote}    Attempting to retrieve that URL seems to randomly vary between succeeding, which returns:  {quote}  ami-id  ami-launch-index  ami-manifest-path  [...]  {quote}    and failing, which returns:  {quote}  <html>   <head>    <title>500 Internal Server Error</title>   </head>   <body>    <h1>500 Internal Server Error</h1>    Remote metadata server experienced an internal server error.<br /><br />         </body>  </html>  {quote}  These failures may be contributing to observed sporadic {{ssh}} key injection failures."
1,Liaise with Long-Haul Network group on Base Site to NCSA network,Coordinate with network group to establish Base Site-to-NCSA LHN.    Assignees: Paul Wefel  Duration: September 2015 - February 2016
3,Evaluate file management technologies,"Evaluate technologies for data management (e.g., iRODS)."
2,Consult on contracting and execution of Chilean Data Center contract,Continue to consult and review design documents for the construction of the Chilean DAC.    Assignees: Tom Durbin  Duration: September 2015 - February 2016
3,Calibration products preparation,Develop the calibration products pipeline plan and begin initial implementation.
3,Stack documentation Part I,Original documentation based on expressed team needs.     [JS 100%]    
3, Integration and test monitoring architecture Part II,Work to be assessed on the basis of the outcome of Part I   [DM-2050]    [100% JMP]        
3,Supertask design and prototyping - Part I,"This epic covers design and prototyping work for an encapsulation of  tasks that allows for the chaining and interleaving of tasks with  QA/metrics tasks, intergration tests and/or KPM/analysis  afterburners. The emphasis at this point is on speed of development  and customisation; performance is assumed not to be an issue as it  will eventually be guaranteed by the process execution framework.     Only SQuaRE's effort is covered by this epic; additional effort on  this investigation is provided by the Middleware WBS and not captured  here.     The outcome of this epic is a proposal for moving forward. In the  event that it is acceptable to the Project Engineer, further epics  might be define, hence Part I.    [100% GPDF] "
3,Communication Toolchain support,"This epic covers support of communication tools primarily used by DM  and/or supported by DM on behalf of other parts of the project - JIRA,  Discourse, Hipchat, etc     The source of this work is primarily driven by short-term user  requests, and so the outcome is timeboxed rather than planned.     [JS 50% FE 50%]  "
2,QA Architecture Documents,Design document outlining SQuaSH elements and implementation plan.    [FE 100%]
3,"Stack Build, Packaging and Testing Improvements Part II",This epic is an umbrella for RFC-69 work.     [JH 90% JS 10%]  
0,Web design fixes DM Design Documents on Sphinx/Read The Docs,"Solve fit-and-finish issues with the stock readthedocs.org Sphinx template when rendering DM design documents. Issues include:    * Sections need to be numbered and those numbers need to appear in TOC  * RTD's TOC does not properly collapse sub-topics  * Appropriate styling for document title and author list  * Wrapping the changelog table  * Adapt section references so that just the section number can be referenced, independently of the section number and title in combination  * Section labels given explicitly in the reST markup are different from the anchors that Sphinx gives to the {{<hN>}}tags; the former are simply divs inserted in the HTML.    The solutions may involve    # reconfiguring the Sphinx installation of individual documents  # forking the RTD HTML template, and/or  # developing extensions for Sphinx in {{sphinxkit}}."
3, Integration Dataset for metrics and regression tests - Part I ,  This epic covers the defintion of a compact but rich dataset to  support regression testing and surrogate metric development for  regular automatic integration tests. It also involes a definition of  interim/surrogate metrics where they can aid testing and development  and/or where KMPs cannot be calculated.    [DN 75% MWV 25%]
3, Processing of DECAM and other Verification Datasets,"This epic covers work to lead and co-ordinate the processing of  precursor datasets, DECAM in the first instance, through the stack and  produce an assessmment of progress and preliminary metrics. In W16  execution will be done using the orca/HTCondor setup previously used  at NCSA for Data Challenges.    [DN 50% AF 25% JS 25%]      "
1,Do basic tests of CModel ellipticity measurements,"I have seen enough anomalies that I have had to go back a bit and do some basic tests of CModel and its ellipticity bias.  This involves running the same pipeline as before, but with controlled galaxy profiles, ellipticities, and angles.  These are zero-shear, zero-seeing tests which I probably should have run first thing.    It I understand everything I see in these tests, I will be confident that the results I am seeing for CModel with varying footprint size, varying nInitialRadii, and varying stamp size are correct."
2,Simultaneous astrometry requirements,The HSC group needs improvements to the simultaneous astrometry fitter delivered by Astier et al.  In particular we'll need to do simultaneous photometry as well.  This epic is to determine the superset requirements for such a system.
3,Refactor Jointcal to use stack functionality,"Jointcal currently has a lot of built-in features that already exist elsewhere in the stack (e.g. GTransfo, StarSelector, Points, etc.). These features should be removed and replaced with the equivalent stack functionality."
3,Implement simultaneous photometry,The simultaneous astrometry framework should be able to be extended to also fit the photometry at the same time.  This task is to do just that.  The task to create a pluggable framework should help with this.
3,Clean up Wcs classes,The Wcs classes as they currently exist are not easy to extend and also contains overrides that are a bit ad hoc.  This task is to clean up the existing classes so that there is a single abstract base class.  It should also be a priority of this task to determine whether an upgrade in wcslib helps with the special cases (e.g. TAN-SIP).
3,Gather requirements for improved Wcs classes,The new Wcs class should be able to apply transforms in a stack so that many different distortions as different scales can be corrected for separately (rather than trying to correct the whole mess with a single 2D polynomial).  There is some work going on in the community around this.  This task should include conversations with community leaders in this area.
3,Produce a design for the new Wcs classes,This will require generating a design as well as getting it reviewed via RFC.  The hope is that other work can be ongoing while the RFC process is carried out.
3,Implement new Wcs classes,"Once a design is accepted implement the new design. This epic covers the replacement of afw:image:wcs with afw:transform/afw:mapping (names currently in flux), but does not involve changes to XYTransform."
3,Make Wcs persistable,"Regardless of the mechanism used by the new Wcs classes, they will need to be persistable.  This will probably require some significant work unless classes that are already persistable are used in the design."
2,Identify all corrections ISR needs to handle,"This is informed by DECAM etc.  This is just for the corrections that do not require detection.  For example, we will push out CR rejection until the question of snaps is decided."
3,Implement all ISR corrections for LSST,Most corrections have some implementation.  Perhaps the most difficult will be crosstalk since the implementation should allow for correction over multiple chips.
2,Make up a test for dipole measurement,"To facilitate work on the dipole measurement, a test will be helpful.  This could simply be an image with dipoles made with Gaussian PSFs with Poisson noise on top.  The important thing is to be able to determine whether the algorithm is returning the right answer."
3,Improve the dipole measurement task,The current dipole measurement algorithm has some issues.  It also doesn't work in the current measurement framework.  This task is to improve the dipole measurement.  The result of this epic should be improved results when run on the test data.
0,Gather requirements to inform a redesign of the CalibrateTask,The current calibrate task is fairly brittle and hard to extend.   This task is to gather the necessary requirements for a redesigned calibrate task.
3,Implement the new CalibrateTask design,Implement the redesigned CalibrateTask.
0,"Create initial cluster design, send internally for feedback and planning",Gather feedback on initial designs for FY16 purchase plans.
1,Create data products description,Addition of [https://confluence.lsstcorp.org/display/~petravick/Products+of+Image+Ingest+and+Processing] to understand more of the requirements necessary for the functional design  
1,"LSE-78: W16 revisions, harmonization with existing design",Review LSE-78 for self-consistency and consistency with the current DM and overall system design.
1,"Revise early integration milestones, LCR-323 and beyond","Revise the list of early integration milestones with OCS, TCS, CCS, and DAQ to form a coherent plan.  Coordinate with NCSA and other interested parties in DM."
1,Review ICD flowdown to DMSR and design documents,NULL
0,Add missing space after if in Qserv code to conform to standard,"Replace ""if("" with ""if ("" to follow standard.    find core/modules/ -name ""*.cc"" |xargs grep ""if(""|wc -l  852    "
1,Review LCR-323 proposal for integration milestones,Prepare for CCB action on LCR-323.  Ensure that DAQ integration is included (it's not in the original LCR proposal).
1,"Review current version of LSE-78, prepare for LCR",Do a comprehensive read-through of the previous released version of LSE-78.  Look for self-consistency and for consistency with the rest of the DM and overall system design.  Report issues to appropriate people.
1,Research existing DHT-based FS approaches ,The previous prototype provided confidence that a DHT overlay could work for routing and placement at the scales and time constants needed for chunk distribution.  The next prototype will need actual data transport and storage management facilities layered on the node/key management provided by the DHT layer.  Explore existing works at this level to a greater depth pick from among proven approaches.
0,Provide values for relative astrometry KPMs in FY15,"Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
0,Provide values for PSF ellipticity KPMs in FY15,"Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
1,Provide values for photometric repeatability KPMs in FY15," Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
1,Provide value for DRP computational budget KPM in FY15,"Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
0,Fix xrootd compiler warnings with clang,"h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}"
0,support shared_ptr<Statistics>,"I would like to write some functions that return afw::math::Statistics objects and wrap them with SWIG. Unfortunately SWIG requires that any object returned by value must have a default constructor, and Statistics does not. Rather than try to add such an object I propose to make my functions return a shared_ptr to Statistics.    This ticket is a request to support that by adding the following to statistics.i:   {code}  %shared_ptr(lsst::afw::math::Statistics);  {code}  "
1,Review of [DM-2983],I was asked for a revision of [DM-2983] which is part of the Backport HSC parallelization code
1,Update some tests to support nose and/or py.test,When {{sconsUtils}} is migrated to use {{nose}} or {{py.test}} some test scripts will need to be modified because test discovery will be slightly different and the namespace of test execution will change.    Two things to consider:  * People would still like the option of running a test as {{python tests/testMe.py}}.  * We have to work out how to run the memory test case.  
0,Fix protobuf compiler warning with clang,NULL
0,Base Site Data Access Center Description Page,https://confluence.lsstcorp.org/display/~petravick/Data+Access+Center
1,Gathering use cases for verification data sets,Seeking out developer use cases of incoming data sets. Need to determine if datasets will be accessed for verification only or by developers and QA in general. Determine access methods. 
2,"Future infrastructure, common IT, and facility planning","Includes consulting on developing use cases for Base Site Commissioning Cluster    Jason Alt, Paul Wefel, Tom Durbin"
2,Specify FY15 Equipment Purchasing Plan,"The hardware contract was finalized at the end of July 2015, leaving 2 months of FY15 for spending the fiscal year hardware budget. In lieu of an annual acquisition strategy document, we draft a one-off FY15 purchasing plan.    Assignees: Jason Alt, Bill Glick, Paul Wefel  Duration: September 2015"
3,W16 Work on Alert Production Simulator,Continued development of Alert Production simulator.    Assignees: Steve Pietrowicz  Duration: September 2015 - February 2016
3,W16 Work on OCS Software Integration,"Integrate the OCS Software, delivered from the Camera Team, into AP.    Assignees: Steve Pietrowicz  Duration: November 2015 - February 2016"
1,Run and document multi-node test with docker,"In order to validate Docker setup on CC-IN2P3 cluster, it is required to launch some test on consistent data. S15 LargeScaleTest data doesn't seems to be compliant with latest Qserv version so running multi-node test would be interesting. Nevertheless the multi-node setup doesn't seems to be documented and, hence, is difficult to reproduce."
0,HSC backport: avoid I/O race conditions config write out,"This is a port of [HSC-1106|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1106]    When running tasks that write out config settings files ({{processCcd.py}}, for example), if multiple processes start simultaneously, an I/O race condition can occur in writing these files.  This is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation.  Also, to avoid similar race conditions in the backup file creation (e.g. config.py~1, config.py~2, ...), a {{--no-backup-config}} option (to be used with --clobber-config) is added here to prevent the backup copies being made.  The outcome for this option is that the config that are still recorded are for the most recent run."
0,some ctrl_events tests execute outside of execution domain,"There are a couple of ctrl_events tests that attempt to execute outside of the valid domains acceptable by the tests, when they shouldn't be.  There's a check in place for tests to find this, but a couple of the tests do not have this check."
0,transfer and update orchestration documentation,"The self-service orchestration documentation needs to be transferred from Trac to Confluence, and updated."
0,Misc work for this reporting week. ,"Deal with comments,  coordinate with Jason Alt. Deal with comments on existing work from KT and GDF"
1,Begin thinking about governance aspects of DM operations,"Thinking, (but not delivered use case)  -- what is the flow of tickets and the division of use cases between the Science and Data Operations?  What kind of tooling needs can be assumed to exist to support service management what does that mean for the support of processes?  (specifically  looked at a summary of the state the market, and took specifics look at what ITSM process supporting tools are open source. -- as may be useful to prototype processes before committing to something..  Looked at several, ITOP seemed to be mature/documented to a level that may be useable.    "
0,"Conduct, document, initial follow through on Sept JCC meeting ","Developed agenda items, considered JCC meeting.  Minutes are on the LSST confluence.  A major item of discussion was related to operations, since we are told this is a priority.  Befall followup on CCIN2P3 ISM practices. "
1,General Management ,"Hiring,  Internal relationships within the NCSA organization. LSST meetings, general management"
0,Replace boost::regex with std::regex,"Boost 1.59 causes a ""keyword hidden by macro"" warn under clang in the regex package.  We should be using std::regex now anyway, so this is a good motivator to go ahead and convert."
3,Create and deploy a git-lfs prototype,"Create and deploy a git-lfs-s3 server.    High level requirements:    * The server should use github to authenticate users. Any github user who is a member of the lsst organization has write access. This means they can push objects to the git-lfs server.  * The server should allow for anonymous read access. This means anyone can clone, pull, fetch, etc.  * Use an S3 compliant API to store objects.  * Simple, well defined method to redeploy.  * Uses https.  * Backs up to Amazon Glacier (or similar) periodically. The data should be ""slow"" so backing up approximately once a day is OK."
0,Update multi-node setup documentation,Workers in multi-node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.
3,W16 ISO Work,Continuous work as LSST ISO.    Assignees: Alex Withers  Duration: September 2015 - February 2016
0,Centralize Sphinx configuration for Design Documents,Centralize Sphinx configuration for design documents in {{documenteer}} and provide a facility for design document authors to use YAML files to store document metadata rather than editing {{conf.py}} files.
0,Implement iostream-style formatting in log package,Implement proposed in RFC-96 change to log macros. This ticket only covers defining new set of macros (LOGS() and friends) which use ostringstream for formatting messages. Migration of all clients and removal of LOGF macros will be done in separate ticket.
1,port dax_*serv to python3,NULL
1,Handle queries with no database,"Sqlalchemy is generating some queries that are currently killing czar, the list is:    {code}  set autocommit=0  SHOW VARIABLES LIKE 'sql_mode'  SELECT DATABASE()  SELECT @@tx_isolation  show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin'  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  select @@version_comment limit 1  {code}    Czar should survive unfriendly syntax, and this will be addressed through DM-3764.    In this story we will make sure that  sqlalchemy-generated queries are properly handled (not just these particular queries, but all queries that do not involve any database and any table). We should run such queries on a local mysql instance (alternatively, perhaps redirect to one of the workers?)"
0,"AP, Co-add, Image cache definitions","Added physical breakdown for Alert Postage Stamp Images file system, co-add images file system and image cache file system"
1,sandbox-stackbuild issues,"A number of issues with sandbox-stackbuild, or rather its infrastructure.    1. Problem with the librarian-puppet plugin and its mismatch with the puppet forge API ([~jhoblitt] has a PR open apparently)    2. As a workaround to above, one needs to    {code}  gem install librarian-puppet  librarian-puppet install  {code}    but that runs into an issue with swap_file needing a downgrade to work with Ubuntu 14.04. Working state as of the time of this bug report for the Puppetfile is:    {code}    forge 'https://forgeapi.puppetlabs.com'    mod 'puppetlabs/stdlib'  mod 'camptocamp/augeas', '~> 1.4'  mod 'stahnma/epel', '~> 1.1'  mod 'petems/swap_file', '1.0.1'  mod 'jhoblitt/sysstat', '~> 1.1'  mod 'maestrodev/wget', '~> 1.7'    mod 'jhoblitt/lsststack', :git => 'https://github.com/lsst-sqre/puppet-lsststack.git'  {code}    3. Which brings us to the fact that the Vagrant puppet-install plugin is broken with Puppet 4, and new platforms are not supported under Puppet 3. Ergo, as is, can't bring up Ubuntu 15.05 etc.     Ticket is to get PRs merged, fork and fix them ourselves, or find alternatives.   "
1,Histogram calculation for image stretch has infinite loop ,"When load the big.fits file, the image never came out.  It stopped at Histogram.  There was an infinity in Histogram.   "
1,Measurement plugin errors,"When doing measurements on coadds, several errors are thrown within the measurement plugins.    {code}  Error in base_GaussianFlux.measure on record 283467884979: Input shape is singular  {code}    {code}  Error in base_GaussianFlux.measure on record 283467883979:     File ""src/SdssShape.cc"", line 842, in static lsst::meas::base::FluxResult lsst::meas::base::SdssShapeAlgorithm::computeFixedMomentsFlux(const ImageT&, const lsst::afw::geom::ellipses::Quadrupole&, const Point2D&) [with ImageT = lsst::afw::image::MaskedImage<float, short unsigned int, float>; lsst::afw::geom::Point2D = lsst::afw::geom::Point<double, 2>]      Error from calcmom {0}  lsst::pex::exceptions::RuntimeError: 'Error from calcmom'  {code}    The measurements were done on tiger, and the command used was:  {code}  measureCoaddSources.py /tigress/HSC/HSC/rerun/nate/old_clip/ --output=/tigress/HSC/HSC/rerun/nate/old_clip/ -C /home/nlust/options_temp.py --id tract=0 patch=2,2 filter=HSC-I^HSC-R   {code}  The data can be found within the rerun directory specified as the input to the command. The data was created using the commands:  {code}  assembleCoadd.py  --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-R --selectId visit=1208 ccd=56^64^72 --selectId visit=1206 ccd=64^65^72^73^79^80 --selectId visit=1212 ccd=64^65^72^73^79^80 --selectId visit=23704 ccd=64^65^72^73 --selectId visit=23706 ccd=56^64^72 --selectId visit=23694 ccd=64^65^72^73^79^80 --selectId visit=1204 ccd=64^65^72^73^79^80 --selectId visit=1220 ccd=63^64^71^72^78^79 --selectId visit=1218 ccd=56^57^64^65^72^73 --selectId visit=23718 ccd=64^65^72^73^79^80 --selectId visit=23692 ccd=64^65^72^73^79^80 --selectId visit=1210 ccd=63^64^71^72^78^79 --selectId visit=1216 ccd=56^57^64^65^72^73 --selectId visit=1214 ccd=64^65^72^73^79^80 --selectId visit=23716 ccd=63^64^71^72^78^79 --selectId visit=1202 ccd=64^65^72^73^79^80  {code}  and   {code}  assembleCoadd.py --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-I --selectId visit=19658 ccd=64^65^72^73^79^80 --selectId visit=1248 ccd=56^64^72 --selectId visit=19696 ccd=65^66^73^74^80^81 --selectId visit=19684 ccd=64^65^72^73^79^80 --selectId visit=1238 ccd=64^65^72^73^79^80 --selectId visit=19710 ccd=56^64^72 --selectId visit=19680 ccd=56^64^72 --selectId visit=1230 ccd=64^65^72^73^79^80 --selectId visit=1236 ccd=63^64^71^72^78^79 --selectId visit=19694 ccd=64^65^72^73^79^80 --selectId visit=1232 ccd=64^65^72^73^79^80 --selectId visit=19698 ccd=64^65^72^73^79^80 --selectId visit=1228 ccd=64^65^72^73^79^80 --selectId visit=1246 ccd=63^64^71^72^78^79 --selectId visit=19682 ccd=63^64^71^72^78^79 --selectId visit=19708 ccd=64^65^72^73^79^80 --selectId visit=19662 ccd=64^65^72^73 --selectId visit=1240 ccd=64^65^72^73^79^80 --selectId visit=1244 ccd=56^57^64^65^72^73 --selectId visit=1242 ccd=56^57^64^65^72^73 --selectId visit=19660 ccd=64^65^72^73^79^80 --selectId visit=19712 ccd=56^57^64^65^72^73  {code}"
0,"Fix ""Executive error executing job"" on the cluster",NULL
0,HSC backport: temporary file handling in butler,The HSC fork includes additional work to improve temporary file usage in the butler:  * [HSC-1275|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1275]: Probable resource leakage by butler  * [HSC-1285|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1285]: eups.version files ignore umask  * [HSC-1292|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1292]: Prevent opening files that are already open
3,Pastry prototype C++,NULL
1,move camera factory methods from obs_lsstSim to afw,"The methods defined in obs_lsstSim/bin/makeLsstCameraRepository.py can be easily adapted for use in generating arbitrary, non-LSST cameras.  This is useful for the sims stack, both for testing purposes, and because members of other projects have begun asking us to use our code.    This ticket will take those methods, make them fully LSST-agnostic, and place them in afw as utility functions.  The code in obs_lsstSim will refer back to these afw methods."
0,NaiveDipoleCentroid/NaiveDipoleFlux algorithms should not require centroid slot,"The {{NaiveDipoleCentroid}} and {{NaiveDipoleFlux}} algorithms in {{ip_diffim}} have members which are instances of {{meas::base::SafeCentroidExtractor}}. Due to the prerequisites that imposes, it is impossible to initialize these algorithms without first defining a {{centroid}} slot.    However, there is nothing in these algorithms which actually uses the {{SafeCentroidExtractor}} or any of the information stored in the slot; this seems to be an entirely arbitrary restriction which is likely a legacy of the port to the {{meas_base}} framework. We should remove the  use of {{SafeCentroidExtractor}} to simply the code and make it easier to run the test suite (since it will no longer be necessary to run a centroider)."
0,Split FY16 plan into audience specific documents,"Fy16 purchase planning needs to be split for specific audiences (NCSA planning, Aura purchase approval)"
2,Preliminaries for the LSST vs. HSC pipeline comparison through single-frame processing,"This ticket is in preparation for DM-2984, which is to run both the HSC and LSST pipelines on 2-3 visits of HSC data, and do a detailed comparison of the science quality and robustness for the single-frame processing (ProcessCcdTask) stage only.  Essentially, it is a detailed audit of all the functionality currently used in HSC single-frame processing and ensuring all relevant features have been pulled over to the LSST stack such that the comparisons to be done in DM-2984 will be meaningful and informative."
0,QMeta thread safety,"Initial QMeta implementation is not thread safe, it uses sql/mysql modules which also do not have any protection (there are some mutexes there but not used). Need an urgent fix to avoid crashes due to concurrent queries in czar."
1,orchestration slide set for DM bootcamp,"Create the ""Orchestration and Control"" slide set for DM Bootcamp, which is being held from Oct 5-7, 2015.  After review (and any revisions), the slide set will be uploaded to confluence, and a link to it will be put here."
1,Simplify task queuing / Runner code,NULL
1,Cleanup cancellation-related worker code,NULL
0,Remove dependency on mysqldb in wmgr,Move remaining code that depends on mysqldb to db module
0,Remove dependency on mysqldb in qserv,Remove remaining dependencies on mysqldb in qserv.:  {code}  ./core/modules/tests/MySqlUdf.py  ./core/modules/wmgr/python/config.py  {code}    and use the sqlalchemy from db module instead.
1,Remove qserv_objectId restrictor,"qserv_objectId restrictor can be replaced by the IN restrictor. This story involves checking if performance is acceptable if we use IN restrictor instead of qserv_objectId restictor, and if it is, doing the switch and removing the qserv_objectId restictor code."
0,Cleanup lua miniParser,"Maybe some cleanup can also be performed in lua code. Indeed ""objectId"" hint and parseObjectId() which seems useless.    Indeed miniParser.parseIt and miniParser.setAndNeeded seems useless.    Removing this code will ease maintenance of objectId management."
0,E/I training and interview,Interviewed and attended training for E/I concerns.
1,Investigate services for backups/data replication on Nebula openstack,Files generated on instances of the Nebula openstack  should be managed with some commensurate  level of data replication/backups.     We investigate services that might serve this task within the cloud context.
0,Package SQLAlchemy in eups,"Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups."
0,Enable CModel in CalibrateTask prior to PhotoCal,"CModel needs to run in CalibrateTask before PhotoCal in order to compute aperture corrections, but it also needs a Calib objects as input, and that isn't available until after PhotoCal is run.    On the HSC side, we dealt with this by adding preliminary PhotoCal run before CModel is run, but we could also deal with it by removing the need for a Calib as input, at least in some situations."
3,Revisit provenance design / built proof-of-concept prototype,"Discuss existing provenance design with the team, brainstorm and improve. Primary focus on capturing provenance information. Deliverable: proof-of-concept prototype + description."
1,Revisit provenance sizing,Revisit estimates of the size of provenance information.
2,First optimizations of provenance querying,Think through the issues of querying provenance. Deliverable: write up attached to this story. IT will be transferred to more official provenance documentation through DM-3961
1,Document provenance design,NULL
2,Build prototype of provenance,"Building a proof-of-concept prototype of provenance.    Deliverable: a working, standalone (not connected to any data producer) prototype of the Data Provenance. Not optimized / alpha version."
0,Meetings Sep 2015,verification datasets weekly meetings and tech-talks
1,Add SQLite-based v01. unit tests for dbserv,NULL
1,"Meetings, institute events, or other LOE, Sep 2015","NCSA or Astronomy Department activities.   - NCSA All-hands  - Local LSST group meetings   - DES-Illinois meetings  - Colloquia  - other seminars, info sessions, or other local meetings"
0,detailed out alert processing/ transmission to event brokers. ,"Detailed out the interactions with the event broker, after consulting with John Swinbank.  Dealt with outputs of alert processing.  Dealt with corrections and comments.   Began considering annual processing"
2,weekly management,"Dealt with impending new hire.   Good deal of intergroup coordination, and deal with NCSA re organization."
0,Package sqlalchemy in eups,"Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups."
0,FY pricing estimates,NULL
0,Network / Storage reviews,Initial (internal) review with networking and storage delegates
0,research items needed for wan simulator procurement,locate part numbers and pricing information
1,Consultation on system and network design,NULL
1,Prepare pricing estimates for networking infrastructure,"provide switch quantities, weight, power and budgetary costs"
0,Recommend network infrastructure for Openstack expansion,NULL
0,Discussion of Base site network proposal,Email based discussion with Ron Lambert on his network infrastructure proposal for the Base site.
0,Post SQLAlchemy-migration tweaks,"Implement some minor tweaks take came in late through PR comments, mostly related to sqlalchemy related migration"
0,Improve the performance for making the image plot,Make FitsRead work better for multi-threads
1,Larger Statistics needed for CModel Studies,"The stampsize and nInitalRadius tests were not conclusive in the September Sprint. and the error estimates appeared to be overly large. The nGrowFootprints test was barely significant.  This is a continuation of work started on DM-1135 (DM-1135, 3375, 3376) , after a study of the sizes of our error estimates was conducted (DM-3984).    We started with a sample with 12 million galaxies (not all of which were used in DM-3375 and 3376.  They appeared to all be useful once we had new error estimates, so the studies were run against with this larger sample. "
1,Errors for shear bias fits,"DM-1135 mostly was inconclusive or at least not highly significant, due to the fact that the error bars were around the same size as the differences in most of the tests. This in spite of the fact that we ran 6x as many sample galaxies as great3sims.    Investigate the reason for this, and see if we can estimate the errors more accurately.  Investigate what the best way (or at least the way it is done in GREAT3) to estimate the errors on the bias parameters.     If it is not from the covariance matrix of the regression parameters, there could be some work here."
1,Run multinode test using docker on one unique host,"Qserv multinode test can be launched on n hosts using docker, but not on one unique host.  This ticket will allow developers to run multinode test on their workstation."
1,Deploy developer code on in2p3 cluster in Docker images,Qserv latest release can be deployed easily on cc-in2p3 cluster using Docker. This ticket will allow developers to prepare worker and master containers using a specific Qserv version and deploy it on cc-in2p3 cluster.
0,remove unnecessary 'psf' arg to SourceDeblendTask.run(),"{{SourceDeblendTask.run}} takes both an {{Exposure}} and a {{Psf}}, even though it can get the latter from the former and always should."
1,Update FY2015 hardware budget plan,"Update the FY2015 hardware purchasing plan with new budget, equipment specifications, and general costs. "
1,Week end 09/05/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 5, 2015."
0,Provide detail specs to AURA,Provided Josh Hobblitt details specs for procurement request
1,Week end 09/12/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 12, 2015."
0,Display.dot origin swaps x and y,Correcting for xy0 in {{dot}} currently does:  {code:hide-linenum}  r -= x0  c -= y0  {code}  which is backwards.
0,Backup Pugsley,Created backup of pugsley in anticipation of new hardware and shutdown of temp Mac OS solution
0,Research using vSphere on Mac Pro,Researched setting up vSphere on Mac Pro.
1,Week end 09/19/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 19, 2015."
0,Learn about Lenovo storage and server options,Went to lunch with Lenovo to learn about systems that might be suitable for LSST deployment. Learned about server and storage options.
1,Week end 09/26/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 26, 2015."
2,Revamp FITS Viewer scrolling to stop using large div,The current scrolling system will not work as well with masking layer.  It is also suspected to use too much browser memory since is creates a very large div.  Firefly will scroll images manually now.
1,Write next-generation stack doc writing guide,Write a guide in the prototype LSST Stack Docs (https://github.com/lsst-sqre/lsst_stack_docs) covering how to document the LSST Stack under the new doc infrastructure.    This exercise will implicitly involve designing how the new docs will work. Content includes:    # How to write a user guide to a package (both content wise and in terms of organizing a package's doc files)  # How to write python doc strings  # Coverage of reStructuredText and Sphinx as implemented by the LSST Stack Docs.  
0,Replace zookeeper CSS with mysql,To switch from QservAdmin to CssAccess interface in our Python tools we will need to replace zookeeper with mysql implementation because we do not have C++ KvInterface implementation for zookeeper.
2,Investigate what is missing to run ISR with DECam raw data and processCcd,"Currently raw DECam data can not be processed with the stack for Instrumental Signature Removal (ISR).  This issue includes efforts to understand how ISR is done in the stack, learn about processCcd, basic DECam ISR, and what is missing in the code stack to proceed.    "
0,"ctrl_execute templates still use ""root"" instead of ""config""","The templates that ctrl_execute fills in still use ""root"", instead of the new ""config"".  This causes extraneous warning messages to appear from pex_config when executing ""runOrca.py"""
1,Allow FlagHandler to be used from Python,"The {{FlagHandler}} utility class makes it easier to manage the flags for a measurement algorithm, and using it also makes it possible to use the {{SafeCentroidExtractor}} and {{SafeShapeExtractor}} classes.  Unfortunately, its constructor requires arguments that can only be provided in C++.  A little extra Swig wrapper code should make it usable in Python as well."
0,Rename forced photometry CmdLineTasks to match bin scripts,"We use names like ""forcedPhotCcd.py"" for bin scripts but ""ProcessCcdForcedTask"" for class names; these need to be made consistent, and it's the former convention that was selected in an old (non-JIRA) RFC."
0,Initial discussion EFD with Dave Mills,Review LTS-210 and discuss design of EFD cluster with Dave Mills.    Are there opensource clustering solutions?
0,Replace boost::tuple with <tuple>,Replace boost::tuple with <tuple>    This ticket will be completed as part of the DM bootcamp at UW.
0,Document Revision,"Document cleanup. Added PDUs, networking estimates, power costs, vsphere licensing."
1,Write developer workflow documentation,Write a developer workflow guide to walk through and document best practices for developing against the LSST Stack.
0,Fix procedure for building docker image for 2010_09 release,This procedure should be straightforward but is currently failing due to gcc-4.9/boost problem (DM-4018)
0,Replace boost::unordered_map with std::unordered_map,DM boot camp tutorial.    
0,forcedPhotCoadd.py fails on HSC data,"When trying to run {{forcedPhotCoadd.py}} on HSC data, I see the following error:    {code}  $ forcedPhotCoadd.py /raid/swinbank/rerun/LSST/bootcamp --id filter='HSC-I' tract=0 patch=7,7  : Loading config overrride file '/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py'  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/meas_base/11.0+2/bin/forcedPhotCoadd.py"", line 24, in <module>      ForcedPhotCoaddTask.parseAndRun()    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/cmdLineTask.py"", line 433, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 360, in parse_args      self._applyInitialOverrides(namespace)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 475, in _applyInitialOverrides      namespace.config.load(filePath)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 529, in load      self.loadFromStream(stream=code, root=root)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 549, in loadFromStream      exec stream in {}, local    File ""/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py"", line 10, in <module>      config.deblend.load(os.path.join(os.environ[""OBS_SUBARU_DIR""], ""config"", ""deblend.py""))  AttributeError: 'ForcedPhotCoaddConfig' object has no attribute 'deblend'  {code}    This is with the stack version 11.0+3 and {{obs_subaru}} 5.0.0.1-676-g4ae362c."
0,Local LSST IAM Meeting,Meeting notes:    * [10 minutes] Review LSST identity management statement of work    On the confluence wiki. Let me know if you have any follow-up questions/comments.    * [10 minutes] Plan initial project tasks    All: Review materials at https://confluence.lsstcorp.org/display/LAAIM. Jim to add updates from last week's meeting.    * [5 minutes] Schedule follow-on project meetings    Alex will schedule meeting with Bill Glick about LDAP at NCSA.  Terry and Daniel will attend https://community.lsst.org/t/dm-boot-camp-announcement/249 sessions of interest if available.
0,Local LSST IAM Meeting,#NAME?
0,First day activities,"Meeting with HR at 8:30AM.  Taken around for introductions through 10AM.  Remainder of day was proceeding through HR punch list for new hires such as:  	NCSA Intranet account setup  	Gain familiarity with NCSA Intranet  	Kerberos and Enterprise ID setup  	Read and Sign-off  on Security and NCSA policy.  	Outlook mail and calendar setup  2pm DM meeting"
0,Second day start activities,Requisitioned used macbook and monitor from IT and set it up.  Located LSST stack site and read build and install documentation  More HR account items.  Requested confluence credentials from LSST.org  Learned about SDSS/Stripe 82
0,groundwork for file management,Installed correct Xcode + misc. for stack version.  Downloaded LSST stack from GitHub repo and built/installed.  
0,Incidental items and jobs for file mgmt. groundwork,"Installed and built LSST tutorials package.  Setup, fixed minor issues and ran First tutorial to check that initial stack  installation was successful.  Learned about CCD operation and how the LSST CCD is laid out.  Learned about raw CCD data from amplifiers, as well as other camera attributes  Studied stack code for these things.  Read description of Astronomy Associative relations as well as CoAdds.  Pulled PhoSim from repo. Built and Installed.  Had trouble getting PhoSim to run due to dynamic link library issues.  Instrumented PhoSim code with PDB commands.  Walked to bookstore and picked up new ID badge."
0,Development for Developer activities!,"Completed setup of confluence access.  Set up LSST HipChat.  Jumped into PhoSim a bit more and resolved errors by dropping in a few symlinks here and there;  not a solution for a proper execution environment, but a time-save just to see PhoSim work and  have a platform for tinkering with camera attar's.  PhoSim ran successfully.  Learned about FTTS format and how the file is built up. Learned about how Key/Val pairs   can proceed ANY data section throughout file by using data offset values."
1,integration of confluence data into learning curve,"Started studying LSST coding policies and best practices via confluence.  Colloquium.  Small meetings with other LSST team members throughout day.  Rebuilt PhoSim/LSST-Stack to take advantage of multiple cores when rendering portion of sky to a file. Built these commands into Stack code. They would have to be custom #defined by ./configure at build time depending on computer arch. which is too much to do when just gaining familiarity so stuck to MacBook  multi-core specs, where I was working.  Logged into JIRA and studied how tasks were proposed, realized, and checked as done.  Extensive talk about BBQ in group area of NCSA/LSST.  Close reading and note taking of LDM-230 and related docs."
1,"Add doc directory, and fix doxygen warnings",Add doc directory and fix doxygen warnings.
1,obs_sdss should use pydl.yanny instead of it's own copy thereof,"Inside obs_sdss there is a yanny.py that looks like it was copied from either sdss_python_module or pydl. We should just depend on pydl (https://github.com/weaverba137/pydl), so we can use whatever improvements it gets for free, and to prevent yet annother yanny reader floating around."
0,Display DECam focal plane mosaics using showCamera,Try out some new functionalities of afw.cameraGeom.utils (from DM-2437) for DECam raw data. Raw data in {{testdata_decam}} are retrieved through Butler and displayed as a focal plane mosaic.  
1,Replace boost::array with std::array,"Replace all use of boost::array with std::array in the DM stack    A quick search turned up use in 17 files spread over these packages:  ip_diffim, meas_base, meas_extensions_photometryKron and ndarray (which is presumably out of scope for this ticket)"
1,Change from boost::math,"Most boost::math contents (not including pi) are now available in standard C++. Please convert the code accordingly.    In addition to the packages listed above, boost/math is used in ""partition"" a package I don't recognize and not a component JIRA accepts."
0,New YAML config for community_mailbot,"Per review comments to DM-3690, the configuration should move to YAML with the following goals    - Have a Configuration object that can be tested and passed around  - Redesign the configuration to allow for additional types of message handlers, such as twitter, hipchat/slack, etc.  - Move secret keys entirely into the configuration file  - Provide a configuration template  - Move any sort of hard-coded configuration to the expanded YAML file (e.g., Mandrill templates)"
0,Refactor Scripts and Discourse interface in community_mailbot,"Per review comments for DM-3690, the community_mailbot can have slight code refactoring    - Refactor scripts into smaller testable units  - Refactor the Discourse feed classes around an ABC  - More testing"
0,Produce demo video for git lfs,Produce a screencast tutorial of the DM git-lfs implementation.
1,update memory management in jointcal,"jointcal currently uses a combination of raw pointers and a custom reference-counted smart pointer class, {{CountedRef}} (similar to {{boost::intrusive_ptr}}).  The code needs to be modified to use a combination of {{shared_ptr}} (most code), {{unique_ptr}} local-scope variables and factory functions, and {{weak_ptr}} (at least some will be necessary to avoid cycles in some of the more complex data structures).  As part of this work, we'll also have to remove a lot of inheritance from {{RefCounted}}, which is part of the {{CountedRef}} implementation.    This ticket looks like it will require a lot of work, because we'll have to be careful about every conversion to avoid cycles and memory leaks.  Nevertheless, I think it will be necessary to do this conversion before attempting any other major refactoring, as I'm worried that having a newcomer make changes to the codebase without first making the memory management less fragile could be very dangerous."
1,"integrate jointcal geometry primitives (Point, Frame, FatPoint) with afw","jointcal currently has three simple geometry classes that can be integrated relatively easily with existing classes in afw and meas_base:   - {{jointcal.Point}} is equivalent to {{afw.geom.Point2D}}.   - {{jointcal.Frame}} is equivalent to {{afw.geom.Box2D}}.   - {{jointcal.FatPoint}} is equivalent to {{meas.base.CentroidResult}}.  We should probably move {{CentroidResult}} to {{afw.geom}}, perhaps rename it ({{MeasuredPoint}}?), and reconsider its relationship with {{Point}}.  This will require a bit of refactoring in {{meas_base}}, but the usage in {{jointcal}} makes me think it's a sufficiently fundamental object to be included in afw.    We may find aspects of the interfaces in {{jointcal}} that we should add to {{afw.geom}}, but I think we'll mostly end up making trivial modifications to {{jointcal}} to use the {{afw}} interfaces."
1,integrate Gtransfo functionality with XYTransform,"{{meas_simastrom}} includes a {{Gtransfo}} class hierarchy that is similar to {{afw.geom.XYTransform}}, but with more functionality and some intentional differences, including:   - {{XYTransform}} objects are immutable; {{Gtransfo}} objects are not.   - {{Gtransfo}} objects expose their parametrization, and can compute various derivatives with respect to those parameters.  {{XYTransforms}} are essentially black-box functions, and expose no parameterization.    Unifying these classes is not entirely straightforward, and should include an RFC for the design prior to implementation.  Overall, I think {{XYTransform}}'s simpler, lower-functionality interface and immutability is worth mostly preserving somehow; I think it's a more fundamental interface than {{Gtransfo}} that can be used in more places.  But obviously we need to provide the more extensive {{Gtransfo}} interface somehow as well.    My initial thought is that we should have two parallel class hierarchies (with a concrete class for each type of transform, such as polynomial distortion, in both), and an ultimate base class shared by both hierarchies.  That ultimate base class would contain most of the current {{XYTransform}} interface but not require immutability, and one side of the tree would contain simple immutable objects while the other would contain the more extensive parameterized interface of {{Gtransfo}}."
0,Redis cache for community_mailbot,Switch from a {{json}} cache to a redis cache from the community_mailbot.
1,Add fake secondary index to testIndexMap.cc,"qproc/testIndexMap.cc is sketchy and perform a very poor validation for now. It should at least use a minimal SQL secondary index, embedded in a simple database like SQLLite, inorder to validate secondary index lookup code."
1,Replace QsRestrictor::PtrVector With std::vector<QsRestrictor> and use move constructor,Use of  QsRestrictor::PtrVector introduces a useless indirection. it maybe could be replace by std::vector<QsRestrictor> and use of move constructor. This would simplify code (currently a confusion exists between empty vector and nullptr) and ease maintenance.
1,Security plan renewal,NULL
0,IaM work,NULL
0,bi-weekly IaM meeting,NULL
0,Meetings Oct 2015,#NAME?
1,Final additions and review,"Add remaining components: power estimates, vSphere annual licensing, networking, PDUs, login nodes    review: misc expense fund, decommissioned services, financial targets"
0,Chasing down Pan Starrs Requirements,Pan Starrs data release (PS1) will be used in the integration QServ environment purchased as part of FY16. Catalog and file space requirements must be understood.
0,Prepare Plan for ICI Leadership Review,Prepare plan and ICI-group-specific points of interest for hardware/service deployment plans. 
0,Vendor Discussions,Discussions with vendors on planned procurement. Details of discussions will not be described here. This is for story point tracking only.
0,Begin Approval Process,"Approval process for FY16 procurement plan. This requires approval from Jeff K, Victor and NSF (due to the cost increment being greater than $250K).    Expected approval time frame: Dec 2015."
0,Define policy based upon FY16 plans,Re evaluate previously proposed storage policies: https://wiki.ncsa.illinois.edu/display/LSST/Storage+Policy?src=contextnavpagetreemode    Plan new policies:  https://wiki.ncsa.illinois.edu/display/LSST/Changes+to+Storage+Policy+and+Design?src=contextnavpagetreemode
0,Data access rights and retention policies,"Added Data access center requirements, individual data access rights, data retention policies and other general cleanup."
0,Support new casting requirements in NumPy 1.10,The function imagesDiffer() in testUtils attempts to OR an array of unit16s (LHS) against an array of bools(RHS) {{valSkipMaskArr |= skipMaskArr}} and errors with message  {code}  TypeError: ufunc 'bitwise_or' output (typecode 'H') could not be coerced to provided output parameter (typecode '?') according to the casting rule ''same_kind''  {code}  preventing afw from building correctly. 
1,Revisit database compression trade-offs,"As discussed at Qserv meeting Oct 7, it is not entire clear if it will be worth compressing data. Need to revisit baseline."
0,Discuss with MySQL team,This story captures issues/topics that we want to bring up with mysql team.
1,SuperTask structure implementation ,Starting to implement the structure of the Super Task framework for process execution
1,SuperTask framework extension,Incorporate Configuration and data reference to implementation
1,Bootcamp meeting,I've attended LSST DM bootcamp
1,SuperTask framework documentation and  refactorization,While still prototyping I need to fill documentation on new code as well as do some clean up as well
0,testPsfDetermination broken due to NumPy behaviour change,"Old NumPy behaviour (tested on 1.6.2):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  /usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars    return mean(axis, dtype, out)    Out[3]: nan  {code}    New NumPy behaviour (1.10.0):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  [...]  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}    This breaks {{testPsfDeterminer}} and {{testPsfDeterminerSubimage}}, e.g.:  {code}  ERROR: testPsfDeterminerSubimage (__main__.SpatialModelPsfTestCase)  Test the (PCA) psfDeterminer on subImages  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""./testPsfDetermination.py"", line 342, in testPsfDeterminerSubimage      trimCatalogToImage(subExp, self.catalog))    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 377, in selectStars      widthStdAllowed=self._widthStdAllowed)    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 195, in _kcenters      centers[i] = func(yvec[clusterId == i])    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3084, in median      overwrite_input=overwrite_input)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2997, in _ureduce      r = func(a, **kwargs)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3138, in _median      n = np.isnan(part[..., -1])  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}"
3,S18 Improve Webserv,NULL
1,Assemble eslint rules for JavaScript code quality control,"Review and assemble eslint rules, which enforce clean JavaScript and JSX code.    Code cleanup to avoid too many rule violations."
0,JavaScript code cleanup - remove unused packages,"Remove es6-promise, react-modal, other cleanup"
0,convert underscore to lodash,"lodash has become a superset of underscore, providing more consistent API behavior, more features, and more thorough documentation. We'd like to convert our underscore package dependencies to lodash while we have only ~20 calls to underscore functions"
1,Create a React component which manages tabs,"We need a React component, which manages tabs. "
1,Shutdown mechanism doesn't work when logging process is disabled.,"If the logging mechanism is turned off in ctrl_execute, the ctrl_orca Logger doesn't get launched.  The current shutdown mechanism waits for the last logging message to be transmitted before shutting down so it doesn't kill off that process.   If the logger.launch config file option is set to false, this process never get launched and ctrl_orca hangs after the shutdown waiting for the message to arrive."
1,Lead  the Firefly conversion from GWT to React/FLUX design meeting,A focused week long design meeting on Firefly conversion from GWT to React/FLUX.   Agenda and notes here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41786446  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  
2,Firefly conversion from GWT to React/FLUX design meeting,A focused week long design meeting on Firefly conversion from GWT to React/FLUX.    Produce the first draft of design document   https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture
1,Firefly conversion from GWT to React/FLUX design meeting,A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Design document at  https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  
1,Firefly conversion from GWT to React/FLUX design meeting,A focused week long design meeting on Firefly conversion from GWT to React/FLUX.  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture.  
0,Attend DM boot camp,"Attend DM boot camp to learn more about DM stack, butler, and task. "
1,Attend DM boot camp,"Attend DM boot camp to learn more about DM stack, butler, and task.     Most of the presentations are located at URL https://community.lsst.org/t/dm-boot-camp-announcement/249. Presentations like afw, eups, tasks, and butler are necessary to participate in LSST, so everyone on LSST must understand these concepts. Look at the list of presentations covering these topics and make sure your understand them. Some of the remaining talks go into more detail or cover more specialized topics. Those talks should be scanned to see if they are of interestß to you."
1,Attend DM boot camp ,"Attend DM boot camp to learn more about DM stack, butler, and task. "
1,Consulting in September,NULL
0,update cat logging information,"The ""cat"" package has a table which the Logger in ctrl_orca uses to insert information from logging messages.  The format of the log messages has changed, and therefore the table in ""cat"" needs to be changed as well."
0,Update qserv for lastest xrootd,"Small API change in latest xrootd, requires a parallel change to qserv.  Paves the way for DM-2334"
1,Update DECam camera geometry descriptions for raw data,"The overscan and prescan regions of instcal data have been trimmed, but they are included for raw data.  The amplifier information in the current camera descriptions were made for instcal data and do not include overscan and prescan regions.      While processing raw data with the current camera descriptions, the bounding boxes from the camera object seem incorrect for raw data.     Code change summary:  - Use non-zero overscan and prescan regions  - Update the pixel array layout. My schematic of pixel array layout is attached in DecamAmpInfo.png    Screenshots are post-ISR images processed with bias and flat correction using the old or updated camGeom.  "
2,Educational Activities for In-Depth Reusable Background,Bucket epic to capture effort spent in educational activities and meetings to gain in-depth reusable background knowledge for the LSST project.
0,Please port showVisitSkyMap.py from HSC,The HSC documentation at http://hsca.ipmu.jp/public/scripts/showVisitSkyMap.html includes a useful script for displaying the skymap and CCDs from a set of visits. It would be convenient if a version of this script was available in the LSST stack.
0,Update Trust Level of all LSST DM Staff to Level 4 via the API,It seems safe to update the Discourse trust level of all members of the LSSTDM group on community.lsst.org to Level 4 (full permissions). See https://meta.discourse.org/t/consequences-of-using-or-bypassing-trust-levels-for-company-organization-staff/34564?u=jsick    This should alleviate concerns that DM staff are being prevented from fully using the forum.    This ticket implements a small notebook to exercise the Discourse API to make this trust level migration possible.
0,Provide upstream improvements to sphinx-prompt,Provide PRs to sphinx-prompt (or decide to own a fork of sphinx prompt in documenteer) that includes    - an actual package you can import  - better error reporting when you forget to include a class with the prompt directive
0,Replace use of image <<= with [:] in python code,Replace all use of the afw image pixel copy operator {{<<=}} with {{\[:]}} in Python code.    See DM-4102 for the C++ version. These can be done independently.
1,Remove use of <<= from C++ code in our stack,"Replace usage of deprecated Image operator {{<<=}} in C++ code with {{assign(rhs, bbox=Box2I(), origin=PARENT)}} as per RFC-102    Switch from [:] to assign pixels in Python code where an image view is created for the sole purpose of assigning pixels (thus turning 2-4 lines of code to one and eliminating the need to make a view)."
0,Update user documentation,"{{ORDER BY}}, {{objectId IN}} and {{objectId BETWEEN}} predicates support have been improved, this should be documented.    "
1,doc & demostrate ceph setup,NULL
1,add lfs remote support to lsstsw/lsst_build,"Support for cloning from lfs backed repos, when indicated via repos.yaml, is needed.  "
0,Update cfitsio to 3.37 (adding bz2 support),"Per RFC-105, we should upgrade to cfitsio 3.37."
1,evaluate git lfs prototype and provide feedback,NULL
0,Clean up lsst_stack_docs for preview,Improve the presentation of the New docs overall:    # Add a Creative Commons license  # Remove stub documents from the presentation  # Put READMEs in all doc directories to explain what content will go in them  # Clean up and update the source installation guide to reflect 11_0
0,Local LSST IAM meeting,"October 14, 2015 (local)    - Drawing identity access management architecture on whiteboard.    - DB access via kerberos       - mariaDB does pam but does it do kerberos tickets?            - could then simply access with a ticket   - if users are exporting VMs/containers, do they need keytabs?  how do we support this?   - what does the ID linking?   - do we need replication to base site so that base site can operate independent     - Can we get 2 LSST VMs?     - NCSA cyber-infrastructure standards?  MIT Kerberos?  OpenLDAP?    - Meeting with Iain Goodenow    "
0,Security Meeting with LSST PO,"October 8, 2015    - Refresh of security plan, sub-plan       - Talk about camera subsystem refusal to buy-in to security plan    - IaM work moving along       - Continue bi-weekly meetings with developers       - Most likely going with kerberos       - Iain meeting with Jim and Co on authn/z work    - Joint technical meeting in February       - when and where?  Santa Cruz, Feb 22-24th 2016    - security plan refresh:       - cover email with old document and instructions       - DM: Don P. and Jeff Kantor       - EPO       - PO: Iain       - Camera       - Incidents: all reports are collected and acknowledged"
2,Build instance and snapshot on Nebula for HTCondor worker with v11_0 LSST stack,Build instance and snapshot on Nebula for HTCondor worker with v11_0 LSST stack.
3,Draft IaM Recommendations,"LSST IaM draft recommendations from NCSA.  Group includes: Jim Basney, Terry Fleury, and Dan Thayer."
0,ctrl_events test failures on CentOS7 VM on Nebula,I am seeing failure to build due to test  failures for ctrl_events on a CentOS7 instance on the Nebula Openstack.  Details to follow.
1,Bootcamp meeting,Slides and attendance at DM Bootcamp
1,Bootcamp meeting,Travel to Princeton and attend DM Boot Camp 
0,pipe_tasks/examples/calibrateTask.py fails,"The self contained example calibrateTask.py in pipe_tasks/examples/ fails when attempting to set field ""coord"" in refCat. Exact error message -     {code}  11:04:19-vish~/lsst/pipe_tasks (u/lauren/DM-3693)$ examples/calibrateTask.py --ds9  calibrate: installInitialPsf fwhm=5.40540540548 pixels; size=15 pixels  calibrate.repair: Identified 7 cosmic rays.  calibrate.detection: Detected 4 positive sources to 5 sigma.  calibrate.detection: Resubtracting the background after object detection  calibrate.initialMeasurement: Measuring 4 sources (4 parents, 0 children)   Traceback (most recent call last):    File ""examples/calibrateTask.py"", line 150, in <module>      run(display=args.ds9)    File ""examples/calibrateTask.py"", line 119, in run      result = calibrateTask.run(exposure)    File ""/home/vish/lsst/lsstsw/stack/Linux64/pipe_base/11.0-2-g8218aaa+5/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/vish/lsst/pipe_tasks/python/lsst/pipe/tasks/calibrate.py"", line 478, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""examples/calibrateTask.py"", line 90, in run      m.set(""coord"", wcs.pixelToSky(s.getCentroid()))    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 2372, in set      self.set(self.schema.find(key).key, value)    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 1064, in find      raise KeyError(""Field '%s' not found in Schema."" % k)  KeyError: ""Field 'coord' not found in Schema.""  {code}    Note that {{wcs.pixelToSky(s.getCentroid())}} is set to {{Fk5Coord(15.007663073114244 * afwGeom.degrees, 1.0030133772819259 * afwGeom.degrees, 2000.0)}}"
0,Mask overlay does not work when the image is flipped,NULL
0,Database connection problems in daf_ingest,"The DbAuth connection fallback in ingestCatalogTask passes the ""password"" keyword argument to {{MySQLdb.connect}} instead of ""passwd"", which fails. Also, the ""port"" command line argument isn't marked as an integer, causing port strings to be passed down to MySQLdb. This results in a type error. "
1,"Have a ""mark as current"" option in lsstsw","Russell explained to me the advantage of having a specific eups tag associated with a given lsstsw installation. However, it would be very handy to have a way to get all installed packages automatically tagged as current as part of the installation process.    I suggest a ""-c"" option to lsstsw which will tag everything as current after the full installation is complete. This way, partially installed packages won't get marked current, and people who do full installations can not have to deal with having to say ""-t bBLAH"" every time they setup things."
0,Change type of LTV1/2 from int to float when writing afw images to FITS,"The LTV1/2 problem is originally my bug.  I used integer LTV1/2 in  {code}  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV1"", -xy0.getX());  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV2"", -xy0.getY());  {code}  whereas a more careful reading of the NOAO page [http://iraf.noao.edu/projects/ccdmosaic/imagedef/imagedef.html] introducing them includes floating point examples.    The fix is to cast the XY0 values to float.  I'm not sure if there'll be any side effects of fixing this, but if so they'll be obvious and trivial.  "
1,DHT prototype: HTTP server library refactor/cleanup,"This experimental library turned out to be quite useful.  Next stage of prototyping will be making greater use of this, and I anticipate this library will also be used in production code.  Spend some time cleaning up and organizing this lib so it doesn't get off to a bad start, and prepare for general review/feedback from the team."
1,DHT prototype: test fixture rework,"The DHT test fixture, developed during the preliminary work with kademlia, needs some updates for the next stage of the prototype:  * adapt to reworked http library  * adapt DHT interface to be more generic  "
0,"Update DECam CCDs gain, read noise, and saturation values","The values of DECam gain, read noise, and saturation value need to be updated.     This ticket is to update them in the Detector amplifier information, which is used in IsrTask.     Talked to Robert Gruendl. These values should take precedence over the values in the fits header. They seem stable and do not seem to vary with time.   "
1,Re-implement packed keys in CSS,Current implementation of JSON-packed keys in CSS has one complication - the name of the container for packed keys is the same as the key itself (plus .json suffix). This complicates handling of the subkeys because these keys need to be filtered out and these names are all different. It would be better to have easily identifiable packed key names.
0,Discover how to create Doxygen XML in build system,We need XML output from Doxygen in order to import existing API documentation into Sphinx. In this story I find out how to achieve this within the stack build system.
1,Demonstrate using Breathe for Python & C++ API reference in New Docs,Demonstrate use of breathe for utilizing the existing Doxygen API documentation in the new Sphinx-based doc platform.
0,Reduce scons output in qserv,"Yesterday AndyH expressed a valid concern that qserv prints too much info which makes it hard to find errors. By default scons prints whole command line for C++ compilation and linking which are quite long (~half screen depending on your screen size). Most of the time we don't need to see that, so it would be better to replace that with shorter messages like ""Compiling Something.cxx"" and have an option to print full command with --verbose option."
0,Networking requirements for design,Meeting with Paul to discuss remaining network design.
0,Finance Contract Discussions,Discussing updated contract hoops and game plan.
0,ICI agenda and mtg,NULL
0,Investigating procurement of individual components,Component breakdown and explanation of components as part of FY16 purchases in order to plan optimal purchasing through vehicles available to NCSA.
0,Search for uses of current afw.wcs in the stack,"Search through the stack for all the uses of our Wcs implementation (Wcs, TanWcs, makeWcs, and any other hidden objects) and make a list of all of those uses (on Community for example). This list should note whether the usage is in C++ or python."
1,Document detailing usage of Wcs in the stack,"The information from DM-4151 should become a brief report on the kinds of usage of Wcs in the stack. This could be posted to Community or Confluence, or it could be a brief LaTeX document attached to the afw repo.    Included in this report should be whether each current usage requires C++, or whether it could be done with e.g. vectors returned from python."
1,RFD to collect current and future use cases of Wcs,"File an RFD requesting information about current and possible future use cases for a Wcs system in the stack. This should get feedback from, at minimum, the alerts pipeline, DRP, and Level III data producers. It should also get feedback from our resident wcs experts.    Whether those use cases are currently implemented in our code, or could be generalized from it, isn't important to this RFD as this information will feed into a subsequent requirements document and RFC about what needs to be written/changed.    Some use cases/buzzwords that will likely be included:   * Efficient x,y -> x',y'   * Stacking a sequence of transformations efficiently   * easy extensibility   * should color terms be included in the Wcs or dealt with elsewhere?   * pixel distortion effects (e.g. tree rings, edge roll off)   * Simultaneous Astrometry (i.e. from image stacks)"
1,Sever side Histogram for variable bin size,"For any given column or columns (expression of columns such as col1+log(col2) ) of a IpacTable data, the variable bin histogram is needed.  The variable bin is based on ""Bayesian Blocks"" algorithm (http://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/).  The output is a new IpacTable (DataGroup) with three columns, numPoints, binMin and binMax."
2,LSST Wcs requirements document,"Based on the information compiled from DM-4153 and DM-4152, prepare a requirements document (LaTeX, with references) describing all of the known Wcs requirements for the various portions of the LSST stack. This document could live in afw or in its own repository, and could potentially become a published technical report/conference proceeding."
3,Evaluate existing Wcs libraries and report on our options,"As part of the requirements document in DM-4155, we need a report-likely section(s) of that same document-on the currently available Wcs libraries (including, but not limited to AST Starlink and astropy.Coordinates) and summaries and references from the current literature about how other surveys and projects have managed their Wcs.    For each of the currently existing options, the report should include at a minimum:   * implementation language.   * API and languages that can interface to that API.   * how well supported (including number of contributors) is the project and how active is ongoing development.   * its performance and optimization for both scalar and vector calculations, and its scalability to large data sets.    We also need to look for and/or request technical reports and other literature from existing and completed surveys, including DES (Robert Gruendl), Pan-STARRS (Paul Price), STScI (Erik Tollerud), and SDSS (Lupton and/or Blanton?)."
1,Provide a recommendation for how to manage Wcs in LSST,"The technical report from DM-4155 and DM-4156 should have as its conclusion a recommendation for what Wcs system the LSST stack should adopt (either our own implementation, a third-party library, or some combination thereof). Beyond the conclusion section of that report, this will be provided as an RFC that includes:     * executive summary   * mock API   * links to the relevant documentation (beyond just the above technical report)   * a bullet list of the rationale for this decision    The conclusion of this RFC represents the end of this Epic."
0,Unused variables in meas.algorithms.utils,"Pyflakes 1.0.0 reports:  {code}  $ pyflakes-2.7 utils.py  utils.py:232: local variable 'chi2' is assigned to but never used  utils.py:481: local variable 'numCandidates' is assigned to but never used  utils.py:482: local variable 'numBasisFuncs' is assigned to but never used  utils.py:487: local variable 'ampGood' is assigned to but never used  utils.py:492: local variable 'ampBad' is assigned to but never used  {code}  In the best case, those variables are simply unnecessary, and they should be removed to simplify the code and avoid wasting time. Alternatively, it's possible that they ought to be used elsewhere in the calculation but have been omitted accidentally. Please establish this for each one, then either remove them or fix the rest of the code."
1,Please implement a warper that works with a single XYTransform,"At present we only warp images based on a pair of {{Wcs}}. This is needlessly restrictive. We should be able to define the transformation by function that computes {{f(x,y) -> x',y'}}, e.g. an {{XYTransform}}.    Note that reversibility, while not strictly necessary, is very desirable. Hence we might as well use {{XYTransform}}.     I suggest we have only one underlying implementation in order to avoid code duplication. This could easily be done by implementing an {{XYTransform}} that combines a pair of WCS."
2,Create a skeleton framework for Firefly using redux and react.,"As part of the GWT conversion to pure JavaScript, we've came up with a new design based on redux and react.  This task is to create all of the major components of the framework with minimal functionalities.  This will allow other developers to build upon this foundation core."
0,Take upstream boost 1.59 patch to squelch warnings for gcc 5.2.1,"Under gcc 5.2.1, use of boost 1.59.0 produces a torrent of compiler warns from within boost headers about use of deprecated std::auto_ptr (see https://svn.boost.org/trac/boost/ticket/11622).    A patch for this is already committed upstream in boost.  It is proposed that we take this patch into the lsst t&p in interim until the next official boost release."
0,Internal documentation of procurement process,NULL
1,"productize ""Data repository selection based on version""",finish & productize work from DM-5608
2,Butler: move configuration (.paf) file into repository,"We want to be able to keep the .paf file in the repository, with the data for which it provides configuration information.    At least for the time being it needs to be optional; it looks first in the repository and second in the ""old"" location for the paf.    In the case of a repository chain, the child should 'win'."
2,Butler: change configuration from .paf to something else,"for ""something else"" there are 2 major options: pex_config (dislike because the 'data' gets executed), and yaml (very well supported, is just data). 3rd option: we could use the sqlite registry, and define a schema for that.     (sqlite benefits: has support for write locks that will be necessary very soon. Except, need write-once-compare-same mechanism; so we'll need that 1. for normal files and 2. across n nodes. so maybe there's no benefit).    also, while we're there:  as noted in DM-4170, registries.py needs to not use astropy and instead should use pyfits. If it's quick, that change should be made here (or turned into a separate ticket)"
0,Build AL2S vlans from Miami to NCSA,To prototype the layer 2 circuit LSST will eventually have.
0,document proposal for Base site to NCSA data transfer,"With Steve and James distill the actual data movement requirements, the mechanism to broker those transfers and the network technology to ensure the real-time transfer of images"
1,Provide network infrastructure support to deployment of new nodes,NULL
0,Create baseline requirements for evaluating server hardware from different vendors,NULL
0,Refine server evaluation specifications into a usable quantifiable form,NULL
0,Discussions with Ron on Base site architecture,NULL
0,update LSE-78 once current updates are applied,There are several sections with inaccurate information about the North American portion of the LHN and the implementation of the networking into NCSA as well as the base site commissioning cluster architecture.
3,Butler: provide API so that a task can define the output dataset type,"The task needs to be able to specify everything that needs to be in the policy file so that the butler can put and get data for a new dataset type.    Consider that the policy data can be split between the camera-specific and the task-specific parts. (KT was thinking of calling the camera part the ""genre""), this potentially reduces the amount the task has to specify.   Another option is to hard-code some some of the policy in the butler itself:  * the path template (it could be assembled out of the data id components)  **  if it's hard coded the task must pass the component dataId keys  ** if it's not hard coded the task must provide a template   * python type  * storage type    there could be user overrides too."
2,Butler: add support for skymap based dataIds,"load the skymap from the repository and use it a la metadata lookup to complete lookups. for example if the script wants all the patches that are in tract x, look in the skymap to get that information.    will be something like:  create a skymap object given a configuration (in the repository) (it has its own dataset type)  create a registry with that skymap object  use that registry to lookup skymap related parameters. Might need to add to the policy file that a given configuration uses the skymap.    requires that data is already indexable by tract & patch, the work is adds iteration over tract/patch specified by the skymap.    example use case:  {code}  {      datasetType = 'coaddTempExp'      key = (?)      format = ['patch', 'visit']      dataId = {'tract':<some numberOrId>}      myButler.queryMetadata(datasetType, key, format, dataId)  }  {code}"
1,Experiment with memcached for secondary index,"Test whether memcached could be used to serve objectId --> chunkId mapping, in particular from the performance perspective."
1,Experiment with xrootd for secondary index,"Test whether xrootd could be used to serve objectId --> chunkId mapping, in particular from the performance perspective."
1,Experiment with c-style arrays for secondary index,"Use simple C-style array (chunk[objID], allocated up to maximum number of objectIDs), or a single-layer set of arrays (chunk[block][objID%blksize], where second index runs from 0 to size of each block) to store index compactly and provide minimum overhead for lookups."
0,Infrastructure Security at Site,NULL
1,Outline of data flow,Outline of data flow between Chile->NCSA and NCSA->Chile.
3,L1 system functions and responsibilities ,Outline functions and responsibilities for all parts of the L1 system
2,Initial code change to run ISR with DECam raw data,"This ticket is for implementing changes in {{obs_decam}} in order to run {{processCcd}} with raw DECam data.    Changes are mostly in {{DecamMapper}} and a new class {{DecamIsrTask}} is added. A test to retrieve defects with Butler is also added. ({{testdata_decam}} is at lsst-dev /lsst8/testdata_decam/ )    The pixels with bit 1 (bad, hot/dead pixel/column) from the Community Pipeline Bad Pixel Masks are used as bad pixels.  The CP BPM fits files are directly used as defect files. Due to their large size, they probably should not go into {{obs_decam}} repository so they are treated similarly as other calibration products.     With the changes of this ticket, the following ingest defects files into {{calibRegistry}}:  {code:java}  ingestCalibs.py . --calibType defect path-to-bpm/*fits  {code}    and the following should run past ISR:  {code:java}  processCcd.py .  --config isr.doFringe=False isr.assembleCcd.setGain=False calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment""  {code}    Running fringe correction with DECam raw data will be in future tickets (DM-4223 and possibly more). Also this ticket does not cover implementing or porting new ISR functionalities that haven't yet been included in ip_isr (such as crosstalk).   "
0,Other LOE -- Oct 2015,"Local LSST group meetings, Ethics training, or other local meetings or tasks to comply with NCSA policies"
1,Automate LSST Firefly standalone releases using Jenkins,"This task involves merging in feature branches, building Firefly standalone, tagging, push changes to github, generating changlog, and using github API to publish the release.  Release should be attached to the latest tag with downloading artifacts and changelog."
0,Python LogHandler does not pass logger name to log4cxx,"Not sure how or why it happened, but presently Python LogHandler for lsst.log does not pass logger name to log4cxx layer and all messages from Python logging end in root logger. "
0,Build proof-of-concept package documentation for lsst.afw,"Build package documentation for {{lsst.afw}} under the new doc platform as a demonstration.    - Install a Sphinx site in lsst.afw/doc  - Implement MVP documentation pages for lsst.afw packages (table, image, etc.)  - C++ API reference from doxygen+breathe  - Python API reference from numpydoc    This ticket *will not* attempt to add new documentation content; only to show how existing content can be re-organized."
3,Build ltd-mason for running a multi-package software documentation build,"LSST the Docs ([SQR-006|http://sqr-006.lsst.io]) is a system for extending our existing Jenkins build infrastructure to build Sphinx-based software documentation for our Eups-based packages. This is a ticket to implement the {{ltd-mason}} service, which runs on Jenkins after the {{buildlsstsw.sh}} step and compiles software documentation.    Specific outcomes of this ticket are    - Full specification of the YAML interface between {{ltd-mason}} and {{buildlsstsw.sh}} (including creating a mock YAML file for local testing)  - Demonstration of a Science Pipelines documentation build on a local lsstsw environment (in conjunction with content from DM-4195)  - Accommodations to the science pipelines documentation repo and documenteer for building sphinx packages are included in this ticket’s scope.    Next steps are    - Standing up the service on Jenkins and testing integration with {{buildlsstsw.sh}}  - Uploading to S3 (which involves building integration with {{ltd-keeper}}."
0,"Improve Qserv master robustness for queries like ""select @@max_allowed_packet""","This king of query crashes Qserv:    {code:bash}  mysql --host 127.0.0.1 --port 4040 --user qsmaster -e ""select @@max_allowed_packet""  {code}"
1,Documentation and technical debt in meas_base/PixelFlags.cc,"The port from HSC of SafeClipAssembleCoaddTask has left some documentation and code quality changes to be made. Specifics include:    * In the process of porting, functionality was added which allowed users to specify additional mask planes to be converted to pixel flags. However this was fundamentally incompatible with the flag handler functionality that LSST was currently using. PixelFlags was thus modified to allow SafeClipAssemble coadd to work with the user defined masks, but that made it fundamentally different than the other plugins. In the future this plugin should be brought more in line with all the other measurement framework. This will most likely involve rewriting sections of the measurement framework, to add the ability for users to more directly set runtime behaviors of the measurement plugins (such as specifying non default mask planes to work with).  * Because PixelFlags could no longer use the the flag handler framework, sections of the SafeCentroidExtractor had to be duplicated within PixelFlags. This duplicated code is non-ideal and should be rectified. Possibly in the process of rewriting the measurement framework, the utils could be expanded to have convenience methods to access functionality when not using flag handlers  * As with all of the measurement framework, PixelFlags needs better documentation. This includes some line to line comments, but more importantly the over all functionality of the routine needs documentation. This includes: what the routine does; how it works; and why various design decisions were made"
0,Revert temporary disabling of CModel in config override files,Revert the temporary disabling of CModel that relates to a bug noted in DM-4033 that was causing too many failures to test that processCcd (etc.) would run all the way to completion (most of the other fixes/updates related to the initial disabling in the multiband tasks have now been completed in DM-2977 & DM-3821).     Relevant files:  {code}  config/processCcd.py   config/forcedPhotCcd.py   config/forcedPhotCoadd.py   config/measureCoaddSources.py  {code}  
0,wmgr should delete database from inventory when dropping it,When wmgr drops database it should also cleanup chunk inventory for that database.    
1,Research web authentication and authorization and gather usage stories,"Research SUI, DAX, Butler, and Qserv authentication and authorization requirements and schemes. Document usage stories for all layers"
1,Create unit tests for SafeClipAssembleCoadd,"Porting SafeClipAssembeCoadd from HSC to LSST left that functionality without a unit test. Currently AssembleCoadd is tested from within tests/testCoadds.py. This test does not call AssembleCoadd directly however. The actual code pertaining to assembling a coadd exists within python/lsst/pipe/tasks/mocks/mockCoadd.py. This called from testCoadds, and is used to build and coadd synthetic images from known psfs amongst other things. This should be expanded to test both AssembleCoadd and SafeClipAssembleCoadd, possibly with some sort of argument to the function call. It is important to keep testing both methods of generating a coadd."
1,Create documentation and examples for SafeClipAssembleCoadd,"SafeClipAssembleCoadd in HSC did not have adequate documentation, and thus neither does LSST post port. Documentation which details the functionality and usage of this function should be created, and should be available either through the [Doxygen task documentation|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/group___l_s_s_t__task__documentation.html] or its successor . Examples of the usage and various options should also be included with the documentation."
1,Investigate shear bias errors from DM-1135,"DM-1135 mostly was inconclusive, due to the fact that the error bars were around the same size as the differences in many of the tests.  This in spite of the fact that we ran 6x as many sample galaxies as great3sims.    Investigate the reason for this, and see if we can estimate the errors more accurately.    "
1,Rerun tests of DM-1135 with a larger number of galaxies,"As a result of the new error estimation, it became apparent that a larger number of sample galaxies were required in DM-1135.  This is an expansion of that test to a larger number of galaxies.  Since the original  great3 tests had about 2 million galaxies, we should be able to do these test reasonably well with the 6 million galaxy pool created in DM-1135.  However, the measurements need to be rerun in some cases, and the error analysis done again."
1,Initial tests of ShapeletApprox,"This will be part of a wider set of test which I am hoping that Jim will fill in as additional stories under DM-1136    Basically, we hope to see how extensive a Shapelet Approximation must be done for a Psf in order to reach a stable result (stable meaning it would not markedly improve with increase in Shapelet order).    For this intial test, I propose to compare SingleGaussian, DoubleGaussian, Full, and 2 higher order models.  I will also add a model with inner and outer defined to see if those have a significant effect.  It will tell use how the existing models line up and which parameters matter.    This test will be done using the great3sims subfield organization, and with a single, randomly chosen Psf image from the 0.7 arcsec FWHM library (raw fwhm from PhoSim, the actual fwhm is closer to 0.8 arcsec)     This will probably be just the beginning of more extensive ShapeletApprox tests, so this story should be a sub-story of a larger test project which I am hoping Jim will define."
1,Butler: SafeFile and SafeFileName can overwrite good with bad in some cases,"A bad file can be written in Butler, in the case where 2 temp files that use SafeFile or SafeFileName to the same location are started, one closes, and then the other fails - and then closes and writes the bad file. Need to handle the exception in a way that does not write.    Also, in the case of no exception (failure), when closing B, need to compare to A and throw if different."
0,Package capnproto for eups,"Prototype is now getting to the point where a wire-protocol package like capnproto or protobuf is needed.  capnproto is the new hotness, and we're probably going to want to migrate qserv from protobuf->capnproto at some point.    This task is to go ahead and get capnproto packaged and published for use in the replication prototype."
1,Convert copyright/license statements to one-liners for RFC-45,"Refactor how we manage copyright and license information in stack repositories    # Identify a list of packages to process  # build and test an automated system of       - adding a global COPYRIGHT to each repo. Content will be “Copyright YYYY-YYYY The LSST DM Developers”. Years will be determined by git history.     - adding a GPLv3 license file to each repo.     - changing the boilerplate in all files to say ""See the COPYRIGHT and LICENSE files in the top-level directory of this package for notices and licensing terms.” Use https://gist.github.com/ktlim/fdaea18ab3d39afdfa8e     - automatically branch, commit, merge and push    And deploy this automated system."
3,X16 Secondary Index - Implementation,Implement / productize optimizations to secondary index proposed through DM-2119
0,IsrTask calls removeFringe in FringeTask but the method does not exist,The method {{removeFringe}} of {{FringeTask}} is called in {{IsrTask}} but there is no {{removeFringe}}.      Not sure if {{removeFringe}} was meant to be a place holder
0,getExposureId not implemented in obs_lsstSim,There is no implementation of the getExposureId method in processEimage.py.  This causes it to fail using a modern stack.
1,Collect single-host performance data for secondary index,"Run production-scale (billions of entries) tests on different index options, collect performance statistics for allocation (CPU, memory) and for queries."
1,Set up multi-host tests for secondary index technologies,"For client-server technologies (memcached, xrootd, etc.), develop multihost test jobs to exercise production-scale indices (billions of entries, millions of queries).  Index should be allocated on single ""master"" machine, with queries generated from one or more separate machines, possibly with multiple threads/jobs per machine."
1,Experiment with bulk updates to secondary index,"The nightly data loader job may need to add new objectIDs, or change the chunks of existing objectIDs, _en masse_.  Develop test code (or add features to existing demonstrators) to handle bulk loading of new data to the index, or to overwrite collections of existing data.  This is significant for client-server technologies, where the bulk data may be transferred in a single transaction, with the server (possibly) handling the loop over individual elements."
1,Collect multi-host and bulk-update performance data for secondary index,"Run secondary index tests across multiple hosts (server and clients), collecting performance data for production-scale indices (billions of entries, millions of queries) with many parallel queries and bulk/block updates."
1,Identify candidate technology for secondary index,"Evaluate results of production-scale performance tests, both single and multiple host.  Identify the technology most likely to meet requirements, and estimate performance capability with respect to those requirements"
0,Port HSC-1355: Improved fringe subtraction,"[HSC-1355|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1355]: ""with this fix, we get much  better fringe subtraction""."
3,Data Distrib proto (nov),NULL
1,Variance is set after dark subtraction,"In the default {{IsrTask}}, the variance is currently set after dark subtraction.  This means that photon noise from the dark is not included in the variance plane, which is incorrect.  The variance should be set after bias subtraction and before dark subtraction.    [~hchiang2] also points out (DM-4191) that the {{AssembleCcdTask}} with default parameters requires amplifier images with variance planes, even though the variance cannot be set properly until after full-frame bias subtraction.  I believe that {{AssembleCcdTask}} only requires a variance plane in the amp images because it does an ""effective gain"" calculation, but I suggest that this isn't very useful (an approximation of an approximation, and you're never going to use that information anyway because it's embedded in the variance plane with better fidelity).  I therefore suggest that this effective gain calculation be stripped out and that {{AssembleCcdTask}} not require variance planes."
1,HSC backport: Jacobian and focalplane algorithms,"Add algorithms to compute the *Jacobian* correction for each object (calculable from the Wcs, but sometimes convenient)  and record the *focal plane* coordinates (instead of CCD coordinates) for sources (useful for plotting).    The standalone HSC commits to be cherry-picked are:    *Jacobian*  {{meas_algorithms}}  May 3, 2013  [Jacobian: add Algorithm to compute the Jacobian.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/88d3bd3f32cf4d0138b80148e57bc275fc8c3454]  May 24, 2013  [Jacobian: fix up some cut/paste oversights.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/ecad0d2559bb9815fc5560234f4502f35f50db73]  May 28, 2013  [Jacobian: fix calculation|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/7f3db53b56279929b9e416173ed09cf00dc81406]    {{obs_subaru}}  May 3, 2013  [config: enable jacobian calculation in processCcd|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d0969911ee1a655fd82998f0b936fa90f443d2fd]  May 6, 2013  [config: set pixelScale for jacobian correction|https://github.com/HyperSuprime-Cam/obs_subaru/commit/e36bd1b4410812ca314f50c01f899d92acc0e7a5]      *focalplane*  {{meas_algorithms}}  May 24, 2013  [add algorithm to calculate position on the focal plane|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/dda3086f411d647e1a3e15451d7f093cd461873a]  May 25, 2013  [fix up building of focalplane algorithm|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/57d718bf51b255adf5789e389dfb776ecaa062d1]  Nov 21, 2014  [Adapt to removal of Point<float> from afw::table.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/95627d55cb7d64718a42027954474df5c3661a65]    {{obs_subaru}}  May 24, 2013  [config: activate focalplane algorithm|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d999a32e7e10b25cceccc94b61890486f96c0bfd]  "
1,HSC backport: countInputs and per object variance functions,"Back port of the following two HSC tickets:    *countInputs*  [HSC-1276|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1276]  {{meas_algorithms}}  Jul 1, 2015  [add measurement algorithm to count input images in coadd|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/51db0fd2624c7f9b641c93aa3cf6366539995d50]    {{obs_subaru}}  Sep 24, 2015  [config: activate countInputs for measureCoaddSources.|https://github.com/HyperSuprime-Cam/obs_subaru/commit/13ecd1317b05b5ff9e65fba41fe27a5cffcc2fda]    *variance*  [HSC-1259|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1259]  {{meas_algorithms}}  Jul 2, 2015  [add measurement algorithm to report background variance|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/86022f4381c3cec3f7f203b831b6a306596cfa3f#diff-7ae7aea69b58dbf075350ccfd3802cfb]    {{obs_subaru}}  Oct 19, 2015  [config: activate measurement of variance for coadds|https://github.com/HyperSuprime-Cam/obs_subaru/commit/cf1e80958bb9164dacf42d2d35a94dd366c78892]  "
0,Specify default output location for CmdLineTasks,"When neither {{\--output}} or {{\--rerun}} is specified as an argument to a {{CmdLineTask}}, any output from that task appears to be written back to the input repository. Note the use of the term ""appears"": from a preliminary inspection of the code and documentation, it's not clear if this behaviour can be overridden e.g. by environment variables.    The HSC stack behaves differently, using {{$INPUT/rerun/$USER}} as a default output location. A [brief discussion|https://community.lsst.org/t/new-argument-parser-behavior-rerun-flag-introduction-discussion/345] suggests that this is the preferred behaviour.    Please update the LSST stack to match the HSC behaviour."
0,unable to upload images to nebula,"I seem to be unable to upload an image to neblua from a URL via either horizon or the nova cli client.  The request seems to queue briefly and then reports a status of {{killed}}. Eg    {code}  glance image-create --name ""centos-7.1-vagrant"" --disk-format qcow2 --container-format bare --progress --copy-from http://sqre-kvm-images.s3.amazonaws.com/centos-7.1-x86_64 --is-public False --min-disk 8 --min-ram 1024  {code}"
0,Fix integer casting error in numpy version 1.10 in obs subaru,Fix type casting in obs_subaru in lates numpy in obs_subaru
1,Identify Qserv areas affected by secondary index,Evaluate Qserv software for the Czars and workers to identify where an interface to the secondary index will be required for efficient operation.
2,Implement secondary index service in Qserv,"Implement the selected seconary index technology (see DM-2119) as a Qserv service, providing a client API to be used elsewhere in the Qserv system.  Depending on the chosen technology, this may including configuration of a separate lightweight server for the secondary index, creation of database tables, etc.  The client-side API should be technology independent."
1,Implement bulk updating of secondary index in Qserv,"Provide a service in Qserv to support creation or modification of secondary index objectID-chunk pairs in bulk, to support data loading."
1,Implement secondary index query in Qserv,"Implement query of secondary index in Qserv, using the technology selected in DM-2119."
0,Draft of Configuration Solicitation,Draft of solicitation to be used for quote and configuration from multiple vendors for FY16 purchase. 
0,Image Viewer memory leak,"When reloading the same 500MB RAFT image into an image viewer (see the script below), it was discovered that single node Firefy server with 3G memory runs out of memory after ~15 reloads    Test case: keep reloading the html file with the following Javascript, creating an image viewer with 500MB image:    function onFireflyLoaded() {          var iv2= firefly.makeImageViewer(""plot"");          iv2.plot({               ""Title""      :""Example FITS Image'"",               ""ColorTable"" :""16"",               ""RangeValues"":firefly.serializeRangeValues(""Sigma"",-2,8,""Linear""),               ""URL""        :""http://localhost/demo/E000_RAFT_R01.fits""});  }    Follow up:    The bug was traced to java.awt.image.BufferedImage objects not being evicted from VIS_SHARED_MEM cache.    Further search showed that java.awt.image.BufferedImage (along with java.io.BufferedInputStream) is in src/firefly/java/edu/caltech/ipac/firefly/server/cache/resources/ignore_sizeof.txt, which lists the classes that have to be ignored when calculating the size of cache.    Testing on single node server (VIS_SHARED_MEM cache is not replicated), using [host:port]/fftools/admin/status page:    BEFORE (java.awt.image.BufferedImage was commented out in ignore_sizeof.txt)    After 14 reloads:  Memory    - Used                      :      3.7G    - Max                       :     3.55G    - Max Free                  :    488.0M    - Free Active               :    488.0M    - Total Active              :     3.55G     Caches:   	VIS_SHARED_MEM @327294449  	Statistics     : [  Size:15  Expired:0  Evicted:0  Hits:246  Hit-Ratio:NaN  Heap-Size:1120MB  ]  OUT OF MEMORY on next reload    AFTER THE CHANGE (Commented java.awt.image.BufferedImage in ignore_sizeof.txt)    After 36 reloads:  Memory    - Used                      :   1672.9M    - Max                       :     3.55G    - Max Free                  :   1968.0M    - Free Active               :   1468.0M    - Total Active              :      3.6G    Caches:   	VIS_SHARED_MEM @201164543  	Statistics     : [  Size:3  Expired:0  Evicted:34  Hits:659  Hit-Ratio:NaN  Heap-Size:1398MB  ]    "
0,Local LSST Sec Meeting,Local cyber security meeting at NCSA with DM group.
1,Image viewer: choosing pixel interpolation algorithm for scaled images,"Pixel values are defined at integer coordinate locations. This means that when an image is rendered in a scaled, rotates, or otherwise transformed coord. system, an interpolation algorithm should be used to provide a pixel value at any continuous coordinate.    Currently, Firefly is using     RenderingHints.KEY_INTERPOLATION = RenderingHints.VALUE_INTERPOLATION_NEAREST_NEIGHBOR,    which means that when an image is rendered in a transformed coord. system, the pixel value of the nearest neighboring integer coordinate sample in the image is used.     ""As the image is scaled up, it will look correspondingly blocky. As the image is scaled down, the colors for source pixels will be either used unmodified, or skipped entirely in the output representation.""    Jon Thaler's team would like to be able to choose a different interpolation algorithm, depending on the situation.      As an example see various resize algorithms in [thttp://stackoverflow.com/questions/4756268/how-to-resize-the-buffered-image-n-graphics-2d-in-java]."
0,LSST PO Security Meeting,Bi-weekly meetings with PO to discuss cyber security issues in LSST.
0,Please include obs_subaru in CI,{{obs_subaru}} should be included in the CI system.
0,Create GitLFS Technical Note,Create a SQuaRE Technical Note describing the architecture of the GitLFS service implementation.
2,Literature search for DCR -- Reiss,Go through the literature to find relevant seminal papers on DCR.    The outcome will be a bibliography and executive summary.  This should be posted on Discourse.
1,Implement simple reference index files,"We would a very simple way to make small reference catalogs for astrometry and photometry. The use case is testing and small projects, where the overhead of making a full up a.net index file (or whatever we replace that with) is excessive."
2,Determine scope and requirements for CalibrateTask redesign,"There are two stories in DM-3579 that define two requirements for the redesigned CalibrateTask.  Those along with the requirement that CalibrateTask be less brittle are a starting point for what the new task needs to do.  All other requirements should be gathered using the RFC process.    Using the input from the RFC and other requirements, the scope for this particular redesign will be defined and stated in a discourse thread."
0,Sphinx support of sqr-001 technical note,Support the distribution of a technical note SQR-001    - Remove oxford comma in author list (documenteer)  - Solve issue where title is repeated if the title is included in the restructured text document  - Solve issue where name of the HTML document is README.html
1,Investigate current dipole measurement examples and tests,There are tests in ip_diffim/tests/dipole.py for the dipole fitting.  There is also an example in the examples directory of ip_diffim.  This story will investigate these tests and examples to see to what precision the tests go as well as how complete the tests are.    An outcome of this will be an understanding of how precise the tests need to be to show that dipole measurement is behaving as we expect.
1,Create a set of tests (or update the current ones) to facilitate refactoring of dipole measurement,This will create a test (not necessarily a unit test) that will simulate dipoles and measure them so that the measurement can be compared to truth values.  This may be simply refactoring the current tests.    This task should also include generating more generalizable utilities needed to create the dipoles and incorporating these and other test data into the stack so that they can be used in other studies.
3,F17 Integrate L1 Database with Alert Production pipeline,Integrate the prototype of the L1 database built through DM-2036 with the Alert Production pipelines.  The design of the Alert Production L1 database is covered [here|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog].
0,Integrate qserv docs into the new doc system,"See Frossie 11/04/2015 email to qserv-l list:    {{5- Docs. So you guys have a sphinx site. Fab, that’ll make it super easy to drop it into the new doc system that hosts sphinx on readthedocs - you can see a small example here http://sqr-001.lsst.codes/en/master/ - the idea is to continuously deploy the docs so the release step should be very lightweight and doable via API calls. (My holy grail is to do a release with no local checkout involved).  }}"
1,SQuaRE design meeting 1,Hold an in-person design discussion with members of the SQuaRE team.  
0,SQuaRE supertask design meeting 2,Hold a teleconference design discussion with members of the SQuaRE team.  
3,Create SuperTask design proposal,"Create a design document covering the initial SuperTask concept and a scheme for how it would lead to solutions to longer-term pipeline construction problems, support provenance recording, etc."
1,Should only read fringe data after checking the filter,The fringe subtraction is not necessarily performed if {{doFringe}} is True. It is only if the filter of the raw exposure is listed in config fringe.filters.      Fringe data should not be read unless the filter is indicated. There are likely no such filter data and it would cause runtime errors.      Seems related to changes from RFC-26 and DM-1299. 
1,Adapt an existing task to be usable as a SuperTask,"Either by wrapping, or by converting, make an existing task usable as a SuperTask subclass so that it can be run under an Activator."
3,FY20 Define Procedures and Build Tools for Schema Evolution,"As described in [LDM-135|http://ldm-135.readthedocs.org/en/master/#data-production-related-requirements], The database system must allow for occasional schema changes for the Level 1 data, and occasional changes that do not alter query results for the Level 2 data after the data has been released. This epic involves preparing for dealing with schema changes in production syste."
0,Week end 10/03/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 3, 2015."
1,Week end 10/10/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 10, 2015."
1,Week end 10/17/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 17, 2015."
1,Week end 10/24/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 24, 2015."
1,Week end 10/30/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 30, 2015."
1,Planning for new equipment setup (week end 10/03/15),"* Planning for hardware upgrades in racks  * Evaluate the setup of the new storage server installs lsst-store101, lsst-store141,lsst-store143,lsst-store144  * Set up and tested ESXi server on new Mac Pro. Worked on getting Mac OS X installed inside of ESXi on the new Mac Pro hardware. Found solution, need to find a better one if we are expecting to bring up and tear down instances on demand."
1,New equipment setup (week end 10/10/15),"* Planning on how to start setting up for new equipment    * Started to coordinate with Josh Hobblitt on what has been ordered and delivered   * Received 15 Dell R730 servers (plan to set up in temporary rack before Thursday’s maintenance)  * Installed spare rack at the east end of NCSA 3003-Row A. Added three 125V 30A drops to spare rack  * Resolved problems with Puppet on lsst-stor101  * Updated OS on lsst-stor101, lsst-stor142-144  * Installed ZFS on lsst-stor101, lsst-stor142-144  * Checking config of stor142-144 and stor101 for Thursday outage  * More ESXi testing with Mac Pro"
2,New equipment setup and regular maintenance (week end 10/17/15)  ,"* Unpack and mount 6 Dell R730’s  * Update the bios and firmware on 6 Dell R730’s  * Install ESXi 6.0 on 6 Dell R730’s  * Rearranged hardware in NCSA 3003, move systems around in racks, reconnect power and networking, troubleshoot startup issues   * Applied kernel and other software updates to infrastructure as it was moved  * Setup vSphere vCenter server and compute nodes  * Discovered need to order drive caddies for lsst10 and updated iDRAC Enterprise license for 15 new R730 servers"
1,New equipment setup and configuration (week end 10/24/15)  ,"* Unbox and mount six UPS in racks, mount two new power panel PDU's for 30 Amp service, connect power cabling to UPS and to each of the supported systems. TODO: Complete the setup on each of the servers.  * Unbox and mount in the rack two R730 servers, connect network and power  * Itemized list of new Dell servers received  * Debugging networking issues on new lsst-esxi1 server - confirmed it's not an issue with switch ports or cables  * Setup vSphere virtual networking and moved 4 test VMs to new setup  * Setup vSphere Data Protection (Vmware backups via snapshots)"
1,New equipment setup and configuration (week end 10/30/15),* Moved spare rack to Row C  * Documented setup of new LSST vSphere setup (https://wiki.ncsa.illinois.edu/display/LSST/LSST+vSphere)  * Debugging networking issues on new lsst-esxi1 server - testing with new/alternate hardware  * Installed 6 new drives for historical log storage on lsst10 MySQL server
0,bi-weekly IaM meeting,Meeting Oct 22nd  Notes on LSST confluence
1,Bootcamp meeting,IAM group attended DM bootcamp
0,"write meeting agenda, for Oct 19 meeting ",NULL
0,"Conduct, document, initial follow through on Oct JCC meeting ",NULL
0,Visit to Argonne - facility coordination,NULL
2,Specify L1 system - design and WBS plan for construction,"Spent 3 days in focused planning to knock out version 1 of the L1 system plan.  SPs cover me, Don."
3,Processing of DECAM COSMOS field - Part I,"This story covers work on the verification plan done in October.     The DECAM Cosmos field was selected, however DECAM ISR is not available so the starting point for now is Community Pipeline reduced data. Have put the data through processCcdDecam and makeCoaddTempExp but had a number of failures that we have so far not been able to pin down. A list of user experience issues is being collated and [~frossie] will generate Summer 16 cycle stories to address those that fall within SQuaRE's defined scope of activities.     Story closed to fit within month, but work is ongoing. "
0,Software Stack Introduction,Installation of LSST stack for initial steps towards further understanding of the software components of the DM architecture. 
1,L1 Function and Design Mtgs,2 days of design discussions and refining functional diagrams of the Alert Productions and Image Ingest system.
0,Vendor Discussions re: specification documents,Specification document sent. Discussions with vendors covering document and schedule.
0,SC15 Scheduling and Prep,"Vendor appt scheduling and conference workshop scheduling, logisitics, etc for SuperComputing 15 in Austin"
0,LSST IaM Project,NULL
2,investigate distributing automated lsst_apps builds via docker containers,NULL
1,Run and document multinode integration tests on Openstack+Docker,"Boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests.    FYI, lack of DNS on OpenStack Cloud cause problems, but a vagrant plugin seems to solve this."
1,Write up some introductory guides for Nebula usage,We write up some introductory guides for Nebula usage to enable new users to get started.  These are located on Confluence under :    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide
0,want equiv of m1.xlarge flavor with smaller disk,"I'd like to be able to build images with vcpus & ram from the {{m1.xlarge}} flavor that can be run on a {{m1.medium}} with it's smaller disk image, this would require a new flavor with 16GiB ram/8vcpus but only 40GiB of disk.  Something along the lines of:    {{openstack flavor create --ram 16384 --disk 40 --vcpus 8 ...}}    Is that possible?"
1,Vagrant for Nebula OpenStack,Create and document a Vagrant configuration to use [~jhoblitt] lsstsw machine images on NCSA's Nebula OpenStack cloud.
1,Convert banner and menu to react/flux,"Add flux data model to capture menu and banner information.  Convert banner and menu UI from gwt to react.  As part of this task, bring in Fetch API to simplify client/server interactions."
1,update obs_lsstSim,"obs_lsstSim has seen some bitrot.  In particular, the ingest task and the addition of the getExposureId methods on processImageTask have not been propagated to obs_lsstSim.  This ticket will deal with those issues."
1,re-deploy lsstsw on Jenkins,Pandas was added to the bin/deploy script in lsstsw to support  sims development.  This has already been merged to master in 4b1d1a0fa.  The ticket is to ask that lsstsw be redeployed so the sims team can build branches that use pandas.
0,Add unit testing into gradle build for Firefly's server-side code,Add a test task to Firefly's common build script.  This can be used by any sub-project to run unit test.  Added unit testing to Jenkins continuous integration job to ensure new code does not break unit testing.
3,FY17 Implement Image Caching,"When a file is requested, a cache maintained by the service is checked. If the file exists in the cache, it is returned. If the file does not exist, configurable rules are consulted to remove one or more files to make room for it in the cache, if necessary. (If no room is currently available because all cached files are being used, the request is blocked.) The file is then regenerated by invoking application pipeline code based on provenance and metadata information stored in the repository. The regenerated file is placed in the cache.    This is documented in [LDM-152 §4.1|http://ldm-152.readthedocs.org/en/master/#image-file-services-baseline]."
0,Please add HSC tests to CI,In DM-3663 we (= [~price]) provided an integration test for processing HSC data through the stack with the intention that it should be integrated with the CI system.    Having this test available and regularly run would be enormously helpful with the HSC port -- we've already run into problems which it could have helped us avoid (DM-4305).
0,Try out nebula for stack developing and data processing,"Follow instructions on:    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide  https://community.lsst.org/t/creating-a-nebula-instance-a-recipe/353    and create a nebula instance with the stack, update and install more packages in the stack, test by constructing a data repository and processing ISR with DECam raw images.    "
0,Missing Doxygen documentation,"As of [2015-11-10 02:53.26|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_02.53.27/] there were 19 ""mainpages in subpackages"" available through Doxygen.    In the next build, [2015-11-10 21:16.19|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_21.16.19/], most of them have vanished and we only provide links for {{ndarray}} and {{lsst::skymap}}.    As of filing this issue, they were still missing from the [latest build|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/].    Please bring them back!"
0,Oct. on-going support to Camera team in UIUC,Attend UIUC weekly meeting and give support as needed. 
0,Nov. on-going support to Camera team in UIUC,Attend UIUC weekly meeting and give support as needed. 
1,Configure sshd with pam_krb5 and document,Configure sshd in our test IAM VM to use pam_krb5 so the user gets a Kerberos ticket when logging in as discussed in our design doc. Document the configuration steps.
3,Configure DAX web app with Kerberos authentication in IAM test VM,Install the DAX server (https://github.com/lsst?query=dax) in our test IAM VM at NCSA and configure it to use Kerberos authentication using standard Python flask Kerberos support. Document the process.
0,Set up test IAM MariaDB instance with Kerberos,Install MariaDB in our test IAM VM at NCSA and configure it for Kerberos password authentication. I understand that Kerberos ticket authentication is not yet supported by MariaDB. Document the process and include references to the ongoing work to add Kerberos ticket support.    Create example tables that demonstrate access control for different logged in users.
1,Configure sssd with NCSA LDAP for accounts in test IAM VM,Configure [sssd|https://fedorahosted.org/sssd/] with NCSA LDAP for accounts in test IAM VM using instructions from Doug Fein.
1,Configure httpd with SSL and mod_krb5 in IAM test VM,Get SSL certificate and configure httpd for mod_auth_krb5 authentication in IAM test VM. Document the setup.
0,Replace fitsthumb in obs_subaru (port HSC-1196),"{{fitsthumb}} is now obsolete; all the functionality we need is available in {{afw}}. Further, we want to drop it as a dependency to make the job of integrating {{obs_subaru}} with CI easier."
0,Bootstrap for new Sphinx/reST/RTD technical reports,"Create a template repository for LSST (DM/SQuaRE) technical reports that are written in reStructuredText, built with Sphinx and published with RTD"
3,investigate distributing automated lsst_apps builds,(aggregation ticket for [previous existing]] tightly related tasks in progress related to binary distribution)
0,Coadd_utils tests should run and skip if afwdata is missing,"Currently, the {{coadd_utils}} tests are completely skipped at the scons layer if afwdata can not be located. This is bad for two reasons:  1. Are there any tests that can be run even if afwdata is missing?.  2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped.  Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch."
0,ActiveMQ Broker upgrade,"Version 5.8.0 of the ActiveMQ broker, which is not part of the regular distribution, is quite out of date.  [~tjenness] is working upgrading the ActiveMQCPP library in DM-4330 to the current version, and this would be a good time to upgrade the broker to the latest release so we try and stay in sync."
2,Sizing model fixes,"* Improve API with LDM-144: need to separate spec of czar nodes from spec of worker nodes  * Revisit model for qserv czar nodes  * Fix chunk count for DiaObject (dbL2 F139)  * Change low volume query result size, 0.5 GB --> 1 MB (sciReq G271).  * Revisit model for shared scans and multiple releases  * Add model for recovery from node failure for Query Access cluster  * Consider modeling intermediate results (concluded that intermediate results are relatively small, and given they are distributed across the cluster, it is not worth modelling them)  * Fix issue with transfer time of nightly products from Archive to Base  ---  * Add Dia* to scans  * Consider modeling nodes for data loading  * LDM-139 and LDM-141 don't document where MOPS Scratch comes from or what it's supposed to hold.  * revisit bulk data transfer from HQ to Base (shipping disks --> over the network)"
0,The astrometry task should print a warning or throw an exception when there is no reference star in the field,When there is no reference star in the field the exception raised by anetAstrometry is identical to the one issued when the fit has not converged.    astrometry (matchOptimisticB) is throwing an exception with the message lsst::pex::exceptions::InvalidParameterError: 'posRefBegInd too big'.  I suggest a test to detect that there is not enough stars to fit the astrometry and to throw an exception accordingly.
3,Global Metadata,"Revisit design of the global metadata for LSST image and file archive. This includes understanding interactions between butler, local per repository metadata, global metadata, and metaserv. Sort out the dividing line between global metadata and dax (e.g., the schema https://github.com/lsst/dax_metaserv/tree/master/sql should live in cat/sql, not in dax)    Deliverable: document describing the architecture of the global metadata, including how pipelines, users and SUI will interact with it. It should cover both writing/ingest and reading aspects."
3,S17 Implement Image and File Archive,Implement fully working system capable of managing images and files. It will be used by the Alert Production system that we will be putting together in FY17
3,F17 Integrate Image and File Archive with Alert Production,NULL
3,FY18 Integrate Image and File Archive with DRP,NULL
3,S17 Butler,NULL
2,Disentangle log messages from different processes,"It's difficult to disentangle interleaved log messages when running with multiprocessing.  Two possible ways to do this would be:  1. Have each process write to a different log file.  Maybe we could allow the user to specify log filenames including {{%(pid)s}} and {{%(hostname)s}}.  More useful would be to allow the full range of keys from a {{dataId}}, but that might require some changes in how processing runs.  2. Prepend each line of the log with context information ({{dataId}}, pid, hostname), allowing the user to use {{grep}} to do the disentangling.  To avoid overwhelming the log with the context information, a hash of the context information could be used instead, with the lookups published in the log before the processing starts.        h5. Brief summary on the changes:  - Chain logs of sub-tasks to their parent task logs  - Add {{PrependedFormatter}} as the new default formatter for log files (specified through {{--logdest}} command line argument for {{cmdLineTask}}). The standard output to the terminal remains the same as before.  - Any string can be used to label a pex_logging Log, and with {{PrependedFormatter}} this label prepends each log message. For {{cmdLineTask}}, the dataId is prepended.  - The HSC's commit on prepending the timestamps is also ported to {{PrependedFormatter}}.  - An example log message:  {code:java}  2016-03-08T22:29:56.889933: [{'filter': 'r', 'tract': 0, 'patch': '2,3'}]: mergeCoaddDetections: Culled 1731 of 7751 peaks  {code}  "
0,Add args to s3s3bucket CLI,Add {{source-bucket}} and {{dest-bucket}} arguments to {{s3s3bucket}} the command line script. This is to allow for one off duplication of buckets.    Increment version to 0.1.10.
0,tested upgraded activemqcpp package,"The activemqcpp package was upgraded as part of DM-4330, and I tested it to be sure the upgrade was backwards compatible with the code that exercises it in ctrl_events.   It is."
0,dax_imgserv 2015_10.0 build error,{{2015_10.0}} has a build error under a current {{lsstsw/bin/deploy}} environment.  Current speculation is that this is related to the conda version of numpy being upgraded to {{1.10.1}}.
2,Replace XML-RPC with in-process communication,"With all recent development in CSS sector we should be able now to get rid of Python in czar entirely. This is also a good opportunity to move from XML-RPC between proxy and czar with direct in-process ""communication"". Daniel said it's a good idea :)  "
0,Fix publishing script async issue and add additional release notes.,Async command execution causes unpredictable and unreliable results.  Switches to synchronous where possible.  Also add additional description to the release notes.
1,Write technote on the new technical note platform,Write a technote about the platform that github.com/lsst-sqre/lsst-technote-bootstrap lets DM members publish in. Discuss current status and outline future plans.
1,"Design Mtg, Review and discussions of L1 processing","Design/planning meeting for L1 system. Materials read and discourse discussions on EFD, MOPs, T&S docs and calibration production"
1,"Reviewing quotes, power requirements, rack layouts","Review of multiple quotes from multiple vendors across full year of purchases. Derived power requirements and rack layouts for placement, networking, electrical work discussions to begin."
0,"SC15 Scheduling, Processor Futures Refresh","Scheduling of several more meeting opportunities for next week. Also, time spent reviewing NDA materials on processor futures. "
3,S19 Run Large Scale Tests,NULL
3,FY19 Implement Qserv Software Upgrading Tools,NULL
3,FY19 Finalize Internal DRP Database,Final changes to make DRP database commissioning ready.
0,obs_subaru fails to compile after DM-3200,"Due to atypical calls in {{obs_subaru}}'s {{hsc/SConscript}} to run scripts in the {{bin}} directory, {{obs_subaru}} fails to compile after the changes made in DM-3200."
1,SuperTask phase 1 implementation,"This story represents the implementation of the first part of the SuperTask framework design,"
0,Implement configuration for activator parsing,Need to add configuration to the CmdLineActivator in new Workflow tasks
1,First implementation demo,First stage demo of Super Task and WorkFlowTask Framework
0,write documentation for registry free repo,NULL
1,Improve overscan correction for DECam raw data,"Currently, the default overscan correction from IsrTask is used for processing DECam raw data. Overscan subtraction is done one amplifier at a time.     However, a bias jump occurs due to the simultaneous readout of the smaller ancillary CCDs on DECam, some images show discontinuity in the y direction across one amplifier, as in the example screen shot.     This ticket is to improve overscan correction for DECam data so to mitigate this discontinuity in the ISR processing.    Arrangement of CCDs on DECam: http://www.ctio.noao.edu/noao/sites/default/files/DECam/DECamPixelOrientation.png      h3. More details:  There are 6 backplanes in the readout system, shown by the colors in DECamPixelOrientation.png. In raw data files, the CCD's backplane is noted in the header keyword ""FPA"".  Examination of some images suggests that science CCDs on orange and yellow backplanes show bias jump at 2098 pixels from the y readout. That is the y size of the focus CCDs.     h3. Actions:  For CCDs on the affected backplanes, divide the array into two pieces at the jump location, and do overscan correction on the upper and lower pieces separately.    "
1,Fix bug and add unit tests for PsfShapeletApprox ,"We discovered during this Sprint that this plugin was giving us faulty values for all the models except for SingleGaussian.  I will fix that bug on this issue.    Obviously, a better unit test would have caught this.  I am adding a DoubleGaussian unit test, plus a test that the default models provide different results.  Also a timing test for all the models, as we do not really have enough information about the performance of the shapelet approximation.  "
0,Duration for various ShapeletPsfApprox Models,This is just a report of the amount of time it takes to run ShapeletPsfApprox and CModel over 10000 galaxies from GalSim
1,Migrate lsst/ci_hsc repo to git-lfs.lsst.codes,"The github lfs backed repo https://github.com/lsst/ci_hsc needs to be migrated to git-lfs.lsst.codes.  A sanity check for any other ""live"" lfs repos under the lsst github org might also be a good idea."
0,Migrate testdata for DECam from disk to git-lfs,"There is a package full with test data for the obs_decam.  It is an eups package currently, but not a git repository.  I would like to migrate that into our hosted git-lfs so the obs_decam package can be built by Jenkins.  [~jmatt] I'm hoping you would be willing to handle this for me.  If not I can find somebody else.  Thanks!"
0,HSC backport: Add tract conveniences,"This is a port of [HSC-715: Add tract conveniences|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-715].  Here is the original description:    {panel}  In regular HSC survey processing, we'll run with a ""rings"" skymap to cover the entire survey area. meas_mosaic does not currently efficiently or conveniently iterate over tracts. For example:  {code}  mosaic.py /tigress/HSC/HSC --rerun price/cosmos --id field=SSP_UDEEP_COSMOS filter=HSC-R  {code}  Note the lack of a tract in the --id specifier — we want to iterate over all tracts. This is not currently possible. Instead, if we do not know the tract of interest (which the user should not be required to know), we have to iterate over all the tracts (e.g., tract=0..12345), but the user should not be required to know the number of tracts, and this is slow (and possibly memory-hungry: currently consuming 11GB on tiger3 just for 12 exposures).  We need an efficient mechanism to iterate over all tracts by not specifying any tract on the command-line.  {panel}    As this functionality was added specifically for {{meas_mosaic}}, it was going to be ported as part of DM-2674.  Due to a recent desire to use this functionality, this ticket will be ported here."
0,A slimmer testdata_decam,"Before this ticket, the files in {{testdata_decam}} are as they are downloaded from the archive.  Some are MEF, and the total is a bit big (1.2G).    I made a trimmed down version of {{testdata_decam}}, 109M and available here:  [https://github.com/hchiang2/testdata_decam.git]  I trimmed it down by only saving the primary HDU and one data HDU.  However the unit tests in {{getRaw.py}} become less meaningful and I am not sure if we really want to do this, because some complexities of DECam files are about the MEF. {{getRaw.py}} tests Butler retrieval of multiple dataset types, in particular tests if the correct HDU is retrieved.     Nonetheless, {{getRaw.py}} can pass  (with branch u/hfc/DM-4375 of obs_decam)    Note: the old testdata_decam still live on lsst-dev:/lsst8/testdata_decam/"
0,Learn and setup nebula as a development machine,Personally establish Nebula as a stack development platform.
0,Review VAO/IVOA protocols for use in LSST IAM,"Following the good principals of re-using prior work, review VAO/IVOA protocols for use in LSST IAM, in particular the [IVOA Credential Delegation Protocol|http://www.ivoa.net/documents/latest/CredentialDelegation.html] and include a summary in our LSST IAM Design Doc."
2,X16 Revisit Public Interfaces / ADQL,"We need to revisit the interfaces we will be exposing to public, wiht particular focus on ADQL vs SQL92 (mysql flavor?).    Deliverable: recommendation which public interfaces should be exposed to users from the Data Access Services, with particular focus on ADQL vs mysql-flavor."
0,"""SHUTOFF"" nebula instances consume core/ramIt  quota","It appears that halted/shutoff instances have no effect on resource quota usage.  Eg:    {code:java}  $ openstack server list  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  | ID                                   | Name                  | Status            | Networks                               |  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  ...  | 1956c6d0-8aec-4f42-a781-8a68fd10179d | el7-jhoblitt          | SHUTOFF           | LSST-net=172.16.1.171, 141.142.208.150 |  ...  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 141    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 44     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 342016 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+  $ openstack server delete 1956c6d0-8aec-4f42-a781-8a68fd10179d  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 133    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 43     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 325632 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+    {code}  "
1,Port registryInfo.py from obs_subaru into Butler,"in butler (probably butlerUtils), users would like the ability to dump info from a repository's sqlite registry to text (console output). This is already implemented in {{obs_subaru/.../registryInfo.py}}, and basically just needs to be ported to butler in a sensible place. There are a few cases that assume certain columns are present and we need either to make the script more generic, or [~rhl] suggests that maybe we need to standardize the registries."
0,Avoid restarting czar when empty chunk list changes,"Currently czar caches empty chunk list after it reads the list from file. This complicates things when we need to update the list, integration test for example has to restart czar process after it loads new data to make sure that czar updates its cached list. Would be nice to have simpler mechanism to resetting cached list in czar without restarting it completely. It could be done via special query (abusing FLUSH for example) or via sending signal (problematic if czar runs remotely).    This can be potentially useful even after we replace empty chunk list file with some other mechanism as I expect that cache will stay around even for that."
0,Clean up ProcessCcdDecam,"ProcessCcdDecam needs some cleanup:  * {{run}} method simply delegates to the base class  * {{propagateCalibFlags}} is a no-op (deliberately in {{cab69086}}, need to explore if the original problem still exists)  * The config overrides (in config/processCcdDecam.py):  ** Uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs.  ** Sets the background {{undersampleStype}} to {{REDUCE_INTERP_ORDER}}, which is the default."
0,Skymap fails tests on testFindTractPatchList,"When skymap is built and healpy is loaded, {{testFindTractPatchList}} fails with:    {quote}======================================================================  FAIL: Test findTractPatchList  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 245, in testFindTractPatchList      self.assertClosestTractPatchList(skyMap, [tractInfo.getCtrCoord()], tractId)    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 284, in assertClosestTractPatchList      tractPatchList = skyMap.findClosestTractPatchList(coordList)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/baseSkyMap.py"", line 146, in findClosestTractPatchList      tractInfo = self.findTract(coord)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/healpixSkyMap.py"", line 97, in findTract      index = healpy.ang2pix(self._nside, theta, phi, nest=self.config.nest)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 367, in ang2pix      check_theta_valid(theta)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 110, in check_theta_valid      assert (np.asarray(theta) >= 0).all() & (np.asarray(theta) <= np.pi + 1e-5).all(), ""Theta is defined between 0 and pi""  AssertionError: Theta is defined between 0 and pi{quote}    This was missed during regular CI testing since healpy is not normally setup. "
0,Update testCoadds.py to accommodate changes in DM-2915,"As of DM-2915, the config setting:  {code}self.measurement.plugins['base_PixelFlags'].masksFpAnywhere = ['CLIPPED']{code}  is set as a default for {{MeasureMergedCoaddSourcesTask}}.  However, this *CLIPPED* mask plane only exists if a given coadd was created using the newly implemented {{SafeClipAssembleCoaddTask}}.  If a coadd was built using {{AssembleCoaddTask}}, the *CLIPPED* mask plane is not present, so the above default must be overridden to exclude it when using {{MeasureMergedCoaddSourcesTask}}.  This is the case for the mock coadd that is assembled in the unittest code in {{testCoadds.py}}, so the config needs to be set for the test to run properly.    Note that the associated tests for {{SafeClipAssembleCoaddTask}} will be added as part of DM-4209."
1,Get analysis script working for HSC/LSST stack comparisons,"A script for performing pipeline output QA is under development for HSC.  The script provides many useful tools for plotting and analyzing pipeline outputs on single visits and coadds.  This is of general use for LSST and, in particular, will be adapted/expanded to include tools for the direct comparisons of identical data sets processed with both the HSC and LSST pipelines (i.e. DM-2984).  Appropriate adaptations for this script to run with the LSST stack will be made here (with the understanding that development is still ongoing on HSC and further adaptations will be accommodated as necessary/desired).    See [HSC-1320|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1320] and [HSC-1359|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1359] for details and examples of the output of this script."
0,Update cmsd configuration for multi-node tests,"A particular cmsd configuration parameter prefixes a hardcoded path for QueryResource, which needs to be removed. This seems to appear only during multi-node tests."
0,ctrl_execute test fails to find test binary,"There's a test in ctrl_execute that exercises the bin/dagIdInfo.py test program.   Since the rewrite_shebang rewrites happen after the tests are executed, the test that looks for the bin/dagIdInfo.py binary fails, since it's not there before the tests execute."
0,Scale CommandLineTask multiprocessing timeout with workload,"The default timeout value for aborting a multiprocessing run in CommandLineTask is too short. Currently if no time length is supplied by the user, the default value gets set to 9999s. However if a processing task is quite large it is possible for the processing pool to take much longer to arrive at the result. Currently if the processing pool does not complete it's run within that time limit, python multiprocessing will throw a timeout error. The timeout value should be scaled such that the supplied value is assumed to be the timeout length for one processing task, and should be scaled by the number of tasks divided by the number of cpus available. The command line task documentation should be updated to reflect this change."
0,Fix regexp for gcc48,"DM-2622 inttoduced some regexes which raise exceptions when built with gcc48 (e.g. on centos7). gcc48 support for regexes is generally broken, so it's better to replace that with boost regexes."
0,ctrl_execute test fails under El Capitan,"The test/testDagIdInfo.py because it runs a script from bin.src, rather than bin.   This test needs to be rewritten."
1,SuperTask demo on other older tasks,"The exampleCmdLine task worked fine, need to show demo for other tasks from pipe_tasks"
3,W16 Qserv Refactoring #2,Additional refactoring of Qserv as found necessary in W16
1,Experiment with light-weight SQL databases for secondary index,"Evaluate the use of light-weight SQL, such as InnoDB, TokuDB (now Kyoto Cabinet), or RocksDB to create and manage the secondary index."
0,Review of [DM-2983] part 2,Second part and final of reviewing DM-2983
1,Debug Qserv on ccqserv125..ccqserv149,"It seems that some chunkQuery doesn't return on long queries like ""Select count(*) From Object""  The query stall and print in czar log:  {code}  2015-11-24T15:07:55.324Z [0x7f06b37fe700] INFO  root (core/modules/qdisp/Executive.cc:395) - Still 9 in flight.  {code}  If we look in the logs with next commands:  {code:bash}  cat  /qserv/run/var/log/qserv-czar.log | egrep  ""Executive::add\(job\(id="" | grep LSST | cut -d'=' -f2 | cut -d' ' -f1 | sort > JOBID_ADD.txt  cat  /qserv/run/var/log/qserv-czar.log | egrep  ""markCompleted""  | cut -d'(' -f 3 | cut -d',' -f1 | sort > JOBID_DONE.txt  {code}  And then  {code}  qserv@ccqserv125:/qserv$ diff JOBID_ADD.txt JOBID_DONE.txt   1826d1825  < 2639  2217d2215  < 2991  2904d2901  < 3609  3088d3084  < 3775  3152d3147  < 3832  3433d3427  < 4085  4227d4220  < 480  5478d5470  < 5926  6937d6928  < 7239  {code}  9 jobs aren't completed on czar.  If we look the the chunk_id of one of this job:  {code}  /opt/shmux/bin/shmux -c ""locate 5299"" ccqserv{125..149}.in2p3.fr  ...  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYD  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYI  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.frm  ...  {code}  Data is on worker node and onccqserv148 xrootd log:  {code}  [2015-11-24T15:07:42.990Z] [0x7ffbc8244700] INFO  root (core/modules/xrdsvc/SsiSession.cc:125) - Enqueued TaskMsg for Resource(/chk/LSST/5299) in 0.000465  {code}  But a thread seems to be locked:  {code}  #gdb on ccqserv148 xrootd process:  (gdb) thread 7  [Switching to thread 7 (Thread 0x7ffbad7fa700 (LWP 192))]  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185  185     ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory.  (gdb) bt  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185  #1  0x00007ffbd4c45c7c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #2  0x00007ffbccbe8db9 in std::condition_variable::wait<lsst::qserv::wsched::BlendScheduler::getCmd(bool)::<lambda()> >(std::unique_lock<std::mutex> &, lsst::qserv::wsched::BlendScheduler::<lambda()>) (      this=0xfba620, __lock=..., __p=...) at /usr/include/c++/4.9/condition_variable:98  #3  0x00007ffbccbe88f9 in lsst::qserv::wsched::BlendScheduler::getCmd (this=0xfba5c0, wait=true) at core/modules/wsched/BlendScheduler.cc:156  #4  0x00007ffbccba3b07 in lsst::qserv::util::EventThread::handleCmds (this=0xfbe890) at core/modules/util/EventThread.cc:45  #5  0x00007ffbccbab137 in std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()>::operator()<, void>(lsst::qserv::util::EventThread*) const (this=0xfbe900, __object=0xfbe890)      at /usr/include/c++/4.9/functional:569  #6  0x00007ffbccbaafff in std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)>::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0xfbe8f8)      at /usr/include/c++/4.9/functional:1700  #7  0x00007ffbccbaae45 in std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)>::operator()() (this=0xfbe8f8) at /usr/include/c++/4.9/functional:1688  #8  0x00007ffbccbaacfa in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)> >::_M_run() (this=0xfbe8e0)      at /usr/include/c++/4.9/thread:115  #9  0x00007ffbd4c49970 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #10 0x00007ffbd50ae0a4 in start_thread (arg=0x7ffbad7fa700) at pthread_create.c:309  #11 0x00007ffbd43b904d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111  {code}    Something seems to prevent return of chunk query result...          "
0,HSC backport: fix memory leak in afw:geom:polygon,"This is a backport of a bug fix that got included as part of [HSC-1311|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1311].  It is not related to that issue in particular, so is being ported here as an isolated bug fix.    {panel}  Original commit message:  pprice@tiger-sumire:/tigress/pprice/hsc-1311/afw (tickets/HSC-1311=) $ git sub  commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b  Author: Paul Price <price@astro.princeton.edu>  Date:   Wed Oct 21 10:59:56 2015 -0400         imageLib: import polygonLib to prevent memory leak             When doing ""exposure.getInfo().getValidPolygon()"", was getting:             swig/python detected a memory leak of type 'boost::shared_ptr< lsst::afw::geom::polygon::Polygon > *', no destructor found.             This was due to the polygonLib not being imported in imageLib.      Using polygonLib in imageLib then requires adding polygon.h to all      the swig interface files that use imageLib.i.      examples/testSpatialCellLib.i              | 1 +   python/lsst/afw/cameraGeom/cameraGeomLib.i | 1 +   python/lsst/afw/detection/detectionLib.i   | 1 +   python/lsst/afw/display/displayLib.i       | 1 +   python/lsst/afw/geom/polygon/Polygon.i     | 1 +   python/lsst/afw/image/imageLib.i           | 2 ++   python/lsst/afw/math/detail/detailLib.i    | 1 +   python/lsst/afw/math/mathLib.i             | 1 +   8 files changed, 9 insertions(+)  {panel}"
1,Port detection task footprint growth changes from HSC,"In hsc the default behavior for the detection task is to updated footprints with a footprint which has been grown by the psf. This behavior needs to be ported to LSST, as some source records have footprints which are too small. When making this change, the new default needs to be overridden for the calibrateTask, as it needs the original size.    The port includes 8e9fb159a3227f848e0db1ecacf7819599f1c03b from meas_algorithms and 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 from pipe_tasks"
2,Write a DM Collaborative Workflow document,"Document our procedures for collaborative development with JIRA, Git and GitHub for the new docs."
0,Add git-lfs to packer-newinstall,git-lfs is not available in our deliverables. Artifacts (binaries) such as VM images and docker data containers should provide a stable version of git-lfs.
0,Meetings - Nov 2015,"Verification dataset meetings, Illinois DES meeting, single-frame processing discussions, supertask meeting, OpenStack User meeting"
1,Other LOE - Nov 2015,"Local LSST group meetings, NCSA postdoc meetings, NCSA Physics & Astronomy theme meeting, or other local meetings, events, or tasks to comply with NCSA policies"
0,Crash course on using git-lfs,Learn to install and use git-lfs; help testing with migrating {{testdata_decam}} to git-lfs; verify tests pass with the new repository (DM-4370).
1,Learn about the task design in ISR processing,"Learn the concept behind the previous API changes (RFC-26) in the tasks of ISR processing, and data storage/retrieval involved. "
1,Explore basic middleware and orchestration tools,Use {{runOrca}} to launch jobs through {{lsst-dev}} and do some single frame processing with it. Also learn a little about process execution. 
0,Avoid bash usage in batch submission,{{ctrl_pool}} currently creates bash submission scripts with an explicit {{/bin/bash}} bang line.  [~rhl] [argues| https://github.com/lsst/ctrl_pool/commit/047f0de5a682ad9e9a6f65ccc7cc296e0a0d4ee7#commitcomment-14573759] on the review of DM-2983 that we should using posix shell constructions instead.
0,faulty assumption about order dependency in ctrl_event unit tests,"A recent change to daf_base uncovered a couple of faulty tests in ctrl_events that incorrectly assumes the order in which assumed the order in which data in a PropertySet would be received.   We can't assume which order these values will be put into the property set, and therefore into the list retrieved from the Event object."
3,Image preparation time at server side measurement ,Setup the mechanism to measure the time needed to prepare the image (generate the image in PNG or other suitable format) for client display. 
3,Image preparation time at server side measurement,Measure the time needed to prepare the image on server side for client display.   Reach the goal of less than one second.
3,Image preparation time at server side measurement,Measure the time needed to prepare the image on server side for client display.   Reach the goal of less than half second.
3,setup the measurement for Image rendering time to display at web UI,Setup the mechanism to measure the time needed to render the image after the data received by the client for display.  
3,measure Image rendering time to display at web UI,"Measure the image rendering time, reach the goal of 1 second."
3,measure Image rendering time to display at web UI,"Measure the image rendering time, reach the goal of 0.5 second.  "
1,Remove Task.display(),"As of DM-927 (included in release 9.2, end of S14), {{lsst.pipe.base.task.Task.display()}} was marked as deprecated. It should now be removed."
2,Revisit mysql connections from czar,"Need to revisit connections we maintain from czar to mysql. This include revisiting whether we need both sql/SqlConnection and mysql/MySqlConnection classes. (In some cases, like in InFileMerger we have instances of both, which gets very confusing.)"
3,setup mechanism to measure the query response time ,Setup the method to measure the response time after query has been submitted from client.   1. query sent to the data provider from client  2. result returns from data provider  3. result displayed in the client    We can measure   1. the time from query submission to been sent to data provider  2. the time from data returned from provider to been displayed in the client
3,measure the query response time ,Measure the time from query returned from data provider to been displayed in the client.
3, measure the query response time ,Measure the time from query returned from data provider to been displayed in the client.
3,Setup the load test for Web UI portal ,Setup the load test system to measure the performance of web UI portal
3,load test of Web UI portal: support 100 concurrent users,Run the load test system with 100 concurrent users. Measure the performance against other KPM epics in the same cycle. 
3,load test of Web UI portal: support 100 concurrent users,Run the load test system with 100 concurrent users. Measure the performance against other KPM epics in the same cycle.  
1,Improve docker storage backend on RedHat-like distributions,"Solve startup log message RedHat-like distro: ""level=warning msg=""Usage of loopback devices is strongly discouraged for production""?    This is due to use of DeviceMapper (default package option on RedHAt-like distros) without a dedicated hard-disk, use of ""overlay"" backed storage seems better.  "
0,Replace sed with stronger template engine in docker scripts,"Dockerfile are generated using templates and sed, this should be strengthened."
0,Remove useless xrootd client parameters,"This extract of etc/sysconfig/docker:    {code:bash}  # used by qserv-czar  export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp  # Disabling buffering in python in order to enable ""real-time"" logging.  export PYTHONUNBUFFERED=1    export XRD_LOGLEVEL=Debug  export XRDSSIDEBUG=1    # Increase timeouts, without that, long-running queries fail.  # The proper fix is coming, see DM-3433  export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000  {code}    Has to be moved to:    {code:bash}   # used by qserv-czar  export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp  export XRD_LOGLEVEL=Debug  export XRDSSIDEBUG=1  # Disabling buffering in python in order to enable ""real-time"" logging.  export PYTHONUNBUFFERED=1  {code}    And then tested in multi-node, and on in2p3 cluster."
0,Remove QSW_RESULTPATH and XROOTD_RUN_DIR if useless,These parameters may be useless (see DM-4395). If yes they can be removed to simplify configuration procedure.
0,IAM process for granting data access rights,Document a process for granting of data access rights to LSST users according to the Data Access White Paper ([Doc-53733|http://ls.st/Document-5373]).    On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Granting+Data+Access+Rights
0,Please document the --rerun option,"DM-3371 adds the {{--rerun}} option to command line tasks. The help for this option reads:  {quote}  rerun name: sets OUTPUT to ROOT/rerun/OUTPUT; optionally sets ROOT to ROOT/rerun/INPUT  {quote}  While essentially correct, that's not particularly helpful in understanding what's actually going on here. A motivation and description of this functionality is available in RFC-95: please ensure that, or some variation of it, is included in the stack documentation."
1,ISR and calibration of a tiny set of DECam raw data,Learn more about the beginning steps of single frame processing by processing a small subset of DECam Stripe 82 raw data (~10 visits) and performing instrument signature removal with features currently implemented.
0,IaM work in November,Work done in support of LSST's IaM efforts.
1,Management for November,"Centered around DMLT meeting, design process and hiring, in addition to general steering of activities at NCSA "
1,November TOWG/opeartions design  work,"Towg attendance/ note + participating in Beth's group.       Detailed  WBS for DM,  Condensed  WBS  to show high level,  Effort estimates, point out problematic thinking in the estimates. "
1,JCC,"Two day JCC activity at NCSA, including extended JCC meeting with HEP centers likely to   host people exploiting LSST Data.      writeup of extended meeting is here: https://confluence.lsstcorp.org/display/JCC/Extended+JCC+meeting+--+2015-11-23   in the JCC section.     organize, coordinate, and write up meeting notes. "
2,Design refinement for the L1 system,"Further specification of L1 design,     Long list of Questions for group but handled by GDF, began procession replies.    Understood Chilean Buffer in L1 system  could be eliminated,  posed question about systems engineering process needed to support this.     Genera work casing further developing the design into WBS -- which is not 30+ lines of detail  Began L1 con ops ,to guide group    Learned of some (possible undocumented) ""fifth device""."
2,Data Distrib proto (dec),NULL
3,S17 Implement Async Queries in Qserv,"* Design and implement *basic* system for determining whether particular query is synchronous or asynchronous. The complete version will come through DM-1490. Note that this work is related to shared scans (e.g., we need to know what scans we have running)  * Design SQL API for starting and interacting with async queries.  * Modify Qserv to support async queries (starting, getting status, retrieving results)    Note, async queries are indirectly related to authentication (users should not see each other' async queries).    Deliverable: Qserv that accepts and executes queries asynchronously, and allows users to retrieve results."
0,HTML5 Sphinx theme for technotes,"Build a Sphinx theme for the Technote platform. Treat this work as exploratory, proof of concept work for customizing the HTML, CSS and JS of the software docs.    Objectives are    1. Create a Sphinx theme repo  2. Show how gulp can be used to help develop web assets for the theme  3. Establish a pattern for table contents columns that scroll independently of the main article  4. Show how we can implement a HTML5 rst builder in documenteer to fix permalink issues and build true HTML5 output."
1,Finish documentation and comments on SuperTask ,"Need to finish documentation, implementation and comments on the code"
0,Fix multiple patch catalog sorting for forcedPhotCcd.py,"{{forcedPhotCcd.py}} is currently broken due to the requirement of the {{lsst.afw.table.getChildren()}} function that the *SourceCatalog* is sorted by the parent key (i.e. {{lsst.afw.table.SourceTable.getParentKey()}}).  This occurs naturally in the case of *SourceCatalogs* produced by the detection and deblending tasks, but it may not be true when concatenating multiple such catalogs.  This is indeed the case for {{forcedPhotCcd.py}} as a given CCD can be overlapped by multiple patches, thus requiring a concatenation of the reference catalogs of all overlapping patches.    There two places in the running of {{forcedPhotCcd.py}} where calls to {{getChildren()}} can cause a failure: one in the {{subset()}} function in {{references.py}}, and the other in the {{run}} function of *SingleFrameMeasurementTask* in {{sfm.py}}."
1,Understand how the proposed interfaces fit with Qserv code,Understand how the interfaces proposed by [~abh] in DM-3755 fit with the existing qserv code.
0,Re-locate LSST PS server and configure to reside on new layer2 circuits,NULL
1,Investigate MemSQL,Take a look at the MemSQL distributed database.
0,Week end 11/07/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 7, 2015."
1,Week end 11/14/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 14, 2015."
0,Week end 11/21/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 21, 2015."
1,New equipment setup and configuration (week end 11/07/15),* Three Dell R730's:  ** Mount in A row racks  ** Complete Bios updates  * Moved ~25 VMs over to new lsst-vsphere infrastructure  * Setup lsst-condor\[1-6\] VMs  * Setup lsst-esxmac1 with networking  * Fixed networking issues on new lsst-esx1 server (an undocumented host was squatting on the IP address)
1,New equipment setup and configuration (week end 11/14/15),* Received Dell iDrac license upgrades for new Dell R730 servers  * Received and configured VMware vSphere licenses from CDW-G & AURA  * Converted three physical systems to VM's:  ** lsst-nagios  *** Problems with software mirror raids.  **** VMware converter does not see software raided drives.  **** Split the raid 1 into discrete drives.  **** Chose sda to modify - failed as sad was faulty.  *** Built new Centos 6.6 VM  **** Used CrashPlan to rebuild.  ** lsst7 - converted after learning how to convert system to a fixed IP subnet  ** lsst8 - converted  *** Debugged lsst8 system migration to vmware.  Partition table was invalid and was preventing move  * Finished moving all VMs to new lsst-vsphere infrastructure
0,"New equipment setup, configuration, and regular monthly maintenance (week end 11/21/15)","* Virtualized physical system lsst-xfer  * Worked with bmather or dell Dirac licensing issue  * Cleaned up old and new hosts in NCSA DNS, Nagios monitoring, and Qualys vulnerability scanner  * Completed connections for six UPS"
0,New equipment setup and configuration (week end 11/28/15),* Obtained Dell iDRAC Enterprise licenses & upgraded 4 of the 13 servers  * Installed base VM templates for OS X 10.8-10.11
0,Decommissioning old equipment (week end 11/14/15),* Shutdown 17 (all) old ESXi servers  * Shutdown 3 old condor servers
0,Decommissioning old equipment (week end 11/21/15),"* Shutdown last 3 old condor servers  * Shutdown lsst-netem, lsst-ps, & lsst-ps-base servers  * Surplussed equipment:  ** NCSA servers ( 5 Dell 1950's, 2 Dell 2950) repurposed from A22 to C17  ** Moved blade chassis to C20  * Move lsst-test systems in A23"
0,Write DM Git LFS Guide,Refactor words from DM-4412 into a top-level Doc page for using Git LFS. This will leave DM-4412 more as a Collaboration Workflow document.
1,Consulting in November,"Review of design documents, correspondence with the design team regarding Data Center details and floor space, and conferences via web links."
2,Shared scan implementation,Fine tune the API proposed in DM-3755 and implement it.
1,Improvements to logging in xrootd,Improve logging in xrootd to make it more compatible with qserv logging.
3,Fit Visualizer porting: Begin,"Display fits image, server round trip, organize initial data structures, initial render"
1,"FITS Visualizer porting: group, group scrolling",NULL
1,"FITS Visualizer porting: zooming: group zooming, zoom fit, zoom fill, active plot selection",NULL
0,Upgrade to react 0.14.3,NULL
1,Fit Visualizer porting: create toolbar,NULL
2,FITS Visualizer porting: Add canvas drawing infrastructure,NULL
1,Fit Visualizer porting: Distance Tools,NULL
1,Fit Visualizer porting: Drawing Target Center ,NULL
2,FITS Visualizer porting: Marker tool,"improve the menu organization from the current one when doing the migration.   The implementation includes:  * DrawObj for marker * Marker dropdown list under the marker icon to show the marker and footprint items (will be added later). * Marker drawing layer rendering and operation including action creator and action dispatch functions. * Marker UI component shown in Drawing Layers popup. The implementation set up some work which can be similarly expanded to footprints in the future. * Title of marker layer on the layer control is like ""show: <title>"".   The title change as the text you enter for the label. * the mouse changes to a pointer once the cursor moves onto the marker, and the mouse becomes a resize sign when the cursor moves on the handler of the marker. * When the cursor becomes a pointer, the marker can be relocated by dragging the mouse, and when the cursor becomes a resize one, the marker can be resized by dragging the mouse. * the marker size changes as the image is zoomed.     Maker drawing layer operation:  * select 'Add Marker' from dropdown list to add a new drawing layer * click anywhere to locate the newly added marker * click and drag the mouse to relocate or resize the marker: for relocation: click and drag inside the circle, then drag the mouse. for resize: click inside the circle, then click and drag the handler to resize * label and its location are set from Marker tool UI  "
2,FITS Visualizer porting: Grid drawing,NULL
1,FITS Visualizer porting: Catalog drawing,NULL
2,FITS Visualizer porting: Region Drawing,"region drawing for the following regions,   circle, ellipse, box, polygon, point (circle, box, diamond, cross, x, arrow, boxcircle), line, text, annulus, box annulus, ellipse annulus. (annulus is made for the case with at least two repeated regions of the same type).    add the following functions to file ShapeDataObj.js    makeRectangleByCenter, makeEllipse  drawEllipse,  makeTextLocationRectangle, makeTextLocationEllipse  update drawRectangle by adding the case which is given the rotating angle and rectangle center."
2,FITS Visualizer porting: Mouse Readout: part 1: projection,show the fits XY readout so that it update the position in the users selected coordinate system. The readout should also show the plot title.    Write the dialog to change the coordinate system readout.    Also show the pixel size and write the dialog that will change it between pixel size and screen pixel size.    [1/28/16]  Move this ticket to the next sprint.  I cannot get it done in this sprint because  # Spent time to work on other two tickets  # Take off from work  # It takes more time than estimates since I am not familiar with reducer and store etc.  More study is needed.      
2,FITS Save Dialog,NULL
0,FITS Visualizer porting: Rotate,NULL
2,FITS Visualizer porting: North/East Arrow,Add the north and east arrow like the gwt system has.
1,FITS Visualizer porting: Layer Control Popup,NULL
0,FITS Visualizer porting: Stretch Pulldown,NULL
0,FITS Visualizer porting: Color bar pulldown,NULL
0,FITS Visualizer porting: Restore to defaults & re-center,NULL
1,FITS Visualizer porting: Show FITS Header,"Task involves several steps:    * Server side: VisServerCommands.Header needs to change to check to the JSON_DEEP parameter. In this case the return from  VisServerOps.getFitsHeaderInfo should be converted into a format that the new javascript tables should understand (see loi how to get this format). Look at VisServerCommands.AreaStat for an example.  * Client side: a call to the server: need to add getFitsHeaderInfo into PlotServiceJson.js. For reference, look at other calls in  PlotServiceJson.js and the java version of the getFitsHeaderInfo PlotServiceJson.java  * Client side: when header toolbar button pushed then make the call to the server.  * Client side: when server call promise is resolved then show a dialog with the table data. Remember 3 color images should have a tab per color."
0,FITS Visualizer porting: Flip,NULL
1,FITS Visualizer porting: Expanded View,NULL
1,FITS Visualizer porting: Expanded Single,NULL
1,FITS Visualizer porting: Expanded View : WCS Match,NULL
1,FITS Visualizer porting: Expanded View: Grid,NULL
1,FITS Visualizer porting: Crop,NULL
1,Fit Visualizer porting: Select Area,NULL
1,FITS Visualizer porting: Statistics - part 1,dialog only
1,"FITS Visualizer porting: selecting points of catalog from image view, showing selected points","able to draw a rectangle on the image, and select the catalog entries overlaid on the image"
0,FITS Visualizer porting: Image Select Panel/Dialog,"Converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: Each ticket should reference this ticket as the base.    Panel includes the following:  * issa, 2mass, wise, dss, sdss tabs  * file upload tab, upload widget might have to be written  * url tab  * blank image tab  * target info reusable widget  * 3 color support - any panel should show for 3 times, for read, green, and blue in 3 color mode  * must be able to appear in a panel or dialog  * must add or modify a plot  * Allow to create version with most or less than the standard tabs. example - see existing wise 3 color or finder chart 3 color  * A plot might need to be tied to specific type of image select dialog, we need a way to tie a plotId to and non-standard image select panel."
1,Fit Visualizer porting: Thumbnail,NULL
0,Fit Visualizer porting: Magnifier,NULL
1,Experimentation and testing of new SuperTask infrastructure,WBS deliberately left unset as this is tracking LOE work.
1,Migrate prototype SuperTask code to upstream repository,The prototype SuperTask code is now ready to be moved from the experimental repo to the upstream {{pipe_base}} repo. This requires the commits to be squashed and tidied.
0,makeDocs uses old style python,{{makeDocs}} is written in python 2.4 style. This ticket is for updating it to python 2.7.
0,Improve reStructuredText documentation,"Enhance docs by covering    - Images as links  - Table spans  - Abbreviations  - :file: semantics, etc."
1,Quoting of paths in doxygen configuration files breaks makeDocs,In DM-3200 I modified {{sconsUtils}} such that all the paths used in {{doxygen.conf}} files were quoted so that spaces could be used. This change broke documentation building (DM-4310) because {{makeDocs}} did not expect double quotes to be relevant. This ticket is to fix {{makeDocs}} and to re-enable quoting of paths in config files.
1,Assess DECam ISR up to currently implemented,"Not all known ISR corrections are applied or implemented to DECam data yet. For example, no cross-talk, edge-bleed, non-linearity, sky pattern removal, satellite trail masking, brighter-fatter, or illumination correction.    But we have most of the basic ISR already. With what we already have, identify issues that would severely affect the quality of post-ISR processing."
1,Flag out the glowing edges of DECam CCDs,"Pixels near the edges of the DECam CCDs are bigger/brighter and correcting them is not trivial. One way to move forward is to mask them out.      DESDM and CP mask 15 pixels on each edge.  The cut was later raised to 25 pixels, with the inner 10 pixels flagged as SUSPECT.  "
0,Fix startup.py inside Docker container,qserv tag should be replace with qserv_latest
3,Margaret's mgmt. activities in November,DMLT @ Princeton  Weekly DMLT and Standups  Local meetings  TPR  etc.
0,Ops Planning and TOWG attendance - November,"TOWG meetings, review service catalog as input to LOPT, review draft of operations WBS with Don and Athol"
2,L1 design specification and planning - November,"With Steve, Jim, Don, and Jason, detailed design construction and planning for development of the Image Ingest and Processing System. Worked through LDM-230 and OCS design docs, input from discussions on Confluence. Made cleaner drawings (draft) and expanded Project plan. The plan is currently a ""to do"" list, without schedule or resource loading yet. "
1,Facility Coordination Meeting and JCC meeting @ NCSA,"Day 1 : Facility Coordination Day with Argonne, CC-IN2P3, NERSC, and NCSA  Day 2: Extended JCC meeting    Notes posted on Confluence: https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=JCC&title=Extended+JCC+meeting+--+2015-11-23 "
0,Compilation errors from CLang (Apple LLVM 7.0) in XCode 7 on MacOSX,"Compiling on MacOSX Yosemite with XCode 7, a number of files fail compilation.  ----  {{core/modules/util/EventThread.h,cc}} fails because {{uint}} is used as a data type.  This is non-standard (though some compilers support it), and should be replaced with {{unsigned int}}.  ----  {{core/modules/wbase/SendChannel.h,cc}} fails because {{#include <functional>}} is missing.  ----  {{core/modules/wsched/ChunkState.cc}} fails because {{#include <iostream>}} is missing.  ----  {{build/qmeta/qmetaLib_wrap.cc}} (generated by SWIG) fails with many errors because the {{typedef unsigned long int uint64_t}} included in {{qmetaLib.i}} conflicts with MacOSX's typedef of it as {{unsigned long long}}."
0,shellcheck linting of lsstsw bash scripts,This issue is to recover a branch from DM-4113 that was not merged due to issues with installing shellcheck under travis.
0,Create release notes docs template,Create a template for release notes/other release-type documentation in the new docs.
1,Research simulation tools,"We need to do simulations of DCR and other effects when determining appropriate mitigation techniques.  This will require settling on a simulation tool for doing this.    The obvious choices are: phosim, galsim, and a roll your own solution.  Look into which of these is the most reasonable choice and make a recommendation."
1,Roll Qserv into SQuaRE release - part I,Improvements to codekit to support release process. 
1,Revisit short and long term plans for butler,Revisit short and long term requirements and needs and capture it through stories.
0,Review of storage quotes,"Reviewed 4 products from 3 vendors with storage group. Prices and features comparison along with discussions on whether to integrate into condo or not. Also, discussed security considerations.    Preliminary vendor/product chosen. Follow up questions sent to vendor. Will review with storage before we are able to purchase."
0,Update to Sizing Model,"Removed 'memory effectiveness' factor as it was already included in compute efficiency  Updated capacity / tape to follow LTO roadmap  Updated bandwidth / tape drive to follow LTO roadmap  Changed ""tape!Number of new tapes purchased"" to round up to an even number for the archive site  Changed ""tape!Number of new tapes needed"" to round up for both sites  Changed ""tape!Number of tape drives needed based on bandwidth"" to round up for both sites  Fixed 'tape!tape bandwidth' colums to take into account mixed tape drive types  Updated 'tape!HPSS' to take into account retiring mover nodes    Also began work with Spectra Logic to further improve tape predictions."
0,Power requirements and LSST footprint at NCSA,"Finalized power requirements with the UofI engineer. I plan to distribute verification compute across multiple racks in order to reduce per-rack power requirements and reduce per-rack network port counts. This allows us to drop from 3x 60A for the verification cluster to 3x 30A which is the same for the other racks. This will result in some cost savings and simpler planning for future use of the racks.    Also, provided a plan for LSST's footprint in NPCF until 2032. We now have space reserved from the south side of Roger to the north side of Blue Waters. This space should be very visible from the room camera I believe. "
1,Move butler from daf_persistence to daf_butler,NULL
2,Add initial butler support for remote GET,"For Get:  If the mapper returns a URL:  retrieve the URL contents into a file  return the path to the file.    This will be optimized in subsequent stories by ""add read support for various transports to the afw object readers"". This is a degenerate case that will be used if the object does not have a reader for a given transport protocol.    Need to solve the cleanup problem of when to delete the file that was downloaded.    For Put:  serialize the object to a temporary file  transfer the file to the URL"
0,Fix docker workflow,"Some issues where discovered while trying to package DM-2699 in Docker (for IN2P3 cluster deployment), they're fixed here.    - apt-get update times out: why?  - git clone then pull is too weak (if building the first clone fails, pull never occurs) => step merged  - eupspkg -er build creates lib/python in /qserv/stack/.../qserv/... and next install can't remove it for unknow reason => build and install merged."
3,Base site additions to simulator,"Started using login on the Nebula cluster - set up some instances and used snapshot facility. Detected an error with launching from volume.  Met with Chris Lindsey about issue.      Started coding replicator and Foreman functionality.   Began message taxonomy for comm between Foreman and Replicator.     Installed log rotate, Rabbitmq,  pip, pika and GCC on running instance. Successfully tested  rabbitmq with another instance which had pika libs installed.  Finished up prototyping of replicator and foreman code.    Installed Nova Client on foreman instance so it can start and stop replicator instances  as needed at runtime. Tested.    Began a project to simulate realistic camera data from LSST to be used for various timings of DMCS prototype code. This was also a concentrated effort to learn about the nature of astronomical data, how it is represented, its various types of data, and practice with the C and Python libraries for building FITS files. Two sample files were built: One additively built and converted 16bit DES data into scaled 18bit data, then placed that binary representation in a 32bit integer, and the other was additively built up from PhoSim data. Files were built to represent a single LSST camera raft with 9 HDUs - one for each CCD. Files were over 600Mg. The code used in these files has been saved and parts turned into scripts for generating other sample files.      Set up an environment for timing the imprint of these files as they are built and processed through the Base DMCS pipeline code. Initial timing was done on compression techniques.     Wrote result paper on FITS file generation and timing. Errors due primarily to my own ignorance of this type of file format and how the data was truly represented were encountered several times. This project was personally extremely helpful in beginning to learn the vast amount of domain knowledge needed to complete future pieces of the L1 Base site code package.      Added coding for setting up raw data to be moved and assembled into fits files in DMCS , simulating part of the Base DMCS data flow, into existing Base replicator/foreman prototype.    "
3,"Functional drawings, specification writing, and info gathering ","Initial meeting with Don about spec work   Began gathering architecture ideas for Base DMCS 4    Discussed requirements gathering for Base site network operations with  Steve P., Paul, and Don.   1    Began specification draft for base site, and integrated drafts into project wiki 4    More spec ideas and posted them on Dons pages under the OCS Bridge page.  4    DMCS Planning meetings begin in earnest.  16    Planned DMCS diagrams for L1 base site and NCSA site as a group and drew them in support of Margaret’s trip to DM meeting. These were refactored a couple of times and are in confluence.  8    Camera meeting: about 1"
1,Bug fix and improvement for DECam processing,- Bug fix in DecamMapper policy of fringe dataset  - Improve readme documentation about ingesting and processing raw data  - Bug fix on translating Community Pipeline's Bad Pixel Mask (BPM) --- Previously in DM-4191 I looked up the wrong table for the BPM bit definition.  - Flag the potentially problematic edge pixels as SUSPECT (DM-4515)  - Add data products for coaddition processing
0,Local LSST Sec Meeting,NULL
0,Local LSST IaM Meeting,NULL
0,Security Playbook,NULL
1,Security Plan Renewal,NULL
3,Convert basic table functionalities to JS.,"Task includes server-side json conversion, data modeling, and a simple React table for presentation."
1,Table (JS): selection feature.,This task is composed of:  - converting java class SelectionInfo.  - reducing data into its table model state  - rendering SelectionInfo onto the TablePanel  - creating action and action creator
1,Upgrade to the latest react-highcharts library,"We need to upgrade from the early version of react-highcharts to the latest one, compatible with React 0.14.3. Just switching to the new library does not work, need to resolve issues."
2,Table (JS): large table handling,This task is composed of:  - creating and adding a paging toolbar to TablePanel  - adding external data loading feature to TablePanel  - creating prefetch and background data fetching mechanism  - use websocket events for reporting background statuses    - requires new server-side code.    - depends on  DM-4578 - Integrate websocket messaging into flux
1,Table (JS): sorting,This task is composed of:  - introduce sorting feature into TablePanel  - creating action and action creator  - reducing data into its table model state
2,Histogram View of a Table,"Combine HistogramOptions and Histogram widgets into a Histogram viewer:  - define histogram state tree, actions (getting/updating table statistics, getting/updating column data), and reducers  - write a smart widget, which shows options and histogram side-by-side"
1,Table (JS): filtering,"This task is composed of:  - adding filter toolbar into TablePanel  - filter validation syntax  - creating action, action creator, and reducing data into its table model state  - -generating meta info for enumerated columns-  not sure if we wanted this.    Also, added actOn feature to FieldInput."
1,Suggestion Box widget,"We need to find or implement a suggestion box widget in JS. Currently, it is used to suggest table column names in XY plot and in some forms."
1,Table (JS): table options,"This task is composed of:  - adding table options panel to TablePanel.  - providing features:    - show/hide units in header    - show/hide columns, reset to defaults, etc    - page size"
1,JS expression parsing library,Since we are allowing column expressions we need a way to validate them on client side.
0,Table (JS): text view,This task is composed of:  - adding text view option to TablePanel
1,XY Plot view of the table (JS): define state tree,"Define state tree, actions, and reducers for XY Plot view of the table."
1,XY Scatter Plot (JS) ,Implement basic scatter plot widget using react-highcharts library
2,XY Scatter Plot Options (JS),NULL
2,Integrate websocket messaging into flux,This task is composed of:  - design and implement messaging concept into flux    Implementation thoughts:  - convert inbound messages into actions  - convert selected actions into outbound messages  - add message action reducer with the concept of a message consumer.    - consumer can be a predefined action creator or a function      - allow consumers to be added/removed into/from the system after bootstrap
2,XY Plot view of a table (JS),"Implement smart widget which shows toolbar, plot options, and XY plot."
1,XY Plot view of a table (JS) - Toolbar,"Toolbar, which toggles plot options, selection and filter buttons    Extra:   - handling zoom from the toolbar rather than using built-in zoom  - ability to switch between histogram and scatter plot view"
2,XY Plot Viewer (JS) - density plot,Implement density plot using react-highchart library (Highchart's canvas-based heat map).
1,XY Plot View of a table (JS) - selection support,"Show/change selected/highlighted points. Ideally, this should be done without redrawing the whole plot. "
0,SUIT: search returning images in a directory,#NAME?
1,XY plot view of a table (JS) - density plot zoom support,Density plot zooming requires server call.
1,XY Plot view of a table (JS) - density plot selection support (?),density plot - how do we support selection?    (In current version we turn off selection support when the plot is density plot)
1,GWT Conversion: Login,"This task is composed of:  - adding user info into banner    - includes user name and links for login, logout, and profile.  - convert server-side code to return json  - use messaging to handle current user state.    - depends on DM-4578	Integrate websocket messaging into flux"
2,GWT Conversion: Search Panel,"This task is composed of:  - converting server-side code to return json  - defining and loading search info data into application state    - loading should be implemented so that it can be from a server fetch or a client declaration  - creating action, action creator and reducing functions  - rendering SearchPanel from search info data    - attach SearchPanel to application:  depends on GWT Conversion: layout"
3,Create ctrl_platform_nebula package to exercise ctrl_orca orchestration on Nebula,We create a ctrl_platform_nebula package to support processing with the LSST framework as orchestrated by the ctrl_orca/ctrl_execute  packages within a HTCondor pool that resides on the Nebula OpenStack.
1,GWT Conversion: basic layout for results,"This task is composed of:  - creating a results container that handle the layout of its components  - define and load layout info into application state  - creating action, action creator, and reducing functions  - components include:    - vis toolbar    - last searched description    - layout options: tri-view.  side-by-side, single and popout can be added at a later time.    - tables, image plots, xy plots.  - depends on DM-4590: GWT Conversion: advance resizable layout panel"
1,GWT Conversion: advance resizable layout panel,Create an advance React component for layout.  Features should include:  - a set of predefined layouts  - resize strategies  - generic for reuse
1,GWT conversion: System notifications,"This task is composed of:  - adding notification panel to the application  - convert server-side code to use messaging for notifications  - use messaging on client-side to handle notifications  - creating action, action creator, and reducing functions  - depends on DM-4578	Integrate websocket messaging into flux  "
1,GWT Conversion: History and routing,First pass at the implementation of history and routing.  Define a framework in which the application can be:  - bookmarked  - record state in history  - retore application from a url  
0,Remove deprecated versions of warpExposure and warpImage,"afw.math supports two templated variants of warpExposure and warpImage, one that takes a warping control and the other which does not. The latter have been deprecated for a long time and are no longer used. I think it is time to remove them."
0,Build docs.lsst.io Doc Index Page,"Create an HTML landing page for all DM documentation/documents    - Software Documentation  - Developer Guides  - Requirement and Design Documentation  - Technotes  - Papers  - Presentations    The page will be implemented as a static site. The page build will be template driven, with content scraped from the metadata.yaml resources of technotes (among other sources).    Since this is the first SQuaRE web project, this ticket will also involve effort in establishing a CSS+HTML pattern library and Gulp-based development workflows. Long term, this investment will be returned with new dm.lsst.org, technote and Sphinx documentation web designs."
0,sconsUtils tests should depend on shebang target,Some tests rely on code in the {{bin}} directory. Whilst these tests have been modified to use {{bin.src}} the general feeling is that the test code should be able to rely upon the {{shebang}} target having been executed before they are run.
0,make codekit repos.yaml aware,Up to now codekit assumed the repo name is the same as the eups package name. FIx it by using repos.yaml. 
0,Partition package should use the standard package layout,The partition package does not build on OS X El Capitan because the package is not laid out in the standard manner and whilst {{sconsUtils}} is used most of the default behaviors are over-ridden. This means that fixes implemented for DM-3200 do not migrate over to {{partition}}. I think the best approach would be to reorganize the package so that it does build in the normal way.
0,Research Kerberos and LDAP replication options,"IAM components, including the Kerberos KDC, need to be replicated between NCSA and Chile machine rooms. This may impact whether LSST can use NCSA's production Kerberos instance (if it supports selective replication) or needs a separate Kerberos instance that can be replicated outside NCSA. This task is to research and document the options, in consultation with NCSA Kerberos admins, and propose a Kerberos replication approach."
1,"Receive, verify and test network equipment",NULL
1,Install networking hardware into openstack and verify operation,NULL
1,Verify Network Emulator operation,NULL
0,Design network emulator integration into workflow,NULL
1,Deploy heirarchical queuing to test image precedence,"Once we have a working workflow ready to move data between the ""base site"" and ""archive site"" (both at NCSA), deploy a base site exit router with stacked queuing to test prioritization of image traffic over various network conditions."
2,Migrate Qserv code to stream-based logging,"Migrate Qserv code from LOGF_* to LOGS_*.     While doing it, we will also revisit logging levels: in particular we are abusing INFO, most of what is now in INFO should be on DEBUG, in some places where INFO is used to cover unusual conditions, it should go to WARNING.     Further, we will unify how we initialize logging structures. Per discussions at 2015/12/09 Qserv meeting, we like best {code}LOG_LOGGER _log = LOG_GET(""lsst.qserv.<module>.<file>""){code} in anonymous namespace in cc files. Logging from .hh files is strongly discouraged.    This involves changing ~600 places."
1,Send all chunk-queries to primary copy of the chunk,"We are planning to distribute chunks / replicas across worker nodes such that each node will have a mix of primary copies for some chunks, and backup copies for some chunks. While doing shared scan, we are going to always rely on the primary chunks (e.g., all queries that need a given chunk should be sent to the same machine so that we read that chunks only once on one node). This story involves tweaking xrootd to ensure we don't send chunk-queries to nodes hosting non-primary copies."
1,Design for butler support of multiple repositories,"Work on design for Gregory/SUI's request:    We need to understand how put()/writing works when multiple repositories are made visible through a single Butler.  For get()/reading a single search order makes sense.  For put() it may be desirable to support alternative destinations (local disk, user workspace, Level 3 DB) or even multiple destinations for a single put()."
1,Explore coadd processing with DECam data with default config,"Starting with raw DECam data, perform single frame processing and then try image coaddition with a few visits of images. "
2,Provide input to CalibrateTask design work,Provide requirements and advice as input to the effort to redesign CalibrateTask (DM-3881).
1,Create IDL pipeline workflow for DRP processing - processCcdDecam,"For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages.  Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  The first step is processCcdDecam."
1,Create IDL pipeline workflow for DRP processing - makeCoaddTempExp,"For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages. Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  processCcdDecam is working.  The next step is makeCoaddTempExp.  "
1,lsstsw should symlink afwdata or allow an envvar,"To reduce disk usage, it is very handy to be able to make build/afwdata and stack/afwdata/BLAH be symlinks. build/ is easy: just make the symlink and then never have touch it unless you rm your whole stack. stack/afwdata/BLAH is harder: each time you rebuild something that depends on afwdata, it will install a new copy of afwdata, which you'll have to manually remove and declare your symlink with a new tag.    A couple of ways to make this more automatic:     * lsstsw checks whether build/afwdata is a symlink, and if so just makes the new stack ""install"" a symlink to the same directory.   * Check for some environment variable (e.g. AFWDATA_BASE_DIR or something) and if that exists, just make a symlink to it, or make a dummy eups table that points to that directory and don't put anything in stack at all."
1,modernize afw code and reduce doxygen errors,"I would like to make some simple modernizations afw code and reduce doxygen warnings as much as practical. The modernizations I had in mind were:  - Move doc strings from .cc files to .h files and standardize their format  - Use {{namespace lsst { namespace afw { ...}} in .cc files to make the code easier to read  - Eliminate all {{<Class>::Ptr}} and {{<Class>::ConstPtr}} typedefs (replacing with {{PTR(<Class>)}} and {{CONST_PTR(<Class>)}}).  - Make sure .py files import the appropriate packages from future and (where practical) pass the flake8 linter    Regarding doxygen warnings: I think moving the documentation to .h files will help in many cases. Some warnings may be impractical to fix, such as complaining about not documenting ""cls"" for python class methods.  "
1,Implement mouse interaction with the drawing infrastructure,NULL
1,Migrate scisql and mysqlproxy to mariadb,"MySQLproxy and SciSQL relies on MySQL, they should now move to MariaDB"
0,Add utility function to handle client-side download requests.,Create utility function to handle client-side download requests.  It needs to be done in a way that does not mess with history and current page state.
0,Add workflow code to lsst-dm github,"Since we have split the code for Super Task, all the workflow code should  go in a different repository"
2,L1-CONOPS,Contribute to L1-CONOPS document
1,investigate replicating EUPS published packages,"(This ticket is for work that has already been done, per internal discussion in SQRE, but accidentally without an open ticket)    https://github.com/lsst-sqre/lsyncd-eupspkg  https://github.com/lsst-sqre/sandbox-pkg"
1,Support sqlalchemy use with qserv,"When one tries to connect to qserv using sqlalchemy there is an exception generated currently:  {noformat}  $ python -c 'import sqlalchemy; sqlalchemy.create_engine(""mysql+mysqldb://qsmaster@127.0.0.1:4040/test"").connect()'  /u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py:298: SAWarning: Exception attempting to detect unicode returns: InterfaceError(""(_mysql_exceptions.InterfaceError) (-1, 'error totally whack')"",)    ""detect unicode returns: %r"" % de)  Traceback (most recent call last):    File ""<string>"", line 1, in <module>    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2018, in connect      return self._connection_cls(self, **kwargs)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 72, in __init__      if connection is not None else engine.raw_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2104, in raw_connection      self.pool.unique_connection, _connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2078, in _wrap_pool_connect      e, dialect, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1405, in _handle_dbapi_exception_noconnection      exc_info    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 199, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2074, in _wrap_pool_connect      return fn()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 318, in unique_connection      return _ConnectionFairy._checkout(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 713, in _checkout      fairy = _ConnectionRecord.checkout(pool)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 480, in checkout      rec = pool._do_get()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1060, in _do_get      self._dec_overflow()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 60, in __exit__      compat.reraise(exc_type, exc_value, exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1057, in _do_get      return self._create_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 323, in _create_connection      return _ConnectionRecord(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 454, in __init__      exec_once(self.connection, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 246, in exec_once      self(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 256, in __call__      fn(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 1312, in go      return once_fn(*arg, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/strategies.py"", line 165, in first_connect      dialect.initialize(c)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/dialects/mysql/base.py"", line 2626, in initialize      default.DefaultDialect.initialize(self, connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 256, in initialize      self._check_unicode_description(connection):    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 343, in _check_unicode_description      ]).compile(dialect=self)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute      self.errorhandler(self, exc, value)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler      raise errorclass, errorvalue  sqlalchemy.exc.InterfaceError: (_mysql_exceptions.InterfaceError) (-1, 'error totally whack')  {noformat}    The reason for that is that sqlalchemy generate few SELECT queries to figure out unicode support by the engine, and those selects are passed to qserv which cannot parse them. Here is the list of SELECTs which appears in proxy log:  {code:sql}  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  {code}"
0,Create and rename the sims_dustmaps repository to sims_maps,"Create and rename the sims_dustmaps repository to sims_maps.    This is my plan after talking to [~jhoblitt]:    Add sims_maps to {{lsst_build/repos.yml}}. Change related dependencies and create ticket branches, run CI to confirm the changes."
0,Update sims_dustmaps/sims_maps repository to use git-lfs,Update sims_dustmaps to use git-lfs.
3,Convert GWT code to pure JavaScript (X16),We plan to continue the GWT to JS conversion in Summer 16. The goal is to finish it.
0,CI debugging,diagnosing build failures and refreshing build slaves
3,Ci Deploy and Distribution Improvements part III,NULL
1,Port code style guidelines to new DM Developer Guide,Verbatim port of DM Coding style guidelines to Sphinx doc platform from Confluence.    - https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy  - https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666 and contents    I’m unclear whether these pages should be included:    - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++ ‘using’)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190 (how to use C++ templates)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++11/14; which should seem to belong in the code style guide)    Any temptation to amend and update the style guideline content will be avoided.
1,Port RFC/RFD/Decision Making Page to new docs,Port to new Sphinx docs: https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process?src=contextnavpagetreemode
1,Read and extend SuperTask technical note,Read the current version of DMTN-002 and comment.    Write new sections describing the overall architecture and the expected role of SuperTask in the system.
1,Review existing CmdLineTask instances' inputs and outputs,"Review most or all existing DM CmdLineTask subclasses to understand their external inputs and outputs.  This will inform the design of the successor to the interim SuperTask.execute( DataRef ) interface.    The issue is that the single-DataRef interface supports only 1:1 input:output relationships, or N:1 relationships where a list of inputs is derivable from an output dataid.  This is believed to be insufficiently general."
0,Improve sphgeom documentation,"Per RFC-117, the sphgeom package needs decent overview documentation, linked from the top-level README.md. The doxygen output should also be reviewed."
0,audit obs_subaru defaults and move them to lower-level code,"The obs_subaru config overrides contain many useful settings that aren't actually specific to HSC or Suprimecam.  These should be moved down to the low-level defaults in the config classes themselves, so new obs_ packages don't have to copy these configurations explicitly."
1,configure WebDAV with Kerberos/LDAP on lsst-auth1,"Configure WebDAV for Kerberos authentication and LDAP authorization. Create example subdirectories where LDAP groups determine access (using .htaccess files):  * lsst: anyone in lsst group can read/write to this directory  * ncsa: anyone in all_ncsa_employe can write, anyone in lsst can read"
0,IAM process for managing L3 data access,Document a process for managing access to L3 Data Products.    On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Managing+L3+Data+Access
0,Prototype LSST User/Group Manager,NULL
1,Design Interfaces for Memory Management for Shared Scans,Part of the shared scans involve memory management - a system that will be used by Qserv that will manage memory allocation / pin chunks in memory. This story involves designing the API between Qserv and the memory management system.  
0,"qserv/cfg has to be removed by ""scons -c""","qserv-meta.cong was still pointing on MySQL instead of MariaDB, even after running ""scons -c"". This error-prone behaviour should be fixed."
3,work with database team to exercise all the APIs for data access (F16),SUI will continue to work with database team to exercise all the APIs for data access. All known issues should be worked out in S16 cycle.
2,Provide API for tabular data display using Firefly,We need to provide JavaScript API access to all the table displaying features to give user more control when using Firefly API to displaying table data in their own web page or to build customized web UI 
3,Provide a prototype version of LSST web UI ,"SUIT deployment at NCSA to access SDSS strip82 data processed by DM stack in 2013.  * Use the data access API, or TAP API  * Light curve for time series data  * Connection between the light curve data point and the image that the data point coming from  "
3,Implementation of multiple repositories v1,NULL
2,Implementation of multiple repositories v2,NULL
0,Changed the implementation of HistogramProcessorTest due to the minor change about the algorithm in the HistogramProcessor,"In Histogram, when the data points fall on the bin edges,  the following rules are used:  #  For each bin, it contains the data points fall inside the bin and the data point fall on the left edge.  For example, if binSize=2, the bin[0] is in the range of [0,2].  The data value 0 is in bin[0] .  #  For each bin, the data point falls on the right edge is not included in the number point count. For example if binSize=2, the bin[0] is having the range of [0,2].  The data value 0 is in bin[0] but the data value 2 is not in the bin[0].  # For the last bin, the data points fall inside the bin or fall on the left or right bin are counted as the number of bin points.    The last rule is newly introduced.      "
3,Firefly visualization Java/JS code refactoring and bug fixes(F16) ,"This epic will capture all the Java and JS code refactoring in Firefly, bug fixes, JS code optimization and performance enhancement. "
1,Design worker scheduler for shared scans,NULL
3,Data Distrib proto (Jan),NULL
3,Refactor ProcessCcdTask and sub-tasks,"Based on conversation spurred by DM-3881 as discussed [on clo|https://community.lsst.org/t/requirements-for-overhauled-calibration-task/370], this ticket will refactor ProcessCcdTask to be easier to extend and instrument, easier to understand, and more modular.    The main work will be to break up ProcessCcdTask into it's component modules and, and reconfigured to meet the requirements as outlined by the clo discussion."
2,distributed loader,NULL
2,Implement memory mgmt for shared scans,NULL
3,Implement worker scheduler for shared scans,NULL
2,Add initial butler support for remote PUT,"For Get:  If the mapper returns a URL:  retrieve the URL contents into a file  return the path to the file.    This will be optimized in subsequent stories by ""add read support for various transports to the afw object readers"". This is a degenerate case that will be used if the object does not have a reader for a given transport protocol.    Need to solve the cleanup problem of when to delete the file that was downloaded.    For Put:  serialize the object to a temporary file  transfer the file to the URL"
2,Create ALERT framework for qserv,"At the moment Qserv code will throw exception when something wrong/unusual happens. That is not always the best idea to do in a server code that is needs to run 24x7. If we don't throw exception and just log the issue, it might get unnoticed in the log files. So, it'd be useful to have some sort of alert framework where we could send alerts when something strange / unexpected happens in Qserv code and we are able to ""ignore it"" and continue running the server. It can be as naive as writing to a special place, or sending an email, or messaging the DBA etc. This story involves designing and implementing such framework. The sooner we do it the better so that we don't accumulate new code that is throwing exceptions where it should not."
2,Revisit Qserv code that throws exceptions,We have ~500 places where we throw exceptions in qserv/core/modules/*/*. Revisit all of them and make sure we catch these exceptions properly.
1,Promote IsrTask to command line task.,"As pointed out by [~nidever] in DM-4635, it would be quite useful to have the IsrTask callable as a command line task without having to do all the other steps in ProcessCoaddTask."
1,Promote CharacterizationTask to command line task,"In refactoring the processCcd.py script, we'd like to make each component callable by command line as well.  This is to promote the image characterization task to a command line task.  A requirement will be that this task be able to run on data without IsrTask having been run (command line tasks should be insulated as much as possible from knowing about previous processing)."
1,Promote CalibrateTask to command line task,"The task that takes care of measurement and calibration on characterized images will be promoted to a command line task.  As with the other command line tasks, it should be possible to run the calibration and measurement command line task on data without necessarily running IsrTask or CharacterizeTask.    Of course, this means the task will have to get a PSF from somewhere, see [clo|https://community.lsst.org/t/requirements-for-overhauled-calibration-task/370] for suggestions."
0,Qserv integration tests fail on CentOS7 with gcc 4.8.5,"The version of gcc that ships with CentOS7, {{gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)}}, appears to miscompile the qserv worker source in a way that makes it impossible to actually run queries. Installing {{devtoolset-3-toolchain}} and {{devtoolset-3-perftools}} via {{yum}} to get gcc 4.9 resolves the issue."
0,qdisp/testQDisp fails with mariadb,"Fabrice fried to build qserv with mariadb and it caused failure in one of the unit test: qdisp/testQDisp with the message:  {noformat}  pure virtual method called  terminate called without an active exception  {noformat}    Runnig it with GDB it' obvious that there is a problem with resource lifetime management in qdisp/testQDisp.cc. The problem is that XrdSsiSessionMock is destroyed sooner than other objects that use it.     One way to resolve this problem is to instantiate XrdSsiSessionMock earlier than other objects that use it (to reverse the order of destructors), possibly make it a global instance.    Big mystery here is how mariadb could trigger this interesting behavior and why did not we see this earlier."
0,Rerun and create a repository for CFHT astrometry test.,"Understand, re-rerun, and recreate clean version of [~boutigny] 's CFHT astrometry test for the astrometry RMS for two sample CFHT observations.  This test is on the NCSA machines in  /lsst8/boutigny/valid_cfht    Create a repository for this test with an eye toward it becoming integrated in a validation suite for the stack.        "
0,Adapt CFHT astrometry test for DECam COSMOS field validation,Adapt the CFHT astrometry validation test to the DECam reprocessing effort.      Will focus on the repeat observations of the COSMOS field.  Goal is to just do a simple two-observation comparison.  Doing a full test of all of the observations will be a later story.  
0,Integrate astrometry test into SDSS demo lsst_dm_stack_demo,Incorporate the astrometry test as an optional component in lsst_dm_stack_demo.    This is chosen because lsst_dm_stack_demo currently serves as the very loose stack validation and understanding how to do astrometric repeatibility testing in this demo will help explore how it would make sense to put in a fuller CFHT validation test of the DM stack.
0,Prototype a validation module of the stack using CFHT data.,"Create a prototype standalone validation test of the astrometric performance of the stack on suitable CFHT data.  Module is called `validate_drp`     http://github.com/lsst/validate_drp_cfht    Decide how those data should be provided (testdata_cfht being one obvious possibility), and determine if obs_cfht tests and the tests for this validate_drp module should use the same test datasets.    This is prototyping for DM-2518."
1,host identification info needs to be part of log message,The EventAppender needs to add host identification (host/process/id) information to the log message it transmits.   This was inadvertently left out.
0,Edit testdata_cfht to pass obs_cfht unit tests,This ticket covers the first half of the issues in DM-2917.     {{testdata_cfht}} was left unedited while some past changes in {{obs_cfht}} {{MegacamMapper}} required coordinated changes.  The goal of this ticket is to simply pass the unit tests currently in {{obs_cfht}}.   
0,Fix documentation and restructure workflowTask,NULL
0,Improve documentation on pipe_base/supertask,NULL
1,Bad OpenBlas setting in miniconda/numpy causes very poor performance for running multiple processes,"I have been running many processes of processCcdDecam.py on my new linux machine in Tucson (bambam).  To my surprise, running 40 processes at once gets very poor performance (~70 sec per process) compared to running a single process (~16 sec). I expected some performance hit because of larger overheads but not a factor of 4!    I ran it both on a spinning HDD and PCIe SSD but they both had the same problem.  I also tried running it on multiple visits versus multiple chips for a single visit (all accessing the same MEF FITS file) but this made no difference.  I tested it with various numbers of processes and found that the time per processes increases linearly with the number of processes running.      [~jmatt] has been helping me track this down.  We used some performance tools (htop, iotop, and perf) to figure out what was going on.  It was clear that the issue was not a RAM or I/O problem.  By watching htop while the 40 processes were running it became clear that once some of the processes hit ""deblending"" everything slowed down considerably and all cores were maxed out and showing lots of kernel traffic.  I also ran processCcdDecam.py with deblending turned off and the performance was much more reasonable (~24 sec. per process).    After more digging (with perftop), we found that there was a lot of swapping going on during the deblending step by ""openblas"".  This is a package that numpy uses for speeding up certain computationally intensive tasks using multithreading (e.g. linear algebra).  By default each openblas instance takes advantage of ALL cores on a machine.  So all 40 processes were trying to use all available cores and most of the time was spent swapping between all of these threads.    OpenBlas can be configured to use a more reasonable number of cores/threads, but the version that LSSTSW uses is installed by miniconda via a dependency of numpy and, as far as we could tell, it's not possible to configured NUM_THREADS for OpenBlas with miniconda.    We ended up compiling our own version of OpenBlas with NUM_THREADS = 6 (the maximum threads that OpenBlas uses) and the performance was great, 24 sec.    I'm not sure what the solution is for this but we probably don't want to go with miniconda for the default LSSTSW installation (uses currently done by bin/deploy).    JMatt might have comments to add.   "
0,Track down reason for slow performance when running many jobs of processCcdDEcam on bambam,"During the processing of the COSMOS data for the verification dataset work I ran many jobs of processCcdDecam.py on the new linux server, bambam.  The performance was very slow, 4x longer than running a single job at a time.  Figure out what is going on."
1,Meetings Dec 2015,"Verification dataset meetings, RFD meetings, DES Chicagoland meeting and preparation"
1,Other LOE -- Dec 2015,"weekly LSST local grouop meetings, NCSA meetings (All-hands, software, etc), code review, other local meetings, postdoc meetings and tasks"
0,Vendor input on sizing predictions,Discussions about tape-pricing and disk-pricing predictions from Spectra and DDN respectively in order to improve our forecasting. This information needs to be incorporated into LDM-144.
1,More preparation for FY16 hardware,"More pricing iterations with several companies and incorporating that information into our final decision. More Q&A with storage companies re: comparable features. Compiled all storage option quotes into a spread sheet which now forms as a good comparison and helps LDM-144  forecasting.     Awaiting quotes for racks and PDUs. Now that rack size is known for the Chilean DC, this will serve us well for LDM-144 costs.     Power issues for FY16 hardware are settled and we are ready to schedule installation as soon as the hardware purchase contract is complete.    Tagging issues for FY15 hardware complete. Looking into pre FY15 tagging. Requested that LSST/Aura perform an inventory request to complete the circle and prove the process.    Working with Spectra / NetSource to create a sustainable tape condo that can serve LSST through 2030."
0,Plan DM’s communication / documentation / information architecture strategy,"Plan and write a technote outlining communication and documentation platforms from a DM perspective. The technote will specify    - how each platform is used  - what developments need to be done  - address integrations with LSST-wide communications projects  - address information architecture (generally, the ease of discovering the right information)"
1,File tickets for list of stack deficiencies and suggested upgrades,"K-T suggested that I take my list of ""stack deficiencies and suggested improvements"" [https://confluence.lsstcorp.org/display/SQRE/Stack+Deficiencies+and+Suggested+Upgrades] on confluence and (with Tim J.'s help) create tickets for each item (as much as possible) so that the work could be scheduled.  "
1,Continue learning about middleware,"Learn more about orchestration, task execution, and logging.  "
1,Implement zenodio.harvest,Harvest metadata about records in a Zenodo Community collection using the {{oai_datacite3}} format. See https://zenodo.org/dev    Part of the [zenodio|https://github.com/lsst-sqre/zenodio] Python package. This tool will be used by our technote and the documentation indexing platforms.
0,Doxygen package fails to build with flex 2.6,"To wit:    {code}  $ flex --version  flex 2.6.0    $ bash newinstall.sh    LSST Software Stack Builder  [...stuff...]  eups distrib: Failed to build doxygen-1.8.5.eupspkg: Command:  	source /Users/jds/Projects/Astronomy/LSST/stack/eups/bin/setups.sh; export EUPS_PATH=/Users/jds/Projects/Astronomy/LSST/stack; (/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.sh) >> /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log 2>&1 4>/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.msg  exited with code 252    $ grep error /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log  commentscan.l:1064:55: error: use of undeclared identifier 'yy_current_buffer'  commentscan.l:1126:58: error: use of undeclared identifier 'yy_current_buffer'  {code}    Builds fine using {{flex 2.5.35 Apple(flex-31)}}."
1,HSC backport: Add functions to generate 'unpacked matches' in a Catalog,"The qa analysis script under development (see DM-4393) calls to HSC {{hscPipeBase}}'s [matches.py|https://github.com/HyperSuprime-Cam/hscPipeBase/blob/master/python/hsc/pipe/base/matches.py] which adds functions to generate ""unpacked matches"" in a Catalog (and vice versa).  It will be ported into {{lsst.afw.table}}.    The port includes following HSC commits:  *Add functions to generate 'unpacked matches' in a Catalog.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186    *Adding check to prevent more obscure error.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5    *Some little error handling helps.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/61cc053b873d42802581adff8cbbdb52a348879e  (from branch: {{stage-ncsa-3}})    *matches: add ArrayI to list of field types that require a size*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5  (from branch {{tickets/HSC-1228}})    Please also include a unittest."
2,Adapt qa analysis script for LSST vs. HSC single visit processing comparison,"The qa analysis script ported from HSC and adapted to LSST on DM-4393 currently performs qa on single visit processing by comparing outputs of a single run from the different measurement algorithms and comparing those with the astrometry/photemetry reference catalog.  Here we will add functionality to directly compare two different runs of the same dataset (requiring accommodations for two butlers).  Since the goal is to compare outputs from runs on the LSST vs. HSC stacks, this will require a mapping of the different schemas of the persisted source catalogs of the two stacks."
0,Add labels to qa analysis plots for better interpretation,The plots output by the qa analysis script (see DM-4393) currently do not display any information regarding the selection/rejection criteria used in making the figures and computing the basic statistics.  This includes magnitude and clipping thresholds.  This information should be added to each plot such that the figures can be interpreted properly.
0,lsst-build should support enabling Git LFS in an already-cloned repository,"A repository which does not use Git LFS is created and described in {{repos.yaml}}. It runs through CI, and is cloned onto a Jenkins build slave. Subsequently, the repository configuration in {{repos.yaml}} is updated to enable LFS. The build system should notice this change and update the cloned repository on disk appropriately. Currently, it doesn't."
0,afw fails to build on a machine with many cores,"The afw package does not build reliably (if at all) on a linux box at UW (""magneto"", which has 32 cores and 128 Gb of RAM). The failure is that some unit tests fail with the following error:  {code}      OpenBLAS: pthread_creat error in blas_thread_init function. Error code:11  {code}    For the record, /usr/include/bits/local_lim.h contains this:  {code}  /* The number of threads per process.  */  #define _POSIX_THREAD_THREADS_MAX	64  /* We have no predefined limit on the number of threads.  */  #undef PTHREAD_THREADS_MAX  {code}    It appears that the build system is trying to use too many threads when building afw, which presumably means it is trying to use too many cores. According to [~mjuric] the package responsible for this is {{eupspkg}}, and it tries to use all available cores.    A workaround suggested by [~mjuric] is to set environment variable {{EUPSPKG_NJOBS}} to the max number of cores wanted. However, I suggest we fix our build system so that setting this variable is unnecessary. I suggest we hard-code an upper limit for now, though fancier logic is certainly possible.    A related request is to document the environment variables that control our build system. I searched for {{NJOBS}} on confluence and found nothing."
1,Remove dead code from configuration procedure, - remove scratch db?  - cleanup tmp/sql/*.sql filesi  - remove xrootd configuration script if useless?   - cleanup configuration script style (i.e. tmp/*.sh)  
1,Study if mysqlproxy can be compatible with mariaDB client,mysqlproxy is not compliant with mariaDB client: see https://mariadb.com/kb/en/mariadb/mariadb-vs-mysql-compatibility/#incompatibilities-between-mariadb-and-mysql-proxy    Nevertheless the trivial fix proposed (remove progress-report options) doesn't seems to work...  {code:bash}    mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv    ERROR 1043 (08S01): Bad handshake  {code}
1,Improve 'unit' tests using database,- Add mock database for it to work during unit tests or run it apart from unit tests?  - Fix testLocalInfile (read configuration file)  - Try all core/modules/sql/testSql*           
1, Improve  LOAD LOCAL INFILE management on czar side,"Option ""mysql_options( m, MYSQL_OPT_LOCAL_INFILE, 0 );"" is added to all C++ sql client instance due to common sql interface, but is only required on master (for merging results) is it possible:   - to remove LOCAL keyword (on czar virtfile is on the same machine that mariadb server)  - or to set it in Qserv czar/master configuration  - or to set it in master MariaDB instance only?"
1,Update kernel on IN2P3 cluster,"The ""Kernel Panic"" issue is non-blocking right-now for John due to machine automated reboot, but we have to solve it to target a stable production system.    With Yvan, we're converging on next update for the cluster:    - on my side I update Qserv metadata on ccqserv100 w.r.t. new Qserv metadata format, and then I test Docker+Qserv on ccqserv100->ccqserv124,  - then CC-IN2P3 team launches a upgrade of the kernel to kernel-ml ( ""mainline stable"" branch of The Linux Kernel Archives) on ccqserv100->ccqserv124, this could be done in January,  - I control Qserv behaviour is still the same than before,  - then Qserv developpers can use this cluster to see if ""Kernel Panic' issue is solved.    If it work we'll update to kernel-ml on ccqserv125->ccqserv149, if it doesn't, cc-in2p3 and Qserv developpers will have to find an other solution.    Regards,    Fabrice"
1,Audit and document obs_subaru scripts,"{{obs_subaru}} has a {{bin.src}} directory containing a variety of miscellaneous scripts. Some of these may be actively useful; others could be useful, but require modernizing to work with the latest version of the LSST codebase; others are obsolete or duplicate functionality available elsewhere. Throughout, documentation is lacking.    Please audit this directory: remove the scripts which are useless and ensure the others are working and properly documented."
0,Cyber security infrastructure document,This document details anticipated security infrastructure and roles needed for the operations of LSST at the base and summit observatory site.  
0,Bi-weekly LSST IaM meetings for December,Bi-weekly IaM meeting between NCSA and LSST for the month of December 2015.  Local coordinating meetings also included.
0,Make deblender more robust against weird PSF dimensions,"[~boutigny] reports two problems with PSF dimension calculations in the deblender that result in fatal errors, because earlier checks for bad dimensions intended to cause more graceful failures are incomplete.    The first appears to happen when the PSF dimensions are highly non-square, and the image width is smaller than 1.5x FWHM while the image height is more than 1.5x FWHM (or the opposite).  {code:hide-linenum}  measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,2'}:     File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw:  :image::ImageBase<PixelT>::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, cons  t typename lsst::afw::image::detail::types_traits<PixelT, false>::view_t&) [with PixelT = double]      Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15'    Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 321, in __call__      result = task.run(dataRef, **kwargs)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, in run      self.deblend.run(exposure, sources, exposure.getPsf())    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 231, in run      self.deblend(exposure, sources, psf)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 308, in deblend      clipStrayFluxFraction=self.config.clipStrayFluxFraction,    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 354, in deblend      psf, pk, sigma1, patchEdges)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 1073, in _handle_flux_at_edge      psfim = psfim.Factory(psfim, Sbox, afwImage.PARENT, True)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4630, in Factory      return ImageD(*args)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4472, in __init__      this = _imageLib.new_ImageD(*args)  LengthError:     File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, const typename lsst::afw::image::detail::types_traits<PixelT, false>::view_t&) [with PixelT = double]      Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15'  {code}    The second problem may occur when the overlap region between a PSF image and the data image it corresponds to is only 1 pixel in either dimension.  In any case, there's a gap in the graceful-failure logic that could let such a problem through, which would result in received error message:  {code:hide-linenum}  measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,1'}:   Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.  py"", line 321, in __call__      result = task.run(dataRef, **kwargs)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, i  n run      self.deblend.run(exposure, sources, exposure.getPsf())    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 231, in run      self.deblend(exposure, sources, psf)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 308, in deblend      clipStrayFluxFraction=self.config.clipStrayFluxFraction,    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 312, in deblend      tinyFootprintSize=tinyFootprintSize)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 575, in _fitPsfs      **kwargs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 752, in _fitPsf      sx1, sx2, sx3, sx4 = _overlap(xlo, xhi, px0+1, px1-1)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 721, in _overlap      (xlo <= xhi)  and (xmin <= xmax))  AssertionError  {code}"
1,Demonstrate web authentication using CILogon and Globus,Configure mod_auth_oidc on lsst-auth1 with CILogon and Globus.
2,CONOPS to  support design activities.,"One foundational document  required for how NCSA Astronomy Core Services working methods that is missing is a concept of operations (CONOPS) for the L1 system.     Better late than never,  I wrote a L1 cops illustrating the uses of the L1 system, allowing for further specifications and context for all staff involved in the project. the document currently exists a a draft in google docs.  It's been review by Mario and KT.  This is more properly a systems engineering document, not at all sure where a final home for it belongs (or how it acquires status)"
1,"Prepare  activity diagrams and backing conops  for LI provisioning and ARP, including satellite computing centers.","Prepared two longer con accompanied by (hand drawn activity diagrams).      One conops /activity  diagram describe the work at the archive center to provide and support the L1 services used by telescope operations.    the second activity diagram and conops respdes to Chuck Clavers' request to have materials that explain the relationship of the stiletto computing center at CCIN2P3 to Archive Center at NCSA.    Both are DRAFT; and coops seem to be systems engineering documents, and where to deliver a blessed version and who is responsible for this is unclear to me. "
0,Beth Willman visit,self explanatory
1,"Review gartner materials relating ITIL, Devops and related topic","Read Gartner materials related to ITIL, devops  IT organization in preparation for more detailed thinking about Data Operations.    One major category of thought is ""Mode 1 and Mode 2"" type organizations. Mode ! is the current typical controlled environment the strength is when something precious needs to be managed.  For LSST this might be the data release production, which is baselined to be 9 months on a unique resource that the project procured.  Mode 2 is ""doves"" which is best used for nimble, fall fast software.  An example e testing algorithms."
2,Management for December,"includes attendance at the NSF CI for facilities workshop (2d).  Hiring,  hiring related presentation at U of I  ACI.   Management of group and effort distributed at NCSA.  Oversight of some legacy projects (Chileand data center, etc)"
1,Connecting table with histogram viewer,"Create a demo, which takes a URL and shows a table with a histogram viewer connected to it."
0,Cleanup location of anonymous namespaces,"we place anonymous namespace in two ways: (a), INSIDE lsst::qserv::<module> namespace, or (b) BEFORE. This story involves cleaning it up - move them to before lsst::qserv::<module>"
1,Add mysql connection to QueryContext ,"We need access to database schema for various reasons (analyzing queries, checking authorization, for queries like ""show create table"" and others)."
1,Implement globally unique queryId,NULL
1,Support human-friendly Thread ID in logging messages,"Per discussion 1/6/2016, it'd be nice to have a function that generates user-friendly threadId  on Linux to simplify debugging."
2,create multi image viewer,"* port of MultiDataView.java  * support grids, rows, finger chart type grid  * support paging with table data sets  * support DatasetInfoConverter port  * The components should be able to display any group of data  * Critical for Firefly Viewer"
1,Port Data  set info converter achitechture,"defines various image data types, how to get them, groupings, artifacts.   I am not quite happy with how we did in in GWT so the design needs to be improved.  Must be less complex."
2,L1 Concept of Operations (December work),"Assist in developing a ConOps for the L1 system. This is the first step to making a detailed design and plan for construction.  	- Revised/cleaned up/added to L1 ConOps  	- Meeting to clarify calibration products production use case requirements  	- Discussions about operational use cases, processes, functions of L1 system"
1,Ops Planning - December,"- LOPT and TOWG meetings  - Beth Willman 2-day visit; discussions about operations, proposal timeline and deliverables    - Prepared FTE estimates for IT roles  	- Reviewed ITIL roles and clarified work descriptions  	- Met with NCSA ICI leads to get input on FTE estimates for various roles  - Timing diagrams  	- Worked on first draft of cycle diagram showing 24 hours of operations at NCSA"
2,Margaret's mgmt. activities in December,"- Meetings: security, IdM, DMLT, supertask coordination, standups, etc.  - Staffing  	- ARI meeting and preparation  	- Reviewed resumes, discussed staffing plan  - TPR, invoice breakouts, milestones  - Discussed EV process and MIS tool design for internal management  - etc.  "
2,Exploration of In-memory database packages used in time critical applications,"Begin evaluation of potential in-memory data storage tools - selecting memcached and redis to start. - 4    With the intent  to gain familiarity with these tools, procurred introductory volume on redis and began writing prototype python code to prototype lists, hashes, and lists of hashes. Sketched out and implemented base python class with virtual save method, then wrote child classes for replicators, replicator health, replicator jobs etc. and tested this code and implemented the save methods. - 4    Installed the above code on a nebula instance that acts as a job manager, then ran real job messages through the system and simulated task assignment and completion, using redis to track jobs. - 2    Exploring how a logging and visualization harness might be included so job activity could 1) be observed in realtime, and 2) so a session could be played back as an after action review to investigate errors, bottlenecks, cold restart behavior, etc. - 2    To finish out this story, redis replication must be included in the above prototype."
0,CONOPS-V1,Evaluated first cut of CONOPS document. Formulated queries for clarifying specific questions regarding ldm-230 - 2
0,Track and provide feedback on base site facitlity,NULL
1,Build network testbed,NULL
0,add CSS import and image import and clean up some existing jsx,NULL
1,Port W16 CModel improvements from HSC,Three significant changes were made to CModel in [HSC-1339|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1339]. They were described by [~jbosch] in a [post to {{hsc_software}}|http://jeeves.astro.princeton.edu/pipermail/hsc_software/all/4568.html]. They include:    * Changing the method by which the initial approximation is determined;  * Changing the determination of the pixel region to use in fitting;  * A new prior on ellipticity and radius.    Please port these changes to LSST.    Also include the results of fixing the bug described in [HSC-1384|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1384].
0,Week end 12/12/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 12, 2015."
0,Week end 12/19/15,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 19, 2015."
1,New equipment setup and configuration (week end 12/05/15),"* Setup IPMI on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Created ipmitools binary - Ubuntu 12.04 has the correct libraries for ESXi6  ** Installed on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Configuration worked well on lsst-esxi5, lsst-esxi6.  ** lsst-esxi3 is actually lsst-esxi4.  System will not respond to ipmitool commands - suggest waiting for scheduled outage and manually setting up ipmi.  * Debugged networking issues with new equipment  * Cleanup of crashplan archives for new lsst system  * Mac VMs  ** Working on figuring out Mac VM requirements and process with Josh Hobblitt  ** Attempted to install and configure puppet on Mac VMs - running into configuration issues  "
0,New equipment setup and configuration (week end 12/12/15),"* Set up IPMI on lsst-test1, lsst-test2,…lsst-test9  * More research on using Puppet on Mac VMs – little progress  * Cleanup of NFS space in ITS  "
0,New equipment setup and configuration (week end 12/19/15),* Research on using puppet on Mac VMs  ** Considering using Vagrant to manage VirtualBox or Fusion Mac VMs  ** Tried to setup LDAP auth for Mac user auth  * Cleanup of NFS space in ITS
1,Updates to the Sizing Model,"Updated processor projections based upon Haswell and Skylake expectations. Added Shipping rates, Chilean and US power and cooling rates, updated memory pricing projections. Working with Spectra Logic on updating and improving tape predictions, library space and power requirements, upgrade options and mapping of bandwidth and capacity requirements to hardware (need to figure in fudge factors for latency of mounting tapes, latency of seeks times, maybe space for tape migrations, replace replacement tapes with updating pricing that includes tape replacement). Inclusion of that into the document will be in the next story."
0,"Contractual work, justifications, inventory for LSST hardware","Reviewing hardware purchase contracts, reviewing internal hardware budget justifications (and attending related meetings), working with purchasing on 'vendor specific' purchasing options, incorporating updated vendor-quoted pricing into expected hardware expenditures. "
0,meas_extensions_shapeHSM seems to be broken,"I have installed the meas_extensions_shapeHSM package together with galsim and tmv (I documented it at : https://github.com/DarkEnergyScienceCollaboration/ReprocessingTaskForce/wiki/Installing-the-LSST-DM-stack-and-the-related-packages#installing-meas_extensions_shapehsm) and tried to run it on CFHT cluster data.     My config file is the following:    {code:python}  import lsst.meas.extensions.shapeHSM  config.measurement.plugins.names |= [""ext_shapeHSM_HsmShapeRegauss"", ""ext_shapeHSM_HsmMoments"",                                      ""ext_shapeHSM_HsmPsfMoments""]  config.measurement.plugins['ext_shapeHSM_HsmShapeRegauss'].deblendNChild=''  config.measurement.slots.shape = ""ext_shapeHSM_HsmMoments""  {code}    When I run measCoaddSources.py, I get the following error :    {code}  Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/bin/measureCoaddSources.py"", line 3, in <module>      MeasureMergedCoaddSourcesTask.parseAndRun()    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run      if self.precall(parsedCmd):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall      task = self.makeTask(parsedCmd=parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 363, in makeTask      return self.TaskClass(config=self.config, log=self.log, butler=butler)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/python/lsst/pipe/tasks/multiBand.py"", line 530, in __init__      self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/task.py"", line 255, in makeSubtask      subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pex_config/2015_10.0-1-gc006da1/python/lsst/pex/config/configurableField.py"", line 77, in apply      return self.target(*args, config=self.value, **kw)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/sfm.py"", line 247, in __init__      self.initializePlugins(schema=self.schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins      self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 15, in __init__      self.cpp = self.factory(config, name, schema, metadata)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 223, in factory      return AlgClass(config.makeControl(), name, schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_extensions_shapeHSM/python/lsst/meas/extensions/shapeHSM/hsmLib.py"", line 964, in __init__      def __init__(self, *args, **kwargs): raise AttributeError(""No constructor defined - class is abstract"")  AttributeError: No constructor defined - class is abstract  {code}"
0,MariaDB does not work together with mysql-proxy,We have switched to MAriaDB but there is one issue that complicates things - mysql client from mariadb fails to connect to mysql-proxy with an error:  {noformat}  ERROR 1043 (08S01): Bad handshake  {noformat}  so Fabrice had to find a workaround for our setup to use client from mysqlclient package instead. This workaround is not perfect and it complicates other things. Would be nice to make things work transparently for mariadb.    
0,JIRA project for the publication board,The LSST Publication Board requests a JIRA project for managing its workload.       
0,Rename temporarily mariadb client,"MariaDB client isn't compliant with mysqlproxy and eups doesn't allow to override it with regular mysql client, so it will be rename, so that MYSQLCLIENT_DIR reference in qserv table file can be removed (indeed, it brokes qserv_distrib setup, but not qserv setup, ...)"
1,Consulting in December,NULL
0,Update provenance in baseline schema,Current provenance schema in baseline (cat/sql) is very old and no longer reflect latest thinking. This story involves bringing cat/sql up to data and replacing existing prv_* tables with tables we came up with in the epic.
0,Packge mysqlproxy 0.8.5,See https://mariadb.atlassian.net/browse/MDEV-9389
1,FITS Visualizer porting: Mouse Readout: part 2: flux value,"Call the server when mouse pauses, include the flux value in the readout. The should also include support for 3 color."
1,FITS Visualizer porting: Mouse Readout: part 3: Lock by click & 3 color support,add toggle button that make the mouse readout lock to last position click on.  It will not longer update on move but by click  Include: 3 Color Support
3,S17 Refactor MySQL Connection in Qserv,NULL
3,F17 Setup Qserv and ImgServ with PanSTARRS data,"Once the PanSTARRS data becomes public, we should load it to qserv. This epic involves partitioning data, loading to qserv and making it ready for analysis by friendly scientists (and for our internal testing). We should also make the panstarrs images available through imgserv, The work involves setting up webserv instance and configuring imgserv for panstarrs."
1,Refactor prototype docs into “Developer Guide” and Science Pipelines doc projects,"Refactor [lsst_stack_docs|https://github.com/lsst-sqre/lsst_stack_docs] into two doc projects    - LSST DM Developer Guide that will be published to {{developer.lsst.io}}, and  - LSST Science Pipelines that will be published to {{pipelines.lsst.io}}"
0,Write Zoom Options Popup,Write the simple zoom options popup that is show when the user clicks zoom too fast or the zoom level exceeds  the maximum size.      activate this popup from visualize/ui/ZoomButton.jsx
0,DetectCoaddSourcesTask.scaleVariance gets wrong result,"DetectCoaddSourcesTask.scaleVariance is used to adjust the variance plane in the coadd to match the observed variance in the image plane (necessary after warping because we've lost variance into covariance). The current implementation produces the wrong scaling in cases where the image has strongly variable variance (e.g., 10 inputs contributed to half the image, but only 1 input contributed to the other half) because it calculates the variance of the image and the mean of the variance separately so that clipping can affect different pixels.    Getting this scaling very wrong can make us dig into the dirt when detecting objects, with drastic implications for the resultant catalog.    This is a port of [HSC-1357|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1357] and [HSC-1383|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1383]."
1,Rotate Popup,NULL
0,Update the ground truth values in the lsst_dm_demo to reflect new defaults in deblending,"In DM-4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. This breaks the lsst_dm_demo, as now the results of processing are slightly different. The short term solution as part of DM-4410 was to run the demo with the defaults overridden to be what they were prior to DM-4410. In the long term the values used in the compare script should be updated to reflect what would be generated with running processCcd with the stack defaults. "
1,Some wcs keywords need to be removed from the metadata of raw DECam data,"Header keys such as PVi_j left in the raw metadata confuse the making of wcs in later processing steps.   For example, when {{calexp}} is read in {{makeDiscreteSkyMap.py}}, {{makeCoaddTempExp.py}}, and so on, this message appears:  {code:java}  makeWcs: Interpreting RA---TAN-SIP/DEC--TAN-SIP + PVi_j as TPV  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV-SIP/DEC--TPV-SIP  {code}  These {{calexp}} are created by running {{processCcd.py}} on raw data, and are mis-interpreted as TPV.  "
0,Test stack with mariadbclient,"Now that we switched Qserv to mariadb, it'd be good to switch the rest of the stack. This story involves trying out if things still work if we switch mysqlclient to mariadbclient."
1,Add Shared Scan Table Information to CSS ,Some information should be added to CSS to indicate if a table should be locked in memory for shared scans and the effect the table is likely to have on the time it takes to complete a query.
0,Package mariadbclient,"There are some very low level modules that depend on mysqlclient (for example daf_persistence). It'd be too harsh to make them depend on mariadb, so we should package mariadb client."
3,X16 Fine-tune Shared Scans,"Fine tune shared scans code, in particular take advantage of unique queryId."
3,Provenance Prototyping,"Build a proof-of-concept provenance prototype for a selected pipeline, perhaps HCS."
0,Create validation_data set for DECam validation test,Create a `validation_data_decam` to provide a few images for DECam validation tests.    Use the COSMOS field data as currently available on NCSA being processed by [~nidever].    Select just a few images for now.
1,Planning for Software Documentation Deployment Service,Write initial draft of [SQR-006|http://sqr-006.lsst.io] that specifies how the documentation deployment service will work.
0,Read and understand `ci_hsc` and plan relationship with `validate_drp`,Read through and run the `ci_hsc` tests and plan for how this module and efforts should relate to `validate_drp`.    a. Add capabilities to `validate_drp` to run the tests in `ci_hsc`.  (/)  b. Compare frameworks. (/)  c. Plan for how such validation and continuous integration data sets should be constructed. (/)  
0,Improvement of raw data handling in DecamMapper,Two minor improvements with better coding practice:  - Be more specific copying FITS header keywords. Avoid potential problems if unwelcome keywords appear in the header in the future. Suggested in the discussions in DM-4133.   - Reuse {{isr.getDefectListFromMask}} for converting defects. A more efficient method that uses the FootprintSet constructor with a Mask and a threshold has just been adopted in DM-4800.     Processing is not changed effectively.  
0,HSC backport: Remove interpolated background before detection to reduce junk sources,This is a port of [HSC-1353|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1353] and [HSC-1360|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1360].    Descriptions from HSC:   {panel:title=HSC-1353}  We typically get a large number of junk detections around bright objects due to noise fluctuations in the elevated background. We can try to reduce the number of junk detections by adding an additional local background subtraction before object detection. We can then add this back in after detection of footprints and peaks.  {panel}  {panel:title=HSC-1360}  I forgot to set the useApprox=True for the background subtraction that runs before footprint and peak detection. This will then use the Chebyshev instead of the spline.  {panel}
1,Code review,"DM-4133, DM-4800, DM-4709, DM-4707, DM-4814"
0,Add Dropdowns to Vis toolbar,Add the dropdown to the vis tool bar
0,Clean up div and css layout on FitsDownloadDialog,FitsDownload dialogs html and css is not quite right. Needs some clean up.
0,makeDiscreteSkymap has a default dataset of 'raw',"The default dataset type for command line tasks is raw.  In the case MakeDiscreteSkyMapTask is asking the butler for calexp images.  This shouldn't be a problem, but in my case I have calexp images, but no raw images.  This causes the task to think there is no data to work on, so it exits."
2,Understand async queries in Qserv,"Try to understand, without doing actual implementation what is involved in  implementation of support for asynchronous queries in Qserv and possibly web interface. Should result in a roadmap for implementation at all levels.  "
0,Adapt `validate_drp` to standard python and bin subdir sturcture,Move Python files into python/lsst namespace convention.  Decide on where {{validateCfht.py}} and {{validateDecam.py}} executables should live  Add package requirements to {{ups/validate_drp.table}}
1,"Finish Fits View Decoration: context toolbar, title, expand button, etc",NULL
1,Add error handling to PsfFitter in meas::modelfit,"The {{ShapeletPsfApprox}} Task uses a class called {{PsfFitter}} which is not a {{SimpleAlgorithm}} and does not support error handling.  Add error handling to this class, and modify the Task definition in {{psf.py}} to call an {{errorHandler fail()}} function when the algorithm's {{optimizer.run()}} call fails.    Also, add unit tests"
1,Add bright object masks to pipeline outputs,"Given per-patch inputs providing   {code}  id, B, V, R, ra, dec, radius    {code}  for each star to be masked, use this information to set:  * A bit in the mask plane for each affected pixel  * A flag in the source catalogues for each object that has a centroid lying within this mask area    This is a port of [HSC-1342|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1342] and [HSC-1381|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1381]."
0,Update configuration for Suprime-Cam,The {{obs_subaru}} configuration for Suprime-Cam needs updating to match recent changes in the stack.    Port of [HSC-1372|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1372].
0,Preliminaries for LSST vs HSC pipeline comparison through coadd processing,This is the equivalent of DM-3942 but through coadd processing.    Relevant HSC tickets include:    * [HSC-1371|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1371]
0,Allow slurm to request total CPUs rather than nodes*processors.,"On some systems, we are asked to request a total number of tasks, rather than specify a combination of nodes and processors per node.    It also makes sense to use the SMP option this way.    This is a port of [HSC-1369|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1369]."
1,Fix logic for applying aperture corrections,"With the current flow, the aperture corrections are being applied only after all the measurement plugins have run through, independent of their execution order.  This results in plugins whose measurements rely on aperture corrected fluxes (i.e. with execution order > APCORR_ORDER) being applied prior to the aperture correction, leading to erroneous results.  The only plugin currently affected by this is {{base_ClassificationExtendedness}}.    This ticket involves applying a temporary fix to ensure proper application and order of aperture corrections.  However, the problem highlights the fact that the current logic of how and when aperture corrections are applied should be reworked (on another ticket) to be less error-prone."
1,Implement brighter-fatter correction,"Please port the prototype Brighter-Fatter correction work by Will Coulton from HSC.    This covers [HSC-1189|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1189], [HSC-1332| https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368], [HSC-1368|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368]. Note also the stand alone commits [783b124|https://github.com/HyperSuprime-Cam/obs_subaru/commit/783b124b6813f5745ce1e444f61fb0114d055907] and (if this work is performed after DM-3373) [9fc5e78| https://github.com/HyperSuprime-Cam/obs_subaru/commit/9fc5e78247e7173e095255dba34e994f73a6bd1d]."
1,High-level overview of DRP processing,"Create high-level overview of Data Release Production, probably as an annotated flowchart, for use in sizing model work and as a graphical table of contents for more detailed descriptions."
1,Add sky objects,"Please add ""sources"" corresponding to empty sky (ie, at positions where nothing else has been detected) and include them in multiband processing.    This is a port of [HSC-1336|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1336] and [HSC-1358|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1358]. "
0,Use high S/N band as reference for multiband forced photometry,We are currently choosing the priority band as the reference band for forced photometry as long as it has a peak in the priority band regardless of the S/N.  Please change this to pick the highest S/N band as the reference band when the priority band S/N is sufficiently low.    This is a port of [HSC-1349|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1349].
0,Don't write HeavyFootprints in forced photometry,There's no need to persist {{HeavyFootprint}}s while performing forced photometry since retrieving them is as simple as loading the _meas catalog.    This is a port of [HSC-1345|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1345].
1,Add new blendedness metric,[HSC-1316|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1316] shifts the calculation of blendedness from {{meas_deblender}} to {{meas_algorithms}} and defines a new blendedness metric in the process. Please port it.
1,Measure photometric repeatability and correctness of reported errors,1. Calculate and plot photometric variability across series of N images.  Compare to reported photometric errors.  Designed for N > 5.  2. Calculate and plot Delta flux / sigma_flux for multiple observations of stars in field.  This is related to 1. but is focused on N=2 to N=5.  3. Fit uncertainty distribution vs. magnitude to identify any floor in the photometric uncertainty and to check performance vs. photon counts.
0,LDM-151 - comments from Jacek,"I am reading your https://github.com/lsst/LDM-151/blob/draft/DM_Applications_Design.tex, and I have some minor comments suggestions. I am going to add comments to this story to capture it. Feel free to apply to ignore :)"
0,Factor out duplicate setIsPrimaryFlag from MeasureMergedCoaddSourcesTask and ProcessCoaddTask,{{MeasureMergedCoaddSourcesTask.setIsPrimaryFlag()}} and {{ProcessCoaddTask.setIsPrimaryFlag()}} are effectively the same code. Please split this out into a separate task which both of the above can call.    This is a (partial) port of [HSC-1112|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1112] and should include fixes from [HSC-1297|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1297].
1,XY Plot action and reducers,Write action and reducers for XY Plot
0,Implement zenodio.metadata to mediate Zenodo's API with local YAML metadata,"[Zenodio|http://zenodio.lsst.io] is a Python package we’re building to interact with Zenodo. For our various doc/technote/publishing projects we want to use YAML files (embedded in a Git repository, for example) to maintain deposition metadata so that the upload process itself can be automated.    The {{zenodio.metadata}} sub package provides a Python representation of Zenodo metadata (but not File or Zenodo deposition metadata).    See DM-4725 for the upload API work, which consumes the metadata objects."
0,Add __setitem__ for columns in afw.table,"It's confusing to have to use an extra {{[:]}} to set a column in afw.table, and we can make that unnecessary if we override {{\_\_setitem\_\_}} as well as {{\_\_getitem\_\_}}."
0,Replace killproc and pidofproc with kill and pidof,Running at NCSA on OpenStack revealed that our qserv-stop.sh and qserv-status.sh fail because of missing killproc and pidofproc. It looks like (see eg http://stackoverflow.com/questions/3013866/killproc-and-pidofproc-on-linux) these are not very portable and it is better to use kill and pidof.
0,imagesDiffer doesn't handle overflow for unsigned integers,"I'm seeing a test failure in afw's testTestMethods.py, apparently due to my numpy (1.8.2) treating images that differ by -1 as differing by 65535 in both {{numpy.allclose}} and array subtraction (which doesn't promote to an unsigned type).    Does this still cause problems in more recent versions of {{numpy}}?  If not, I imagine it's up to me to find a workaround for older versions if I want it fixed?    (assigning to [~rowen] for now, just because I know he originally wrote this test and I hope he might know more)"
1,"Please provide ""getting started"" documentation on writing meas_base algorithms","{{meas_base}} provides a framework for writing measurement algorithms in a uniform way. However, documentation on exactly how this should be done is fragmentary:    * There's some basic documentation in [Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html] which provides a useful introduction, but doesn't discuss common idioms and helpers such as {{FlagHandler}}, {{SafeCentroidExtractor}} and transformations.  * The [design notes on Confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390] are not intended as documentation and aren't kept up-to-date as new features are added, but can still be a useful reference.  * [~jbosch] gave a [nice introduction|https://github.com/lsst-dm/Oct15_bootcamp/blob/measurement/measurement/measurement.pdf] at the October 2015 Bootcamp, but a set of slides is no substitute for proper documentation, and again there's no expectation that these will be kept up-to-date.    Please provide a centralized, maintained guide to writing {{meas_base}} plugins."
0,Add point selection,"click and highlight a point.  Is on when mouse readout ""Lock by Click"" is on. However, can me turned on externally by adding toolbar context menu options."
2,Setup webserv for SUI,"This story involves setting up a webserv in a VM (NCSA OpenStack) with a small data set: images and corresponding database catalog. We need to   * setup VM   * build the stack for webserv and qserv   * identify images to load   * run qserv in the VM   * run ingest to load the data to mysql (mysql will run on lsst10) and qserv (qserv will run directly in the VM)   * run two webservers - one with mysql backend, one with qserv backend   * open the port numbers for the IPAC team"
1,Port HSC background matching routines,HSC has its own implementation of background matching: see {{background.py}} in {{hscPipe}}. Please port it to the LSST stack.
0,Filter mask planes propagated to coadds,"Some mask planes -- {{CROSSTALK}}, {{NOT_DEBLENDED}} -- do not need to be propagated to coadds. Add an option to remove them.    This is a port of work performed on [HSC-1174|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1174] and [HSC-1294|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1294]."
0,scisql build scripts are buggy ,"The scisql build script logic for MySQL/MariaDB version checking is broken on all platforms. There are also assumptions about shared library naming that do not hold on OS/X, which means that the deployment scripts are likely broken on all platforms other than Linux."
2,Setup LSST stack for verification datasets work,Created a script to setup required LSST stack packages for bulge survey processing.  
1,Installing a reference catalog to use in bulge survey processing,Install Astrometry.net Index Files for 2MASS all sky catalog
2,Setup orchestration environment at lsstdev for bulge survey processing,Follow instructions at   https://confluence.lsstcorp.org/display/DM/1.+Quick+Start+-+LSST+Cluster+Orchestration    to set up the required packages to run the lsst stack at lsst cluster.
2,Debugging lsst.astrometry task in bulge survey processing,The DECam bulge survey is being processed as part of the verification data sets effort. During astrometry calibration task a large number of failures (affecting ~100 of 213 visits) have been found in calibrate.astrometry.matcher. We report here details of the investigation around this issue. Part of the task is to learn how to use the task built in debug.     More info:    https://confluence.lsstcorp.org/display/SQRE/Bulge+Survey+Processing#BulgeSurveyProcessing-Results
2,Setup firefly example for image visualization ,Start from example provided by the firefly team    https://github.com/lsst/suit
1,Test the matchOptimisticB astrometric matcher,"The matchOptimisticB matcher fails on many visits of the bulge verification dataset.  This prompted a deeper investigation of the performance of the matcher.  Angelo and David developed a test script and discovered that the matcher works well with offsets of the two source catalogs of up to 80 arcsec, but fails beyond that.  This should be robust enough for nearly all datasets that the LSST stack will be used on."
1,Write a firefly search processor that retrieves image paths from the butler,NULL
0,base has no readme,"The base package does not have a readme file, so it's unclear what it's for. The package name is also somewhat unfortunate, being so generic, but at least with a readme it would be clearer how important it is (if it is, in fact, important)."
1,Compile list of DM simulation needs for Andy Connolly,Compile list of DM simulation needs over the next ~6 months to give to Andy Connolly (simulations lead).
1,Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.,"Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.    - Use the butler to iterate over the data ids, read the src catalog and count the number of sources per ccd.    - Use afw.display.ds9 to display the image and overlay the sources"
0,Propagate flags from individual visit measurements to coadd measurements,"It is useful to be able to identify suitable PSF stars from a coadd catalogue. However, the PSF is not determined on the coadd, but from all the inputs. Add a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the PSF.    Make the inclusion fraction threshold configurable so we can tweak it (so we only get stars that were consistently used for the PSF model; the threshold might be set it to 0 for ""or"", 1 for ""all"" and something in between for ""some"").    Make the task sufficiently general that it can be used for propagating arbitrary flags.    This is a port of work carried out on [HSC-1052|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1052] and (part of) [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293]."
0,Make coadd input catalogs contiguous,"It's convenient if we can assume that coadd input catalogs are contiguous -- it simplifies the implementation of {{PropagateVisitFlagsTask}} (DM-4878), for example. Make it so.    This is port of work carried out on [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293]."
3,Test capabilities of python bokeh plotting library for making interactive plots - I,"We are testing the bokeh library plot for interactive visualization in the web, the python API is atractive and allows rapid prototype which is good for SQuaRE build up its QA system.    Some examples are available in this repo, including a scatter plot with linked histograms in both axis which seems really useful.    https://github.com/lsst-sqre/bokeh-plots    A more complete demonstration of bokeh is available in this webminar:     https://continuum-analytics.wistia.com/medias/f6wp9dam91    "
0,base_Variance plugin generates errors in lsst_dm_stack_demo,"Since DM-4235 was merged, we see a bunch of messages along the lines of:  {code}  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record  {code}  in the output from {{lsst_dm_stack_demo}}. (See e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/7482/console#console-section-3]). It's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem."
2,S17 Improve Qserv Integration Tests,"Integration tests need improvements, in particular, we want to run multi-node integration tests easily (possibly without docker), get rid of mono-node test. We should catch errors from individual tests."
2,Improve/simplify multi-node tests,NULL
3,Script launch of HTCondor pool on Nebula OpenStack,"The ability to automate the launch of htcondor pools of worker nodes  (e.g., with LSST software installed) on the Nebula OpenStack can be useful  in several ways for LSST DM.  On one hand, users can start up their own customized  pool should the standard pool available on other resources such as lsst-dev not  be suitable (e.g., not enough cores, not enough memory per core/slot,  customized software not installed on the systems, etc.)  Also, scripted launch  of a pool can be part of the development of a solution for offering  ""batch"" scheduling to the Nebula OpenStack, following an approach similar to e.g.,  CANFAR ( http://www.canfar.net/docs/batch/ , http://cloudscheduler.org)  whereby  a ""cloud scheduler"" is used in conjunction with an htcondor central manager  to provide batch access to the cloud (i.e., submitted jobs are placed into an htcondor queue,  that will execute on launched instances, when those instances join the working pool.)  "
1,Refactor measurement afterburners into a new plugin system,"Some of the operations we currently run as part of measurement (or would like to) share some features that make them a bit different from most plugin algorithms:   - They must be run after at least some other high-level plugins, and may be run after all of them.   - They do not require access to pixel data, as they derive their outputs entirely from other plugins' catalog outputs.   - They may require an aggregation stage of some sort to be run on the regular plugin output before they can be run.    Some examples include:   - Star/Galaxy classification (with training done after measurement and before classification).   - Applying aperture corrections (estimating the correction must be done first).   - BFD's P, Q, R statistics (requires a prior estimated from deep data).    We should move these algorithms to a new plugin system that's run by a new subtask, allowing these plugins to be run entirely separately from {{SingleFrameMeasurementTask}}.  This will simplify some of the currently contorted logic required to make S/G classification happen after aperture correction, while making room for hierarchical inference algorithms like BFD and Bayesian S/G classification in the future.    (We will not be able to support BFD immediately, as this will also require changes to our parallelization approach, but this will be a step in the right direction).    This work should *probably* be delayed until after the HSC merge and [~rowen]'s rewrite of {{ProcessCcdTask}} are complete, but it's conceivable that this refactoring could solve emergent problems there and be worth doing earlier as a result."
1,Update git-lfs documentation to work with git-lfs 1.1.0+,"The git-lfs client (1.1.0+) does not support empty username and passwords. To work around this, users can store the appropriate credentials directly with the credential helper."
0,Update git-lfs repositories to point to the git-lfs documentation.,Update git-lfs repositories to point to the git-lfs documentation.    All documentation should be generic and point to:    http://developer.lsst.io/en/latest/tools/git_lfs.html  
3,Take DECam data with collimated beam projector,[~mfisherlevine] and [~rhl] will travel to CTIO to observe on the Blanco 4m telescope.
0,Write tutorial describing remote IPython + ds9 on lsst-dev,"[~mfisherlevine] recently figured out how to set up his system to run a remote IPython kernel on {{lsst-dev}} and interact with it from his laptop, including streaming image display from the remote system to a local instance of {{ds9}}.    He will write all this up so that others in the community can easily do the same."
1,Ingest DECam/CBP data into LSST stack,"[~mfisherlevine] will ingest the data taken in DM-4892 into the LSST stack. Initial experiments indicate problems with:    * Bias subtraction  * Flat fielding  * Bad pixel masks    These may already be remedied by work on {{obs_decam}}; if not, he will file stories and fix them."
1,Prepare calibration products for analysing DECam data,"Determine if existing bad pixel masks, flats, etc are adequate for analysing the DM-4893 DECam data, and, if not, provide alternatives."
1,Qualitative exploration of the CBP/DECam data,"Having got the CBP/DECam data loaded into the stack, explore the parameter space and understand data.    This should result in a series of stories describing more detailed analysis with quantitative results."
3,Implement the simulation and testing framework for analyzing image differencing,"We need to be able to test all aspects of image differencing.  This includes template generation, astrometric registration, and differencing.  We know that DCR will be an effect that will need to be mitigated so we will have to be able to simulate it and show how well various techniques deal with it."
2,Implement simulations for testing image differencing.,"Implement a suite of simulations tools for testing the image differencing techniques, specifically with an eye toward dealing with DCR."
0,Use yaml configuration files to store camera-specific data ID and ref image information for validation testing.,"Currently there is {{validateCfht.py}} and {{validateDecam.py}} as code.  These differ in just having {{defaultData}} functions that specify the dataIds to consider and the dataIds to use as a reference for comparison.    Storing the information necessary to create these sets of dataIds in separate data files, to be stored as YAML would  1. Improve the separation of code and data  2. Clarify the usage and necessary information to run on a new or different set of data  3. Make it easier to run different subsets easily by specifying a different input file    The proposals is that {{validateCfht.py}} and {{validateDecam.py}} would disappear from {{bin}} and be replaced by just {{validate_drp.py}}.  The examples in {{examples/runCfhtTest.sh}} and {{examples/runDecamTest.sh}} will be updated to show the new usage.  The README will also be updated."
1,"Expand button hide/show, delete button hide/show, display title options,","Expand button hide/show, delete button hide/show, display title options,  support pv.hideTitleDetail to control showing zoom level and rotation info (used by planck)  support external title bar (planck as well)  support checkbox on title bar (planck)  "
0,Buffer overrun in wcslib causes stack corruption,"The buffer 'msg' in wcsfix.c is used to report attempts by wcslib to re-format units found in fits files. It is allocated on the stack (in function 'unitfix') using a pre-processor macro defined size of 160 chars (set in wcserr.h). When attempting to run the function 'unitfix' in wcsfix, this buffer can overflow on some fits files (the raw files generated by HSC seem particularly prone to triggering this behavior) and results in the session being terminated on Ubuntu 14.04 as stack protection is turned on by default i.e. the stack crashes with a 'stack smashing detected' error. We have reported the bug to the creators of wcslib. As a temporary workaround, users affected by the bug should increase the default size of 'msg' by increasing WCSERR_MSG_LENGTH defined in wcserr.h      We are providing a small python example that demonstrates the problem. Run it as  python test.py <path to ci_hsc>/raw/<any fits file in this directory>    We are also providing a simple c program to demonstrate the bug. Compile it as  cc -fsanitize=address -g -I$WCSLIB_DIR/include/wcslib -o test test.c -L$WCSLIB_DIR/lib -lwcs (on Linux)  cc -fsanitize=address -g -L$WCSLIB_DIR/lib -lwcs -I$WCSLIB_DIR/include/wcslib -o test test.c (on Mac OS X)"
2,Investigate astrometry warnings from processing raw DECam data,"This ticket includes efforts to troubleshoot and improve processing DECam raw data in Jan 2016.   Investigations of the warnings from solving astrometry led to DM-4805, DM-4859. This ticket also includes partial efforts in DM-4859.         For validation, I ran {{processCcd.py}} with two visits of raw Stripe 82 DECam data with and without the changes in DM-4859 (using the first camera geometry fix, which is different from the final fix).  The script {{validateDecam}} in {{validate_drp}} is used to check astrometric scatter of the sources between the two visits.  Output plots are in the attachments.  By fixing DM-4859, the median astrometric scatter (mag < 21) decreases from 31.3 mas to 26.1 mas.  The number of matches between two visits increases from 56768 to 71265.  Also attached are the ra/dec patches plots, using the {{showVisitSkyMap.py}} script from DM-4095. The two colors represent the two visits of calexp wcs.  Southern CCDs had bad astrometric solutions before DM-4859.  The validation results are consistent with the patches visualization.   "
0,Cyber security infrastructure requirements,"Documenting cyber security operational requirements by LSST, particularly at the obs. site."
0,Security plan renewal,"Continuing work on cyber sec. plan renewal.  DM moving along, PO slated next."
0,DM security meeting,Security meeting/planning with LSST DM team at NCSA.
0,LSST IaM meetings at NCSA,NULL
0,LSST IaM bi-weekly coordinating meeting,"Meeting between NCSA CSD group, NCSA LSST DM group, and other LSST groups."
1,Add option for object name resolution,"For some object names resolved by NED, the position is not right. In this situation, it would be better to get the position from Simbad. Currently, Firefly offers two options: first NED, then Simbad; first Simbad, then NED. The third option to be added would be ""the best position according to the object type"".     It should check the object type returned by NED, making a decision whether to get position from Simbad instead; and vice versa. "
1,Test obs_decam with processed data,"Sometimes DECam-specific bugs only reveal in or affect the processed data. For example the bug of DM-4859 reveals in the {{postISRCCD}} products.  If the bugs are DECam-specific, some changes in {{obs_decam}} are likely needed.  It would be useful to have a more convenient way to test those changes. In this ticket I modify {{testdata_decam}} so that those data can be processed, and then allow wider options in the {{obs_decam}} unit tests.    I add {{testProcessCcd.py}} in {{obs_decam}} that runs {{processCcd.py}} with raw and calibration data in {{testdata_decam}}.  Besides a short sanity check, I add a test (testWcsPostIsr) that tests DM-4859. {{testWcsPostIsr}} fails without the DM-4859 fix, and passes with it.  "
0,Porting encodeURL of the java FitsDownlaodDialog code to javascript ,"When download an image,  the proper name needs to be resolved based on the URL and   the information about the image.  In Java code, it has the following three methods:  {code}   encodeUrl  makeFileName  makeTitleFileName  {code}    These method should be ported to javascript.  Thus, the javascript version of the FitsDownloadDialog will save the file in the same manner. "
1,Test performance of vertical-partition joins in mysql,"We are planning to vertically partition some tables (for example Object). We should make sure such joins across say 5, 10 or 20 tables are not a problem for mysql from performance standpoint. The testing involves creating a wide table (say 200 columns) and testing a speed of full scan, then slicing that table vertically into different number of columns and using join to assemble the pieces together."
1,Support Multi image fits and controls,"add toolbar: next, prev arrow buttons, title when multi image fits has image specific titles or title would be cube number.    Make sure the store will support multi images, add next, prev actions, etc"
0,Make obs_subaru build with OS X SIP,"Because of OS X SIP, {{obs_subaru}} fails to build on os x 10.11. In the {{hsc/SConscript}} file, the library environment variables need properly set, and scripts need to be delayed until the shebang rewriting occurs. "
0,want to see locations in trace when butler raises because multiple locations were found,"daf_persistence 11.0-2-g56eb0a1+1 gives the unhelpful error message:    {code}  > RuntimeError: Unable to retrieve bias for {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: No unique lookup for ['calibDate', 'calibVersion'] from {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: 2 matches  {code}    (the old butler did this too).  The user wants to know what the 2 matches were -- it's user error, but the user needs help and  printing the first few options (nicely formatted) is very useful.  I think I did this on the HSC side.      The butler code in question is actually in butlerUtils/python/lsst/daf/butlerUtils/mapping.py and my post-doc gave me the wrong package.    It's in need():  {code}  >         if len(lookups) != 1:  >             raise RuntimeError, ""No unique lookup for %s from %s: %d matches"" % (newProps, newId, len(lookups))  {code}"
0,"Make FlagHandler, SafeCentroidExtractor usable from Python","The {{meas_base}} framework includes {{SafeCentroidExtractor}}, a convenience routine for extracting a centroid from a source record, setting a consistent set of flags if that's not possible or if the centroid is in some way compromised. This consistent flag handling is made possible by the use of the {{FlagHandler}} class.    Unfortunately, {{FlagHandler}} is not meaningfully usable from Python, not least because it's impossible to define flags:  {code:python}  >>> import lsst.meas.base as measBase  >>> measBase.FlagDefinition(""flag"", ""doc"")  [...]  TypeError: __init__() takes exactly 1 argument (3 given)  >>> fd = measBase.FlagDefinition()  >>> fd.name = ""flag""  [...]  AttributeError: You cannot add attributes to <lsst.meas.base.baseLib.FlagDefinition; proxy of <Swig Object of type 'lsst::meas::base::FlagDefinition *' at 0x10a82b900> >  {code}    Looking further, even were we able to create {{FlagDefinitions}}, the {{FlagHandler}} is initialized with pointers to the beginning/end of a container of them, which seems like a stretch for Python code.    Please add Python support for these routines."
1,Centroids fall outside Footprints,"In DM-4882, we observed a number of centroids measured while running the {{lsst_dm_stack_demo}} routines fall outside their associated {{Footprints}}. This was seen with both the {{NaiveCentroid}} and the {{SdssCentroid}} centroiders.    For the purposes of DM-4882 we quieted the warnings arising from this, but we should investigate why this is happening and, if necessary, weed out small {{Footprints}} entirely."
0,Fix intermittent testQdisp failure,"The mocks used in the executive class don't mock cancellation correctly and doing so would require significant effort. When Executive::squash() is called, the mocks threads are already running but waiting on the _go barrier. squash() calls JobQuery::cancel() for each thread and cancel() calls markComplete() for the job because a QueryResource has not been aquirred from xrootd. Once all the jobs are cancelled and _go is set to true, the ex.join() command doesn't wait for the jobs to complete since markComplete() has already been called for all of the jobs. If any of the jobs take longer to complete than the main thread, they call markComplete for an Executive that no longer exists and cause the test to fail."
0,Fix build of MariaDB on OS X El Capitan,The current MariaDB EUPS package does not build on OS X El Capitan because OS X no longer ships with OpenSSL developer files. MariaDB has a build option to use a bundled SSL library in preference to OpenSSL but the logic for automatically switching to this version breaks when the Anaconda OpenSSL libraries are present.
1,Deploy 4 bare metal hosts for testing Base to Archive transfer implementation,"James needs to test network communication methodologies in an environment that mimics the expected real-world conditions. In order to minimize the complications with debugging, using bare metal machines in the first phase is preferred.    We can use 4 of the machines bought off the 2015 purchase or purchase new machines just for this purpose."
0,Qserv build fails on El Capitan with missing OpenSSL,Qserv does not build on OS X El Capitan due to the absence of OpenSSL include files. Apple now only ship the OpenSSL library (for backwards compatibility reasons). Qserv only uses SSL in two places to calculate digests (MD5 and SHA). This functionality is available in the Apple CommonCrypto library. Qserv digest code needs to be taught how to use CommonCrypto.
1,Track kernel panic issue,"The line that caused the kernel panic is in modules/mysql/MySqlConnection.cc line 151.  Currently the line is fine and is:          std::string const killSql = ""KILL QUERY "" + std::to_string(threadId);    This version of the line will occasionally cause the kernel panic (note the missing %1% that should be after KILL QUERY).          std::string killSql = boost::str(bo  ost::format(""KILL QUERY "") % threadId);  "
0,Create a utility function do do spherical geometry averaging,"I would like to calculate a correct average and RMS for a set of RA, Dec positions.    Neither [~jbosch] nor [~price] knew of an easy, simple function to do that that existed in the stack.  [~price] suggested:    {code}  mean = sum(afwGeom.Extent3D(coord.toVector()) for coord in coordList, afwGeom.Point3D(0, 0, 0))  mean /= len(coordList)  mean = afwCoord.IcrsCoord(mean)  {code}    That makes sense, but it's a bit unobvious (it's obvious how it works, but would likely never occur to someone that they should do it that way in the stack).    Pedantically it's also not the best way to do a mean while preserving precision, but I don't anticipate that to be an issue in practice.    Creating a function that did this would provide clarity.  I don't know where that function should live.    Note: I know how to do this in Astropy.  I'm intentionally not using astropy here.  But part of the astropy dependency discussion is likely ""how much are we otherwise rewriting in the LSST stack""."
0,on-going support to Camera team in visualization at UIUC,Attend the weekly meeting and answer questions as needed
0,Enable validateMatches in ci_hsc,"{{python/lsst/ci/hsc/validate.py}} in {{ci_hsc}} [says|https://github.com/lsst/ci_hsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#L78]:  {code:python}      def validateMatches(self, dataId):          # XXX lsst.meas.astrom.readMatches is gone!          return  {code}  {{readMatches}} (or its successor) should be back in place as of DM-3633. Please enable this test."
0,multiple CVEs relevant to mariadb 10.1.9 and mysql,"Multiple CVEs have been released this week for mysql & mariadb.  The current eups product for mariadb is bundling 10.1.9, which is affected.  Several of the CVEs do not yet provide details, which typically means they are ""really bad"".    https://github.com/lsst/mariadb/blob/master/upstream/mariadb-10.1.9.tar.gz    https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0505  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0546  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0596  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0597  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0598  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0600  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0606  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0608  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0609  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0616  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-2047"
0,Update scisql to v0.3.5,"In order to update MariaDB to v10.1.10 {{scisql}} needs to also be updated to deal with the hard-coded version checking. For the current version we get this error with the latest MariaDB:  {code}  :::::  [2016-01-28T16:51:40.539306Z]     user_function(self)  :::::  [2016-01-28T16:51:40.539334Z]   File ""/home/build0/lsstsw/build/scisql/wscript"", line 63, in configure  :::::  [2016-01-28T16:51:40.539346Z]     ctx.check_mysql()  :::::  [2016-01-28T16:51:40.539392Z]   File ""/home/build0/lsstsw/build/scisql/.waf-1.6.11-30618c54883417962c38f5d395f83584/waflib/Configure.py"", line 221, in fun  :::::  [2016-01-28T16:51:40.539410Z]     return f(*k,**kw)  :::::  [2016-01-28T16:51:40.539432Z]   File ""tools/mysql_waf.py"", line 85, in check_mysql  :::::  [2016-01-28T16:51:40.539451Z]     (ok, msg) = mysqlversion.check(version)  :::::  [2016-01-28T16:51:40.539473Z]   File ""tools/mysqlversion.py"", line 74, in check  :::::  [2016-01-28T16:51:40.539514Z]     if not comparison_op(version_nums, constraint_nums):  :::::  [2016-01-28T16:51:40.539547Z] UnboundLocalError: local variable 'constraint_nums' referenced before assignment  Failed during rebuild of DM stack.  {code}"
0,IRSA developer mentoring effort,IRSA is contributing to the Firefly package development.  we need to put in time to mentor the developers. 
0,IRSA developer mentoring effort,IRSA is contributing to Firefly development. We need to mentor the new developers.
0,Fix type inference and return types makeMaskedImage et al,"The {{makeMaskedImage}} function and cousins like {{makeExposure}} don't do the type inference they're supposed to do in C++, because they use the old {{typename Image<T>::Ptr}} approach instead of {{PTR(Image<T>)}}.    They also return raw pointers, which is dangerous.  They should be converted to return shared_ptrs.  Note that this will have to include adjusting or removing Swig code (probably {{%newobject}} statements) that deal with taking ownership of the raw pointers."
2,Switch to MemManReal in the worker scheduler,"First iteration of worker scheduler uses a skeleton of the memory manager that doesn't actually look at any of the tables, files, or memory. The scheduler needs to be switched to MemManReal. "
1,butler should transparently allow files to be compressed or not,"see the conversation on c.l.o. at [https://community.lsst.org/t/how-does-the-butler-support-compression/502].    The summary is, when the mapper returns e.g. a non compressed file name e.g. {{foo.fits}}, that file may be compressed and the filename may reflect this e.g. in reality it might be named {{foo.fits.gz}}. On a posix system some component of the butler framework should discover this and transform the filename to the correct filename and pass that to the deserializer.    TBD if the list of allowed extensions is hard coded someplace (in a mapper subclass?) or specified another way, perhaps by the policy (could be for dataset type or globally)."
2,Schemas for QA information,"This ticket is to capture preliminary design work we are doing for storage of QA system information, which we are working with the Database team on.     As well as prior experience, Jacek has made us aware of the sdqa tables in the schema:    https://lsst-web.ncsa.illinois.edu/schema/index.php?sVer=baseline    and also plan on mining pipeQA for quantities of interest.     Once we have a draft, there will be an RFD for soliciting further input. "
1,afw Wcs object copying does not copy exactly,"A probable bug in WCSLIB is causing {{wcscopy}} to create copies of {{Wcs}} objects which are not the same as the object that was copied. In some cases when this object is passed to {{wcsset}} it fails, as the {{Wcs}} object contains impossible values.    This has behaviour is non-deterministic (failure is only seen occasionally). The error has only been observed on OSX, but we do not believe it to be operating system dependent (except insofar as different systems and compilers produce different memory layouts and hence different failure modes). This reliably causes {{ci_hsc}} to fail when running on a Mac.    Relevant lines in {{afw}} are {{image/Wcs.cc:140}} and the {{Wcs}} copy constructor in {{image/Wcs.cc:468}}    Additionally a bug has been found in {{image/Wcs.cc}} on line 485 where the flag property should be set on an element, and not on the object itself, ie {{_wcsInfo[i]->flag = -1;}}."
0,Please improve the documentation for TransformTask and derivatives,"While working on DM-4629 (overhauling {{ProcessCcdTask}}) [~rowen] stumbled over {{TransformTask}}, which he wasn't previously familiar with. Existing Doxygen documentation covers what this task does, but lacks context as to why it's useful. Please provide a high-level overview of what the intention is here."
1,Improve MySQL proxy code and add unit tests,QServ's proxy needs some cleanup:    1. Standardize passing of q and qU parameters to methods  2. Comment removal may have never worked  3. Whitespace translation is likely buggy  4. A few unit tests verifying routing of queries would be nice  
2,Build MVP of ltd-keeper web app covering ltd-mason interface,This ticket is to create an MVP of the ltd-keeper web app (RESTful API) that tracks versions of LSST the Docs’ published software documentation. Specifically this ticket will implement the RESTful endpoints needed by ltd-mason. See [SQR-006|http://sqr-006.lsst.io] for design information.    [SQR-006|http://sqr-006.lsst.io] will be updated in this ticket as the design is clarified in implementation.
1,Add S3/Route53 project provisioning capabilities to ltd-keeper,"An **authenticated** user should be able to provision (and likewise, delete) an entire published software documentation project via ltd-keeper’s RESTful API. This includes creating an S3 bucket in SQuaRE’s AWS account and setting up Route 53 DNS. The user should also be able to delete a project. This ticket will add AWS affordances to ltd-keeper. DM-4950 will be responsible for hooking this functionality into the methods that service API calls."
0,delegate argument parsing to CmdLineTask instances,"Command-line argument parsing of data IDs for {{CmdLineTask}} s is currently defined at the class level, which means that we cannot make data ID definitions dependent on task configuration.  That in turn requires custom {{processCcd}} scripts for cameras that start processing at a level other than ""raw"" (SDSS, DECam with community pipeline ISR, possibly CFHT).    Instead, we should let {{CmdLineTask}} *instances* setup command-line parsing; after a {{CmdLineTask}} is constructed, it will have access to its final configuration tree, and can better choose how to parse its ID arguments.    I've assigned this to Process Middleware for now, since that's where it lives in the codebase, but it may make more sense to give this to [~rowen], [~price], or [~jbosch], just because we've already got enough familiarity with the code in question that we could do it quickly.  I'll leave that up to [~swinbank], [~krughoff], and [~mgelman2] to decide."
0,Update pyfits,The final version of {{pyfits}} has just been released. This ticket covers updating to that version. This will be helpful in determining whether the migration to {{astropy.io.fits}} will be straightforward or complicated.
1,Adapt SRD-based measurements of astrometric performance for validate_drp,"Adapt the SRD-based specifications for calculation of astrometric performance.  Follow the examples for AM1, AM2 as presented at    https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41785659    and detailed in DM-3057, DM-3064"
0,Generate JSON output from validate_drp for inclusion in a test harness,Generate JSON output from validate_drp for inclusion in a test harness.    Generate a file that summarizes the key metrics calculated by `validate_drp`.      Develop naming conventions that will make it easy to plug into the eventual harness being developed as part of DM-2050.
2,Hard copy support- saving regions,This ticket will only do the region saving.    The scope has change somewhat since region saving will talk a little longer and making the png requires some server side work. DM-6139
0,ci_hsc fails to execute tasks from with SCons on OSX 10.11/SIP,"The {{ci_hsc}} package executes a number of command line tasks directly from SCons based on {{Command}} directives in a {{SConstruct}} file. On an OSX 10.11 system with SIP enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks:  * -The {{scons}} executable starts with a {{#!/usr/bin/env python}}. Running through {{/usr/bin/env}} strips {{DYLD_LIBRARY_PATH}} from the environment.- (duplicates DM-4954)  * SCons executes command using the [{{sh}} shell on posix systems|https://bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/SCons/Platform/posix.py?at=default&fileviewer=file-view-default#posix.py-105]. By default, that means {{/bin/sh}} on a Mac, which, again, will strip {{DYLD_LIBRARY_PATH}}.    Please make it possible to run {{ci_hsc}} on such a system."
1,LSST vs. HSC stack comparison: PSF estimation,"In order to determine the cause of the output differences between single frame processing runs of the same data using the LSST vs. HSC stacks (see figures attached to DM-4730), a detailed look at some of the image characterization steps is required.  This ticket involves a detailed investigation of the initial PSF estimation including:  {panel: title=LSST vs. HSC stack runs:}  - a comparison of the initial object detection (will likely involve looking at the initial background estimate as well as the specific assignment of footprints)  - which objects are selected as PSF candidates  - the initial PSF model (as a function of position)  {panel}"
0,Obs_Subaru camera mapper has wrong deep_assembleCoadd_config,"When lsst switched to using SafeClipAssembleCoaddTask, the camera mapper for hsc was not updated accordingly. This causes ci_hsc to fail when it attempts to verify the config class type for the deep_coadd. Camera mapper should be updated accordingly"
1,January Operation Support Related Tasks,"Account cleanup process for existing infrastructure (Identify accounts, assign sponsors)    Reconcile inventory between NCSA and Aura (on going). Mock request was generated by Aura for dry run audit. Several machines have been found not included in inventory. Task to be completed in February."
0,Investigate Roger as fallover for Nebula,"Investigate Roger OpenStack as fallback for Nebula during outages. Internally, this required technical and coordination meetings. Externally, this required interfacing with Square in order to facilitate a proper evaluation.     This task is ongoing. "
0,January AAA Tasks,Attended local AAA meetings and reviewed documentation. 
1,January Tasks,"Security meeting with Paul, Bill, Eyrich to review goals and coordinate efforts.   Initial draft of the procurement plan. Waiting for hardware contract.   Updates to internal FY16 cost estimate spreadsheet and planning (not LDM-144).  Updating expected expenditures based on update quotes.   Meetings and discussions with OBFS with respect to new vendors within MHEC and procurement approval processing for FY16 components."
2,January Tasks,Technology and pricing updates to LDM-144. Explanation update in LDM-143.    Meetings with multiple vendors re: longer term technology forecasts.
0,January Tasks,Mtg w/ IN2P3 re: ITIL implementation experiences.     Mtg w/ IN2P3 re: tape recall ordering
1,Jason January Tasks,"Activities this month include: IT sys admin meetings, LSST internal project meetings, conducting, coordinating, discussing interviews. Meeting with candidates. ICI coordination meeting (Randy). Discussion of work-to-be-done with onboarded teammates. Relaying task prioritization to IT for LSST-related activities. "
0,DM Power Requirements,Further discussions about power requirements at the Chilean DC.
3,"Investigate logging, monitoring and metrics technologies and architecture","Investigate technologies and architectures to use with panopticon, our logging system. Perform preliminary research and evaluations into ELK (Elasticsearch, Logstash and Kibana), extensions to ELK and other alternatives."
0,"Meetings, Jan 2016","verfication dataset meetings, TechTalk, RFD, local middleware-related meetings, etc"
0,"LOE, Jan 2016","LSST local group meetings, postdoc meeting, other local meetings, etc"
1,Reconsider high detection threshold in CharacterizeImageTask,"[~price] makes the [reasonable recommendation|https://community.lsst.org/t/why-was-detection-includethresholdmultiplier-10-for-old-processccdtask/500/6] that we consider providing PSF estimation with the N brightest sources in the image, rather than only detecting bright sources.    [~rowen] reasonably believes that this is beyond the scope of DM-4692, hence this new issue.    There should be very little new code needed here, but it may involve quite a bit of experimentation and validation."
1,upstream patches/deps from conda-lsst,"Where ever possible, missing dep information and patches from conda-lsst should be upstreamed.  The patches have already been observed to cause builds to fail due to upstream changes."
1,Finish data distribution prototype (March),NULL
1,Prepare for auth session at JTM,"Prepare for JTM session with a working title of “How Authentication/Authorization technology can be used to implement and enforce data access rights and operational processes for LSST"".  Prepare a final title and agenda for the session. Tuesday from 3:30pm - 5:00pm."
1,Save algorithm metadata in multiband.py,"The various {{Tasks}} in {{multiband.py}} do not attach the {{self.algMetadata}} instance attribute to their output tables before writing them out, so we aren't actually saving information like which radii were used for apertures.    We should also make sure this feature is maintained in the processCcd.py rewrite."
1,work flow of light curve visulizaiton,"Generate a description document of work flow that a scientist would go through in order to do time series research, visualize the light curve."
0,review of dependency on the third party packages,"We need to periodically review the status of the third party software packages that Firefly depends on. Making a plan to do upgrade if needed.   package.json lists out the dependencies Firefly has on the third party software. The attached file was last modified 2016-02-09.    package.json_version lists the current version of the third party packages, major changes were indicated by (M). The attached file was created on 2016-02-29.     bq.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)          "
1,Design single-sign-on authentication system for webserv,"Outline a design of the authentication system (based on components provided by NCSA) that will support single sign-on. Current thinking involves two tokens: application token to certify the app is legitimate and to determine which users it can represent, and user token"
1,Extend webserv API to pass security tokens,Extend the [API|https://confluence.lsstcorp.org/display/DM/AP] to pass security tokens.
0,Update validate_drp for El Capitan,validate_drp does not work on El Capitan due to SIP (System Integrity Protection) stripping DYLD_LIBRARY_PATH from shell scripts. The simple fix is to add  {code}  export DYLD_LIBRARY_PATH=${LSST_LIBRARY_PATH}  {code}  near the top of the scripts.
1,Benchmark dipole measurement (dipole fitting),"Benchmark dipole measurement (dipole fitting), compare speed directly to psf-fit (and/or galaxy measurement) task. Runtime should be comparable (~factor of two?) - if not understand why. Evaluate new implementation vs. current impl. Accuracy?"
0,Fix rotation for isr in obs_subaru,"Approximately half of the HSC CCDs are rotated 180 deg with respect to the others.  Two others have 90 deg rotations and another two have 270 deg rotations (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]) .  The raw images for the rotated CCDs thus need to be rotated to match the rotation of their associated calibration frames prior to applying the corrections.  This is accomplished by rotating the exposure using the *rotated* context manager function in {{obs_subaru}}'s *isr.py* and the *nQuarter* specification in the policy file for each CCD.  Currently, *rotated* uses {{afw}}'s *rotateImageBy90* (which apparently rotates in a counter-clockwise direction) to rotated the exposure by 4 - nQuarter turns.  This turns out to be the wrong rotation for the odd nQuarter CCDs as shown here:   !ccd100_nQuarter3.png|width=200!  top left = raw exposure as read in  top right = flatfield exposure as read in  bottom left = _incorrectly_ rotated raw exposure prior to flatfield correction"
2,Implement new dipole fitting algorithm as SimpleAlgorithm,"Implement new dipole fitting algorithm as SimpleAlgorithm -- implement measure, fail methods, define flags"
0,Make ci_hsc resumable,"if ci_hsc fails for any reason, (or is cancelled) it must start from the beginning of processing again. This is because of the use of functools.partial to generate dynamic function. These differ enough in their byte code that scons thinks each build has a new function definition passed to the env.command function. Using lambda would suffer from the same problem. This ticket should change how the function signature is calculated such that scons can be resumed.    This work does not prevent this from being used as a ci tool, as the .scons directory can be deleted which will force the whole SConstruct file to run again."
0,Please trim config overrides in validate_drp,"validate_drp will test more of our code if it uses default config parameters wherever possible. To that effect I would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides.    For DECam there are no overrides that are different than the defaults, so the file can simply be emptied (for now).    For CFHT there are many overrides that are different, and an important question is whether the overrides in this package are better for CFHT data than the overrides in obs_cfht; if so, please move them to obs_cfht.    As a heads up: the default star selector is changing from ""secondMoment"" to ""objectSize"" in DM-4692 and I hope to allow that in validate_drp, since it works better and is better supported.    Sorry for the incorrect component, but validate_drp is not yet a supported component in JIRA (see DM-5004)"
0,remove REUSE_DATAREPO in testCoadds in pipe_tasks,"When the test fails and the output directory is written but not populated, subsequent test executions fail every time until the output directory is deleted or REUSE_DATAREPO is set to False. This is misleading for users who don't know about this hidden feature.    Furthermore, the REUSE_DATAREPO=False feature is broken; setting it False causes NameError: global name 'DATAREPO_ROOT' is not defined.    It would be better if the test cleaned up after itself (deleted all outputs) every time. If it's really important to reuse the outputs then the dir should be cleaned up in the case of failed writes and/or corruption.    "
3,F16 Data Access Model Refresh,"A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science. "
1,Resolve development issues by testing using WAN Emulator,A test plan draft was written and some short meetings were held regarding the use of the WAN Emulator. The manuals for the Apposite Netropy 40G emulator were retrieved and read. The test plan draft for three test projects is attached.
2,Continued WBS planning,"Finished out all necessary fields in the first cut at the WBS.  Split project into work phases,   Began drilling down into the milestones for each phase, with accurate estimation the goal.  Provided miscellaneous diagrams to capture expected functionality for various state transitions throughout the system."
1,Convert Confluence DM Developer Guide to Sphinx (hack day) ,"This is a hack day sprint to convert all remaining content on https://confluence.lsstcorp.org/display/LDMDG to reStructuredText content in the Sphinx project at https://github.com/lsst-sqre/dm_dev_guide and published at http://developer.lsst.io.    The top priority for this sprint is to port all content into reST and have it tracked by Git.    h2. Sprint ground rules    # Before the sprint, clone {{https://github.com/lsst-sqre/dm_dev_guide.git}} and {{pip install -r requirements.txt}} in a Python 2.7 environment so that you can locally build the docs ({{make html}}).  # Claim a page from the list below by putting your name on it. Put a checkmark on the page when you’ve merged it to the ticket branch (see below).  # See http://developer.lsst.io/en/latest/docs/rst_styleguide.html for guidance on writing our style of reStructuredText. Pay attention to the [heading hierarchy|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#sections] and [labelling for internal links|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#internal-links-to-labels].  # If you use Pandoc to do an initial content conversion, you still need to go through the content line-by-line to standardize the reStructuredText. I personally recommend copy-and-pasting-and-formatting instead of using Pandoc.  # Your Git commit messages should include the URL of the original content from Confluence.  # Merge your work onto the {{tickets/DM-5013}} ticket branch. Rebase your personal work branch before merging. JSick is responsible for merging this ticket branch to {{master}}.  # Put a note at the top of the confluence page with the new URL; root is {{http://developer.lsst.io/en/latest/}}.    h2. Planned Developer Guide Table of Contents    We’re improving the organization of DM’s Developer Guide; there isn’t a 1:1 mapping of Confluence pages to developer.lsst.io pages. Below is a proposed section organization and page structure. These sections can still be refactored based on discussion during the hack day.    h3. Getting Started — /getting-started/    * ✅ *Onboarding Checklist* (Confluence: [Getting Started in DM|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM]). I’d like this to eventually be a quick checklist of things a new developer should do. It should be both a list of accounts the dev needs to have created, and a list of important developer guide pages to read next. The NCSA-specific material should be spun out. [[~jsick]]  * *Communication Tools* (new + DM Confluence [Communication and Links|https://confluence.lsstcorp.org/display/DM/Communication+and+Links]). I see this as being an overview of what methods DM uses to communicate, and what method should be chosen for any circumstance.  * *Finding Code on GitHub* (new). This should point out all of the GitHub organizations that a developer might come across (DM and LSST-wide), and point out important repositories within each organization. Replaces the confluence page [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]    h3. Processes — /processes/    * ✅ *Team Culture and Conduct Standards* (confluence)  * ✅ *DM Development Workflow with Git, GitHub, JIRA and Jenkins* (new & Confluence: [git development guidelines for LSST|https://confluence.lsstcorp.org/display/LDMDG/git+development+guidelines+for+LSST] + [Git Commit Best Practices|https://confluence.lsstcorp.org/display/LDMDG/Git+Commit+Best+Practices] + [DM Branching Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Branching+Policy])  * ✅ *Discussion and Decision Making Process* (new & [confluence|https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process])  * ✅ *DM Wiki Use* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/DM+Wiki+Use]) [[~swinbank]]  * ✅ *Policy on Updating Doxygen* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Updating+Doxygen]); needs to be addressed with TCT. Inter-link with the developer workflow page. [[~jsick]] (we’re just re-pointing the Confluence page to the workflow document)  * ✅ *Transferring Code Between Packages* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Transferring+Code+Between+Packages]) [[~swinbank]]  * -*Policy on Changing a Baseline Requirement*- ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Changing+a+Baseline+Requirement])  * ✅ *Project Planning for Software Development* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Project+Planning+for+Software+Development]) [[~swinbank]]  * ✅ *JIRA Agile Usage* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/JIRA+Agile+Usage]) [[~swinbank]]  * -*Technical/Control Account Manager Guide*- ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=21397653]) (Do not port; see discussion below.)  * *Licensing* (new) Need a centralized page to discuss license and copyright policies; include boilerplate statements.    h3. Coding Guides — /coding/    * ✅ *Introduction* and note on stringency language (confluence: [DM Coding Style Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy])  * ✅ *DM Python Style Guide* (confluence: [Python Coding Standard|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard])  * ✅ *DM C++ Style Guide* (confluence pages: [C++ Coding Standard|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666] + [C++ General Recommendations|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908756] + [C++ Naming Conventions|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685] + [C++ Files|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908674] + [C++ Statements|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706] + [C++ Layout and Comments|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908737] + [Policy on use of C++11/14|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399] + [On Using ‘Using’|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283856])  * Coding Style Linters (new; draft from confluence [C++ Coding Standards Compliance|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283861] and [Python Coding Standards Compliance|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standards+Compliance]  * ✅ *Using C++ Templates* ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190]); this page needs to severely edited or re-written, however.  * ✅ *Profiling* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Profiling|]). Also add a section ‘Using Valgrind with Python' (new) [[~jsick]]  * ✅ *Boost Usage* ([TRAC|https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal]) [[~tjenness]]  * ✅ *Software Unit Test Policy* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Software+Unit+Test+Policy]) [[~swinbank]]  * ✅ *Unit Test Coverage Analysis* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Coverage+Analysis]) [[~swinbank]]  * ✅ *Unit Testing Private C++ Functions* ([trac|https://dev.lsstcorp.org/trac/wiki/UnitTestingPrivateFunctions]) [[~swinbank]]    h3. Writing Docs — /docs/    * *Introduction* (new): Overview of DM’s documentation needs; links resources on technical writing.  * *English Style Guide* (new): Supplement the [LSST Style Manual|https://www.lsstcorp.org/docushare/dsweb/Get/Document-13016/LSSTStyleManual.pdf] and provide English style guidance specific to DM. Capitalization of different heading levels; use of Chicago Manual of Style; a ‘this, not that’ table of spelling and word choices.  * ✅ *ReStructuredText Style Guide* (new)  * ✅ *Documenting Stack Packages* (new)  * ✅ *Documenting Python Code* (new)  * ✅ *Documenting C++ Code* (confluence, adapted from [Documentation Standards|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards]); needs improvement  * ✅ *Writing Technotes* (new; port README from [lsst-technote-bootstrap|https://github.com/lsst-sqre/lsst-technote-bootstrap/blob/master/README.rst])    h3. Developer Tools — /tools/    * ✅ *Git Setup and Best Practices* (new)  * ✅ *Using Git Large File Storage (LFS) for Data Repositories* (new)  * ✅ *JIRA Work Management Recipes* (new)  * ✅ *Emacs Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development]). See DM-5045 for issue with Emacs config repo - [~jsick]  * ✅ *Vim Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Config+for+VIM]) - [~jsick]    h3. Developer Services — /services/    * ✅ *NCSA Nebula OpenStack Guide* (Confluence: [User Guide|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide] + [Starting an Instance|https://confluence.lsstcorp.org/display/LDMDG/Introduction+to+Starting+a+Nebula+Instance] + [Using Snapshots|https://confluence.lsstcorp.org/display/LDMDG/Start+an+Instance+using+a+base+snapshot+with+the+LSST+Stack]. Add the [Vagrant instructions too from SQR-002|http://sqr-002.lsst.io]? [[~jsick]]  * ✅ *Using lsst-dev* (Confluence: [notes Getting Started|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM] + [Developer Tools at NCSA|https://confluence.lsstcorp.org/display/LDMDG/Developer+Tools+at+NCSA]  * ✅ *Using the Bulk Transfer Server at NCSA* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Using+the+Bulk+Transfer+Server+at+NCSA]) [[~jsick]]    h3. Build, Test, Release — /build-ci/    * *Eups for LSST Developers* (new) [[~swinbank]]  * ✅ *The LSST Software Build Tool* → ‘Using lsstsw and lsst-build' ([confluence|https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool]); lsstsw and lsst-build documentation. [[~swinbank]]  * *Using DM’s Jenkins for Continuous Integration* (new) [~frossie]   * ✅ *Adding a New Package to the Build*([confluence|https://confluence.lsstcorp.org/display/LDMDG/Adding+a+new+package+to+the+build]) [[~swinbank]]  * ✅ *Distributing Third-Party Packages with Eups* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Distributing+third-party+packages+with+EUPS]) [[~swinbank]]  * ✅  *Triggering a Buildbot Build* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Triggering+a+Buildbot+Build]) [~frossie]  * ✅ *Buildbot Errors FAQ* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+FAQ+on+Errors]) [~frossie]  * * Buildbot configuration ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+Configuration+and+Setup] [~frossie]    * *Creating a new DM Stack Release* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Creating+a+new+DM+Stack+Release]); though this page or a modern equivalent should probably belong with the software docs? [~frossie]    _A lot of work should go into this section._ Have something about Scons? Or maybe that belongs in the doc of each relevant software product.    h2. Leftover Confluence pages    h3. The following pages should be moved to a separate Confluence space run by NCSA:    * [NCSA Nebula OpenStack Issues|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+Issues]  * [DM System Announcements|https://confluence.lsstcorp.org/display/LDMDG/DM+System+Announcements]  * [NCSA Development Servers|https://confluence.lsstcorp.org/display/LDMDG/DM+Development+Servers]    h3. The following pages are either not relevant, generally misplaced, or need to be updated/recalibrated:    * [Git Crash Course|https://confluence.lsstcorp.org/display/LDMDG/Git+Crash+Course]  * [Basic Git Operations|https://confluence.lsstcorp.org/display/LDMDG/Basic+Git+Operations]  * [Handling Git Push Problems|https://confluence.lsstcorp.org/display/LDMDG/Handling+Git+Push+Problems]  * [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]; see the proposed “Finding Code on GitHub” page for a replacement.  * [Standards and Policies|https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies]: this is a good TOC for the Confluence docs; but not longer needed for the new docs.  * [Documentation Guidelines|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Guidelines]. Some of this could be re-purposed into an intro to the ‘Writing Documentation’ section; some of this should go in a ‘Processes' page.  * [DM Acknowledgements of Use|https://confluence.lsstcorp.org/display/LDMDG/DM+Acknowledgements+of+Use]: this probably belongs in documentation for the software projects that actually used this work."
0,Set doRenorm default to False in AssembleCcdTask,Change the default value of {{AssembleCcdConfig.doRenorm}} to {{False}} for the reasons given in RFC-157 and to implement that RFC.
1,Optionally report do-nothing config overrides,"As discussion on DM-4692 and in various HipChat rooms, it's too easy for camera-level config override files to contain many options that don't actually change anything, because they simply override the defaults with the same default values.  To aid in tracking these down and removing them, we should have an option in which {{CmdLineTask}} s (delegating to {{pex_config}}) refuse or warn about overrides that have no effect.    We should probably not make failing on do-nothing overrides the default behavior, but we could consider making warning the default behavior.  Mostly, I think it's important just to be able to find such options when wanted."
0,Modernize version check scripts in matplotlib and numpy packages,The version check scripts in the stub {{matplotlib}} and {{numpy}} eups packages use old Python conventions. They should be updated to work with 2.7+.
1,FITS Visualizer porting: Expanded mode single - part 2,I split DM-4497 into two part so I can demonstrate code reviews. This part has paging controller & layout cleaned up.  This tickets is messy because it involves a lot of refactoring of the reducers.  Therefore I am going to end it and move the rest of the UI work to DM-5088.
0,Modernize python code in Qserv scons package,"The {{site_scons}} Python code is not using current project standards. For example, print is not a function, exceptions are not caught {{as e}}, {{map}} is called without storing the result and {{map/filter/lambda}} are used where list comprehensions would be clearer.    Most of these fixes are trivial with {{futurize}}."
1,"Adds, Moves, Change support for DNS, network, IP addressing, etc",NULL
1,"define rack, pdu specifications and obtain pricing quotes",NULL
2,Base site and summit RFP,Working with Ron to create a RFP for the acquisition of network equipment for the summit and base. 
0,Fix dependencies for eups-packaged sqlalchemy,"Eups-packaged sqlalchemy lists {{mysqlclient}} as required dependency which is not really right. sqlalchemy does not directly depend on mysql client stuff, instead it determines at run time which python modules it needs to load depending on what exact driver client code is requesting (and {{mysqlclient}} does not actually provides python module so this dependency does not even make anything useful). So dependency on specific external package should be declared on client side and not in sqlalchemy, {{mysqlclient}} should be removed from sqlalchemy.table."
0,eval NCSA vSphere/OSX support -- first attempt,NULL
1,Design interconnect for GPFS cluster prototype,NULL
1,Create physical and logical network diagrams for first phase of purchases,NULL
0,Tests fail on Qserv on OS X El Capitan because of SIP,OS X El Capitan introduced System Integrity Protection which leads to dangerous environment variables being stripped when executing trusted binaries. Since {{scons}} is launched using {{/usr/bin/env}} the tests that run do not get to see {{DYLD_LIBRARY_PATH}}. This causes them to fail.    The same fix that was applied to {{sconsUtils}} (copying the path information from {{LSST_LIBRARY_PATH}}) needs to be applied to the test execution code used by Qserv's private {{site_scons}} utility code.
3,X16 Data Access and Database Documentation,"Update the documentation for Data Access and Database - bring it up to date with the design. This includes LDM-135 (Database Design), and creating a new LDM document or DAX Design)."
1,"DAX & DB Docs (Fritz, March)","* Document Data Distribution  * Create structure for DAX doc  * Bring over Provenance documentation from prov_prototype  * Update LDM-135 to reflect the updates to the storage/IO model  * Update LDM-152  * Fix LDM-135: 3.3.6.4 and 3.3.6.5 should be 3rd level, so 3.3.7 and 3.3.8  "
2,DAX & DB Docs (John),* Refresh shared scans design documentation (in LDM-135)  * Add info about query cancellation (in LDM-135)
1,DAX & DB Docs (AndyS),* Document db and table metadata  * Document async queries  * Document data loader
1,DAX & DB Docs (Nate),* Improve butler documentation
2,DAX & DB Docs (Brian),* Document webserv/imgserv/metaserv/dbserv
1,DAX & DB Docs (Mike),Document secondary index
1,DAX & DB Docs (Serge),"* Document spatial indexing  * Document database ingest  * Refresh ""Stored Procedures and Function"" in LDM-135"
3,Load panstarrs data to qserv,NULL
1,Setup webserv with panstarrs data,NULL
1,"update ""newinstall.sh"" nebula images & docker containers",[~hchiang2] is looking for nebula images newer than {{w_2015_45}} (from the exploratory work in DM-4326) and [~gdaues] is interested in images with a complete {{lsst_distrib}} install for orchestration testing.  New builds should incorporate the pending change to {{newinstall.sh}} that converts from {{anaconda}} to {{miniconda}}.
0,SingleFrameVariancePlugin takes variance of entire image,"{{SingleFrameVariancePlugin}} takes the median variance of the entire image, rather than within an aperture around the source of interest.  A {{Footprint}} is constructed with the aperture, but it is unused.    This means that this plugin takes an excessive amount of run time (255/400 sec in a recent run of processCcd on HSC {{visit=1248 ccd=49}} with DM-4692)."
0,Design replacement for A.net index files,We need a simple way to hold index files that will be easy to use and simple to set up.
3,Some small things slipped through in winter 2016,Fix up things that slipped through or were delayed in winter 2016.  The individual things are small parts of larger epics and typically are the result of emergent work or increased scope.
3,Implement Approx/Interp improvements,"We are making due with the current approximation and interpolation scheme, but the two should be merged.  This must really be done after the HSC merge because of the difficulty of doing large refactoring before then."
1,Assess priority of Aprox/Interp upgrades.,This is to assess the priority of a major approximation and interpolation refactor.
2,RFC corrections for ISR.,Create a list of ISR requirements and have it RFCd.
1,Assess the corrections that need to be imlemented,The stack can do many of the corrections needed.  Assess the status of the current algorithms and identify any deficiencies.
2,Improve and implement crosstalk in ISR,#NAME?
2,Design the refactoring for ProcessCcd,There is a significant design issue when refactoring a piece of this importance.  Carry out a design study to implement in DM-4692.
0,Week end 1/09/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 9, 2016."
0,Week end 1/16/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 16, 2016."
0,Week end 1/23/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 23, 2016."
0,Week end 1/30/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 23, 2016."
0,New equipment setup and configuration (week end 1/23/16),* Finished setting up Mac vSphere infrastructure with Paul
1,New equipment setup and configuration (week end 1/30/16),* Set up new lsst-dev7 as CentOS 7 server  * Continuing to set up IPMI on new test servers (working with Dell on issue with iDRAC license upgrade)
0,Decommissioning old equipment (week end 1/16/16),* Recovery of old LSST used equipment  ** Moved remaining surplussed last servers to wiping bench  ** Started wiping drives  ** Re-purposed 10 Dell 1950  
0,Decommissioning old equipment (week end 1/23/16),* Complete the cleanup of last used NCSA systems
1,Lenovo test server,* Mount Lenovo test server in LSST1 rack. Install fiber card and networking. Test PXE boot to 10G nic.  * Work on getting Lenovo to PXE boot to 10G card  * Booted satisfactorily to 1GB interface – loaded Centos 7  ** Abruptly ends after Menu with 10GB card  
0,PcaPsf can hit an assertion failure,"This is bad for multiple reasons:  1. When multiprocessing, the assertion failure kills a single process, which prevents the final join of the multiple processes, so the job hangs forever.  2. The failure is not logged.  3. Hard assertions like this should only occur when we break the system integrity, which this does not (i.e., it's too big a hammer for the problem).    {code}  pprice@tiger-sumire:/tigress/pprice/dm-4692 $ eups list -s  afw                   tickets.DM-4692-gd8ad35cd96+1     b1901 setup  afwdata               2016_01.0         b1901 b1902 setup  astrometry_net        0.50.lsst2+5      b1901 b1902 setup  astrometry_net_data   sdss-dr9-fink-v5b         setup  base                  2016_01.0         b1901 b1902 setup  boost                 1.59.lsst5        b1901 b1902 setup  cfitsio               3360.lsst4        b1901 b1902 setup  coadd_chisquared      2016_01.0+6       b1901 setup  coadd_utils           2016_01.0+6       b1901 setup  daf_base              2016_01.0         b1901 b1902 setup  daf_butlerUtils       tickets.DM-4692-g048b33c50e+3     b1901 setup  daf_persistence       2016_01.0-1-gf47bb69+1    b1901 b1902 setup  display_ds9           2015_10.0+43      b1901 setup  doxygen               1.8.5.lsst1       b1901 b1902 setup  eigen                 3.2.5             b1901 b1902 setup  fftw                  3.3.4.lsst2       b1901 b1902 setup  geom                  10.0+50           b1901 b1902 setup  gsl                   1.16.lsst3        b1901 b1902 setup  ip_diffim             tickets.DM-4692-g543ea8fde5+3     b1901 setup  ip_isr                2016_01.0+6       b1901 setup  lsst_build            LOCAL:/tigress/pprice/lsstsw/lsst_build   setup  mariadbclient         master-gf2dee38289        b1901 b1902 setup  matplotlib            0.0.1+5           b1901 b1902 setup  meas_algorithms       tickets.DM-4692-g3d073a93d7+1     b1901 setup  meas_astrom           tickets.DM-4692-gbbf15418e6+1     b1901 setup  meas_base             LOCAL:/tigress/pprice/dm-4692/meas_base   setup  meas_deblender        2016_01.0+6       b1901 setup  minuit2               5.28.00.lsst2     b1901 b1902 setup  ndarray               10.1+58           b1901 b1902 setup  numpy                 0.0.1+5           b1901 b1902 setup  obs_subaru            LOCAL:/tigress/pprice/dm-4692/obs_subaru  setup  obs_test              tickets.DM-4692-g1533aee20f+1     b1901 setup  pex_config            2016_01.0         b1901 b1902 setup  pex_exceptions        2016_01.0         b1901 b1902 setup  pex_logging           2016_01.0         b1901 b1902 setup  pex_policy            2016_01.0         b1901 b1902 setup  pipe_base             2016_01.0+6       b1901 setup  pipe_tasks            LOCAL:/tigress/pprice/dm-4692/pipe_tasks  setup  psfex                 2016_01.0         b1901 b1902 setup  pyfits                3.4.0             b1901 b1902 setup  python                0.0.3             b1901 b1902 setup  python_d2to1          0.2.12            b1901 b1902 setup  pyyaml                3.11.lsst1        b1901 b1902 setup  scons                 2.3.5             b1901 b1902 setup  sconsUtils            2016_01.0         b1901 b1902 setup  skymap                2016_01.0+6       b1901 setup  skypix                10.0+347          b1901 setup  stsci_distutils       0.3.7-1-gb22a065  b1901 b1902 setup  swig                  3.0.2.lsst1       b1901 b1902 setup  utils                 2016_01.0         b1901 b1902 setup  wcslib                5.13.lsst1        b1901 b1902 setup  xpa                   2.1.15.lsst3      b1901 b1902 setup    pprice@tiger-sumire:/tigress/pprice/dm-4692 $ processCcd.py /tigress/HSC/HSC --rerun price/dm-4692 --rerun price/dm-4692 --id visit=1248 ccd=100 --clobber-config  /tigress/pprice/lsstsw/miniconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM (    File ""src/Utils.cc"", line 42, in std::string lsst::utils::getPackageDir(const std::string&)      Package meas_extensions_shapeHSM not found {0}  lsst::pex::exceptions::NotFoundError: 'Package meas_extensions_shapeHSM not found'  ): disabling HSM shape measurements  : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/HSC/HSC/rerun/price/dm-4692  CameraMapper: Loading registry registry from /tigress/HSC/HSC/rerun/price/dm-4692/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.isr: Applying linearity corrections to Ccd 100  processCcd.isr.crosstalk: Applying crosstalk correction  processCcd.isr: Set 0 BAD pixels to 3147.74  processCcd.isr: Flattened sky level: 3847.800781 +/- 2114.507723  processCcd.isr: Measuring sky levels in 8x16 grids: 3884.324645  processCcd.isr: Sky flatness in 8x16 grids - pp: 15293.248379 rms: 1173.423587  processCcd.isr: Setting rough magnitude zero point: 34.678409  processCcd.charImage: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.charImage.repair: Identified 6044 cosmic rays.  processCcd.charImage.detectAndMeasure.detection: Detected 127 positive sources to 5 sigma.  processCcd.charImage.detectAndMeasure.detection: Resubtracting the background after object detection  processCcd.charImage.detectAndMeasure.measurement: Measuring 127 sources (127 parents, 0 children)   processCcd.charImage.measurePsf: Measuring PSF  /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:354: RuntimeWarning: invalid value encountered in less    bad = numpy.logical_or(bad, width < self._widthMin)  /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:355: RuntimeWarning: invalid value encountered in greater    bad = numpy.logical_or(bad, width > self._widthMax)  processCcd.charImage.measurePsf: PSF star selector found 6 candidates  meas.algorithms.psfDeterminer WARNING: You only have 3 eigen images (you asked for 4): reducing number of eigen components  meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 3): reducing number of eigen components  meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 2): reducing number of eigen components  python: /tigress/pprice/lsstsw/stack/Linux64/eigen/3.2.5/include/Eigen/src/Core/Redux.h:202: static Eigen::internal::redux_impl<Func, Derived, 3, 0>::Scalar Eigen::internal::redux_impl<Func, Derived, 3, 0>::run(const Derived&, const Func&) [with Func = Eigen::internal::scalar_max_op<double>; Derived = Eigen::CwiseUnaryOp<Eigen::internal::scalar_abs_op<double>, const Eigen::Matrix<double, -1, -1> >; Eigen::internal::redux_impl<Func, Derived, 3, 0>::Scalar = double]: Assertion `size && ""you are using an empty matrix""' failed.  Aborted  {code}    Note:  * This occurred while testing DM-4692.  The LOCAL pipe_tasks and obs_subaru are on that ticket branch.  The LOCAL meas_base is for the fix from DM-5050.  * One root cause of the bad PSF modeling may be bad rotations in the application of the calibs ([~lauren] is looking into that; don't know if there's a ticket number), but this should never happen regardless."
2,Operations planning ,"Draft Operations planning w.b.s.  Create additional activity Diagrams,  Draft DPPD Operations processing talk."
2,January Management ,develop design and project management methods in conjunction with L1 design.  Deal with ARI labor component  General management of staff
0,PropagateVisitFlags doesn't work with other pipeline components,"{{PropagateVisitFlags}}, which was recently ported over from HSC on DM-4878, doesn't work due to some inconsistencies with earlier packages/tasks:   - The default fields to transfer have new names: ""calib_psfCandidate"" and ""calib_psfUsed""   - We're not currently transferring these fields from icSrc to src, so those fields aren't present in src anyway.  I propose we just match against icSrc for now, since it has all of the fields we're concerned with.   - It makes a call to {{afw.table.ExposureCatalog.subsetContaining(Point, Wcs, bool)}}, which apparently exists in C++ but not in Python; I'll look into seeing which HSC commits may have been missed in that port."
0,"Please add a package that includes obs_decam, obs_cfht and all validation_data datasets","It would be very helpful to have an lsstsw package that added all supported obs_* packages (certainly including obs_cfht and obs_decam, and I hope obs_subaru) and all validation_data_* packages. This could be something other than lsst_apps, but I'm not sure what to call it."
0,Enable aperture correction on coadd processing,"Aperture corrections are now coadded, so we can enable aperture corrections in measurements done on coadds."
1,"Add auto play,select which dialog, close button working,  to expanded mode",Add the auto play to expanded mode.  Add the choose which dialog to expanded mode. Make close button work.    I am breaking this up the expanded mode ticketa because the task is getting so big and ticket DM-5019 involved reducer refactoring.  Also the refactoring needs to get into the dev branch.
1,Add task discovery on command line activator,I'll add a way to specify on the command line the path or the package to discover for CmdLineTask or SuperTasks
1,Investigate alternative for networkx before RFC,I'll make sure I explored other alternatives before creating a RFC for adding networkx which by itself require other packages. This is needed for the pipe_flow_x work. I tried one stand-alone package before pygraphviz but then decided to migrate to networkx as it is more complete and allow other possible future features
0,LDM documentation of butler basics & multiple repositories,NULL
1,Security plan renewal,Renewal of the LSST security plan.  Starts with DM.
1,HSC backport: Set BAD mask for dead amps instead of SAT,This is a port of [HSC-1095|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1095] and a leftover commit from [HSC-1231|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1231]: [isr: don't perform overscan subtraction on bad amps|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4] (noted in DM-3942).
0,Redirect confluence based pages to new developer guide.,Delete and apply redirects to all migrated pages in old Confluence-based Developer Guide
0,Make validateDrp a Task.,"Make validateDrp a Task so   1. it can easily be run from the command line or programmatically.  2. it can import the standard command line arguments  3. it can be logged in the same way.    This eventually should fit into DM-2050, and DM-3859."
0,Update validate_drp to use TransformTask to store calibrated measurements,"Currently validate_drp uses some manual crude addition of calibration information and constructs new schemas to store this information.  This is essentially what TransformTask is meant for.  Using this would simplify the code, make it less fragile, and ideally eventually integrate more transparently with future calibration improvements or redefinitions of how zeropoints are tracked..    1. Learn how to use TransformTask.  Note DM-4948 is the doc task for this.  2. Adapt the code.  3. Verify unchanged results on existing validation_data_decam and validation_data_cfht."
1,Add tests to validate_drp to verify SRD calculations and utility function behavior,The current validate_drp is woefully lacking in tests.    1. The key SRD metrics definitely need to have test cases that verify the calculation of these important metrics.  2. Overall the utility functions would benefit from testing.
1,Polish IN2P3 cluster upgrade to CentOS7,"What remains:    - problem with Docker 1.9.1+overlay+xfs => switch to Docker 1.10.1? Then switch back from devicemapper to overlay?  - problem with qserv uid: go back to 1000, instead of 1008?"
0,Docs for ltd-keeper,Create a documentation project within ltd-keeper that documents the RESTful API while it is being developed. This will allow the [SQR-006|http://sqr-006.lsst.io] technote to have a place to link to for detailed information.
0,"Fix --id examples in processCcd.py and friends to correctly show ""ccd=1^2"".","The required '^' convention for lists of things, e.g. {{ccd}}, {{filter}}, {{visit}} and such is surprising.  But, worse, the documentation is currently wrong in its examples and presents several {{ccd=1,2}}, {{patch=1,2}} examples.    * Fix the {{--id}} examples in {{pipe_tasks}} and other uses of processCcd.py in obs_* packages to correctly match the required syntax.    Here's the current list in {{pipe_tasks}}, but check other packages as well.    {code}  [serenity tasks] grep '[0-9],[0-9]' *.py | grep '""'  assembleCoadd.py:                               help=""data ID, e.g. --id tract=12345 patch=1,2"",  coaddBase.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"",  imageDifference.py:        parser.add_id_argument(""--id"", ""calexp"", help=""data ID, e.g. --id visit=12345 ccd=1,2"")  makeDiscreteSkyMap.py:            boxI = afwGeom.Box2I(afwGeom.Point2I(0,0), afwGeom.Extent2I(md.get(""NAXIS1""), md.get(""NAXIS2"")))  multiBand.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"",  multiBand.py:                               help=""data ID, e.g. --id tract=12345 patch=1,2 filter=g^r^i"")  multiBand.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"",  processCoadd.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"",  {code}"
1,Rewrite integration test queries with spatial constraint returning empty results,"Some queries in the integration test suite return empty results, here's how to catch them:  {code:bash}  # this should be done for alll tests cases  egrep ""^0$"" ~/qserv-run/2016_02/tmp/qservTest_case02/outputs/mysql/*  # empty results files have also to be tracked  {code}    There parameters should be fixed to query a region containing data (use select * on object)."
1,Add scans for DRx-1 to the model,"Per RFC-134 we need to support scans for DRx-1. This story involves building this into the model, costing it, and changing the baseline."
1,Add scans for DRP-produced Dia* tables to the model,"Per RFC-133, we need to support scans on DiaObject table, possibly Dia*Source tables as well. This story involves adding it to the model, costing it and adding it to the baseline."
1,new conda 'mkl' dependent packages break meas_base tests,"Continuum release/rebuilt a number of packages last friday to depend on the the Intel MKL library.     https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimizations    There are [new feature named] versions that continue to use openblas but the MKL versions appear to be installed by default.  This causes at least multiple {{meas_base}} tests to fail.After extensive testing, I have confirmed that the meas_base tests do not fail with the equivalent 'nomkl' package.  In addition, mkl is closed source software that requires you to accept and download a license file or it is time-bombed to stop working after a trial period.      {code:java}      docker-centos-7: [ 36/36 ]  meas_base 2015_10.0-9-g6daf04b+7 ...      docker-centos-7:      docker-centos-7: ***** error: from /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/build.log:      docker-centos-7: tests/sincPhotSums.py      docker-centos-7:      docker-centos-7: tests/measureSources.py      docker-centos-7:      docker-centos-7: tests/testApertureFlux.py      docker-centos-7:      docker-centos-7: tests/testJacobian.py      docker-centos-7:      docker-centos-7: tests/testScaledApertureFlux.py      docker-centos-7:      docker-centos-7: The following tests failed:      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/sincPhotSums.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/measureSources.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testApertureFlux.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testJacobian.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testScaledApertureFlux.py.failed      docker-centos-7: 5 tests failed  {code}  (the exact cause of the test failures was not investigated as this should not have happened)    This change has also broken the ability to import an existing conda env from 2016-02-05 or earlier that uses scipy due to some sort of package version resolution problem.  Explicit declaring it as the scipy package without mkl fixes the resolution problem.    There is a new 'nomkl' package, when installed, any subsequent package installations will default to versions without mkl.  However, this does not fix any already installed packages.    I am traumatized by the lack of reproducible  build envs even within a few days of each other.  After discussion in the Tucson office, I'm going to pin the lsstsw and newinstall.sh conda package versions with a commitment from square to update them on a monthly basis.  I already have a test version of lsstsw/bin/deploy that defaults to a bundled package but with a option flag to use bleeding edge.  "
0,"newinstall.sh fails with ""eups: command not found""","[~jgates] has reported the following output when running {{newinstall.sh}} on el6.    {code:java}  Installing EUPS (v2.0.1)... done.  setup: No module named utils  Installing Miniconda2 Python Distribution ...   newinstall.sh: line 277: eups: command not found  {code}    Clearly, a command failure which should have been fatal was ignored.  "
0,Fix effective coordinates for defects in obs_subaru,"The defects as defined in {{obs_subaru}} (in the {{hsc/defects/20NN-NN-NN/defects.dat}} files) are defined in a coordinate system where pixel (0, 0) is the lower left pixel.  However, the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, the defect positions are being misinterpreted for the rotated CCDs in HSC (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]).  This needs to be remedied."
0,Offset in gaussian-psf in ci_hsc,"I'm seeing what looks like an aperture correction problem in psf-gaussian on {{ci_hsc}} coadds.  This gets in the way of our ability to do star/galaxy classification, and suggests potentially more serious problems elsewhere.  "
3,S17 Data Access and Database Documentation,Update the documentation for Data Access and Database
3,S17 Data Access and Database Documentation,Update the documentation for Data Access and Database
3,FY17 Data Access Model Refresh,"A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science."
3,S18 Data Access and Database Documentation,Update the documentation for Data Access and Database.
3,S18 Data Access and Database Documentation,Update the documentation for Data Access and Database
3,FY20 Data Access and Database Documentation,Update the documentation for Data Access and Database.
3,FY18 Data Access Model Refresh,"A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science."
3,FY19 Data Access Model Refresh,"A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science."
3,FY18 Data Access Model Refresh,"A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science."
3,FY19 Data Access and Database Documentation,Update the documentation for Data Access and Database.
1,"Add intelligence to `validate_drp` so it does ""A Reasonable Thing"" on an unknown output repo","validate_drp current takes as input both a repository and a configuration file.  The configuration file contains information to construct the list of dataIds to analyze.    However, these dataIds could be extracted from the repo itself, in cases where the desired is to analyze the entire repo.      1.  Add a function that loads the set of dataIds from the repo. (/)  2.  Select reasonable defaults for the additional parameters specified in the config file. (/)  3.  Design how to handle multiple filters. (/)"
0,Add multiple-filter capabilities to `validate_drp`,"Design and refactor `validate_drp` to produce results for multiple filters.    1. Decide on the syntax for the YAML configuration file that denotes the multiple filters.  E.g., which visit goes with what filter? (/)  2. Organize the running of multiple filters in `validate.run` to sequentially generate statistics and plots for each filter. (/)  3. Add a filter designation to the default output prefix. (/)    Note: matching objects *across* filters is out-of-scope for this ticket."
0,LOAD DATA LOCAL does not work with mariadb,"After we un-messed mariadb-mysqlclient we see errors now when trying to run integration tests:  {noformat}    File ""/usr/local/home/salnikov/dm-yyy/lib/python/lsst/qserv/wmgr/client.py"", line 683, in _request      raise ServerError(exc.response.status_code, exc.response.text)  ServerError: Server returned error: 500 (body: ""{""exception"": ""OperationalError"", ""message"": ""(_mysql_exceptions.OperationalError) (1148, 'The used command is not allowed with this MariaDB version') [SQL: 'LOAD DATA LOCAL INFILE %(file)s INTO TABLE qservTest_case01_mysql.LeapSeconds FIELDS TERMINATED BY %(delimiter)s ENCLOSED BY %(enclose)s                          ESCAPED BY %(escape)s LINES TERMINATED BY %(terminate)s'] [parameters: {'terminate': u'\\n', 'delimiter': u'\\t', 'enclose': u'', 'file': '/home/salnikov/qserv-run/2016_02/tmp/tmpWeAj6u/tabledata.dat', 'escape': u'\\\\'}]""}"")  2016-02-10 14:17:40,836 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/usr/local/home/salnikov/testdata-repo/datasets/case01/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/salnikov/qserv-run/2016_02/etc/wmgr.secret --delete-tables --chunks-dir=/home/salnikov/qserv-run/2016_02/tmp/qserv_data_loader/LeapSeconds --no-css --skip-partition --one-table qservTest_case01_mysql LeapSeconds /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.schema /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.tsv.gz  {noformat}    It looks like mariadb client by default disables LOCAL option for data loading and it needs to be explicitly enabled.  "
1,Adapt all HSC calibration data to LSST camera geometry,"In the [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png], approximately half of the HSC CCDs are rotated 180 deg with respect to the others, two others have 90 deg rotations and another two have 270 deg rotations.  The HSC camera geometry defined a coordinate system where pixel (0, 0) is always the lower-left corner.  However, the new camera geometry in the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, accommodations have had to be made for the rotated CCDs on {{obs_subaru}}.  See DM-4998 and DM-5107 in particular for details.  The need for these accommodations, and the accommodations themselves, should be removed.  This entails a re-ingestion of the HSC calibration data files (BIAS, DARK, FLAT, etc.) as well as a redefinition of the defects files in {{obs_subaru}}."
0,qserv fails when it mixes mariadb and mariadbclient directories,"When I tried to run qserv-configure after installing qserv 2016_01-7-gbd0349f I got this error:  {noformat}  2016-02-10 16:03:16,915 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh  {noformat}    Running script configure/mysql.sh:  {noformat}  $ sh -x /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh    + echo '-- Installing mysql database files.'  -- Installing mysql database files.  + /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov  + echo 'ERROR : mysql_install_db failed, exiting'  ERROR : mysql_install_db failed, exiting  + exit 1  {noformat}    and     {noformat}  $ /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov    FATAL ERROR: Could not find mysqld    The following directories were searched:        /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/libexec      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/sbin      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/bin  {noformat}    So it looks for mysqld in mariadbclient, the same directory as mysql_install_db script, mysql_install_db should be actually running from mariadb.  "
3,FY18 Centralize access to database servers,"We will have multiple services: L1 live database, multiple DR databases, calibration databases, EFD etc. It'd be nice if users would not have to know which server / which port / which dialect (plain mysql or qserv etc) to use. Instead, it'd be good to have a single entry point that redirects to the right place."
0,Cost adding the support for Object / DiaObject joins in Qserv,"Per RFC-133, we should support Object / DiaObject joins. That requires changes to query analyzer (and possibly elsewhere), currently we only support self-joins on objectId for director table. We'd need to either make DiaObject a director table and allow director-director joins, or allow director-child joins. This story involves costing how much effort it will be to implement it (and making a straw-man proposal how to implement it)"
0,Create InputField for generic use cases.,"Create a composable, validating InputField so it can use outside of the form/submit use-case."
0,"B-F correction breaks non-HSC custom ISR, ci_hsc","The addition of brighter-fatter correction on DM-4837 breaks obs_cfht's custom ISR, since it slightly changes an internal ISR API by addding an argument that isn't expected by the obs_cfht version.  It also breaks ci_hsc, since the B-F kernel file isn't included in the calibrations packaged there.  "
0,make the fits statistics call work with JSON,NULL
0,obs_subaru install with eups distrib fails,"Thus:  {code}  $ eups distrib install -t w_2016_06 obs_subaru  ...    [ 52/52 ]  obs_subaru 5.0.0.1-60-ge4efae7+2 ...    ***** error: from /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/build.log:  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/hscRepository.py"", line 91, in setUp      self.repoPath = createDataRepository(""lsst.obs.hsc.HscMapper"", rawPath)    File ""tests/hscRepository.py"", line 63, in createDataRepository      check_call([ingest_cmd, repoPath] + glob(os.path.join(inputPath, ""*.fits.gz"")))    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 540, in check_call      raise CalledProcessError(retcode, cmd)  CalledProcessError: Command '['/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/bin/hscIngestImages.py', '/var/folders/jp/lqz3n0m17nqft7bwtw3b8n380000gp/T/tmptUSKuf', '/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/testdata_subaru/master-gf9ba9abdbe/hsc/raw/HSCA90402512.fits.gz']' returned non-zero exit status 1    ----------------------------------------------------------------------  Ran 8 tests in 9.928s    FAILED (errors=7)  The following tests failed:  /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/tests/.tests/hscRepository.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  {code}  Please fix it."
2,Make meas_simastrom a stack package,"Currently the simastrom code is sitting outside LSST which makes it not very visible and does not get built regularly making it sensitive to bitrot.  Before we can really continue to gather requirements and develop the system, we need to bring it inside the fence.    This should make the package located [here|https://github.com/lsst-france/meas_simastrom] buildable and usable in the LSST system with a minimum of external dependencies.  By usable, I mean that it should be callable as a task.  This does not need to solve the problem of persistence."
0,Make ci_hsc buildable by Jenkins,"1. Make sure {{ci_hsc}} is buildable by {{lsstsw}} / {{lsst_build}}  (/)  2. Add {{ci_hsc}} to lsstsw/etc/repos.yaml so that one can request that Jenkins builds it.  (/)  3. Verify that the test in {{ci_hsc}} fails on known broken tags and passes on known successful tags. (/)    No dependencies will be added to {{lsst_sims}} or {{lsst_distib}}.  This is meant to provide the ability to request that Jenkins do these builds and to fail if something has broken them.    This will later be expanded to new packages {{ci_cfht}}, {{ci_decam}}, and {{ci_sim}}.    The key goal is to make sure one hasn't broken obs_ packages in their butler interface or in their processCcd    Additional Notes and Thoughts from HipChat Discussion  [~ktl]  Sounds good to me; we might have an ""lsst_ci"" top-level metapackage depending on all of them which is what Jenkins would run regularly.     If the goal is to test obs_ packages, then my first instinct would be to put that in the obs_ package.  Longer term goal to test the stack with different precursor datasets.  If this is testing obs_ packages on a slower cadence than the built-in tests, it's OK for that to be a separate package.    [~jbosch]  Eventually, I think we need to run a CI dataset for each camera, then run some camera generic tests on each of those, then run some camera-specific tests on each of those.  So we don't want to go too far down a road in which all tests are camera-specific, but maybe we don't have a choice until we have some better unifying framework for them.  I've certainly been putting some checks in {{ci_hsc}} that would be valid for all other cameras, if we had a CI package for them that went through to coadd processing."
0,Increase key_buffer_size,"I just looked at my qserv-run/etc/my.cnf and I don't see us setting key_buffer_size there. Looking at mysqld run as part of qserv I can see it is set to 128 MB. That is pretty low given we are planning to do lots of joins. Please add an entry in my.cnf that sets it to something higher with a comment that ""~20% of available RAM is recommended""."
1,on-going support to Camera team in visualization (Feb. 2016) ,Attend the weekly meeting and answer questions as needed.  Help with the Python and JS debug 
0,document adding git-lfs repos to CI,NULL
0,Update apr and apr_util,{{apr}} and {{apr-util}} are outdated and lagging behind the versions on RHEL6. They should be updated as agreed in RFC-76.
0,Move luaxmlrpc to lsst-dm/legacy-,"We no longer need luaxmlrpc because we run czar inside proxy. We should move it to lsst-dm/legacy-, and remove mentioning it in readme."
1,DM Power Requirements Justification,"The power requirements for the base site appeared to have increased greatly since LSE-239 or LDM-144 v140. Significant effort was spent digging through LDM-144 for precise rack counts, rack weights, rack power. Further time was spent on the analysis of why the power requirement is greater then expected. This involved analyzing swing space power requirements, max swing space needed, investigation into what LSE-239 refers to 'expansion' (turns out to be alert processing), attributing alert processing power requirements to the base (LDM-144 contributes to archive site but contingency is still in place for base site operations), comparing peak and steady state power needs. Also discussions around reinforcing the floors for greater rack weights.  "
1,Jason Feb Tasks,"Weeks 1&2 - Interviews, Team mtgs, uptime institute tier discussions: 1.5 pts  Weeks 3&4 - Team mtgs, ICI meetings, set/prioritize IT goals 4 pts"
0,Jason Feb Educational Activities,"Learning DM stack deployment and layout, reading on redesign of butler 1.5"
2,Avoid merge table (i.e. result_m) creation on czar side,"When launching a query which require an aggregation/merge, Qserv first creates a result_m table to collect chunk query results and then a result table. On the other hand, for a query which doesn't require a merge, only result table is created.    If merge query was send to mysql-proxy right after query parsing (like it is currently done with ""order by"" clause only, this would be then generalized to all merge queries), creation of result_m table could be avoided. This would lead to simpler C++ code, and aggregation would be performed at the same time that returning result, which may lead to better performance. Queries wich requires or not aggregation step would be processed exactly the same way on the C++ side (store results of chunk queries), and mysql-proxy would release lock on result table when running aggregation/merge query (here, one can consider that simply concatening results of query would also be a kind of aggregation).    Please not that removal of result_m table would also free some space on master, which is a bottleneck.    [~jbecla], I propose you to plan this interesting feature for next sprint, feel free to postpone it. I think that intersection with ""Query coverage"" story might not be empty."
1,Provide usable repos in {{validation_data_*}} packages.,"Re-interpreted ticket:  1. Provide already-initialized repositories in the `validation_data_cfht`, `validation_data_decam`, and `validation_data_hsc` packages alongside the raw data.  The goal is to allow both easy quick-start analyses as well as comparisons of output steps from processCcd.py and friends at each step of the processing. (/)  2. Add (Cfht,Decam,HSC).list files to provide for easy processing of the available dataIds in the example data. (/)  3. Update README files to explain available data.  (/)    [Original request:]  In validation_drp when I run examples/runXTest.sh I find that any data I had saved in CFHT or DECam is lost, even if I have carefully renamed it. This is very dangerous and I lost a lot of work due to it. At a bare minimum please do NOT touch any directories not named ""input"" or ""output"".    Lower priority requests that I hope you will consider:  - Have the the input repo be entirely contained in the validation_data_X packages, ready to use ""as is"". That would simplify the use of those packages by other code. It would also simplify validate_drp, and it would just leave the output repo to generate (which already has a link back to the input repo).  - Have runXTest.sh accept a single argument: the path to the output. (The path to the input is not necessary if you implement the first suggestion)."
1,IN2P3 cluster worker nodes failed to start due to Innodb error,"Next error happens when starting mariadb on worker (with existing data from 35TB dataset, which were generated by mysql):  {code:bash}  2016-02-13 22:02:36 139632684558144 [Note] InnoDB: Completed initialization of buffer pool  2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: auto-extending data file ./ibdata1 is of a different size 640 pages (rounded down to MB) than specified in the .cnf file: initial 768 pages, max 0 (relevant if non-zero) pages!  2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: Could not open or create the system tablespace. If you tried to add new data files to the system tablespace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt. InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data!  2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' init function returned error.  2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.  2016-02-13 22:02:36 139632684558144 [Note] Plugin 'FEEDBACK' is disabled.  2016-02-13 22:02:36 139632684558144 [ERROR] Unknown/unsupported storage engine: InnoDB  2016-02-13 22:02:36 139632684558144 [ERROR] Aborting  {code}"
0,Create an easy place to add tests to ci_hsc,Create a single file where tests for validating source can be added. The tests will be duck typed to a class method and be registered to the corresponding validation class with a decorator.
0,"Code review, Feb 2016","DM3733,DM4825"
0,"Meetings, Feb 2016",verification dataset meetings
1,Process a tiny set of raw DECam Stripe 82 data,Process some DECam data to gain familiarity with process execution and learn to debug issues
3,Continue learning middleware,"Ramp up with the middleware status and development. Look into packages pipe_base, pex_config, pex_logging. "
2,"LOE, Feb 2016","Local LSST meetings, postdoc meetings, NCSA All hand meetings, RFDs, NCSA software meeting, astronomy events, workshops, travel to JTM, other local meetings. "
0,Please document MemoryTestCase,"{{lsst.utils.tests.MemoryTestCase}} is used extensively throughout our test suite, but it is lacking in documentation and it's not clear under what circumstances its use is required or encouraged. Please add appropriate documentation to the [Software Unit Test Policy |http://developer.lsst.io/en/latest/coding/unit_test_policy.html].    See also [this thread on clo|https://community.lsst.org/t/what-is-the-policy-for-using-lsst-utils-tests-memorytestcase]."
0,"Record CCD, visit of input catalog in `validate_drp`",1. Record the CCD and `visit` of the individual source in the catalog so that it is available for later analysis.  3. Update `analyzeData` to use these newly available CCD and `visit` information in the catalog.  
0,HSC backport: Support a full background model when detecting cosmic rays,"This is a port of the following two standalone HSC commits:    [Support a full background model when detecting cosmic rays|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb]  {code}  If there are strong gradients (e.g. M31's nucleus) we need to do more than  treat the background as a constant.  However, this requires making a copy  of the data so the background-is-a-constant model is preserved as a special  case  {code}  [Fixed cosmicRay() in RepairTask for the case background is subtracted.|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b]  {code}  When the background is subtracted with finer binsize, new exposure  will be created and cosmic rays will be detected on that exposure.  But the image of that exposure was not properly returned back.  {code}  "
1,Audit the LSST and HSC codebases for differences,"We've already merged a lot of code from HSC to LSST, and are optimistic that we've captured most of the big ticket items. However, we need to perform a thorough comparison of the codebases to check there's nothing we're missing. Please do that, and file tickets in the DM-3560 and DM-3568 epics to describe outstanding work."
1,Modify System layout to support expanded views,Each of the visualizers needs to expand to full screen.  We need to modify our current layout system so each and expand and collapse so that the old view is restored. The system needs to be flexible enough so an 'expanded version' of the component can be used.
0,Tests in daf_persistence should skip properly,Some of the tests in {{daf_persistence}} have a couple of problems that cause difficulties with modern test frameworks:  # unittest is not being used at all in some cases  # Skipping is done with a print and a {{sys.exit}}    They need to be modernized.
0,Mouse Readout: part 1.5 - update flux server call to work in JS,NULL
1,Analyze catalog-comparison CmdLineTasks,Analyze the QA CmdLineTask collection being generated by [~lauren] sufficiently well to determine the interface requirements needed to represent them as Supertasks.    Does not include actually designing that interface.
1,Standup Fastly infrastructure for LSST the Docs,LSST the Docs will use Fastly to serve docs out of an S3 bucket with well-formatted URLs thanks to routing at the Varnish layer. See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for an overview of the desired setup and http://sqr-006.lsst.io for an overview of LSST the Docs. Specific outcomes are:    * Create S3 bucket for LTD  * Create Fastly account (may be a demo account pending negotiations with Fastly)  * Basic configurations for Fastly account  * Research pricing/configure a TLS certificate for *.lsst.io domains  * Set up base VCL configuration on Fastly.  
1,Fastly API interactions for LSST the Docs,"Using Fastly’s API, have ltd-keeper setup new builds and editions:    - Add {{Surrogate-Key}} to headers of objects uploaded to S3 (happens on ltd-mason side)  - Configure Varnish to serve specific bucket directories as specific domains (DM-4951 has added Route 53 interactions to ltd-keeper)  - Purge content when editions switch or content is deleted.    DM-5167 is covering non-API driven work to configure fastly.    See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for a write-up on serving static site via fastly. See also http://sqr-006.lsst.io for an overview of LSST the Docs."
3,New XY  functions to be developed (F16),There are several new functions requested by users
1,Manipulating masks is confusing,"A possible bug exists in afwImage.Exposure.getMaskedImage(). This function returns a copy of the Exposure's masked image, and not the actual maskedImage owned by the Exposure. This means that any changes made to the mask are done only on the copy, and are not reflected in the Exposure's maskImage. The intended behavior seems to be that a shallow copy be returned with pointers to all the original objects (such as the mask). This however does not seem to be the case, a deep copy is always made instead. Verify that the intended behavior is indeed happening. Steps to reproduce    {code:python}  coaddExposure = afwImage.ExposureF()  coaddExposure.getMaskedImage().getMask().addMaskPlane('TEST')  print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict())  m = coaddExposure.getMaskedImage().getMask()  print(m.getMaskPlaneDict().asdict())  m.removeAndClearMaskPlane('TEST')  print(m.getMaskPlaneDict().asdict())  print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict())  {code}     A second concern, though not necessarily a bug, is that adding and removing mask planes is confusing due to inconsistent manipulation of global state. For example:  {code}  In [1]: import lsst.afw.image as afwImage    # Create two separate Masks  In [2]: mask1 = afwImage.MaskU()  In [3]: mask2 = afwImage.MaskU()    # Neither Mask contains a ""TEST"" plane  In [4]: 'TEST' in mask1.getMaskPlaneDict()  Out[4]: False  In [5]: 'TEST' in mask2.getMaskPlaneDict()  Out[5]: False    # Adding a plane to one updates a shared list of planes, so it appears in the other  In [6]: mask1.addMaskPlane('TEST')  Out[6]: 9  In [7]: 'TEST' in mask1.getMaskPlaneDict()  Out[7]: True  In [8]: 'TEST' in mask2.getMaskPlaneDict()  Out[8]: True    # But deleting a plane from one affects only that particular Mask and not the global state  In [9]: mask1.removeAndClearMaskPlane('TEST')  In [10]: 'TEST' in mask1.getMaskPlaneDict()  Out[10]: False  In [11]: 'TEST' in mask2.getMaskPlaneDict()  Out[11]: True  {code}"
0,Add CSS information for shared scans to integration test data.,Some tables int the integration tests need to be flagged as needing to be locked in memory and given a scan rating.
0,lsstsw deploy on OS X fails in miniconda install,Testing the fixes for the {{deploy}} script in DM-4359 it seems that the part of the script installing {{miniconda}} no longer works on OS X because the list of packages to be installed has been derived from a Linux system and not all the Linux packages have OS X equivalents. There needs to be a per-OS list of packages. The default OS X list seems to be:  {code}  # This file may be used to create an environment using:  # $ conda create --name <env> --file <this file>  # platform: osx-64  astropy=1.1.1=np110py27_0  conda=3.19.1=py27_0  conda-env=2.4.5=py27_0  cycler=0.9.0=py27_0  cython=0.23.4=py27_1  freetype=2.5.5=0  libpng=1.6.17=0  matplotlib=1.5.1=np110py27_0  nomkl=1.0=0  numpy=1.10.4=py27_nomkl_0  openssl=1.0.2f=0  pandas=0.17.1=np110py27_0  pip=8.0.2=py27_0  pycosat=0.6.1=py27_0  pycrypto=2.6.1=py27_0  pyparsing=2.0.3=py27_0  pyqt=4.11.4=py27_1  python=2.7.11=0  python-dateutil=2.4.2=py27_0  pytz=2015.7=py27_0  pyyaml=3.11=py27_1  qt=4.8.7=1  readline=6.2=2  requests=2.9.1=py27_0  scipy=0.17.0=np110py27_nomkl_0  setuptools=19.6.2=py27_0  sip=4.16.9=py27_0  six=1.10.0=py27_0  sqlalchemy=1.0.11=py27_0  sqlite=3.9.2=0  tk=8.5.18=0  wheel=0.29.0=py27_0  yaml=0.1.6=0  zlib=1.2.8=0  {code}
0,miniconda2 eups package fails to install on OS X,The {{miniconda2}} eups package attempts to install the relevant conda packages by downloading a list from the {{lsstsw}} repository. This fails for the same reason that {{lsstsw}} fails in DM-5178 in that the list of packages is not OS-specific. This means that {{newinstall.sh}} does not work any more on OS X.
0,"update ""newinstall.sh"" nebula images & docker containers - w_2016_08",NULL
1,Hook up help system,We need to help system like we have in GWT.
1,Implement Lock plot button on toolbar,* Write a button on the toolbar that monitors the active plot view's group and shows the locked or unlocked icons  * Add an action and reducer functions the will toggle the lock state of the group.
1,Add Xrdssi plugin configuration file,"Xrdssi plugin configuration file could be useful for sharedscan.  to pass plugin configuration file path to xrootd  http://xrootd.org/doc/dev42/xrd_config.htm#_Passing_Plug-In_Command (use -+xrdssi)  to get this argument from C++  http://xrootd.org/doc/dev42/ssi_reference.htm#_Toc431242001    then an add-hoc config file parser needs to be used (not to be xrootd dependant), json/yaml could be used."
1,Set Qserv master in env variable for Docker containers,"This would allow use of pre-configured container on all clusters, indeed the only parameter which currently change in cluster install is master fqdn.  See http://xrootd.org/doc/dev42/Syntax_config.htm  and  {code}  if defined ?~EXPORTPATH    set exportpath = $EXPORTPATH    else    set exportpath = /tmp    fi    all.export $exportpath    {code}"
2,Add fftools API: Image Viewer plus foundational work,NULL
1,Add fftools API: Table,NULL
1,Add fftools API: XYPlots and Histgram,NULL
1,"Coverage, Coverage API, ImageMetaData API",NULL
3,Add remote (python) API support ,"The python interface needs to be ported.  This involves the following:    * Modify FireflyClient.py  * Change all the API to work by dispatching remote actions. There is currently a dispatchRemoteAction method in  FireflyClient.py  * On the server side clean up file PushCommands.java, PushJob.java and ServerParams to remove the old api.  * Move the fftools/python to firefly/python  * clean up the test notebooks.  There are currently several, some don't work and should be removed.  Others should be clean test cases.  * Make sure the python support for RangeValues is consistent with the Java and JavaScript. I suspect it is not.  * Make sure events can be received into the python.  "
1,attend the bi-weekly meeting authentication and authorization discussion,attend the bi-weekly meeting authentication and authorization discussion. provide input and feedback to IAM. 
2,Deploy ltd-keeper as a Docker Container,ltd-keeper should be deployed as a Docker container as a best practice for maintainable cloud microservices.    This involves writing a Dockerfile committed to the ltd-keeper repo and demonstrating that the container can be stood up on Google Container Engine.    I plan on use data-containers attached to the service’s container to maintain the sqlite DB. This ticket should document how to operate ltd-keeper and apply updates to both the ltd-keeper app *and* DB migrations..    This ticket also involves initial overhead in researching Docker/Kubernetes.
0,swift API availability?,"The downtime announcement email for {{Nebula unavailable Feb 9-10}} mentioned a ""roadmap"" for swift.  I have checked and post maintenance, there is not a swift endpoint available in the catalog.  Is there a time line for availability?"
1,Test and robustify shapelet PSF approximations,"The CModel code ported from HSC only works as well as the ShapeletPsfApproximation algorithm that runs before it, but we've switched on the LSST side to a more flexible algorithm that isn't as nearly as battle-tested as what's been running on the HSC side, and there are some concerning indications from [~pgee]'s work that it can be catastrophically slow on some reasonable PSFs.  On this issue, I'll run it on some real HSC data and try to improve it, even if that means reducing the flexibility back to what was on the HSC side in some ways."
1,FITS Visualizer porting: Statistics - part 2 - drawing overlay & 3 color support,drawing overlay 3 Color Support
0,instance I/O errors,"The kernel dmesg for Instance {{bbfd7458-6dd6-4412-a8ba-8d417c3df56b}} has started reporting thousands of block I/O errors and these are starting to trickle up as a filesystem I/O errors.  I suspect this is likely a hypervisor I/O issue.    {code}  [687301.556430] Buffer I/O error on device dm-3, logical block 3768490  [687301.556433] Buffer I/O error on device dm-3, logical block 3768491  [687301.556436] Buffer I/O error on device dm-3, logical block 3768492  {code}    {code}  $ openstack server show bbfd7458-6dd6-4412-a8ba-8d417c3df56b  +--------------------------------------+-----------------------------------------------------------------------+  | Field                                | Value                                                                 |  +--------------------------------------+-----------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                |  | OS-EXT-AZ:availability_zone          | nova                                                                  |  | OS-EXT-STS:power_state               | 1                                                                     |  | OS-EXT-STS:task_state                | None                                                                  |  | OS-EXT-STS:vm_state                  | active                                                                |  | OS-SRV-USG:launched_at               | 2016-02-11T23:36:25.000000                                            |  | OS-SRV-USG:terminated_at             | None                                                                  |  | accessIPv4                           |                                                                       |  | accessIPv6                           |                                                                       |  | addresses                            | LSST-net=172.16.1.115, 141.142.209.121                                |  | config_drive                         |                                                                       |  | created                              | 2016-02-11T23:36:12Z                                                  |  | flavor                               | m1.xlarge (5)                                                         |  | hostId                               | f7fbf308022d02f52e1111c91cf578d852784d290d0e0ddb0d69635c              |  | id                                   | bbfd7458-6dd6-4412-a8ba-8d417c3df56b                                  |  | image                                | centos-7-docker-20151116230205 (59a2a478-11ab-41c5-affc-29706d38d65a) |  | key_name                             | vagrant-generated-comshorc                                            |  | name                                 | el7-docker-jhoblitt                                                   |  | os-extended-volumes:volumes_attached | []                                                                    |  | progress                             | 0                                                                     |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                      |  | properties                           |                                                                       |  | security_groups                      | [{'name': 'default'}, {'name': 'remote SSH'}]                         |  | status                               | ACTIVE                                                                |  | updated                              | 2016-02-11T23:36:25Z                                                  |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                      |  +--------------------------------------+-----------------------------------------------------------------------+  {code}"
2,sph-partition does not support BLOB fields,"Next command fails with BLOB field in input file:  {code:bash}  dev@clrinfoport09:~/src/qserv_testdata/datasets/case01/data$ sph-partition --out.dir /home/dev/qserv-run/git/tmp/qserv_data_loader/Object --part.prefix chunk --config-file /home/dev/src/qserv_testdata/datasets/case01/data/common.cfg --config-file /home/dev/src/qserv_testdata/datasets/case01/data/Object.cfg --in Object.tsv  CSV record contains an embedded line terminator, a trailing escape character, or a quoted field without a trailing quote character.  {code}  Note that the command works in input file is reduced to its first line.    Note that mysql import works fine:  {code:sql}  MariaDB [qservTest_case01_mysql]> LOAD DATA INFILE ""/tmp/Object.tsv"" INTO TABLE Object FIELDS TERMINATED BY '\t' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n';  {code}"
0,Remove LOGF macros from log package,We have removed all uses of LOGF macros from qserv and as far as I know no other clients use those macros. It's time to clean up log package itself from those macros.
1,Add support for 3 Color,Most of this is done.  I just need to plot a few 3 color images and work out the bugs.
0,Remove remaining LOGF macros from qserv,"There are still few cases of LOGF macros in qserv, have to replace them all."
2,replace Associations::CollectRefStars with LoadReferenceObjectsTask,"AstroUtils.cc has code for loading USNO catalog which is used by Associations:CollectRefStars to build a reference list. We should replace this with  LoadReferenceObjectsTask from meas_algorithms to both make it more generic, and to remove problematic endian handling in AstroUtils."
0,Please do not write garbage to the FITS EQUINOX,"The equinox is not relevant when dealing with ICRS coordinates.    When {{afw}} manipulates {{Wcs}} objects, it simply doesn't bother initializing the {{equinox}} field of its {{_wcsInfo}} struct when dealing with an ICRS system.    When {{afw}} persists the {{Wcs}} to FITS, it blindly writes whatever happens to be in that uninitialized field to the FITS header. Thus, we end up with something like:  {code}  EQUINOX =      9.87654321E+107 / Equinox of coordinates  {code}  This should be no problem, since, per the [FITS standard|http://fits.gsfc.nasa.gov/standard30/fits_standard30aa.pdf] (page 30), the {{EQUINOX}} is ""not applicable"" if they {{RADESYS}} is {{ICRS}}. The reader should thus ignore this value.    However, [SAOimage DS9|http://ds9.si.edu] version 7.4.1 (the latest release at time of writing) does _not_ ignore the {{EQUINOX}}. Rather, it refuses to handle the WCS for the image. Note that version 7.3 of DS9 does not seem to have the same issue.    While this does seem to be a bug in DS9, it's easy enough to work around by simply not writing {{EQUINOX}}."
0,Evaluate MariaDB GSSAPI Authentication Plugin,"As a follow-on to DM-4315, deploy the new [Maria DB GSSAPI Authentication Plugin|https://mariadb.com/kb/en/mariadb/gssapi-authentication-plugin/] in the IAM testbed for Kerberos ticket-based authentication, to provide single sign-on access."
2,Improve worker configuration files.,"Configuration on the worker is done by setting environment variables in a script, which is lacking in flexibility, but there is a question of if it is worth changing to some form of text configuration file. The code that reads the configuration could be improved in either case."
1,Run Qserv multinodes integration tests inside Travis,This aims at preparing integration of this procedure inside Jenkins CI
1,Add configured requirements parameters.  Pass/Fail test.,1. Add pass/fail routine to report success/fail against metrics.  Do this for    * SRD  (/)    * Configured metrics  (/)    2. Add pass/fail reporting to running of `validate.drp.run`  (/)
1,Add a ci_hsc daily build,"Please add a daily build of `ci_hsc` to the Jenkins system.    This does not need to explicitly build `lsst_distrib` or `lsst_sims`.  The only product to list is `ci_hsc`.    In the slightly near future, I anticipate that this build will be replaced by a daily build of a metapackage `lsst_ci`."
0,implement cycle change in DLP,"Summer --> Fall, Winter --> Sprint, add X16"
3,X16 Object Characterization Bucket,Catch all epic for essential bugs and improvements in object characterization emerging during X16.
0,Segfault in shapeHSM centroid extractor,"[~boutigny] reports a segfault in {{meas_extenstions_shapeHSM}}. He provides the following backtrace:  {code}  Program received signal SIGSEGV, Segmentation fault.  0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  61	typename Field<T>::Element * getElement(Key<T> const & key) {  (gdb) bt  #0 0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  #1 0x00007fffdc8775f2 in set<lsst::afw::table::Flag, bool> (value=<synthetic pointer>, key=..., this=0x21c8d60)  at /home/boutigny/CFHT/lsstsw/stack/Linux64/afw/11.0-8-g38426eb/include/lsst/afw/table/BaseRecord.h:137  #2 setValue (value=true, i=0, record=..., this=0x1da2500) at include/lsst/meas/base/FlagHandler.h:73  #3 lsst::meas::base::SafeCentroidExtractor::operator() (this=<optimized out>, record=..., flags=...)  at src/InputUtilities.cc:134  #4 0x00007fffd03655c6 in lsst::meas::extensions::shapeHSM::HsmPsfMomentsAlgorithm::measure (this=0x1da2410,   source=..., exposure=...) at src/HsmMoments.cc:115  #5 0x00007fffd06708d5 in _wrap_HsmPsfMomentsAlgorithm_measure (args=0x7fffccc67b90)  at python/lsst/meas/extensions/shapeHSM/hsmLib_wrap.cc:14337  #6 0x00007ffff7aee37f in ext_do_call (nk=-859407472, na=<optimized out>, flags=<optimized out>,   pp_stack=0x7fffffff7d18, func=0x7fffd0c21878) at Python/ceval.c:4345  #7 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2720  #8 0x00007ffff7aefdbe in PyEval_EvalCodeEx (co=0x7fffd0a9ceb0, globals=<optimized out>, locals=<optimized out>,   args=<optimized out>, argcount=3, kws=0x7fffccd43b08, kwcount=0, defs=0x0, defcount=0, closure=0x0)  at Python/ceval.c:3267  {code}    See the discussion at DM-4780."
0,Implement background gradient fit in pre-sub. images for dipole fit,Add a linear background gradient fit to the integrated pre-subtraction and dipole fitter (for testing).  This will eventually be implemented in the measurement plugin.
2,Implement a way to pass more than one exposure to a SingleFrameMeasurement (DipoleMeasurementTask),NULL
0,Add background gradient fit to new dipole measurement task,NULL
0,lsstsw breakage with spaces in paths,There are still some issues relating to using {{lsstsw}} to build the stack when spaces are in the path to the {{$LSSTSW}} location. This is a fine thing to sort out on Rodeo Day...
0,"Base ""bright star"" cut on S/N instead of magnitudes","The astrometry histogram generated by validateDrp.py conflates astrometric and photometric calibration because it uses magnitude for brightness, and this relies on the accuracy of the photometric calibration. [~ctslater] suggests (and I agree) that brightness should be based on signal to noise ratio, thus making the astrometry histogram independent of photometric calibration.  "
3,X16 Butler,Work on improving Butler:  * Refactor butler multiple repository support based on user feedback.   * Formalize butler configuration mechanism and define configuration persistence.   * RFC and implement support for repo selection based on version.   * Design and RFC mechanism for Butler to define output dataset type.   * R&D & stub implementation of Butler storage factorization (python type + file type + storage location)  * implement spatial lookups in butler  * minor bug fixing
0,Modernize python in lsst_build,The python in {{lsst_build}} uses old-style print and exception handling. These should be updated to the current standard.
0,Turn on bias-jump fix for all CCDs ,"The overscan fix to handle bias jump in an amplifier done in DM-4366 introduced a new config parameter {{overscanBiasJumpBKP}}, and the fix is applied for CCDs on the backplanes specified in {{overscanBiasJumpBKP}}.  Previously, the default is to only fix CCDs on backplanes next to the focus chips. But [~mfisherlevine] also see the bias jump features in other CCDs.  It would make more sense to turn it on for all CCDs by default. "
2,Prepare narrative description of Level 3 operations from the perspective of the SUIT,"Also known as the ""Level 3 ConOps"" needed by NCSA."
1,Provide comparison routines for comparing two repos of the same data,Adapt the HSC capabilities from DM-4730 as represented on pipe_tasks u/laurenm/DM-4730 (as prepared by [~lauren] and [~price])  into generally available {{pipe_tasks}} routines for comparison of two different repositories of the same data.  The intended use case is comparing two different algorithms or configurations on the same data and providing individual source-measurement to source-measurement comparisons for debugging new algorithms and understanding the behavior.
0,Audit SuprimeCam policy and update to current standards,{{obs_subaru}}'s {{policy/SuprimecamMapper.paf}} contains a number of entries that look wrong (e.g. {{deep_forcedPhotCoadd_metadata}} should be {{deepCoadd_forced_config}}) or do not apply to LSST (e.g. {{stack_config}}) and doesn't contain a number of entries that might be expected (e.g. {{transformed_src}}).    Please ensure that this file is updated to comply with current expectations.
0,rename meas_simastrom to jointcal and flatten namespace,"Moving meas_simastrom from lsst_france/ to lsst/ also resulted in a name change per RFC-123, and a namespace flattening: it's not derived from meas; it's a task. This is the necessary first step in getting it integrated into the stack."
1,Filtering from XY Plot table view (JS),Allow to filter in a selected area from XY Plot.
0,make floating point exception handling cross-platform (or remove it),"jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on OSX. Were these an important part of error handling in meas_simastrom, or can I just remove them?"
0,plan to upgrade the third party packages,"The following packages need to be reviewed and maybe upgraded.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)  "
1,replace buildbot with jenkins job(s),Removing buildbot and replacing it with jenkins would provide a number of benefits    * one less dashboard for developers to know about / interact with  * one less system for SQRE to maintain  * lessening the cost of refactoring the CI drivers scripts as synchronized updates to two CI system configurations would no longer be necessary    It should also be easy to go one step further and try to eliminate the need for developers to manually log into the {{lsstsw}} account on {{lsst-dev}} to publish eups distrib packages. 
1,Attend JTM,"Joint Technical Meeting 2/22-24, Santa Cruz"
0,arrays not properly transmitted,Sending a property set with an array as one of the entries only passes the last element of the array.
1,Port HSC afw changesets to LSST,"We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 2c12255372bde846ba0429b5b542960e57d169f0, 0aec617e0ea604cde85105de3dade279a4fe10df: Footprint::overlapsMask  * 76f3706f6688b23d5b0c71e66af3e94095a9f821: copyWithinFootprint: respect image size  * f49676d7f1348f9de8ca21ee633e0c25473251ae: Implemented Linear and ZScale transformations, HSC-1206, 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04  * c369a8ad53962aba950f7710210be4b23f45a523: utils: make Mosaic.nImage a property  * Maybe 0a2647a4f57addc3b3adb347da995fa0d36b43cb: Add display.utils function to plot the bboxes of inputs to a coadd in ds9  * 386a4b71d974d9e5672fe8690d0db3e56c9fb40f: Box2?.getCorners  * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint  * 6c1845474f28f528a95190eeb88f095b11999078: Check in #3092 (iterable coord) directly on master  * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint  * 5d1934cc9fe7d8c43aa8f9318a1ac9a3ce85e94e, 0d1ab12db604d5e42a5d72f028411a64294283ce: Footprint merging  * b8578746d69920bc1e1089cca4b4acb230f0e8d5: Interpolate: add support for ndarray  * 3de3339aa075f869d73a5bc66fc65dbee8ae3d16: Fix unit test fallout from Interpolate std::vector change  * 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04, f73544e15abd2760bf84794798cf4b84e97e938d: Added xSize, ySize, and rescaleFactor arguments to {make,write}RGB; HSC-1207  * 88d838b74898d9572bbc8c46121da029958c1c72: rgb: fix makeRGB so it can replace saturated pixels and produce an image  * 254d7248ea20d98de481d968f6503d1610b16ae7: Remove tests of writing rgb images to jpeg and tiff  * 3252a40ad55222b882acf14d2f7cf0f3fe80f585: Added MatchControl and implemented it for matchXY and MatchRaDec  * 81c6063a32883b748f3770b7124d74ced7b480f5: Implement and test includeMismatches option in MatchControl object  * fd4c0baec617155fac0816607a5acba88e8970f5: Add support for renaming without replacing the full field in SchemaMapper  * 79337bb6d1ee3a0b73bcd2b2d0ca506a44d3fa56: Handle empty Footprints when normalising  "
0,"Port HSC skymap, shapelet changesets to LSST",We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * skymap:  ** f83f71718eac5307d575d3113ee3757a63a16de2: Set vertex list in ExplicitTractInfo.    * shapelet:  ** bb928df3fc2fafe5183e0d075da19994f0af4fc7: Let the value to normalize to be specified in [Multi]ShapeletFunction  
1,Port HSC meas_algorithms changesets to LSST,"We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 1293a31c19c238ba2c2acd8f67ec1be742764b66: BinnedWcs  * 9f392b134502f6e4fbbd8759806b15f89a267e5a: detection: additional debugging plots for background  * ad74fe8595ec523d6269e36ec2db051534bf3e9a: Add initial.flags.pixel.interpolated.any to ObjectSizeStarSelectorConfig.badFlags  * 69f5db0eba69225cff917fa4c96a94dc8b765aa0, 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab: Check for bad PSF measurements (HSC-1153)  * a54b1ac52678025d3317e8a379c2849d3eb567ba: pcaPsfDeterminer: catch case of no good PSF candidates in debugging  * c4fcab3251e6f41da2248d63fdf28c0bf80e30f8: Indent seems to be wrong for debug display  * 2a889c17d47c879dbb4345bafba6aed9869b5984, f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018: Make IdSpanCompar more deterministic  * f99eb46f484609673b45290eaaba47688d7b4a24: CR code has to take care of 'NO_DATA' mask  * 6f6b786bce8ca34bf4c67f75f965130dea027147: Handle small numbers of psfCandidates (HSC-1176)  * d744e6514feaf67b87068ac502bca677306f9fc2: tests: add test for MeasureApCorrTask  * 65f617089038fe19179fca4f959bf23ea061a6b8, 1b7e3cc48ed347b0afa31e81c821b38f87d18d64: Test case for measurement of negative objects    There are also a couple of issues that were identified in the DM-5162 review:  * Delete tests/config/MeasureSources.py --- mere configuration, old-style measurement  * testPsfDetermination has method 'xtestRejectBlends'"
0,Port HSC daf_butlerUtils changesets to LSST,"We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * daee24edba01b01a0412df7f9b4cf70be5b10860: CameraMapper: allow a default filter name to be provided  * e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0: CameraMapper: normalize path names, and remove leading double slash  * 476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694: preserve timestamps in cameraMapper.backup()  * b2491ef60e5e23afa7d9f0297f257e694aa1af35: Only attempt to update Wcs if it's available  * 9f62bcce588fa9abc8e1e44ff2f0275e5230f629: Registry: hold registry cache for a single thread only (HSC-1035)  * 412f03b95b7a5e82003ab33a61bd43adbf465188: Registry: use a pool of registries to avoid having too many open files"
1,Port HSC meas_extensions_simpleShape package to LSST,"HSC uses a package, meas_extensions_simpleShape, which needs to be ported to LSST.  The package is used for basic shape measurements for determining focus, and also serves as a simple guide for writing measurement plugins."
0,Port HSC meas_extensions_multiShapelet changesets to LSST,We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * bf5f753133ae4b41357f9789ff4763949ebb6ffb: FitPsf: reduce outerOrder to 1  * a54d6cbd41baf916fac2a1bb235a8502af14edfd: Provide explicit instantiations for the sake of clang 6.0 on os/x 10.9  * a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15: FitProfileAlgorithm: bail if the PSF flux failed  
0,Port HSC meas_deblender changesets to LSST,"We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * a8cf6c22df14494d6dcf2d7354c695cba9506301: Clarify tiny footprint limit  * 624790aa63a38fb7a328ebc21abfd1b10503aa26: config: change default strayFluxRule  * db7d705de93b43a5f32f771c716b1c5c7368d124: consolidate failed peak logic and downgrade warning    We also identified a few differences that should be resolved:  * clipStrayFluxFraction defaults to 0.01 for LSST, 0.001 for HSC  * Stray file, src/Baseline.cc.orig, on LSST side  "
0,Port HSC ip_isr changesets to LSST,We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * f1cee734998f1faf86c02af42ea599b077847eeb: IsrTask: allow fallback to a different filter when loading calibrations  * 89cd629bb8e1a72a545176311b1ef659358d95af: saturationDetection: apply to overscan as well as image  
1,Port HSC pipe_tasks changesets to LSST,"We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiBand: save background-subtracted coadd as deepCoadd_calexp  * e99e140feafe28e6f034143e8ee2ae58e9a9358d: Rejig interface for DetectCoaddSourcesTask to provide non-dataRef-centric API  * 829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects  * MeasureMergedCoaddSources.doMatchSources defaults to False  * ProcessImageConfig.doWriteHeavyFootprintsInSources defaults to False ?  * 56666e8feba6893ac95fd4982d3e0daf6baf2d34: WcsSelectImagesTask: catch imagePoly is None    We also noticed some differences:    * * CalibrateConfig.setDefaults doesn't call parent  * CalibrateTask.run isn't returning apCorrMap  * reserveFraction=-1 instead of 0.2  "
1,Port HSC obs_subaru changesets to LSST,"We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * 69d35a890234e37c1142ddbeff43e62fe36e6c45: Set radius for flux.naive, adjust comment for flux.sinc  * 8ea54d10f5ae56f8b6f244bca76d5796ae015216: config: disable sigma clipping in coadd assembly  * 8d2f4a02d0d668fc82e853b633444d8e0fe80010: config: reduce coadd subregionSize  * e36bd1b4410812ca314f50c01f899d92acc0e7a5: config: set pixelScale for jacobian correction  * Remove processCcdOnsiteDb.py, processStack.py  * Rename stacker.py to coaddDriver.py or whatever Nate chooses in DM-3369  * 49e9f5dcf16490f6be6438b89b17911a0cd35fb2: Fixed obvious errors caused by introducing VignetteConfig  * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * daa43eeac46e8708de6f37feeb5d5d16a3caca11: HscMapper: set unit exposure time for dark  * 77ff7c89d56bed94bca4f320f839dbd20fbab641: Set BAD mask for dead amps instead of SAT    We also noticed the following need to be done:    * Forced photometry configuration (CCDs and Coadds)  * Sanitize config of OBS_SUBARU_DIR (use getPackageDir)  * multiband config files need ""root"" --> ""config""  * No astrometry in measureCoaddSources  * Narrow bands missing from priority list  * detectCoaddSources removed from multiband  * Move filterMap from config/processCcd.py into own file"
0,Add z-index for dialogs components,Some of the outside modules that we have brought in have a z-index.  We need to make sure that our dialog components stay on top of them.
0,Docker-ready configuration system for LTD Keeper,"To deploy LTD Keeper in a Docker container (DM-5194), it’s best practice to handle all configurations through environment variables. In DM-4950, LTD Keeper was configured through files for test and dev/deployment profiles. What we should do is continue to allow hard-coded configurations for test and dev environments, but have a third fully fledged configuration environment that’s driven entirely by environment variables.    The environment variables should allow fine grained configuration (for example, to turn off calls to individual external services for testing).    This should also resolve how to deal with Google Container Engine/Kubernetes auth flow works with environment variables, config files, and profiles."
2,Add queue support,"The ctrl_events package currently only supports ActiveMQ topics.  This change will add support for queues.    Additionally, there will be some minor code and assertion clean up as noted in DM-5279."
1,ImageDifferenceTask: Refactor Image DifferenceTask,"The original DM-3704 was to refactor all ImageDifference task. This issue was split into 3 tasks:  1) Split image difference task into two tasks (1) to generate an image difference, and (2) to run detection and measurement on it: processDiffim.py  2) Refactor the Image Difference portion  3) Refactor the processDiffim portion    This ticket refactors the new task that just generates and image difference. "
1,Rewrite unit tests for new dipole measurement task,NULL
1,Make jointcal buildable under CI,"Once jointcal is part of the stack, we need to get it under continuous integration, buildable by Jenkins, etc. There is only one unittest in the package currently, but at least getting that test built and run will catch a number of basic problems.    This requires having its dependencies (CHOLMOD from SuiteSparse) under CI as well.    As part of this, it would be good to have at least one ""integration test"" that runs jointcal as part of processCcd, to catch problems that appear when that interface changes."
1,Document simple simulator,Document the simple simulator produced in DM-4899.  This will also involve some refactoring and adding unit tests to make it usable by others in the group.
3,X16 Design Discussions,Design discussions within the team and with other teams to ensure the plan is complete and properly scoped
2,Compare LSST and HSC pipelines through through single-frame processing,"Following DM-2984, we are confident that the ISR performed by the LSST and HSC stacks is equivalent. Extend that work to cover the whole of single-frame processing ({{ProcessCcdTask}}).    There are two deliverables for this issue:  * A collections of plots and/or comparison scripts that run on the pipeline outputs that can be used to compare their quality in all the ways we think matter. This should probably be in a new repo. Some of these tests should compare the results of the two pipelines against each other (as that's easier to do), but others should be useful for tracking the quality of the pipeline even after we abandon the HSC fork.  * A set of new JIRA issues that capture the code transfers we think we will need to get the LSST pipeline up to the same level of quality as the HSC one."
1,manage jenkins core + plugin versions,There have been a couple of issues that have arisen when deploying test instances vs updating an existing instance due to slight differences between plugin versions.  This would be avoided by putting all plugin versions under change control.    Including:  * The versions of all jenkins components need to be explicitly specified  * The stored job {{config.xml}}'s should be updated to reflect plugin version changes  * The hipchat notification configuration should be updated to fix breakage caused by the production core/plugin update earlier this week  
2,Get high volume test script working again at IN2P3 cluster,"Currently runquerys.py script falls over when running high-volume tests:    * ""LV"": 75,  * ""FTSObj"": 3,  * ""FTSSrc"": 1,  * ""FTSFSrc"": 1,  * ""joinObjSrc"": 1,  * ""joinObjFSrc"": 1,  * ""nearN"": 1    We need this to be working again to validate recent work on schedulers and to support upcoming work on large results, etc."
1,Test removal of response queuing on czar to see if this provides useful flow control,NULL
1,Enable automated publication of qserv-dev release,This would allow integration tests in CI not to break when some Qserv dependencies change. Indeed CI uses a Docker container which include qserv-dev to build current Qserv version.
1,Additional vertical partitioning tests,"Test potential improvements in many-vertical-shards test (20,50) run-times with query optimizer settings."
2,Refine MemManReal implementation per design discussion w/ John,NULL
1,Implement unique query-id generation,"There are currently two separate query IDs defined for queries in czar code:  - ""user query ID"" - defined in {{Czar::submitQuery()}}, used for constructing table names for result table and message table  - ""QMeta query ID"" - ID obtained from QMeta after registering the query (by {{UserQuerySelect::_qMetaRegister()}})    Currently user query ID is used by the rest of the czar code to track the processing of this query, QMeta ID is not used yet for anything except QMeta registration and updates.     QMeta ID will be used for async query identification and there is no actual reason to keep two IDs around, so we should replace user query ID with the QMeta-generated one everywhere. One minor issue is that currently message table name is built and table is locked before we register query in QMeta. Need to understand it and see if we can reverse that logic."
3,DM Verificational Plan Document for CoDR,"This epic captures work resulting from a Systems Engineering request for a document on the DM Verification plan.     In summary, this document must describe how are DM deliverables are going to be verified against the requirements?    George agrees that structuring this DM Verification Plan around the already drawn KPMs is the right thing to do.     SQuaRE will draft a document for internal DM circulation and eventually leading to a project-led Conceptual Design Review of the DM Verification Plan.     The skeleton plan is:  * Go through the KPMs  * List the method (== tools) by which is KPM will be measured      => Describe the requirements for that method      => Criteria of success    * State when it can be measured    * Describe the data necessary or planned for doing the measurements    Additionally, a process (most likely an end2end run) will be described that can verify that external to DM interfaces are being correctly met.     There is no requirement from the point of the CoDR for a resource-loaded plan leading to this work. That is expected to follow from a successful CoDr.           "
1,Research alert production database design,NULL
1,Identify specs within VO stack which should be implemented by database team,NULL
1,Begin exploratory TAP implementation within dbserv,"This is a quick coding foray, to try to shake loose unforseen implementation dependencies or speed-bumps with TAP integration.    Time-boxed at 4 points to fit into a single sprint with Brian's current resource loading -- this is intended to be only a clarifying start."
0,Fix mariadb CI,patch package is missing in docker container used by travis-CI.
0,Make Bright Object Masks compatible with all cameras,"Currently all of the logic that goes into using bright object masks falls into obs_subaru and pipe_tasks. This ticket should move parts (such as the bright object mask class) out of obs_subaru, into a camera agnostic location. The work should also duplicate relevant camera configurations and parameter overrides in the other camera packages. Bright object masks were originally introduced in DM-4831"
0,MeasureApCorrTask should use slot_CalibFlux as default ref flux,"{{MeasureApCorrTask}} uses ""base_CircularApertureFlux_17_0"" as its default reference flux. It should use ""slot_CalibFlux"" instead.    Also check obs_sdss packages for overrides that can be removed; obs_sdss certainly has one in {{config/processCcdTask.py}}"
0,Remove any redundant or unused datasets,Please remove any redundant or unused dataset names from policy files throughout the stack.
1,estimateBackground should not make a deep copy of the exposure,Implement RFC-155: change {{estimateBackground}} as follows:  - Always subtract the background  - Modify the exposure in place  - Replace {{estimateBackground}} with the run method of a new task {{SubtractBackgroundTask}}  - Replace {{getBackground}} (which fits a background) with {{SubtractBackgroundTask.fitBackground}}
3,"Convert GWT code to pure JavaScript (X16, part2, basic)",Continue to work on the GWT code conversion to JavaScript.
1,Create network monitoring dashboard for nebula sys admins,NULL
2,JTM meeting in Santa Cruz,"Met with many individuals, had lots of good conversations. Helped bring me closer to the working of the project and provided a level-set for where activities are currently at. Attended LHN session for most of Wednesday. "
0,Assist in OSX VM environment deployment,NULL
1,Add ExposureIdInfo class,"Implement RFC-146: add ExposureIdInfo class to daf_butlerUtils    This will be implemented in daf_butlerUtils as part of DM-4692, with a unit test in obs_test because daf_butlerUtils has no camera mapper or camera repo in its test directory."
1,Add usesMatches to star selectors,Implement RFC-126 add usesMatches to star selectors    This will be implemented as part of DM-4692
2,a Catch all bug fix epic (X16),A epic for reported bugs in this scycle
1,GWT Conversion: Table results container,"Create a result container for table data.  This task is composed of:  - create actions, action creators and reducing functions  - dynamically add/remove table from view  - support expanded mode  - TabPanel support for deleting tabs."
0,Fix minor issues in docker procedure,"- params.sh was missing at configuration  - startup.py wasn't importing correctly module ""utils""  - remove unused parameters in params.sh"
0,"Planning for GPFS, etc.",* Gathered filesize statistics from existing NFS for planning GPFS  * Assisted with GPFS client setup on test servers  * Reviewed infrastructure changes for Jason
1,Week end 2/07/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 7, 2016."
1,Week end 2/14/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 14, 2016."
0,Week end 2/21/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 21, 2016."
1,Week end 2/28/16,"Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 28, 2016.  "
1,Jason Feb Tasks,"Procurement activities to prepare ""procurement plan activity 1"". PDU, rack, network selection and review. Refresh quotes for compute, storage, rack, pdu, electrical. Refresh, finalize, present (internally) and review design for FY16 infrastructure.    Draft and review of Procurement Plan Activity 1 document."
1,New equipment setup and configuration (week end 2/07/16),"* Updated lsst-dev7 with few missing pieces after initial user testing  * Setup 3 of 8 lsst-test servers  * Confirmed IPMI setup on new test servers (working with Dell on issue with 1 iDRAC license upgrade)  ** Completed and verified IPMI setups  *** Installed licenses for lsst-test1 - lsst-test6  *** re-associated IPMI lsst-test1m last-tsst6m with the correct systems.  *** Installed CentOS7 on lsst-test1 - lsst-test6. (in progress)  * UPS setup  ** Setup table with location of systems in 3003 racks  ** Setup apcusbd on lsst-stor141, lsst-stor142, lsst-stor143, lsst-stor144, lsst20, lsst13"
1,Jason Mar Tasks,"Week 1: Admin mtg, group mtg, ICI mtg, interview. 1pt  Week 2: Meetings, ICI task planning and prioritization 2pt  Weeks 3&4: Interviews, admin and group meetings, ARI SOW"
0,New equipment setup and configuration (week end 2/14/16),"* Still pushing at Dell to fix broken iDRAC license  * Added 5 systems to RSA OTP system  * Completed the setup of lsst-test1, lsst-test4, lsst-test5, lsst-test6  ** Reinstalled lsst-test1 to correct error in puppet install, Completed CentOS7 install, Installed puppet and ran puppet  ** Corrected network error on lsst-test4, Completed CentOS7 install, Installed puppet and ran puppet  ** Completed the setup of Installed lsst-test5, Completed CentOS7 install, Installed puppet and ran puppet  ** Installed lsst-test6, Completed CentOS7 install, Installed puppet and ran puppet"
3,Recover from accumulated technical debt,"Through the S15 and W16 cycles the DRP group focused on merging functionality from HSC. To expedite this process, we accepted lower quality documentation and poorer test quality than would normally be required. We need to recover from this, and other, accumulated technical debt."
1,Add tests for recent improvements to CModel,"In DM-4768 we ported a number of improvements to CModel from HSC. However, these were not accompanied by test cases. Please add them."
0,Get rid of ProcessCcdSdssTask and ProcessCcdDecamTask,"Update {{ProcessCcdTask}} so that it can be used with different datasete types as appropriate for the ISR task. This will allow us to get rid of obs-specific variants {{ProcessCcdSdssTask}} and {{ProcessCcdDecamTask}}    The plan is to change {{ProcessCcdTask}} as follows:  - set {{doMakeDataRefList=False}} in the call to {{add_id_argument}}  - get the dataset type from the ISR task (default to ""raw"") and set it in data container  - make the dataRef list by calling {{makeDataRefList}} on the data container    Question for DECam folks: do you want two executable scripts for DECam (one that processes data from the community pipeline and one that performs ISR)? Or do you prefer one exectutable (in which case you switch between performing ISR and reading the output of the community pipeline output by retargeting the ISR task)? If you prefer one binary, then which should be the default: perform ISR or read the output of the community pipeline?"
1,Revise LSE-140 to account for recent changes to calibration instrumentation,"Produce a revision of LSE-140, the DM - to - auxiliary instrumentation ICD, taking into account recent changes to the calibration instrumentation."
0,Establish goals and create EA framework for LSE-140 update,"Deliverable: together with [~pingraham], identify the changes needed and develop initial content in EA."
0,Create change request for LSE-140,Deliverable: change request and document diffs for LSE-140
0,Add Vis toolbar to expanded mode,NULL
0,Upgrade minuit2,Minuit2 5.34.14 came out in 2014. The current version in the stack is 5.28 from 2010. Minuit2 is annotated on the DM third party software page as being approved for 6-monthly uprev. Minuit2 is only used by AFW.    Release notes for 5.34.14:    * Several fixes and improvements have been put between this verion and the previous stand-alone one (5.28). Main new features is now the support for using the {{ROOT::Math::Minimizer}} interface via the class {{ROOT::Math::Minuit2Minimizer}} also in the standalone version. A new test has been added ({{test/MnSim/demoMinimizer.cxx}}) to show this new functionality  * Other major improvements is in the control of the error messages. One can now use the class {{MnPrint::SetLevel(int value)}} to control the output level. The same can be achieved by calling {{ROOT::Math::Minuit2Minimizer::SetPrintLevel}}.
0,meas_algorithms uses packages that are not listed in table file,{{meas_algorithms}} directly uses the following packages not expressed in the table file:  * Minuit2  * daf_persistence  * daf_base  * pex_config  * pex_exceptions  * pex_policy  
1,Test consistency of Shear Measurements with different Psfs,"DM-1136 was done with a single Psf, partly to avoid some of the problems we found with PsfShapeletApprox.  In this issue, I will look at consistency of the measurement for different Psfs."
1,Test error estimation with bootstrap resampling,Test the error estimation code using bootstrap resampling.
1,Move supertask code our from pipe_base,Create a new package {{pipe_supertask}} and move all supertask code and activator there. Will soon create a poll to pick a better name.
0,Update DMTN-002 to reflect last changes,"Need to update documentation with latest changes on {{pipe_base}}, {{pipe_supertask}} and {{pipe_flow}}"
0,Update {{pipe_flow}},Update {{pipe_flow}} to change dependencies and examples to reflect migration to {{pipe_supertask}}
2,Begin Image Select Panel/Dialog,"This is the first stop in image select panel:    * Design basic panel  * Implement tabs: issa, 2mass, wise  * implement ability to replace a image"
2,Image Select panel: finish tabs,"Implement     * dss, sdss, blank image tabs  * a reusable radius input field   * implement upload file tab  ( the reusable upload file component is done in DM-5584)  "
2,Image Select Panel: 3 color support,"Add 3 color support.  Basically take the panel an be able to repeat of red,green, and blue.  We might want to use a disclosure component.    What has been done: (copied from github commit message)  Add 3 colors support to image selection dropdown, disclosure component is used for r, g, b field group tabs.    For SizeInpuFiield component, fixed validation, add props 'showFeedback' for feedback display and add valid range in error message popup.  (this update is based on the following issues as SizeInputField is used in other panel     - Popup message should have the range are part of the message     - The feedback at the bottom should be optional, turned on by a property.     - When a value such as ‘111d’ is entered,  It does not validate correctly. )    Some Issues:   - should 'disclosure' component's status be kept?   - image creation doesn't work if any of r, g, b is disabled."
1,Image Select Panel: Support add or modify of plot,previously the image select panel would only modify a plot.  Now give it the ability to add a plot.
1,Enable CC-IN2P3/Qserv team communication in order to prepare for Pan-STARRS large scale tests,"The goal of this ticket is to enable communication between CC-IN2P3 and Qserv team in order to prepare for Pan-STARRS data ingestion into Qserv. This data ingestion step is necessary for the large scale tests of Qserv foreseen for summer 2016.    Specifically, we need to understand:    - What is the size of the data set to be imported to CC-IN2P3?  - Where the Pan-STARRS data set to be imported is currently located?  - What mechanisms will the host of Pan-STARRS data make available to CC-IN2P3 for downloading the data set?  - Does the envisaged ingestion mechanism into Qserv requires that the data transit through the Qserv master server or will each Qserv worker be able to ingest its own chunk of data?  - After the ingestion process is finished, do we need to keep a copy of the ingested data out of Qserv?      Given the size of the dataset likely involved in this process, this project will probably require that we (both Qserv and CC-IN2P3 experts) set up specific mechanisms and equipment for efficient transport, storage and ingestion of these data. Timely planning and several testing campaigns seem necessary for this project to make progress.      "
0,Change default value of MeasApCorrConfig.refFluxName to slot_CalibFlux,"The default value of {{MeasApCorrConfig.refFluxName}} is presently ""base_CircularApertureFlux_17_0"". This should be changed to ""slot_CalibFlux"". That is what the slot is intended for. The slot usually points to ""base_CircularApertureFlux_17_0"", but {{obs_sdss}}, at least, overrides this.    Additional jobs:  - Update {{obs_sdss}} {{config/processCcd.py}} to remove the override for this value, since it will no longer be needed.  - Check for and remove unnecessary overrides in other obs_ packages"
0,Use modern TAP package declarations for all EUPS third party packages,In DM-4670 the TAP-ness of the packages was declared using a {{.tap_package}} file. The modern fix is to use a {{$TAP_PACKAGE}} environment variable in the {{eupspkg.cfg.sh}} file. This is how {{pyyaml}} was implemented.
0,Create {{lsst_ci}} package as a continuous integration build target,"Create an {{lsst_ci}} package to be built for the continuous integration testing.    Plan:  1. Create empty package that has dependencies on {{obs_cfht}}, {{obs_decam}}, {{obs_subaru}}, {{testdata_cfht}}, {{testdata_decam}}, {{testdata_subaru}}. (/)  2.  Ensure above builds. (/)  3.  Add {{obs_lsstSim}} and ensure that it builds. (/)    The following were moved to DM-5381:  [ [~tjenness] : How can I get strikethrough to work in the following list?]  3. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validate_drp}}.  4. Run CFHT, DECam quick examples in {{validate_drp}}.  5. Test for successful running of the above examples.  Fail and trigger Jenkins FAILURE message if these examples fail.  6. Check performance of CFHT, DECam runs against reference numbers.  Fail if there is a significant regression.  7. Decide how to include {{ci_hsc}}, which currently can take at least 30 minutes to process the image data.--"
1,butler planning for X16,NULL
0,Fix obs_* packages and ci tests broken by DM-4683,"The butler changes in DM-4683, in particular the removal of {{.mapper}} from the interface exposed by a {{Butler}} object, broken {{obs_cfht}}, {{obs_decam}}, and {{ci_hsc}}.    This issue will fix those changes, and search for additional broken things.    This work is proceeding in conjunction with DM-5370 to test that the CI system, e.g. {{lsst_ci}}, is sensitive to these breakages and fixes."
1,Add to baseline a dedicated replica of L1 database just for scans,"Per RFC-133, users will sometimes need to do full table scan through L1 catalogs, and our baseline does not allow for full scans on the L1 catalog. It'd be good to maintain a replica of L1 for such scans. This story involves changing LDM-141 and adding hardware for the replica. "
1,Remotely attend JTM 2016 sessions,SSIA.  The final hours of the final day were very valuable.
2,L1 base messaging topology.,"Make sequence diagram for Base site messaging and enumerate each message, then provide a narrative description of each message (including logical flow control if applicable) AND an example message payload for the message dictionary. 4  Meetings about message exchange style.  Meetings about using queue fanout for return messages from forwarders and distributors, or binding using routing keys. 4  Rapid prototyping of some of these ideas for evaluation. 4"
1,Create {{lsst_qa}} package as a daily build target for regression testing,"1. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validation_data_hsc}}.  (/)  2. Add dependency on {{validate_drp}}.  (/)  3. Run CFHT, DECam quick examples in {{validate_drp}}.  (/)  4. Test for successful running of the above examples.  (/)  5. Implement in a testing framework.  6. Check performance of CFHT, DECam runs against reference numbers. Fail if there is a significant regression.  (/)  7. Include {{ci_hsc}}, which currently takes 1000 seconds  to process the image data.  8. Check performance of HSC runs against reference numbers.  Fail if there is a significant regression."
1,Port SdssShape changes from HSC meas_algorithms to LSST meas_base,"In porting {{meas_algorithm}} changes from HSC to LSST, modifications to the {{SdssShape}} algorithm were discovered. These changes should be transferred to LSST."
0,calib_psfReserved is only defined when candidate reservation is activated,"The schema should in general not be a function of whether particular features are enabled or disabled so that users can have confidence looking for columns.  However, {{MeasurePsfTask}} only creates the {{calib_psfReserved}} column when {{reserveFraction > 0}}.  This causes warnings when attempting to propagate flags from calibration catalogs to deep catalogs."
1,Filter  editor,"A dialog to edit all the filters on the data for table and XY plot.     AND, OR conditions?    implemented:  * display column's units and descriptions  * add single column filter with auto-correction  * add free-hand filters field with validation and auto-correction  * reset, clear filters as well.  * 'Column' is sticky... scrolling left/right will not affect it.    "
2,catalog search panel,The search panel to do the catalog search
1,GWT Conversion: Dropdown Container,"Create drop down container to display search panel, catalog search panel, image search panel, etc.     "
0,JavaScript loading/caching plan,"We need to ensure that the latest version of the application(javascript) is loaded. Conditions: 1. once loaded, it should be cached by the browser. 2. name of the script has to be a static, so it can be referenced by api user. 3. it also has to load dependencies(gwt scripts) after the main script is loaded.  To do this, we created a tiny firefly_loader.js script whose role is to load the main script and then its dependencies. firefly_loader.js is configured to never cache so that the latest main script is always picked up. The main script is appended with a unique hash on every build.  This ensures that the browser will pick up the new script the very first time, and then cache it for future use. "
0,Please stop leaving repoCfg.yaml files around,"After a recent change to {{daf_persistence}} and possibly other packages I'm finding that many packages leave {{repoCfg.yaml}} files lying around after they run unit tests.    I'm not sure what is best to do about these files. If they are temporary, as I am guessing, then I think we need some way to clean them up when the tests that generated them have run. If they are intended to be permanent (which would be surprising for auto-generated files) then they should probably be committed?    I hope we can do better than adding them to .gitignore."
0,Investigate boost compiler warnings and update boost to v1.60,"As reported in comments in DM-1304 clang now triggers many warnings with Boost v1.59:  {code}  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/archive/detail/check.hpp:148:5: warning: unused typedef 'STATIC_WARNING_LINE148' [-Wunused-local-typedef]      BOOST_STATIC_WARNING(typex::value);      ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:100:33: note: expanded from macro 'BOOST_STATIC_WARNING'  #define BOOST_STATIC_WARNING(B) BOOST_SERIALIZATION_BSW(B, __LINE__)                                  ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:99:7: note: expanded from macro 'BOOST_SERIALIZATION_BSW'      > BOOST_JOIN(STATIC_WARNING_LINE, L) BOOST_STATIC_ASSERT_UNUSED_ATTRIBUTE;         ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:544:28: note: expanded from macro 'BOOST_JOIN'  #define BOOST_JOIN( X, Y ) BOOST_DO_JOIN( X, Y )                             ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:545:31: note: expanded from macro 'BOOST_DO_JOIN'  #define BOOST_DO_JOIN( X, Y ) BOOST_DO_JOIN2(X,Y)                                ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:546:32: note: expanded from macro 'BOOST_DO_JOIN2'  #define BOOST_DO_JOIN2( X, Y ) X##Y                                 ^  <scratch space>:25:1: note: expanded from here  STATIC_WARNING_LINE148  ^  {code}  v1.60 is the current version so we should see if these warnings have been fixed in that version."
3,Prototype afw/AstroPy integration,"Investigate and prototype options for integrating {{afw}} with AstroPy. In particular, this epic focuses on establishing how tightly, if at all, AstroPy and {{afw::table}} should be coupled."
3,Transfer relevant HSC documentation to LSST,"Audit the HSC [questions site|http://hsca.ipmu.jp:8080/questions//] and [Pipeline How To|http://hsca.ipmu.jp/hscsphinx/] documentation. Identify the parts which are still relevant to LSST. Transfer them to the LSST documentation.    First assumption is that questions go to [clo|http://community.lsst.org] in an appropriate category, while the howto documentation is incorporated into the developer or user guides as appropriate. Confirm this with SQuaRE before starting work.    The intention is to carry out most of this work as a group in a ""dockathon"" session."
3,X16 Framework bucket,Catch all epic for emergent work in 02C.04.01.
3,Support for DM replanning process,Throughout the X16 cycle we expect to have to assign effort to support the ongoing DM replanning process. This work takes two forms:  * Tasks assigned by the DMLT Working Groups;  * Face-to-face discussions with other parts of the DM project.    Both are captured in this epic.
3,HSC port: data release verification,"Throughout S15 and W16 we have worked to merge changes from the Hyper Suprime-Cam stack to LSST. This work is now broadly complete, but requires acceptance testing by HSC. In support of that, the LSST stack must be brought to a level at which it is capable of reproducing the January 2016 HSC data release. This work is a continuation of the effort undertaken in DM-3628. It will reach a successful conclusion when the HSC project undertakes future development based on the LSST stack."
0,Cleanup jointcal,"Before we start digging into jointcal, it'd be good to get the whitespace/oldpython/indentation/lint/etc. questions sorted out. This ticket is for that."
3,Calibration Products Pipeline development in X16,Continued investigation and characterization of the DECam CBP data.
1,Make cluster deployment scripts more generic and enable ccqserv100...124,These scripts will be improved (i.e. more genericity) and integrated inside Qserv code. Qserv will be deployed on ccqserv100 to ccqserv125
1,Developer Guide Content & Maintenance Backlog Epic,General maintenance and original content for the DM Developer Guide (http://developer.lsst.io) based on needs during the cycle.
3,LSST the Docs Production Deployment,In DM-1139 we developed LSST the Docs. LSST the Docs is described in [SQR-006|http://sqr-006.lsst.io]. This epic will focus on the deployment of LSST the Docs as a reliable production service for documentation builds and hosting.
0,Re-enable CModel forced measurement on CCDs,"Recent changes from the HSC side (DM-4768) were implemented in a hurry, and break CModel forced measurement when the reference WCS is different from the measurement WCS (as is the case with forced measurement on CCDs).  This was considered an acceptable temporarily, since forced CCD measurement is currently severely limited by our lack of deblending, but we'll need to fix it eventually.    The fix is trivial from an algorithmic standpoint but may require a bit of refactoring (at least changing some function signatures; maybe more).    This should include re-enabling the different-WCS complexity in testCModelPlugins.py,"
0,Require fields listed in icSourceFieldsToCopy to be present,"{{CalibrateTask}} presently treats config field {{icSourceFieldsToCopy}} as a list of fields to copy *if present*. This was required because one of the standard fields to copy was usually missing. However, [~price] fixed that problem in DM-5385. Now we can raise an exception if any field listed is missing (though I propose to continue ignoring {{icSourceFieldsToCopy}} if isSourceCatalog is not provided)."
1,Rename datasets to utilize butler aliases,"Now that the butler has alias features that can allow for some degree of dataset substitutability, we should consider renaming (or adding aliases) for our existing datasets to make the naming consistent and analysis code more generic.    This work should be proceeded by an RFC with a proposal for the new names and a migration plan.    It *might* make sense to defer this until the high-level pipeline descriptions are more mature and we can choose relatively future-proof names, but hopefully the alias features will also make migration easy enough that this doesn't matter a lot."
1,Qserv do not return very same BLOB field than MySQL,"Enabling query {{qserv_testdata/datasets/case01/queries/0007.2_fetchSourceByObjIdSelectBLOB.sql.FIXME}} will reveal this bug.    Qserv chunk table contains next BLOB:  {code:bash}  mysql --socket /home/dev/qserv-run/git/var/lib/mysql/mysql.sock --user=root --password=changeme qservTest_case01_qserv -e ""select blobField from Source_6630 where SourceId=29809239313746172;"" > 29809239313746172.chunk6630    vi 29809239313746172.chunk6630    blobField    ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field(�q.\\�  {code}    But Qserv returns:  {code}  ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field(�q.�  {code}    See DM-991 for additional informations.  "
0,DecamIngestTask is mis-calling openRegistry,"`DecamIngestTask` is mis-calling `lsst.pipe.tasks.RegistryTask`. Line 59:    {code}  with self.register.openRegistry(args.butler, create=args.create, dryrun=args.dryrun) as registry:  {code}  {{openRegistry}} is expecting a directory name, not a butler object for the first argument    Thanks to [~wmwood-vasey] for diagnosing this."
1,Test new dipole fitting task on real data,"Test new task on real data (which data, TBD); inspect results by eye and compare with existing DipoleMeasurementTask output. This is necessary prior to incorporation into the imageDifference command-line task.    This test may also indicate that further optimizations are necessary (DM-5721)."
1,Incorporate new DipoleFitTask into imageDifference command-line task alongside existing DipoleMeasurementTask,"Incorporate the new task into the command-line task. The goal of this ticket is to implement DipoleFitTask along-side the existing DipoleMeasurementTask, eventually to replace it.    This is likely to have additional stories added, including testing, possibly as part of DM-5412."
0,Create buildable SuiteSparse external package,"To get jointcal to build in the stack, we need to satisfy the SuiteSparse dependency by creating an external package for SuiteSparse.    Assuming it builds cleanly, this should satisfy the remaining requirement of RFC-153, now that the licensing question has been answered there."
3,generation of conda binary packages for DM software products,"This epic covers work in generating binaries for stack releases. At this point we are persisting with the plan to produce conda binary packages for ease of use on the user side, though their reliable generation has so far resisted automation.    Conda binaries will be produced for the 12.0 Stack release. "
1,Ci Deploy and Distribution Improvements part IV,This is a bucket epic for ongoing improvements to the CI system
3,Release engineering Part Three,"This epic covers testing and co-ordination work associated with making  engineering and official releases, and code to support them.      (FE:8, DN:6.5, JS:8)"
0,ci_hsc fails test requiring >95% of PSF stars to be stars on the coadd,"Since the first week of March 2016, ci_hsc fails its test that requires that >95% of the PSF stars be identified as stars in the coadd.  I suspect this is related to the DM-4692 merge.    Here is a sample job that fails:  https://ci.lsst.codes/job/stack-os-matrix/9084/label=centos-6/console    The relevant snippet of the failure is:    {code}  [2016-03-10T17:12:06.667778Z] : Validating dataset measureCoaddSources_config for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:06.697383Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:06.697615Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:07.716310Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:07.716443Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:08.663566Z] : measureCoaddSources_config exists: PASS  [2016-03-10T17:12:08.721051Z] : measureCoaddSources_config readable (<class 'lsst.pipe.tasks.multiBand.MeasureMergedCoaddSourcesConfig'>): PASS  [2016-03-10T17:12:08.721077Z] : Validating dataset measureCoaddSources_metadata for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721249Z] : measureCoaddSources_metadata exists: PASS  [2016-03-10T17:12:08.721663Z] : measureCoaddSources_metadata readable (<class 'lsst.daf.base.baseLib.PropertySet'>): PASS  [2016-03-10T17:12:08.721715Z] : Validating dataset deepCoadd_meas_schema for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721878Z] : deepCoadd_meas_schema exists: PASS  [2016-03-10T17:12:08.726703Z] : deepCoadd_meas_schema readable (<class 'lsst.afw.table.tableLib.SourceCatalog'>): PASS  [2016-03-10T17:12:08.726834Z] : Validating source output for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:10.203469Z] : Number of sources (7595 > 100): PASS  [2016-03-10T17:12:10.204166Z] : calib_psfCandidate field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.204772Z] : calib_psfUsed field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.205468Z] : Aperture correction fields for base_PsfFlux are present.: PASS  [2016-03-10T17:12:10.206159Z] : Aperture correction fields for base_GaussianFlux are present.: PASS  [2016-03-10T17:12:10.207193Z]  FATAL: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0): FAIL  [2016-03-10T17:12:10.207455Z] scons: *** [.scons/measure-HSC-R] AssertionError : Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.207481Z] Traceback (most recent call last):  [2016-03-10T17:12:10.207525Z]   File ""/home/build0/lsstsw/stack/Linux64/scons/2.3.5/lib/scons/SCons/Action.py"", line 1063, in execute  [2016-03-10T17:12:10.207556Z]     result = self.execfunction(target=target, source=rsources, env=env)  [2016-03-10T17:12:10.207593Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 133, in scons  [2016-03-10T17:12:10.207611Z]     return self.run(*args, **kwargs)  [2016-03-10T17:12:10.207646Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 122, in run  [2016-03-10T17:12:10.207663Z]     self.validateSources(dataId)  [2016-03-10T17:12:10.207732Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 191, in validateSources  [2016-03-10T17:12:10.207749Z]     0.95*psfStars.sum()  [2016-03-10T17:12:10.207786Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 52, in assertGreater  [2016-03-10T17:12:10.207816Z]     self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [2016-03-10T17:12:10.207853Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 43, in assertTrue  [2016-03-10T17:12:10.207877Z]     raise AssertionError(""Failed test: %s"" % description)  [2016-03-10T17:12:10.207919Z] AssertionError: Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.209935Z] scons: building terminated because of errors.  {code}    This is the test that fails    https://github.com/lsst/ci_hsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#L169  {code}  class MeasureValidation(Validation):      _datasets = [""measureCoaddSources_config"", ""measureCoaddSources_metadata"", ""deepCoadd_meas_schema""]      _sourceDataset = ""deepCoadd_meas""      _matchDataset = ""deepCoadd_srcMatch""        def validateSources(self, dataId):          catalog = Validation.validateSources(self, dataId)          self.assertTrue(""calib_psfCandidate field exists in deepCoadd_meas catalog"",                          ""calib_psfCandidate"" in catalog.schema)          self.assertTrue(""calib_psfUsed field exists in deepCoadd_meas catalog"",                          ""calib_psfUsed"" in catalog.schema)          self.checkApertureCorrections(catalog)          # Check that at least 95% of the stars we used to model the PSF end up classified as stars          # on the coadd.  We certainly need much more purity than that to build good PSF models, but          # this should verify that flag propagation, aperture correction, and extendendess are all          # running and configured reasonably (but it may not be sensitive enough to detect subtle          # bugs).          psfStars = catalog.get(""calib_psfUsed"")          extStars = catalog.get(""base_ClassificationExtendedness_value"") < 0.5          self.assertGreater(              ""95% of sources used to build the PSF are classified as stars on the coadd"",              numpy.logical_and(extStars, psfStars).sum(),              0.95*psfStars.sum()          )  {code}    Note that the assertion failure messages is a bit confusing.  It should say  ""Fewer than 95% of the sources used to build the PSF are classified as stars on the coadd."""
3,Integration and test monitoring implementation Part I,"Configure, develop and deploy an ELK system.    High level requirements:  * es.lsst.codes - An current version Elasticsearch cluster.  * collect.lsst.codes - A server with multiple services to collect and aggregate logging and messages.  * logging.lsst.codes - A server with Kibana hooked up between.  * Packer and ansible deploys to create artifacts and deploys on Nebula and AWS. Optionally Docker.  * Use ELK to monitor git-lfs and our CI system.        "
3,Understand and test real space extension for ZOGY,The ZOGY algorithm (http://arxiv.org/abs/1601.02655) can be implemented as a real space extension of A&L.
3,Documentation,We are reserving time this cycle for people to contribute to architecture and documentation efforts.
0,Switch PropagateVisitFlags to use src instead of icSrc,"On DM-5084 [~jbosch] switched PropagateVisitFlags to match against icSrc instead of src because we weren't yet matching `icSrc` to `src` in ProcessCcdTask.  That's now been done on DM-4692, so we can revert this.    After doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives."
0,Provide an easy way to set Coord fields of a source catalog,"We sometimes need to set the coord fields of a source catalog, e.g. when fitting a new WCS or when studying an `icSrc` catalog (whose Coord field is not set). It would be nice to have a central, easily found way to do this. Right now we have the following as a static method of `TanSipWcsTask`, which works fine but is in a poor location:    {code}      def updateSourceCoords(wcs, sourceList):          """"""Update coords in a collection of sources, given a WCS          """"""          if len(sourceList) < 1:              return          schema = sourceList[1].schema          srcCoordKey = afwTable.CoordKey(schema[""coord""])          for src in sourceList:              src.set(srcCoordKey, wcs.pixelToSky(src.getCentroid()))  {code}    The other direction is also useful for reference catalogs, though from a practical standpoint the only user is probably `meas_astrom`. Even so, I suggest that this be made publicly available in the same way. Again, this is presently a static method of `FitTanSipWcsTask`:    {code}      def updateRefCentroids(wcs, refList):          """"""Update centroids in a collection of reference objects, given a WCS          """"""          if len(refList) < 1:              return          schema = refList[0].schema          coordKey = afwTable.CoordKey(schema[""coord""])          centroidKey = afwTable.Point2DKey(schema[""centroid""])          for refObj in refList:              refObj.set(centroidKey, wcs.skyToPixel(refObj.get(coordKey)))  {code}    I hope this can remain Python code, but admit that the extra speed of C++ might come in handy in some cases. In any case, once the function is in a central location we can implement it in C++ if we find the need."
3,Participate in X16 DMLT Working Groups,"Bosch, Lupton & Swinbank are members of [DMLT Working Groups|https://confluence.lsstcorp.org/display/DM/DMLT+Working+Groups] during X16. This epic captures work related to the activities of those groups."
0,SingleFrameVariancePlugin can give numpy warnings,"SingleFrameVariancePlugin can produce the following numpy warning, with no hint as to where the problem is coming from:  {code}  /Users/rowen/UW/LSST/lsstsw/miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  {code}  I tracked it down by adding the following code to the calling code:  {code}  import warnings  with warnings.catch_warnings():      warnings.filterwarnings('error')  {code}    It would be nice if the measurement plugin handled this situation more gracefully, such as turning the warning into an exception or testing for it and handling it.    One way to reproduce this problem is to run {{tests/testProcessCcd.py}} in {{pipe_tasks}}. However, it is commonly seen when running {{processCcd}} on other data, as well."
0,ObjectSizeStarSelector can produce numpy warnings,"`ObjectSizeStarSelector` can produce the following numpy warning:   {code}  RuntimeWarning: invalid value encountered in less  {code}  This occurs at the following point in the code:  {code}          for i in range(nCluster):              # Only compute func if some points are available; otherwise, default to NaN.              pointsInCluster = (clusterId == i)              if numpy.any(pointsInCluster):                  centers[i] = func(yvec[pointsInCluster])  {code}  where `func` has been assigned to `numpy.mean`. When I have seen this occur I have found that `dist` is an array of `nan`    I suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. If logging a message would be helpful, then please do that (and if RFC-154 is adopted, a log will be available).    One way to reproduce this is to run `tests/testProcessCcd.py` in `pipe_tasks`. However, I often see it when running `processCcd.py` on other data, as well."
2,Tune and improve ngmix MCMC sampling,Improve the ngmix MCMC sampling plugin to get it working well on most sources.  This may require actually contacting Erin Sheldon and getting his help (but he's quite eager to help).
1,Changes to galaxy_shear_experiments Python code,"This ticket describes changes which were made to the test runner and analysis scripts during the Dec 2015 - Feb 2016 period.  Most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible.    The large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules.    Since these changes do not pertain to any single test (though many were done during Dm-1136), I have put them on a separate ticket."
2,Add SFM plugin for ngmix fitting,Add an SFM pluggin for ngmix fitting using one of the simple fitters in ngmix/fitting.py.    This should depend on DM-5429 (or a suitably configured modelfit_ShapeletPsfApprox) for approximating the PSF as a sum of Gaussians.    Testing and tuning this algorithm to get it working well should be deferred to another issue.
2,Ensure that new object loads are added to secondary index,"L3 users might be generating new objectIds that are not in the DR Object table. If that is the case, ad-hoc L3 analysis would be triggering updates to secondary index.  It is possible that L3 users will be bringing data from other surveys and might partition it DR-way and cross match.  We need some mechanism to mark the secondary index dirty when something new gets added to the Object table, and trigger refresh."
1,Provide a shared stack on lsst-dev & other relevant systems,"Following the discussion in RFC-156, ensure that a documented, fast, easy to initialize shared stack is available for developers to use on shared systems, certainly to include {{lsst-dev}}."
0,Create unit test for ip_isr fallbackfilter,DM-5287 introduced a configuration option that allows specifying a fallback filter in the event that getting a specific butler product fails. Currently there is no test for this functionality. One should be created which tests all the logical paths. This may involve just adapting or mimicking another test that already exists.
0,Move tests/negative.py from meas_algorithms to meas_base,"Porting code from HSC to LSST brought over a unit test into meas_algorithms for functionality that exists in meas_base in LSST. This is due to the refactoring of code into meas_base on the LSST some while ago. This unit test currently runs with code from meas_algorithms, which means it can not simply be moved, as meas_base comes before meas_algorithms in the build order. This work may involve rewriting the unit test to use different code, or evaluating if it is worth bringing that functionality to meas_base along with the test. The code in question is the detection task."
1,Data Backbone ConOps,Data backbone first edit : 1pt (week 1)  Data backbone second edit : 1 pt (week 2)  Third edition : 1 pt week 3&4
3,Astropy integration with LSST DM Software,Work covering the investigation of how to integrate Astropy into the LSST DM software stack.
2,Compare Astropy and LSST functionality,This story will examine the overlap between Astropy and AFW and examine different approaches that could be taken to integrate Astropy into the DM software.
3,Write Report on Astropy integration proposals,A report is to be written on the Astropy integration plan. This report will be in the form of an SPIE paper.
3,"Convert GWT code to pure JavaScript (X16, part3 visualization)",This Epic is for the remaining effort in Extra 2016 cycle related to Firefly visualization coed conversion from GWT to pure JavaScript. 
0,Add scipy as a stack dependency,Adding scipy as a stack dependency is still a nebulous term to me.  David is going to follow up on how to do this exactly (it's already in conda_packages.txt).
1,Write technical note describing galaxy shear fitting experiments,"Through S15 (DM-1108) and W16 (DM-3561), [~pgee] has conducted a large-scale investigation into galaxy shear fitting. Please summarize the motivation, methodology and results of this study as a [technical note|http://sqr-000.lsst.io/en/master/]."
1,Familiarization with ngmix codebase,Download the ngmix codebase from https://github.com/esheldon/ngmix. Install it and its dependencies in the same environment as the LSST stack. Experiment with using it and understanding how it works
3,Convert GWT code to pure JavaScript (F16),The remaining work for converting GWT code to pure JavaScript
3,Visualization algorithm related research (S17),We have some algorithm related issues that need some research time. 
1,inter team discussion (X16),This epic is reserved for inter team discussion and supply/collect input to/from other teams.
2,create support in Butler for multiple repositories,"We need to be able to find repositories based on criteria such as version, validity date, etc.  This story is to provide support & proof of concept that demonstrates this."
3,Implement experimental DCR correction,"After the discussion about DCR, a few avenues for dealing with DCR were enumerated.  It was found that using imaging to model the DCR could be a very fruitful approach.  Nate Lust has suggested an algorithm that performs well in simplified, one-dimensional systems.    This epic is to extend this algorithm to 2-dimensions and add realistic SEDs, bandpasses, etc.  The result will be an implementation of the algorithm applied to the simulated data.  With measurements of how well it corrects for DCR in the context of image differencing."
1,Adapt LTD Mason for Single-package doc builds on Travis CI,"LTD Mason was originally intended to build docs for DM’s Eups-based packages from our Jenkins CI/CD servers. There is tremendous value in consolidating all of DM’s Sphinx-based documentation deployments to use LSST the Docs rather than Read the Docs. This ticket will design and implement adaptations to LTD Mason to build single repo doc projects (Technotes, Design Documents, the Developer Guide, and even generic software projects) from a Travis CI environment. Also includes a template {{.travis.yml}} and associated documentation to allow others to enable travis builds for their documentation.    We name Travis specifically because it is the easiest platform for implementing CI for generic open source projects."
1,Update SQR-006 LSST the Docs technote to reflect deployment in DM-5404,This ticket will ensure that [SQR-006|http://sqr-006.lsst.io] reflects the LSST the Docs continuous delivery platform as it is deployed in the DM-5404 epic. (SQR-006 was initially written as a planning/design document).    This story should be closed only once the DM-5404 epic is ready to be closed.
1,Add non-linearity correction to ISR task,"Implement RFC-164    At the moment some preliminary code is on ticket branches, but this need to be redone once the RFC is finished."
0,Don't restore the mask in CharacterizeImageTask.characterize,CharacterizeImageTask.characterize presently restores the mask from a deep copy for each iteration of the loop to compute PSF. This is unnecessary because repair and detection both clear the relevant mask planes before setting new values.
2,Finish getting obs_decam ISR working with CBP data,"Success criteria:    * Flats should be totally flat, i.e. bias jump problem fixed everywhere, amplifier levels fixed (both of these are currently hit & miss at the moment).  * CRs should be properly interpolated over for non-sky images (as this means no PSF estimate).   * Use un-binned images to confirm that bad pixel masks are correct everywhere.  "
3,S17 Qserv Refactoring,"Refactoring of Qserv as found necessary in F16. Specific tasks will be added during F16, and will include bug fixes, fixing major deficiencies discovered during F16, and keeping Qserv code up-to-date (latest compilers, supported OSes, security and alike). The scope of the work is limited by the number of story points assigned to this epic. "
1,Develop C++ code for experimenting with Python binding,Produce a small C++ codebase that can be used for experimenting with the various technologies we can be used for exposing C++ to Python. It should enable us to experiment with as many of the potential pain points with these technologies as possible
2,Wrap example C++ code with Cython,"Take the example C++ codebase developed in DM-5470, and expose it to Python in the most idiomatic possible way using Cython. Produce a [technical note|http://sqr-000.lsst.io] describing how this was carried out and discussing any particular pain points either in implementation or results."
1,Update meas_mosaic for compatibility with new single frame processing,"Following [recent changes to single frame processing|https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581], {{icSrc}} no longer includes celestial coordinates and {{icMatch}} is no longer being written. {{meas_mosaic}} requires this information. Provide a work-around."
0,Jenkins/ci_hsc failure: 'base_PixelFlags_flag_clipped' already present in schema,"Since 15 March, the {{ci_hsc}} build in Jenkins has been failing as follows:    {code}  [2016-03-16T14:23:13.548928Z] Traceback (most recent call last):  [2016-03-16T14:23:13.548956Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/bin/measureCoaddSources.py"", line 3, in <module>  [2016-03-16T14:23:13.548969Z]     MeasureMergedCoaddSourcesTask.parseAndRun()  [2016-03-16T14:23:13.548999Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun  [2016-03-16T14:23:13.549011Z]     resultList = taskRunner.run(parsedCmd)  [2016-03-16T14:23:13.549040Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run  [2016-03-16T14:23:13.549048Z]     if self.precall(parsedCmd):  [2016-03-16T14:23:13.549076Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall  [2016-03-16T14:23:13.549087Z]     task = self.makeTask(parsedCmd=parsedCmd)  [2016-03-16T14:23:13.549115Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 369, in makeTask  [2016-03-16T14:23:13.549132Z]     return self.TaskClass(config=self.config, log=self.log, butler=butler)  [2016-03-16T14:23:13.549160Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/python/lsst/pipe/tasks/multiBand.py"", line 1008, in __init__  [2016-03-16T14:23:13.549179Z]     self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)  [2016-03-16T14:23:13.549206Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/task.py"", line 226, in makeSubtask  [2016-03-16T14:23:13.549846Z]     subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)  [2016-03-16T14:23:13.549901Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0+1/python/lsst/pex/config/configurableField.py"", line 77, in apply  [2016-03-16T14:23:13.549915Z]     return self.target(*args, config=self.value, **kw)  [2016-03-16T14:23:13.549943Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/sfm.py"", line 248, in __init__  [2016-03-16T14:23:13.549954Z]     self.initializePlugins(schema=self.schema)  [2016-03-16T14:23:13.549985Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins  [2016-03-16T14:23:13.550004Z]     self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)  [2016-03-16T14:23:13.550032Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 15, in __init__  [2016-03-16T14:23:13.550616Z]     self.cpp = self.factory(config, name, schema, metadata)  [2016-03-16T14:23:13.550647Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 223, in factory  [2016-03-16T14:23:13.550660Z]     return AlgClass(config.makeControl(), name, schema)  [2016-03-16T14:23:13.550688Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseLib.py"", line 3401, in __init__  [2016-03-16T14:23:13.552891Z]     this = _baseLib.new_PixelFlagsAlgorithm(*args)  [2016-03-16T14:23:13.552924Z] lsst.pex.exceptions.wrappers.InvalidParameterError:   [2016-03-16T14:23:13.552967Z]   File ""src/table/Schema.cc"", line 563, in lsst::afw::table::Key<lsst::afw::table::Flag> lsst::afw::table::detail::SchemaImpl::addField(const lsst::afw::table::Field<lsst::afw::table::Flag>&, bool)  [2016-03-16T14:23:13.552986Z]     Field with name 'base_PixelFlags_flag_clipped' already present in schema. {0}  [2016-03-16T14:23:13.553012Z] lsst::pex::exceptions::InvalidParameterError: 'Field with name 'base_PixelFlags_flag_clipped' already present in schema.'  [2016-03-16T14:23:13.553014Z]   [2016-03-16T14:23:13.613484Z] scons: *** [.scons/measure] Error 1  [2016-03-16T14:23:13.617577Z] scons: building terminated because of errors.  {code}    Please fix it."
0,Bugs in obs_subaru found by PyFlakes,"I ran pyflakes on the code in obs_subaru and found a few bugs (beyond a few trivial ones that I am fixing as part of DM-5462)    {{ingest.py}} has undefined name {{day0}}    {{ccdTesting.py}} has at least three undefined variables: {{x}}, {{y}} and {{vig}} in the following:  {code}      ngood += pupilImage[y[good], x[good]].sum()    vig[i] = float(ngood)  {code}    {{crosstalkYagi.py}} has many undefined names, starting with {{makeList}}, {{estimateCoeffs}}"
1,"Document investigation of logging, monitoring and metrics technologies and architecture",Finish technote SQR-007. Related to DM-4970
1,Revise FlagHandler," {{FlagHandler}} is ""unpolished ... and a bit dangerous to the unwary"" (DM-5247).  It could be improved by leveraging C++11 features, replacing the default constructor with something that defines the (required) general failure flag, and allowing flags to be added individually.    A potential starting point is [here|https://jira.lsstcorp.org/browse/DM-5247?focusedCommentId=45894&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-45894]."
1,Firefly support for Camera team visualization needs (X16),"Attend the weekly meeting with the camera team and UIUC development team, provide support in discussion and API usage. "
3,Write script to derive and collate QA metrics from data repository of processed data,I wrote a python script using stack components to derive QA metrics and collate other QA-relevant information for a data repository of processed data.  This is currently output to a CSV file that can be loaded into a SQL database.
1,Wrote script to print the names of all visits that overlap a patch,In order to finish the IDL workflow module for makeCoaddTempExp I needed a program to say which visits overlap a given path.  That's what this script does.
3,Processing of COSMOS data - Part II,"Continued work on processing and QA work on the COSMOS verification dataset.  Running processCcDecam, making diagnostic plots, and nvestigating the results.  Most recently I've  reprocessed the COSMOS data through processCcdDecam using SDSS as the astrometric and photometric reference catalog and am redoing the QA work on those results."
2,Write software to match up and combine data for sources in a processed data repository,"In order to fully check the outputs of the processed COSMOS data I needed to combine the information on sources from multiple visits.  This code (written in IDL for now) matching up sources astrometrically across different visits, combines all of the information on separate detections, and measures average quantities (phot and astrom) for unique sources.  The information is then output into four binary FITS files."
1,Write presentation on verification datasets for AAS,Prepared and gave a talk at the NSF booth at the Florida AAS meeting on the progress of the verification datasets effort.
1,Work on script to test the astrometric matcher,"We encouraged astrometric matching problems for the Bulge verification dataset.  Therefore, I wrote a script that tests the matcher by systematically shifting the coordinates of one sets of the data to see if the matcher still works.  It worked well until ~80 arcsec."
0,SdssMapper.paf has wrong python type for processCcd_config,[~npease] reports that {{Sdssmapper.paf}} has the wrong python data type for the dataset {{processCcd_config}}: it is {{lsst.obs.sdss.processCcdSdss.ProcessCcdSdssConfig}} instead of {{lsst.pipe.tasks.processCcd.ProcessCcdConfig}}
0,Work on plan to test specific algorithmic components of the stack,"After working on a script to test the astrometric matcher, I decided to put together a plan to run similar tests on our algorithmic code.  The rough plan is here:  https://confluence.lsstcorp.org/display/SQRE/Stack+Testing+Plan"
0,"Work on putting together page of ""tips and tricks for using the stack""","Due to the incomplete state of the stack documentation and tutorials, I decided to write down various ""tips and tricks"" for using the stack as I learn them.  https://confluence.lsstcorp.org/display/SQRE/Tips+and+Tricks+for+using+the+Stack"
0,Revise operations concept for Observation Processing System,"Turn the L1 ConOps document into appropriate sections of LDM-230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.     (Story points are for KTL drafting and initial contributions)"
1,Field group updates,"After some work we have realized that the following needs to be done to field groups:    * Tabs group should have a field group smart wrapper component  * field group needs to reinit on id change   * remove mixin, use Higher-Order Components instead  * support a function for a value, this function will return a value or a promise  * hidden fields - init field group with key/value object  * Sub-field groups? study only, unless it is easy to implement.  * maintain an option to keep unmount field value available  * determine if InitValue needs to be passed around  * passing fieldState around too much  * find reason for react warning every time popup is raised  * look at promise code make sure it is working the way we think  * if practical, remove all export default    FieldGroupConnector.  It is the high order component that replaces the mixin.   FieldGroupUtils.js:  (~line 33): The field value would be a function on the file upload case. Therefore the upload does not activate until validation. In the upload case the function would return a promise. However, It could return a value or an object with a value and a valid status. Now the value key of a field can contain a promise or function or primitive. The function can return a primitive, a promise, or an object with primitive and status.    fftools.js lines 102-158 you can see my experimenting with taking out the connector. It works fine and does eliminate one of the warning messages.    "
0,improvement of the north/east arrow on image,make the compass sticky when scroll the image
1,Develop operations concept for Batch Processing System,"Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the batch processing environment, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)"
1,Develop operations concept for Data Backbone,"Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Backbone that contains, manages, and provides access to the Science Data Archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)"
1,Develop operations concept for Data Access Processing System,"Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Access Processing System that manages L3 computing in and interfaces to the Data Access Center, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)"
1,Develop functional breakdown for Observation Processing System,"Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Observation Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)"
0,Develop functional breakdown for Batch Processing System,"Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Batch Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)"
0,Develop functional breakdown for Data Backbone,"Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Backbone, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)"
0,Develop functional breakdown for Data Access Center Processing System,"Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Access Center Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)"
3,Develop DPS-WG documents,Create documents needed to accomplish the goals of the DPS-WG.
0,Coordinate completion of operations concepts,Coordinate the creation of a new version of LDM-230 incorporating DPS-WG-generated operations concepts.
0,Coordinate completion of functional breakdowns,Coordinate the creation of a new version of LDM-148 incorporating DPS-WG-generated functional breakdowns.
3,Solve the metadata sanitization problem,"Applications need access to visit specific metadata: e.g. pointing, airmass, exposure length.  This information is typically carried around in a FITS header, but there are no conventions on spelling or even necessarily units of these metadata key, value pairs.  There needs to be a easy to use metadata sanitization process that allows data from many different systems to present a standardized interface to observation metadata to the algorithm code."
1,Collect usage of header metadata,"Collect a comprehensive set of exposure oriented metadata used by science code.  This should also include metadata that is not currently needed but that could be utilized in the future.  In practice, I suspect this will involve looking for all calls to PropertySet.get since that is how FITS header metadata is currently passed around."
2,Implement single interface to sanitized exposure metadata,"Currently metadata associated with exposures is accessed in a few different ways: through the Calib object, through the Wcs object, and through the metadata object.    In conjunction with DM-5502, which is to figure out what metadata is needed, this will provide a single interface to exposure oriented metadata.  One tricky thing is that the Calib object has some subset of the metadata we'll need in a sanitized form, but we won't want to have to remember where to look for metadata.  If we can't extend the Calib object to hold all sanitized metadata, we should create a new metadata object to store all sanitized metadata and remove the pieces from Calib that are currently there."
3,Verification via precursor datasets,This epic covers covers timeboxed investigative activities into processing of precursor datasets with the LSST stack. 
1,DAX & DB Docs (ABH),* Add memman documentation (in LDM-135)  * Refresh XRDSSI documentation (in LDM-135)
1,"Design Discussions (AndyS, March)",NULL
3,QA Tasks & Supertasks,"This epic covers stack-side work for the squash MVP (DM-5555)     (JS:8, MWV:14)"
1,alert production database next steps (April),Place-holder for additional alert production database work after investigate design task completes.  We should split this into smaller stories for a total of 18 points this cycle.
3,QA Tasks & Supertasks II,At this time this is epic is a bucket to keep track of backlog for validate_drp etc. 
0,"Design discussions (Brian, March)",NULL
1,"Design Discussions (John, March)",NULL
2,Validate shared scan implementation on IN2P3 cluster,NULL
0,prepare Slack RFC,    https://jira.lsstcorp.org/browse/RFC-140
1,"Design Discussions (Fritz, March)",NULL
1,"Design Discussions (Nate, March)",NULL
2,Create proposal & RFC for Butler API to define output dataset type,"this story represents a spike to  1. ad-hoc gathering of requirements to create butler API that allows a task to define an output dataset type 2. do any proof-of-concept mock up needed 3. write an RFC & gather input  then  A. If there is major dissent, create another design spike story or  B. Close the RFC, and green light work on DM-4180"
3,SQuaRE documentation & design documents,"This epic involves planning, design and usage documentation on SQuaRE products and services.      (JMP:4,FE:15,JS:6)"
2,SQuaSH design proposal document,  Document capturing situation as of beginning of X16 can be found at:    https://dmtn-016.lsst.io    Further extension is planned in F16 to cover X16 development as well as consequences of the LDM-151 rewrite.     
2,Python wrappers for sphgeom,This issue is a pre-req of DM-3472
1,Weekly and monthly releases,  Some manual process at the rate of 1 SP / month is still involved in the releases until the automating publishing process is complete. 
0,Add documentation to BinnedWcs,"DM-5282 ported functionality from HSC to work in ""super-pixels"" which are the result of binning in wcs. This functionality was introduced in binnedWcs.(cc/h). This functionality needs proper doxygen documentation added."
3,Documentation of Firefly functions and API (F16),We are concentrating on the coding in X16. This epic will be capture the effort to write the document for using Firefly functions and API. 
1,Change star selectors to return stars instead of PSF candidates,"Implement RFC-154:  - Make star selectors tasks, but continue to use and prefer a registry  - Add an abstract base class for star selectors with the following methods:    - {{selectStars}} abstract method that takes a catalog of sources and returns a {{lsst.pipe.base.Struct}} containing a catalog of stars    - {{run}} concrete method that takes a catalog of sources and an optional name of a flag field, calls {{selectStars}} to select stars, then sets the flag field (if given) for stars    - {{makePsfCandidates}} make a list of psf candidates from a catalog of stars (does no selection, other than skipping stars that cannot be made into candidates, and logging the rejects)  "
2,Add HTM indexing to sphgeom,"To include Python wrappers, in support of DM-5052"
1,"Design Discussions (Fritz, April)",NULL
1,"Design Discussions (Fritz, May)",NULL
1,"DAX & DB Docs (Fritz, April)",NULL
1,"DAX & DB Docs (Fritz, May)",NULL
1,Finish data distribution prototype (April),NULL
1,Finish data distribution prototype (May),NULL
1,"Design Discussions (John, April)",NULL
1,"Design Discussions (John, May)",NULL
0,AFW rgb.py has undefined variable that breaks a test in some situations,"The {{rgb.py}} test is failing for me with current AFW master:  {code}  tests/rgb.py  .E............  ======================================================================  ERROR: testMakeRGBResize (__main__.RgbTestCase)  Test the function that does it all, including rescaling  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/rgb.py"", line 313, in testMakeRGBResize      with Tempfile(fileName, remove=True):  NameError: global name 'Tempfile' is not defined    ----------------------------------------------------------------------  Ran 16 tests in 7.296s    FAILED (errors=1)  {code}    {{Tempfile}} is definitely only used in line 313. It was introduced with commit c9864f49.    I'm not entirely sure how this is not picked up by Jenkins as the test will run if matplotlib and scipy are installed and Jenkins does have those.    "
1,"Design Discussions (AndyS, April)",NULL
1,"Design Discussions (AndyS, May)",NULL
1,Alert production database next steps (May),NULL
0,"Design Discussions (Brian, April)",NULL
0,"Design Discussions (Brian, May)",NULL
2,persistence improvements to butler config system,"requirements:  - easy to know what to provide  - fails fast  - has a way to be backward compatible with persisted configs & existing scripts    existing issues to fix:  - currently config creation is verbose, difficult to read, and difficult to format properly  - has the butler class hierarchy baked into the format"
1,"Design Discussions (Nate, April)",NULL
1,"Design Discussions (Nate, May)",NULL
0,Add renderer option to js table,"TablePanel and BasicTable now accept optional renderers.  For each column, you can set a custom renderer for the header, cell, or both.  Also, created several commonly used renderer for images, links, and input field."
1,Z-scale stretch for image display,The z-scale stretch in current system is different from the one in OPS
0,"Assist in document investigation of logging, monitoring and metrics technologies and architecture","Assist with tech note SQR-007 and document investigation of logging, monitoring and metrics technologies and architecture."
3,SQuaSH MVP,"This is an epic that covers setting up a minimally viable QA environment for executing processing, calculating metrics, storing them and displaying them. This serves a dual purpose:    - It results in a limited but still useful production service that developers can take advantage of  - It allows us to assess our initial technology stack for suitability for further development.     The test case was picked to be a supertask version of the tests described in DM-4730 (informally ci_lauren) used during the merging of the HSC fork. This was meant to also allow us to use and give feedback on the supertask architecture. When it became obvious that we would not take delivery of that infrastructure in time for X16 work, we switched the test case to one of the KPMs calculated in validate_drp. This switch does not affect the engineering aims of this prototype. The MVP based on validate_drp can eventually be extended to service other types of KPM measurement for regression testing and release characterisation.        (JH:32, JMP:16, JS:32, MVW:14, AF:20)      Outcome: MVP stood up on squash.lsst.codes. Currently collecting AM1, AM2 and PA1 KPMs using validate_drp on validation_data_cfht. After a short period of evaluation of the performance of the toolchains in production we will proceed with more data, more metrics and more features for F16.     "
3,SQuaRE ad-hoc developer requests,"This is a bucket epic for requests from developers/ science users that come up mid-cycle and cannot wait until the new cycle, security vulnerabilities, critical bug fixes, etc.     (JH:16,JMP:8,FE:3)"
1,Present Supertask design to DMLT,Present the Supertask design to the November 2015 DMLT in-person meeting.    Covers preparation of a presentation and related discussions preceding and immediately following the meeting.
0,Participate in October 2015 OCS-subsystems teleconference,"Prepare for, attend, and follow up on the OCS-subsystems teleconference on October 8, 2015."
2,Write tech note on modifications required to use py.test framework,"Following the investigatory work into switching our Python test files to be compliant with pytest, whilst still using {{unittest}}, a tech note needs to be written explaining the required changes."
1,"Participate in November 2015 OCS-subsystems teleconference (LSE-70, LSE-209)","Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic."
0,Participate in November 2015 OCS-subsystems teleconference (LSE-74),"Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic."
0,"Participate in December 2015 OCS-subsystems teleconference (LSE-70, LSE-209)","Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic."
0,Participate in December 2015 OCS-subsystems teleconference (LSE-74),"Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic."
1,"Review of LSE-70 and LSE-209 drafts, September 2015","Arrange, prepare for, and attend a joint call with the Camera team to review the end-of-summer-2015 drafts of LSE-70 and LSE-209 from the OCS group."
0,CCB review of LCR-567 (LSE-70) and LCR-568 (LSE-209),Review the LSE-70 and LSE-209 drafts submitted with change requests LCR-567 and LCR-568 in January 2016.
0,CCB review of LCR-603 (LSE-74),"Review LCR-603, ""LSE-74 document revision"""
1,"LSE-70, LSE-209 refinements X16",There are open LCRs for cleanups to the versions of LSE-70 and LSE-209 approved by the CCB in February 2016.  An initial teleconference will be held on 30 March 2016 with the OCS group to discuss these.
0,making PSF candidates should be simpler,"The code to make PSF candidates is too complicated and repeated in too many places (even after DM-5532). Every time lsst.meas.algorithms.makePsfCandidate is called (except in a few tests) it is called as follows:  {code}              cand = measAlg.makePsfCandidate(source, mi)              if cand.getWidth() == 0:                  cand.setBorderWidth(borderWidth)                  cand.setWidth(kernelSize + 2*borderWidth)                  cand.setHeight(kernelSize + 2*borderWidth)                im = cand.getMaskedImage().getImage()              max = afwMath.makeStatistics(im, afwMath.MAX).getValue()              if not numpy.isfinite(max):                  continue  {code}    This should to be centralized somewhere. I suggest adding this code to {{meas.algorithms.makePsfCandidate}} itself (which could delegate some work to a private function, if desired)."
1,Add Error and Working feedback to FITS visualizer,"* Add working message when plot is loading, (downloading..., plotting..., etc)  * Add error message when plot fails  * for multi-viewer remove and failed plot cells  * work out if image select panel should become visible again.  * Any thing else the is plotting feedback related"
0,Docgen draft from EA content for LSE-140,Create a docgen from the LSE-140 content in Enterprise Architect.
1,SQuaRE Communication and Publication Platforms Document and Presentation,[SQR-011|http://sqr-011.lsst.io] documents the various communication and publishing platforms that SQuaRE operates on behalf of DM. This ticket will complete v1 of the document (DM-4721 created a time-boxed first draft) and also include work to present the document to LSST management.
1,Support LCR-385,Support getting LCR-385 against LSE-78 through the CCB.
1,Create a reusable upload file component,This  component will upload and validate the file as part of the input's validation process.  It will return a token generated by the server which will resolve to the uploaded file if the upload success.   
1,SQuaRE Communication and Publication Platforms Document and Presentation - Clone,This is a clone of DM-5581 tracking [~frossie]'s SPs
1,Fix obs_decam butler level,"There is a bug in {{obs_decam/policy/DecamMapper.paf}}, causing some butler features for the ""visit"" level or above working incorrectly. The {{hdu}} key is irrelevant for the visit level or above, but wasn't included in the policy file.    Because of this bug, the {{DemoTask}} in {{ctrl_pool}} (ctrlPoolDemo.py) runs incorrectly with DECam data. It incorrectly treats dataRef with different {{hdu}}s as they are from different visits, hence reads each ccd image multiple times (61 times for one visit with 61 hdu). Instead, each ccd image should be read once.        Besides fixing the policy file, I also added an optional test that only runs if {{testdata_decam}} is set up. The part with level=""visit"" in the test fails without the ticket changes in the policy.    (p.s. The raw data file in {{testdata_decam}} is modified and has only 2 hdus.) "
1,Add lmfit package to the stack,"The current implementation of the new {{DipoleFitTask}} for {{ip_diffim}} uses {{lmfit}} to perform parameter estimation (least-squares minimization). {{lmfit}} is essentially an API on top of {{scipy}}'s optimizer, adding functionality such as parameter boxing (constraints) and improved estimates of parameter uncertainties. It would be nice to include this small, pure-python package in the stack rather than investigating and re-implementing the optimization using {{scipy}} or {{minuit2}} (which are the two optimizers that I know of that are in the stack already)."
0,Fix afw build issues with recent clang,"{{afw}} fails to build with recent versions of clang:    {code}  include/lsst/afw/image/MaskedImage.h:553:65: error: '_loc' is a protected member of 'lsst::afw::image::MaskedImage<unsigned short, unsigned short,        float>::MaskedImageLocatorBase<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >, Reference>'                                       const_VarianceLocator(iter._loc.template get<2>())  {code}  and issues with statistics.i so far, more errors may turn up as these are cleared.    These problems are apparent with {{Apple LLVM version 7.3.0 (clang-703.0.29)}} (as shipped with the latest release of XCode, hence this now becoming an issue) and {{clang version 3.8.0 (branches/release_38 262722)}} (a recent release from LLVM; note that Apple uses its own versioning scheme). {{clang version 3.7.1 (tags/RELEASE_371/final)}} is not affected."
3,Archive in a box v1 (F16),"Several times we were asked a question about Firefly: Great software. How could I use it for my data now?     We want to create a recipe and stubs of code (archive in a box) so others can take it, with minimal configuration changes and minimal customized data access code, to have a simple archive UI for their data. It will come with all the built-in images and catalogs access, all the image, catalog, and XY plot functions. "
0,fix issue where butler repository search returns list for single item,"Backwards compatible behavior is that when butler returns a single item, it is NOT in a list. A recent change (when the Repository class was added) broke this behavior.     Change it back so that if an operation in repository would return a list with a  single item, it pulls it from the list.    Note this is only related to the case where a repository's parentJoin field is set to 'outer' and since no one is using this yet (they should not be, anyway) then the point is moot.     "
1,Fix qserv service timeout issue,"After Qserv services have been running over ~couple of days, new queries fail and can also lead to a crash. Investigate and implement a solution."
0,daf_persistence build failure on OSX,"I see the following build failure in {{daf_persistence}} on OSX 10.11:  {code}  c++ -o python/lsst/daf/persistence/_persistenceLib.so -bundle -F/ -undefined suppress -flat_namespace -headerpad_max_install_names python/lsst/daf/persistence/persistenceLib_wrap.os -Llib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/mariadbclient/10.1.11-2-gd04d8b7/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_policy/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_logging/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/daf_base/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/utils/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_exceptions/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/base/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/boost/1.59.lsst5/lib -L/tmp/ssd/swinbank/shared_stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -ldl -lpython2.7  ld: file not found: libz.1.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  scons: *** [python/lsst/daf/persistence/_persistenceLib.so] Error 1  scons: building terminated because of errors.  {code}    This happens with the current master ({{3484020}} at time of writing), but also with a recent weekly ({{3878625}}). "
0,Remove obsolete install scripts from ~/src/qserv/admin/tools/,Internet-free install scripts are unused and should be removed with related documentation.
1,runQueries.py fails on IN2P3 cluster,"Launching runQueries.py produces some errors:  {code:bash}  fjammes@ccosvms0070:~/src/qserv/admin/tools/docker/deployment/in2p3 (tickets/DM-5402 *=)$ ./run-test-queries.sh  +--------------------+--------------------+  | ra                 | decl               |  +--------------------+--------------------+  | 29.308806347275485 | -86.30884046118973 |  +--------------------+--------------------+    real    1m20.725s  user    0m0.004s  sys     0m0.012s  Output directory: /afs/in2p3.fr/home/f/fjammes/runQueries_out  Exception in thread Thread-12:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-23:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (1105, '(proxy) all backends are down')    Exception in thread Thread-16:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-21:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (1105, '(proxy) all backends are down')    Exception in thread Thread-17:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-19:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-18:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")  {code}"
0,check & correct comparison operators in daf_persistence and daf_butlerUtils,"per comments in DM-5593, an incorrect comparison operator was found, that used {{is}} instead of {{==}} in a string comparison (e.g. {{var is 'left'}} which is incorrect, it should be {{var == 'left'}}.  This needs to be corrected in {{Repository}} (see DM-5593 for details), and the rest of daf_persistence and daf_butlerUtils should be checked for correct use of is vs. ==."
2,"plan and RFC for ""data repository based on version""","create way using repo of repos to get only the posix root in the returned cfg, update example/test code.  write an RFC, review with KT  post RFC & gather feedback.  incorporate feedback and/or create another design story if needed."
1,Investigate clang issues regarding friendship and protected members ,"In DM-5590, we worked around a problem in which clang 3.8 refused to access protected members of a cousin class given a friend declaration in the base. To our best understanding at time of writing, the code is valid: it seems possible that this is a bug in clang.    Investigate what went wrong, produce a minimal test case, and (if appropriate) report this as an upstream bug."
3,"x16 Operations Planning in LOPT, TOWG, and DM replanning (ConOps development)","Develop ConOps for DM system, including Bulk Batch System, Data Backbone, L3 Hosting, etc. Develop use cases for TOWG. Continued planning for operations, focusing on Data Processing and Products directorate.    Don Petravick, Margaret Gelman, Hsin-Fang Chiang, Stephen Pietrowicz, Jaggi Yedetore, Paul Wefel  "
1,x16 Joint Coordination Council,"Coordination with CC-IN2P3.    Don Petravick, Jason Alt  "
3,Design specification and requirements analysis of Bulk Batch System,"Functional breakdown of Bulk Batch System, including L2 processing, calibration processing, etc. Detailed design, plan, and schedule.    Don Petravick, Margaret Gelman, Jason Alt, Hsin-Fang Chiang, Stephen Pietrowicz, Rob Kooper, Paul Wefel"
3,Design specification and requirements analysis of Data Backbone,"Functional breakdown of Data Backbone. Detailed design, plan, and schedule.    Jason Alt, Don Petravick, Margaret Gelman, Paul Wefel"
3,x16 middlware/workflow package definition and development,Defining future middleware package to support science pipeline processing. Maintaining and adding to current middleware packages. Prototyping processing sequences with DECam data.    
1,x16 LSST Identity and Access Management Program development,NULL
3,Further requirements analysis of the L1 System,"Additional design specification of parts of the L1 system that we haven't looked at in detail yet, such as EFD replication, Observatory Operations Server, Auxiliary Telescope processing, telemetry processing, and Commissioning support.    Margaret Gelman, Stephen Pietrowicz, James Parsons, Paul Wefel  "
3,"WAN emulation testing, project 1","Project 1 of WAN emulation. See https://confluence.lsstcorp.org/display/JP/WAN+Emulator+test+plan+-+January+2016 for details.    James Parsons, Paul Wefel  "
3,L1 Basic Message Topology (x16),"Includes system status and message dictionary, main programs for all L1 entities, and message interaction between L1 entities."
3,Network and Service Monitoring (Comfort Console),This is the development and integration work required for architecting and building a Network monitoring center.
3,Procure FY16 capabilities,"Quotes, discussions with vendors, details specs of procurements.   "
3,Deploy FY16 Cluster Services,NULL
3,Migrate to distributed filesystem,NULL
3,Deploy FY16 Verification Cluster,NULL
3,Deploy FY16 Object Store Discovery Infrastructure,"Deploy infrastructure for FY16 object store evaluation.    Deliverable: Object Store discovery infrastructure  Staff: 3 NCSA ICI engineers (networking, storage, systems)  Effort: 20 days  Planned Start: 6/1/2016  Planned End: 6/30/2016"
3,x16 ISO Work,NULL
2,test run coaddDriver and multiBandDriver with DECam data,"Preparation work to learn about ctrl_pool and pipe_drivers packages.      - Install ctrl_pool and pipe_drivers packages on my OSX desktop (no slurm).   - Run the ctrl_pool mpiexec example to verify if the mpi is working.  - Obtain a HSC data repo from ci_hsc as sanity checks.   - Construct a small DECam data repo, using raw Stripe82 data consisting of two visits, one band, one patch.  - Run ctrlPoolDemo.py with the HSC data repo and then the DECam data repo.  - Run the pipe_drivers scripts {{coaddDriver}} and {{multiBandDriver}} with the HSC data repo and then the DECam data repo.  - All with the default batch system SMP to run on a single machine."
1,Add data products and config in obs_decam for multi-band processing,"Add necessary data products and default config in order to run forcedPhotCcd, coaddDriverTask, and multiBandDriverTask with DECam data. "
0,Python version checking in newinstall.sh is not quite right,There is a recent report on community where {{newinstall.sh}} reports that the python version is too old despite the user having a modern Anaconda python in their path.  In commit e6fc9ed2 the code was changed to check {{$PYTHON}} for version compatibility but that is not correct as the python that will be used for the actual build is the python in their path. {{$PYTHON}} is defined purely as the python to use for EUPS installation.    In the reported error {{$PYTHON}} was not set and their {{/usr/bin/python}} was too old. Confusingly the error message reporting the version problem actually reported the version information for the python in the path and not the {{$PYTHON}} python. The simple fix is to revert e6fc9ed2.    I already made significant comments on this topic in the original https://github.com/lsst/lsst/pull/19 but I really do have to insist on either reverting that PR or at least fixing the error messages to use a consistent python (I'd argue that this is still wrong but at least consistent). The current situation is at best confusing and at worst pointless and wrong.    The version test only makes sense if we are testing that the default python in the path is the correct version to build the stack. {{$PYTHON}} was originally designed to allow a different python to be used to build EUPS. Even that is no longer an issue as EUPS can work with Python >= 2.6 now.
0,Make file upload show feedback when file is uploading,NULL
0,Build a tool to automatically run autopep8 on LSST Stack,Develop a lsst-autopep8 command in [sqre-codekit|https://github.com/lsst-sqre/sqre-codekit] that can run [autopep8|https://github.com/hhatto/autopep8] in an automated fashion across all of the LSST Stack repositories according to the PEP 8 exceptions determined in RFC-162.
0,finish up afw.table to astropy.table view support,"At an LSST/AstroPy summit hack session, we've put together a functional system for viewing afw.table objects as astropy.table objects on branch u/jbosch/astropy-tables of afw and https://github.com/astropy/astropy/pull/4740.    Before merging, we should add support for ""object"" columns for subclasses to hold e.g. Footprints in SourceCatalog, and add some documentation.  We may also want to add a convenience method to return an astropy.table.Table directly."
1,use AstroPy-compliant strings for units in afw.table,"With DM-5641, we'll soon be able to get astropy.table views into afw.table objects.  That will be a bit more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected."
0,"add method to convert Property[Set,List] to nested dict","In interfacing with AstroPy it'd be useful to easily convert PropertySet and PropertyList to nested dict and OrderedDict (respectively), converting elements with multiple values to lists in the process."
0,Add fine-grained authorization to ltd-keeper users,"The initial MVP of ltd-keeper had all-or-nothing authentication; any user was effectively an admin user. It would be useful have fine grained roles that each API user could have (for example, one API user might be able to add a build, but not create an edition or product or add another user). The phases of this ticket at:    1. Design a set of roles that cover current functionality  2. Add these roles to the User DB model and user creation API  3. Authorize users against these roles in specific API calls"
2,Research & Design for object storage & transport factorization,"Start research and design proposals & prototyping on the butler back end factorization problem; need to be able to configure butler to put and get different object types, different storage formats, and different storage locations. Do intermediate KT reviews.  Initial design and stubbed implementation to include the following: *ExposureF* to/from *Fits* file on *local filesystem* (Posix) *ExposureF* to/from *Memory* *SourceCatalog* to/from *database* *SourceCatalog* to/from *Fits* on *local filesystem* (Posix) *SourceCatalog* to/from *Memory*"
1,Finish stubs and write role description for butler back end factorization,NULL
1,visit AP team and work on processing DECam data,"March 13-17, 2016. Work on various topics about processing DECam data:  - Improve documentations on processing raw DECam data, especially about the steps of ingesting calibration data  - Identify future work on improving processing raw data. Updates about Instrumental Signature Removal tasks.  - Learn how to run difference imaging pipeline with DECam data  - Try jointcal (Simultaneous Astrometry meas_simastrom package from IN2P3) with DECam data and identity necessary code changes for doing jointcal with DECam data  - Use the preliminary jointcal astrometry results to examine DECam data’s distortion  - Also more general discussions on data processing"
1,SUIT vision document,Writing down SUIT vision that the group has discussed and shaped in last year off and on.   SUIT will use it as guidance for system design.
0,run jenkins for PRs on all EUPS products - part I,NULL
0,Implement RFC-167,Implement RFC-167 for adding esutil to the stack.  This will be done in the same way as proposed to add scipy.
0,Reduce code duplication in StarSelectors,"Both {{ObjectSizeStarSelector}} and {{SecondMomentStarSelector}} have logic to transform measured moments from pixel coordinates to TAN_PIXELS in order to remove optical distortion.  That's generically useful for any star selector that works on measured moments, and we shouldn't have to repeat it everywhere it is used."
1,Improve Large Test Scale query script,This script is currently located in:   admin/tools/docker/deployment/in2p3/runQueries.py     Here's some improvments:    - use lsst/db instead of mysqlpython  - externalize queries and other parameters in a config file  - add an option to make script stop after a few queries (in order to have deterministic query results for Large Scale integration tests)  - any other minor improvments...
0,Update tables of packages that depend on scipy,"Now that the {{scipy}} package has been added (DM-5446), the table files of other packages to be fixed as soon as possible so that we have an idea of what is silently depending on {{scipy}}. These include {{afw}}, {{ip_diffim}}, {{meas_modelfit}}, {{mops_daymops}}, {{pipe_tasks}}, {{shapelet}} and {{sims_photUtils}}. Many of these are setupOptional that we should consider making mandatory. Some will be setupRequired."
1,multiple dialog are not working well together,When several dialogs are up together.  The most recently click one should be one top. When table are in the dialogs such a fits header view. The scroll bars will go over other dialogs. This needs some though and work.  Another thing- when a message dialog is show because of a dialog error. It should center on the dialog.  Update- I don't think I will do the error centering now.  I am going to leave that and see if it is a real problem.
1,Add motivated model fits to validate_drp  photometric and astrometric scatter/repeatability analysis and plots,"Implement well-motivated theoretical fits to the astrometric and photometric performance measurements based on derivations from LSST Overview paper.  http://arxiv.org/pdf/0805.2366v4.pdf    Photometric errors described by  Eq. 5  sigma_rand^2 = (0.039 - gamma) * x + gamma * x^2  [mag^2]  where x = 10^(0.4*(m-m_5))    Eq. 4  sigma_1^2 = sigma_sys^2 + sigma_rand^2    Astrometric Errors   error = C * theta / SNR    Based on helpful comments from [~zivezic]    {quote}  I think eq. 5 from the overview paper (with gamma = 0.039 and m5 = 24.35; the former I assumed and the latter I got from the value of your  analytic fit that gives err=0.2 mag) would be a much better fit than the adopted function for mag < 21 (and it is derived from first principles).  Actually, if you fit for the systematic term (eq. 4) and gamma and m5, it would be a nice check whether there is any “weird” behavior in  analyzed data (and you get the limiting depth, m5, even if you don’t go all the way to the faint end).     Similarly, for the astrometric random errors, we’d expect        error = C * theta / SNR,    where theta is the seeing (or a fit parameter), SNR is the photometric SNR (i.e. 1/err in mag), and C ~ 1 (empirically, and 0.6 for the idealized maximum likelihood solution and gaussian seeing).   {quote}"
0,Config override fixes needed due to new star selector,"As of DM-5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal CI, which is how I missed this)."
0,"Organize HSC docs ""hackathon""",Liase with SQuaRE to determine the most effective way to transfer HSC docs to LSST. Organize a hackathon session for DRP developers at which we get this done. Bring doughnuts.
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.  
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.
0,Take part in HSC docs hacakthon,Participate in HSC docs transfer hackathon.
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.  
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.  
0,Take part in HSC docs hackathon,Participate in HSC docs transfer hackathon.
2,Prepare detailed L2 plan,"At the scipi-wg meeting of 23 & 24 March 2017, [~jbosch] presented an overview of his plans for L2 processing. The next step is to refine those plans and prepare a more detailed ""deep-dive"" discussion of the L2 plans."
0,Cannot enable shapeHSM because RegistryField fails validation,"When running ci_hsc after setting-up the meas_extensions_shapeHSM, meas_extensions_photometryKron and dependencies using setup -v -r . in the respective cloned folders, I get  {code}  Cannot enable shapeHSM (RegistryField 'calibrate.detectAndMeasure.measurement.plugins' failed validation: Unknown key 'ext_shapeHSM_HsmMoments' in Registry/ConfigChoiceField  For more information read the Field definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/pex_config/2016_01.0+3/python/lsst/pex/config/registry.py"", line 179, in __init__      ConfigChoiceField.__init__(self, doc, types, default, optional, multi)  And the Config definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/meas_base/2016_01.0-13-g779ee14/python/lsst/meas/base/sfm.py"", line 109, in <module>      class SingleFrameMeasurementConfig(BaseMeasurementConfig):  ): disabling HSM shape measurements  {code}  Find out why this is happening and find a fix  "
2,Wrap example C++ code with pybind11,Same as DM-5471 but using pybind11
2,Wrap example code with cffi,"As per DM-5471, but using cffi."
0,Provide single-visit processing capability as required by HSC,"In DM-3368, we provided a means of running multiple processCcd tasks across an exposure, but without performing global calibration etc as provided by HSC's ProcessExposureTask.    Please augment this with whatever additional capability is required to enable HSC data release processing."
1,processCcd.py is failing on some CFHT u band images,"processCcd.py is failing on some u band CFHT data, as reported by [~boutigny] on c.l.o: https://community.lsst.org/t/testing-dm-4692-the-new-processccdtask/507/24    See that posting for sample data to reproduce the problem."
0,Accommodate pixel padding when unpersisting reference catalog matches,"The reference object loader in {{meas_algorithm}}'s *loadReferenceObjects.py* grows the bbox by the config parameter pixelMargin:  doc = ""Padding to add to 4 all edges of the bounding box (pixels)"" . This is set to 50 by default but is not reflected by the radius parameter set in the metadata, so some matches may reside outside the circle searched within this radius. This increase needs to be reflected in the radius set in the metadata fed into {{joinMatchListWithCatalog()}}.  "
1,Table performance on Firefly,"Table seems to perform poorly on Firefox. Firefox gets into the complete refresh state only when the table is visible with charts only, fits view only or fits view and chart it does not happen    Helpful article: http://benchling.engineering/performance-engineering-with-react/    changelog:  - added react performance tools, React.addons.Perf  - fix some performance issues:    - skip render of selection boxes when not needed.    - skip rendering of xyplot options when not needed.    - skip wasted render called for table cell and headers.    - will do a more in depth investigation in another ticket.  - refactor table code and it's state.    - move all table related states into table_space.    - create sub reducers for each data domain    - rename and move functions to better describe what it's doing   - added 'title' to table.  - show mask while loading    "
0,Table needs to fire another action when data completely loaded,When the data for a table is completely loaded fire another action such as TABLE_NEW_LOADED_DONE. This way the xyplots and the image overlays know to go fetch the data.    4/22/2026 from the pull request:  added new action TABLE_NEW_LOADED to table; fired when table is completely loaded.  added table error handling.  fix active table not updating after an active tab is removed.
3,AP Emergent work -- F16,There is emergent work that comes up as a side effect of other work.  This epic will capture that effort in F16.
1,Connect CatSim to StarFast simulation tool,"CatSim can provide a fully realistic simulated catalog, which StarFast could use as an input for simulations. This ticket includes writing the code to connect to the CatSim database and updating the internal catalog format in StarFast to be compatible with CatSim."
1,Write StarFast interface to ProcessCCD,"The simulated images generated by StarFast need to be able to be run through the LSST stack, to test and make use of the existing measurement, fitting, stacking, and image differencing capabilities.   This includes writing or updating a simulations obs package, and determining and supplying the required metadata."
0,Run StarFast simulated images through diffim,Determine the metadata and dependencies needed to fully process two images simulated with StarFast through diffim. 
1,Implement simple 1D DCR correction on simulated data,"Nate Lust wrote a simple DCR correction recipe that runs in 1D in an ipython notebook. This ticket is to re-write the notebook in python modules that can be run on StarFast simulated images prior to image differencing. For this ticket, the simulated images will be 2D, but DCR will be purely along the x or y pixel grid, allowing columns or rows of pixels to be treated separately in 1D."
0,Add support for blank image,We need to add blank image support.
1,Extend simple DCR correction to 2D,"In DM-5695 a simple DCR correction was applied to simulated images in the case that the effect was purely along the pixel grid and could be reduced to 1D. This ticket extends that work to the general 2D case.  Possible approaches include resampling the ""science"" image to match the ""template"", or including neighboring pixels and computing their covariance. Ideally, multiple approaches will be implemented and tested."
1,Add astrometric errors to StarFast,"One concern with the proposed DCR correction is that it might fail in the presence of source position errors. This ticket is to add the capability to simulate a variety of types of position errors, such as atmospheric turbulence or an inaccurate WCS, to test the DCR implementation."
1,Run many sky simulations through DCR correction to find edge cases,"Once a complete DCR correction prototype is finished, we will want to run many different sky simulations from StarFast with different densities of sources, noise properties, airmasses, and astrometric errors to find the limitations and edge cases where it fails. There are likely to be several thousand simulations needed which will take an as-yet undefined number of CPU hours, but this ticket is for the work in setting up and analyzing the results from the run."
1,Put ImageSelectPanel into dropdown,Currently the image select panel is in a dialog.  It also needs to be able to work in a dropdown.
1,Create toy composite (AST/GWCS) model with supported components,"To help us evaluate WCS options, we need to create a relatively complicated composite model in AST and GWCS, using a few models currently available within the existing packages. A minimal composite model to test these things would include:     * FITS linear transform   * ccd distortion   * optical model   * FITS TAN WCS    The middle steps do not need to be realistic models, just something that we can use to compare AST's and GWCS's respective interfaces and capabilities for creating the composite model, and test for differences in their results. We can then use this model to evaluate performance when run on different numbers of pixels."
1,Create a new model in AST/GWCS to represent a complex distortion,"Using lessons learned from DM-5701, create a more complex distortion model that cannot be represented from the basic models in GWCS or AST. A good example for this might be a rapidly varying sinusoidal tree-ring-like function that is not well represented by the standard polynomial basis functions. This will test our ability to extend each framework with new models that have not yet been decided on.    Once completed, we could plug this back into the composite model in DM-5701."
1,Evaluate performance of AST/GWCS over a range of numbers of pixels,"Once we have a composite distortion model from DM-5701, evaluate the performance of AST and GWCS over a range of numbers of pixels, likely from ~100 through full-CCD (4k^2).    As part of this process, we will try to determine whether there is a way to efficiently warp images/postage stamps using python-only models in GWCS and whether bottlenecks could be worked around via optimizations in cython."
1,add cloudbees-folder support to puppet-jenkins ,NULL
1,Produce document describing DRP parallelization use cases,"At various times in the past few months I've promised [~gpdf], [~petravick], [~kooper], and probably a few others a document describing the parallelization needs for DRP in greater detail.  My understanding of the plans for the eventual DRP probably good enough to do this well now, and is unlikely to improve further in the near future (as that will require algorithmic research).    This needs to be prioritized with my other responsibility for documents that describe the DRP system in other ways, most of which are oriented towards scientists and science pipelines developers.  The document on this ticket is essentially the description that would matter the most for the process control middleware team."
1,UI Consistency,There is a need to go though the entire ui and document inconsistencies with the old UI.
2,Firefly Result view architecture/component,"The result view architecture needs to be written.    * A meeting needs to happen with David, Gregory, Xiuqin, Trey, and Tatiana to discuss this.  * Trey, Loi, and Tatiana should have a design meeting.    It should support some or all of the following ideas:    * A search defining a new results view type  * A search adding to an existing result view  * A search replacing the results - any cleanup needs to happen.  * Some sort of controller that know which view should show and which view can be shown   * New search panels easily added. Maybe the html file defines which are visible"
2,"Verification Cluster, Object Store Procurement",Strategy design with pipeline and deployment teams. Discussions of service description and levels of support. Sufficient design to lead to procurement. Discussions with vendors. Quote selection. Budget tracking. Quote submission to finance. GCO follow up questions. OBFS follow up questions. Finance follow up questions. Overall tracking of purchase progression.
0,JIRA fixes,This tracks SPs spent on JIRA requests. 
1,Add table client-side sorting,Convert gwt's client-side sorting to javascript.
0,make sure table can be resized properly,Test table to make sure it can be resized under a variety of layout.
0,attend the weekly meeting with UIUC camera team,"While Tatiana is the assignee of this ticket, Xiuqin and Gregory participate this weekly telecon semi-regularly to lend support. "
0,attend the weekly meeting with UIUC camera team (May 2016),Tatiana will attend the weekly meeting. Xiuqin and Gregory also attends when needed. 
1,Create django project and initial dashboard app,"This ticket captures the steps to create the django project for SQUASH, its configuration and the dashboard app http://sqr-009.lsst.io/en/latest/    The planned tasks are:        - Implement the ``Dataset``, ``Visit`` and ``Ccd`` tables in the django ORM layer, as a minimum set      of tables for the dashboard app      - Prototype home page and dashboard pages      "
0,Config.loadFromStream suppresses NameError,"Within a config override file being executed via {{Config.load}} or {{Config.loadFromStream}}, using a variable that hasn't been defined results in a {{NameError}} exception, but this is silently suppressed and the user has no idea the following overrides have not been executed."
0,Fix the issues in the server side and the client side introduced by FitsHeaderViewer 's work,"*  The testing data ""table_data.tbl"" in the testing tree was accidentally moved.  It should be added back so that IpactTableTest.java can run.    * The request in JsontableUtil was mistakenly moved out from the tableModel by the the line   * {code}  * if (request != null && request.getMeta().keySet().size()>1) {              tableModel.put(""request"", toJsonTableRequest(request));  }  {code}.  The meta can be null but the request is not null, the request should be put into the TableModel.     "
0,Move Camera creation out of CameraMapper base class,"With the new cameraGeom, it's considered desirable that each camera be able to define the serialization format for its static camera data.  Despite this, it's still the base CameraMapper that does loads it (at least for most cameras), going through a circuitous chain of policy files, obs_* package paths, and Python code.    It'd be vastly simpler for each mapper to simply build the Camera object and assign it to {{self.camera}} in its own {{\_\_init\_\_}} method (most would simply delegate all the work to {{afw.cameraGeom.makeCameraFromPath}}).  We could then remove all the camera entries from the PAF files and make it much easier to follow the logic.    Eventually, I think we need to be storing at least some components of the camera definition in the data repository (or something like a calibration repository associated with it), and that would require giving the mapper access to a partially-constructed butler when its time to build the camera.  But we can save that for another day.  "
0,#NAME?,"Using the {{--clobber-config}} option in a child butler repository can cause changes in the parent repository, as we try to rename files to back them up in the parent repository.    This is a critical bug because it can cause pipeline outputs to be unexpectedly modified.    It should be easy to fix, as it's just a matter of checking whether the files to be renamed backed up are in the output repository.    This was originally reported as https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1341  "
1,Create and deploy common Ansible roles for ELK,"Create roles and deploy to Ansible Galaxy.    These are common roles for cloud-init (and any other future cloud dependencies), java (openjdk-jdk) and an editors role."
1,Create and deploy Elasticsearch and Kibana Ansible roles,Create roles and deploy to Ansible Galaxy.
3,Create and deploy an ELK system,"Create a Vagrant configuration and Ansible role to configure and combine Elasticsearch, Logstash and Kibana (ELK)."
1,"Create and deploy Logstash, Fluentd and Riemann Ansible roles",Create roles and deploy to Ansible Galaxy.    Create a role to combine all the individual projects together.
1,Create packer automation for ELK,Build packer automation to create machine images to use for the ELK system.
1,Implement ingestion code for the QA results,"The initial database model was implemented in DM-5728 and outputs of the QA analysis code are being produced by the work described in http://dmtn-008.lsst.io/en/latest/    In this ticket we plan to implement and API for listing and creating jobs, metrics and measurements so that a job or the QA analysis code can register this information in the dashboard app."
1,Build parallel DCR simulator using GalSim,The result of DM-4899 was a simulation tool called StarFast that can quickly make simulated images with realistic Differential Chromatic Refraction. This ticket is to build an equivalent simulator using GalSim to check the accuracy of results and benchmark speed and memory usage. 
1,SQUASH dashboard prototype design,SQUASH dashboard prototype design is described here    http://sqr-009.lsst.io/en/latest/
0,Upgrade mpi4py to latest upstream,"[mpi4py|https://bitbucket.org/mpi4py/] version 2.0 was released in October 2015 with a number of changes. We should upgrade. When upgrading, we should check whether it contains a proper fix for DM-5409 and, if not, file a bug report upstream.    This issue should not be addressed until we have proper test coverage on code which uses mpi4py (DM-3845)."
1,Integration of Django and bokeh server,NULL
1,XCode 7.3 can not link indirect dependencies that use @rpath,"With XCode 7.3 on OS X we have difficulties resolving indirect dependencies when those dependencies are referenced using {{@rpath}}. This can be seen with Qserv:  {code}  Linking shared object build/libqserv_common.dylib  ld: file not found: @rpath/libboost_system.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  {code}  where {{libboost_system}} is being loaded via {{libboost_thread}}:  {code}  $ otool -L $BOOST_DIR/lib/libboost_thread.dylib  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.59.lsst5+fbf04ba888/lib/libboost_thread.dylib:  	@rpath/libboost_thread.dylib (compatibility version 0.0.0, current version 0.0.0)  	@rpath/libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)  	/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.1.0)  	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)  {code}    This problem is also found when doing a {{conda}} build of the stack because in {{conda}} all shared libraries are modified on creation to reference other libraries via the {{@rpath}} mechanism.    This bug has been reported to Apple as [rdr://25313838|http://www.openradar.me/25313838] and a [Chromium bug report|https://bugs.chromium.org/p/chromium/issues/detail?id=597459] indicates that the fix is to simply ensure that {{-L}} directives include a trailing slash.  "
0,Update Scons to v2.5.0,Scons 2.5.0 came out over the weekend. There were many fixes to the dependency determination code. The next version of Scons is intended to be 3.0 which will be the first version to support Python 3. Since we fully intend to switch to Python 3.0 in the summer it is prudent for us to ensuer that 2.5.0 works fine before switching to 3.0.0 so that we do not get confused as to why there is breakage in jumping straight to 3.0.0.
0,FitsHeader's resize and sorting,"DM-4494 has merged to the dev.  However, there are still two issues remained:  * Resize the popup with tabs does not work  * Sorting is depending on the BasicTable's sorting"
1,TabPanel needs a way to keep it state between renders,The TabPanel and CollapsiblePanel loses its state when it is re-rendered.  It is going to have to have a way keeps it state. Therefore it needs an option to take an ID and keep it state in the store.      Use case- tabs of tables then the image plot goes to expanded mode.  The table tabs gets reset to the first one.    
0,XYPlot needs to be expandable,Make XYPlot expandable
2,XYPlot should support selecting columns from a table,There are should be a way to display all the information about table columns in a table and allow user to choose a column using this table.
1,XYPlot: Optimize decimated plot aspect ratio,"Currently decimation process assumes aspect ration 1. For decimated plots, the displayed area size (or user supplied value) needs to be used as an aspect ratio to approximate square bins.  - When aspect ratio changes, decimation process needs to be redone.  - To avoid server calls on resize, disallow flexible aspect ratio for decimated data.   "
1,XYPlot: decimation options,User needs to be able to control number of bins and bin size.
1,XYPlot: separate density plot from scatter plot,"At the moment we display data as scatter plot, when the number of points does not exceed decimation limit, and as density plot when the number of points does exceed this limit.    Scatter plot and density plot should be separate charts. These are the reasons:  - User should be able to create density plot with any number of points  - Chart type and display might be different for density plot in the future  - Scatter plot look should not change when the number of points exceeds decimation limit  - Scatter plot should support errors in the future"
0,Remove unneeded imports in SConstruct,"There's an outstanding pull request from an external contributor (Miguel de Val-Borro) [here|https://github.com/lsst/sconsUtils/pull/9] that makes some minor improvements to sconsUtils by cleaning up the imports. Somebody should review and (if appropriate) merge it. (Or, at least, reply to our community!)"
1,Implement spatial exposure selection task,"Once DM-3472 lands, it will be possible to write an image selection task that uses the SQLite 3 database produced by {{IndexExposureTask}} (from [daf_ingest|https://github.com/lsst/daf_ingest]) to search for exposures overlapping a region (in particular, the spatial extent of a coadd patch). The {{_rtree_search}} method in {{test_index_exposure.py}} (also from daf_ingest) has an example of how to perform spatial queries quickly.    I was originally scheduled to do something in this space, but Paul mentioned that he had plans to refactor the image selection tasks already, and is much more familiar with the pipeline side of things than I am. Therefore, I'm handing off the implementation of the pipeline task mentioned in DM-3472 to him."
0,Create custom basic coaddition code,"Create script to do the following:  * Takes a list of DECam exposure numbers  * for each CCD, loads the corresponding calexps  * creates a naive pixel-by-pixel coadd of the underlying images  * Possibly either ANDs or ORs the masks (though perhaps not necessary)  * Either sums the expusure time info from the headers, or averages them, depending on whether the images were normalised to exposure times or not  * write the corresponding images out as coadded fits"
0,Coadd CPB exposures,"Identify sets of DECam exposures from the CBP run and feed them to the coaddition script created in DM-5767.    This will need to be redone each time a reprocessing is done as the script will run on calexps. I will do it once now, and then again after DM-5465 is completed to satisfactory levels."
0,Write spot visualisation snippets,"Write some snippets to aide in the processing and visualisation of the CBP data/analysis.    Essentially, write some helper functions that you can throw sections of images at to help look at the shape of the CBP spots, as ds9 isn't great ideal this.    Some nice features would be:    A function that takes a list of images or arrays, and plots them side-by-side, which provides some intelligent options for the stretches, and optionally stretches each image as is best for it, or ties them all to be the same. This would be as 2D colour plots.    A function that takes part of an image and displays it as a colour-graded surface.    A function that takes part of an image and displays it as a 3D bar-chart (as in ROOT, but without using ROOT because there is already enough evil in the world)"
0,Investigate image processing for feature enhancement,"Whilst looking at an individual spot from the CBP on DECam I noticed a weird feature, and upon further investigation, several more, though these were very hard to see.    This ticket is to investigate what image processing techniques will make these hard-to-see features pop out so that they can be examined more closely."
0,Update config files,"DM-46921 and DM-5348 changed ProcessCcd to the point where past config files are no longer valid as stuff has moved a lot (see https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581)    This ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processCcd"
0,Firefly API plan and decision,"We need a plan for  all the Firefly APIs development in the new React/Redux based JS framework, including JS API and Python API.    - Backward compatibility  - Syntax format for JS API  - Syntax format for Python API  - Schedule     - convert the existing API first     - list of new ones to be added, when    "
0,Change the TabPanel.jsx and TabPanel.css's properties to allow its children can be resizable,"When an outside container is resizable (using css properties: resize: 'both', overflow: 'auto'...), in order for the child inside the container to be resizable, the child has to specify its height and width properties using percentage format (height: 90%, width:100%).   When the TabPanel is used, the table is put on TabPanel.  The table needs to access the size information of the outside container, ie,, the grandparent's width and height. The TabPanel has to pass the height and width to its child component.  Without specifying the height and width in the TabPanel, by default, the auto is used.  When the width (height) is auto, it allows to use the child's width (height).  However, the child replies on the parent to provide such information.  When this circular relations occur, the default size of the child is used.  That is why the table component forever has 75px when it was put in the TabPanel.  To be able to resize with the outside (root) contains all the ancestors of the component have to specify the width and height explicitly. "
0,Document Configurable concept,"The Configurable concept (a callable that takes a config as an argument) is a fairly important one in pex_config as the guts behind RegistryField and ConfigurableField, and it's mentioned several times in pex_config's documentation, but it doesn't seem to be directly documented itself."
0,Assist schandra with ts_wep Luigi implementation,Assist [~schandra] with an initial implementation of his workflow using Luigi.
2,"Port Data set info converter, part2","Part 2 includes: * 3 color * clean up on plot fail * clean up image in general * better row highlighting * other types of data layout (a FC type view) * artifacts (maybe part 3) * When image is small, like zoom it down 1/16x, behavior of selecting an image is not consistent. Clicking on an edge will select one, but will not work on another."
0,"Include obs_cfht, obs_decam in lsst-dev shared stack",The shared stack on {{lsst-dev}} provided in DM-5435 does not contain the {{obs_cfht}} or {{obs_decam}} camera packages. Please add them.
1,Port region serializer and data structures from GWT,"The region serializer in: firefly/src/firefly/java/edu/caltech/ipac/util  * RegionFactory.java    Region container data structures files in : firefly/src/firefly/java/edu/caltech/ipac/util/dd    * ContainsOptions.java  * Global.java  * RegionFileElement.java  * RegParseException.java  * Region.java  * RegionAnnulus.java  * RegionBox.java  * RegionBoxAnnulus.java  * RegionCsys.java  * RegionDimension.java  * RegionEllipse.java  * RegionEllipseAnnulus.java  * RegionFont.java  * RegionLines.java  * RegionOptions.java  * RegionPoint.java  * RegionText.java  * RegionValue.java      Note - do not port CoordException, there are other ways to do this."
0,Add support for SGE,Jean Coupon has requested support for SGE in ctrl_pool.
0,Why is doSelectUnresolved an argument?,"The {{run}} method in the {{PhotoCalTask}} has an argument that selects whether to use the extendedness parameter to select objects for photometric calibration.  This is a good idea, but it should be configurable, I think. "
1,Support artifacts ,For now this is the artifacts for WISE images.   We should look at the possibilities to generalize this. 
1,Convert  Mask support,NULL
1,Support image and drawing layer subgrouping,NULL
0,Add Python properties for getters and setters in afw::geom and shapelet,"I'm adding properties via Swig %extend in much of afw::geom right now, because:   - I think we've all agreed this is something we want, even if we haven't agreed how much effort we want to put into it.   - I'm getting annoyed writing lots of parentheses for these getters and setters on DM-5197.   - I can get this done in a couple of hours on a weekend, so I don't need a T/CAM to give me permission to spend my own time on it :)  "
0,Using 'CONSTANT' for background subtraction fails,"Running processCcd (on a DECam file) with the following in the config file:    {code}  config.charImage.repair.cosmicray.background.algorithm='AKIMA_SPLINE'  config.charImage.background.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.background.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.background.algorithm='CONSTANT'  {code}    fails, and throws the following:    {code}  Traceback (most recent call last):    File ""/home/mfisherlevine/lsst/pipe_tasks/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run      resultList = mapFunc(self, targetList)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 324, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/processCcd.py"", line 170, in run      doUnpersist = False,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 298, in run      background = background,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 356, in characterize      image -= estBg.getImageF()    File ""/home/mfisherlevine/lsst/afw/python/lsst/afw/math/mathLib.py"", line 5788, in getImageF      return _mathLib.Background_getImageF(self, *args)  lsst.pex.exceptions.wrappers.InvalidParameterError:     File ""src/math/Interpolate.cc"", line 61, in std::pair<std::vector<double>, std::vector<double> > lsst::afw::math::{anonymous}::recenter(const std::vector<double>&, const std::vector<double>&)      You must provide at least 1 point {0}    File ""src/math/BackgroundMI.cc"", line 196, in void lsst::afw::math::BackgroundMI::_setGridColumns(lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, int, const std::vector<int>&) const      setting _gridcolumns {1}  lsst::pex::exceptions::InvalidParameterError: 'You must provide at least 1 point {0}; setting _gridcolumns {1}  {code}"
1,Pass butler to ref loader,"The design of the indexed reference catalogs requires a butler to be sent to the loader.  This requires passing the butler down through the chain of subtasks from the parent command line task.  In this case, I believe only calibrateTask constructs sub-tasks that requires a reference catalog.    This will also require moving the loader and indexer to meas_astrom, otherwise it will introduce a circular dependency."
1,Asinh stretch algorithm corerction,"in DM-2634, the Asinh stretch algorithm  was implemented, but the behavior was not quite right. We need to figure out the issue and make it right. One possibility is that the understanding the relationship  of zero point  and black point, maximum point and white point. "
1,TabPanel:  Tab titles need to shrink to accommodate a large number of tabs.,Should convert GWT's logic over to TabPanel.  - shrink title as needed.  - show full title on mouse over
0,Cmake in mariadbclient finds wrong libz,"When building mariadbclient, cmake identifies libz from a separate python installation than the one setup to run the stack. I have an anaconda installation on the disk, and a miniconda installation set up specifically for the lsst stack. During the building process CMake for some reason finds the alternate libz associated with that python installation."
0,fetchUrl is not handling post requests correctly.,Parameters are not sent to the server when requests are posted via fetchUrl.
0,Use aperture-corrected aperture flux in validate_drp,Shift from PsfFlux flux/magnitude to aperture-corrected aperture-based mag/flux measurements for calculating photometric repeatibility.
0,Improve star/galaxy separation for validate_drp,"Improve the star/galaxy separation for validate_drp.    Many of the LSST SRD KPMs are defined for bright, isolated stars.  There is clear evidence that galaxies are being included in current runs (they have significantly higher photometric scatter at the same mag|SNR).  Improved star/galaxy separation will help generate better numbers    Stretch goal:  Include additional informative plots about how well the `extendedness` value in the catalogs is successfully separating stars and galaxies."
1,Add a paging bar to ImageMetaDataToolbarView,Add a paging bar similar to the one for table to the ImageMetaDataToolbarView.  This pages images instead of rows.
0,Ensure that variance plane in calexps is unchanged HSC⟷LSST,Per discussion in HSC telecon 2016-04-19.
0,Update imageDifferenceTask to cast template ids and use ObjectSizeStarSelector,"A couple recent changes to the stack break imageDifferenceTask.     Requires updates to only a few lines.     While I'm updating it to reflect the star selector API, I'm also changing the default star selector from SecondMoment to ObjectSizeStarSelector (which I learned today is what the stack has been using by default for a while). "
0,Incorporate Price suggestions to make `validate_drp` faster,"Increase the loading and processing speed of {{validate_drp}} following suggestions by [~price]    1. Don't read in footprints  Pass {{flags=lsst.afw.table.SOURCE_IO_NO_FOOTPRINTS}} to {{butler.get}}    2. Work on speed of calculation of RMS and other expensive quantities.  Current suggestions:  a. {{calcRmsDistances}}  b. {{multiMatch}}  c. {{matchVisitComputeDistance}}  d. Consider boolean indexing  {code}     objById = {record.get(self.objectKey): record for record in self.reference}  to:     objById = dict(zip(self.reference[self.objectKey], self.reference))  {code}    Note that while this ticket will involve work to reduce the memory footprint of the processing, it will not cover work to re-architect things to enable efficient processing beyond the memory on one node."
1,3 color and FITS header clean up,"There are some issues with three color when not using all three bands (i.e. using on green and blue):  * Mouse readout is not labeled correctly  * FITS head popup does not come up    Other FITS header popup issues:  * If file size is too big then the text is wrapping  * On safari, the resizable indicator in on every cell  "
1,Intermittent fault building ci_hsc through Jenkins,"Occasionally (see e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-7/10437//console] and [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/9594//console]) the {{ci_hsc}} job in Jenkins fails, reporting:  {code}  RuntimeError: dictionary changed size during iteration  {code}  The fault seems to be intermittent. Please fix it."
0,Afw fails unit test for convolve depending on compiler optimisation level,"On OSX 10.11.4 with Apple LLVM version 7.3.0 (clang-703.0.29) afw fails {{test/convolve.py}} with the following error when either {{-O0}} or {{-O1}} is enabled but works fine for {{-O2}} and {{-O3}}.    {code:bash}  tests/convolve.py    .....FF/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py:283: RuntimeWarning: invalid value encountered in isnan    nan0 = np.isnan(filledArr0)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:113: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), ~nx.signbit(x), y)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:176: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), nx.signbit(x), y)  F.F...  ======================================================================  FAIL: testSpatiallyVaryingAnalyticConvolve (__main__.ConvolveTestCase)  Test in-place convolution with a spatially varying AnalyticKernel  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 437, in testSpatiallyVaryingAnalyticConvolve      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel using brute force) wrote to edge pixels: image planes differ: maxDiff=1.09176e+38 at position (73, 18); value=-1.09176e+38 vs. 2825.0; NaNs differ    ======================================================================  FAIL: testSpatiallyVaryingDeltaFunctionLinearCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of delta function basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 556, in testSpatiallyVaryingDeltaFunctionLinearCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of delta function kernels using brute force) wrote to edge pixels: image planes differ: maxDiff=9.06659e+36 at position (75, 29); value=9.06659e+36 vs. 2865.0    ======================================================================  FAIL: testSpatiallyVaryingGaussianLinerCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of two Gaussian basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 523, in testSpatiallyVaryingGaussianLinerCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel with 3 basis kernels convolved using brute force) wrote to edge pixels: image planes differ: maxDiff=1.22472e+38 at position (74, 3); value=-1.22472e+38 vs. 2878.0; NaNs differ    ======================================================================  FAIL: testTicket873 (__main__.ConvolveTestCase)  Demonstrate ticket 873: convolution of a MaskedImage with a spatially varying  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 623, in testTicket873      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of basis kernels with low covariance, using brute force) wrote to edge pixels: image planes differ: maxDiff=3.19374e+38 at position (1, 46); value=3.19374e+38 vs. 2774.0    ----------------------------------------------------------------------  Ran 13 tests in 43.252s    FAILED (failures=4)  The following tests failed:  /Users/pschella/Development/lsst/code/afw/tests/.tests/convolve.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  {code}"
0,ECL_B1950 coordinate was not defined correctly,"The CoordSys.js defined ECL_B1950 incorrectly.  When I was testing WebGrid, the grid lines for  Ecliptic B1950 were not right.  Looked further, it was caused by wrong equinox value in its definition."
0,Add CI tests for obs_lsstSim,I propose:    1. Create {{testdata_lsstSim}}.  This will be based on 12 images from the current Twinkles Run 1 (or pre-Run 1):  2 epochs each of 6 filters.  (/)    2. Add an optional test method to {{obs_lsstSim}} that runs if {{testdata_lsstSim}} has been declared.  This is the way these tests are set up for the other {{obs_*}} packages.    3. Add {{testdata_lsstSim}} dependency to {{lsst_ci}} (which already depends on {{obs_lsstSim}}).    This will then be run every time a full standard default Jenkins build is processed.
2,Compare LSST and HSC pipelines through through multi-band coadd processing,"Continue the work described in DM-5301 through the standard ""multi-band"" coadd processing workflow.    Performing an end-to-end comparison of the stacks will not be possible until {{meas_mosaic}} is fully operational on LSST (DM-2674). However, until that point is reached, comparisons are still possible by either:    * Shepherding data through {{meas_mosaic}} and coaddition on HSC, then performing further processing and measurement using the LSST stack;  * Omitting {{meas_mosaic}} from the workflow altogether and performing end-to-end comparisons of the stacks without mosaicking.    Obviously, neither of these will ultimately be adequate, but they should enable early identification of any major issues."
0,Create outline of Level 3 ConOps,Create an outline of the sections of the Level 3 ConOps document
1,Level 3 requirements flowdown,"Document the flowdown of Level 3-related requirements from SRD, LSR, OSS, and DMSR."
1,SUIT requirement flowdown,"go through the original requirement of SUIT,  put them in the categories:  done, Tier1, Tier2    "
0,LSE-140 post-CCB implementation,"Following CCB approval of LSE-140, perform minor document work required for full implementation (application of standard cover page, change log, etc.)."
1,SUIT design diagramming,Prepare initial set of SysML diagrams of the SUIT's relationship to other system components.
2,Prepare requirements and design for Fall 2016 SUIT deployments,Prepare functional and quantitative requirements and the SUIT-centric elements of design for the planned Fall 2016 SUIT deployments (SDSS Stripe 82 and WISE).
0,Prepare a draft of the SUIT deployment timeline,"Prepare a draft schedule, with some detail for 2016-2017, for deployments of the SUIT into (test) production, including the datasets that will be served."
1,access to NCSA Nebular to setup servers for SUIT deployment,Get three hosts in NCSA nebular system to deploy the current Firefly application. The goal is workout the possible issues and identify the software needed to be installed for the hosts. Clarify which team is responsible to install what third-party software packages.
0,Document pipe_drivers,"Please provide a minimal level of documentation for {{pipe_drivers}}, to include:    * A {{doc}} directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary)."
0,horizon console interface broken,It appears that at some point in the last few months the horizon console interface has stopped working.  I am still able to access the console log output via the API/CLI.
0,instance limit low vs available cores,The LSST project is currently at 81/100 instances but there are over 200 cores unused.  Is it possible to increase the instance limit or are we being encouraged to use large instance flavors?
0,unable to list nebula lsst project users,"Currently, [with some difficulty] it is possible to discover the {{user_id}} that created an instance (might be possible for other resources as well) but it is not possible to map this back to a username / person.  This can make it difficult to 'self police' instances.    The administrative API endpoints are not publicly accessible and I doubt any end user has the appropriate permission. "
3,automate deployment of qa dashboard server and database instance,"Add a qa server + rds instance to the terraform configuration for the jenkins-demo sandbox for development purposes.  It may make sense to split this off to be an independent sandbox but that is very easy to do, if needed."
0,"ci_hsc fails with ""too many open files""","For example, with thanks to [~wmwood-vasey]:    {code}                ci_hsc: master-g78db638f21 .....................................................................................ERROR (207 sec).  *** error building product ci_hsc.  *** exit code = 2  *** log is in /Users/wmwv/lsstsw/build/ci_hsc/_build.log  *** last few lines:  :::::  [2016-04-25T19:25:59.824660Z]     jobs.run(postfunc = jobs_postfunc)  :::::  [2016-04-25T19:25:59.824699Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Job.py"", line 113:  :::::  [2016-04-25T19:25:59.824709Z]     postfunc()  :::::  [2016-04-25T19:25:59.824752Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Script/Main.py"", line 1294:  :::::  [2016-04-25T19:25:59.824767Z]     SCons.SConsign.write()  :::::  [2016-04-25T19:25:59.824808Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/SConsign.py"", line 109:  :::::  [2016-04-25T19:25:59.824816Z]     None  :::::  [2016-04-25T19:25:59.824869Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/dblite.py"", line 116:  :::::  [2016-04-25T19:25:59.824878Z]     None  :::::  [2016-04-25T19:25:59.935601Z] Exception IOError: (24, 'Too many open files', '.sconsign.tmp') in <bound method dblite.__del__ of <SCons.dblite.dblite object at 0x10dfe9c50>> ignored  {code}    Possibly only happens on OSX?"
0,libxml build issue with mpich on OS X,"On OS X with Xcode installed {{mpich}} fails to build because it can not locate the libxml include files:    {code}  CC       topology-xml-libxml.lo   topology-xml-libxml.c:17:10: fatal error: 'libxml/parser.h' file not found   #include <libxml/parser.h>            ^   1 error generated.  {code}  with {{pkg-config}} 0.29.1 installed. The problem is that {{configure}} determines that {{libxml-2.0}} is available and is installed into {{/usr}} with a CFLAGS of {{-I/usr/include/libxml2}}. {{configure}} does not itself test whether those parameters are reasonable. With Xcode there are no files installed into {{/usr/include}} and {{clang}} knows to look in specific SDK locations. When {{mpich}} builds it assumes that {{libxml2}} can be found but fails to find it.    Strangely, {{pkg-config}} v0.28 does not seem to be able to find {{libxml-2.0}} so there is no issue.    One solution is to install the Command Line Tools but it might be more portable to attempt to disable {{libxml2}}.  "
0,"Investigate Jupyter internals, interactive widgets","In preparation for linking Jupyter notebooks with Firefly and other SUIT components, read Jupyter documentation. Learn how to build a sample widget or interactive dashboard in the Jupyter framework"
0,Investigate Ginga and Glueviz visualization tools,"Ginga and Glue (glueviz) are community visualization tools in Python. Become familiar with the capabilities of both, thinking from the point of view of using Firefly for the display but using Python for many other things."
0,Java array index out of bound error in VisSeverCommand.java,"The class FileFluxCmdJson in VisServerCommand.java is calling   {code}              String[] res = VisServerOps.getFileFlux(fahAry, pt);  {code}    However, when the mouse is outside the image, the VisServerOps.getFileFlux(fahAry, pt) returns:  {code}  new String[]{PlotState.NO_CONTEXT}  {code}  It is fine for a single band.  However, for 2 or 3 bands, the for loop below caused the index out of bound error because res is an array of length=1 and the expected res is an array of length=no of bands.  {code}    JSONObject obj= new JSONObject();              obj.put(""JSON"", true);              obj.put(""success"", true);                int cnt=0;              JSONObject data= new JSONObject();              for(Band b : state.getBands()) {                  data.put(b.toString(), res[cnt++]);              }              data.put(""success"", true);  {code}    Thus,  res\[cnt++\] caused array index out of bound error.     To fix this issue, the for loop is changed as below:  {code}                        int cnt=0;              JSONObject data= new JSONObject();              Band[] bands = state.getBands();              for (int i=0; i<res.length; i++){                  data.put(bands[i].toString(), res[i]);              }              data.put(""success"", true);                JSONArray wrapperAry= new JSONArray();              obj.put(""data"", data);              wrapperAry.add(obj);  {code}    When the mouse is outside the image, the res returns a new String\[\]\{PlotState.NO_CONTEXT\}, it is added to the JSONObject only once.  "
1,Make DipoleFitPlugin mask-safe,"The DipoleFitPlugin does not correctly handle bad pixels and other masks/flags. Make it so it does so, and make tests to ensure it does so."
3,LSST the Docs Production Fall 2016,"DM-5404 introduced _LSST the Docs_ as a production platform for continuous documentation delivery. This Epic covers additional improvements to the platform, such as    - Implementation of a backup system for LSST the Docs’ DB    - Edition and build dashboards at the /v/ and /builds/ directories that help users find the appropriate version of the documentation site. These could be rendered with react from the API. This also serves as a ramping up exercise on UI elements that will be used on the SQuaSH dashboard and DocHub, so learning time has been rolled into the estimate"
0,Table: Add keyboard navigation,#NAME?
2,Create obs_monocam,Make a package to hold the description of moncam.
0,Minor tweaks to Cython and pybind11 tech notes,"I'll be making some superficial changes to the text of DMTN-13 and DMTN-14 for grammar, while updating links to the python-cpp-challenge repo (which has just moved from my private GitHub to lsst-dm)."
2,Improve Qserv CI using multinode tests,"Here's some tracks:    1. Run multinode integration tests during qserv_distrib CI build.  In order to do that we could create a qserv_testmultinodes repository containing a build script which would launch multinode tests (for example see travis.yml)  I can do this on my side but i'll require a recent version of Docker on the build machine.    2. I'll need your help to do next step:  Each time command below succeed:  {code:bash}  rebuild qserv_distrib  {code}  publish this build to eups web repository and docker hub by running:  {code:bash}  # bXXX is the build id and is available at the bottom of rebuild command standard output  publish -b bXXX -t qserv-dev qserv_distrib  # then create and publish to docker hub image ""qserv/qserv:dev""  # which embed current build products and is used for all Qserv deployment  # (bare-metal, openstack, travis, developper machines)  $QSERV_DIR/admin/tools/docker/2_build-dev-image.sh  {code}    As a TODO list, here's what could be done in an additional ticket, in the long term:    - run multinode integration test inside Jenkins instead of Travis?    * Travis/Gitub integration is very good, so I'm not sure this feature is still an active concern?    * on the other hand Travis has to download qserv/qserv:dev at each build, and if there's a timeout here, the build sometime fails. I don't know if this image can be cached in Travis free version?    * current procedure doesn't support yet tickets branch accross multiple repositories, (like qserv+xrootd+qserv_testdata for example). Do you think this feature would be easier to implement in Jenkins?"
2,Test lsst.log with pipeline tasks,"Try to use {{lsst.log}} instead of {{lsst.pex.logging}} for a few science pipeline tasks, based on log {{u/ktlim/getLogger}} branch and DM-3532. Look into RFC-29."
1,Literature research on image subtraction algorithms,"We need to get a good understanding of where the image subtraction implementation in the stack currently stands. This first requires an up-to-date assessment of the literature, including Becker et al. (2012), and ZOGY (2016). Also, the ""preconvolution"" step."
1,Assessment of current state-of-the-stack diffim implementation,"The existing diffim implementation in the stack defaults to the (2000) version of the Alard/Lupton algorithm. Other recent improvements such as ""pre-convolution"", delta-function basis, model selection via BIC, others, seem to be implemented but are not turned on. We need a good understanding of the existing implementation so we can assess how straightforward it is to implement the ZOGY algorithm in real-space in the stack."
0,Update testdata_subaru to support calib changes,Merging DM-5124 broke obs_subaru because the test data in testdata_subaru wasn't updated.  Fix it.
1,"Incorporate ""Bickerton algorithm"" for detecting & masking satellite trails","In [HSC-1272|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1272], [~bick] proposed an algorithm for detecting and masking satellite trails. This has undergone some review on HSC, but has never been incorporated into an HSC software or data release (and hence is not part of the ""HSC port"").    However: the algorithm is certainly relevant to LSST. Please convert it to work with the LSST stack."
2,Produce document describing flavors of coadds,"[~zivezic] has requested a description of the different flavors of coadds, and the tradeoffs between the more experimental optimal coadds and the non-optimal standard ones.    I'll try to do this as both a presentation for the DMLT and a DMTN (with essentially the same content).    This will include a bit of toy-model simulation work to try to predict some of the tradeoffs; this could make the discussion quite a bit more quantitative and I have an idea for how to do it that is pretty easy."
0,Propose text for alternate galaxy models in DPDD,Write a paragraph or two describing alternatives to the constrained bulge+disk model currently in the DPDD.
1,Use Afterburners to clean up aperture correction logic,"This issue has several components; I'm combining them into a single issue because they need to be done atomically:   - Rewrite the base_ClassificationExtendedness SingleFramePlugin/ForcedPlugin as an AfterburnerPlugin (and remove the old versions).   - Move the ""applyApCorr"" subtask out of SingleFrameMeasurementTask and ForcedMeasurementTask, making it instead a subclass of their parent Tasks.   - Add afterburner subtask stages to ProcessCcdTask (within DetectAndMeasureTask) and the multiband tasks wherever measurement is currently being run.  The afterburner tasks should be run after aperture corrections are measured and/or applied.    After these changes, throughout the stack, whenever a MeasurementTask is run, we also run ApplyApCorrTask and AfterburnerTask (in that order), while possibly running MeasureApCorrTask immediately after the MeasurementTask.    This may or may not enable significant cleanups in DetectAndMeasureTask (I haven't looked closely).  If so, they should be done on this issue.    Given all the moving parts, it's important to check that the actual behavior of the pipeline (in the aperture correction and extendedness values) does not change, so it might be useful to start by creating some reference outputs to compare against."
0,Add chi plots to validate_drp output to compare nominal error,"Make histograms of Deltas / nominal error.    where ""nominal"" error is that reported by the pipeline.    Are they distributed with a sigma=1?"
2,Remove use of Boost smart pointers throughout the Science Pipelines,Replace all use of Boost smart pointers through the stack with their standard library equivalents.    This will require an RFC.
2,Audit use of Boost in the stack and remove it where possible,"Consider use of Boost in the stack, and investigate where it can be eliminated by using std library equivalents. Create tickets to remove it, then work on them."
2,Recover from processCcd refactor,"Now that DM-5771 is closed and processCcd runs again, get back to the point where the things are not just not crashing but are actually working correctly (given that this is a non-standard use-case)."
1,Create MySQL account for monitoring,A MySQL account needs to be created in configuration procedure and on existing data on IN2P3 cluster in order to enable ELK access. MySQL password/secret has to be shared accross all containers.
1,Create a JSON file for monitoring stack,"Create a JSON or YAML file with:    - Qserv version  - libraries/deps version  - other idea welcome    This will interesting ""GROUP BY"" in monitoring tool (performance for each Qserv or xrootd version for example)"
3,Provide ngmix-based MCMC galaxy fitting,"DM-2250 provided for ""simple"" fitting of galaxies in single frame measurement using ngmix. Extend this to fit galaxies using ngmix's MCMC sampling facilities. This may include defining a mechanism to store MCMC samples in source records.    This should be available through (an) lsst_apps (or _distrib) based meas_extension package(s)."
0,"Suppress gcc warnings about ""unused local typedefs""","We should add {{\-Wno\-unused\-local\-typedefs}} to our gcc options.  This cleans up the build significantly, because there's a flood of warnings of this type coming from boost.  If we suppress those, it might become possible to notice warnings that we care about."
0,LSST the Docs Fastly should redirect /en/latest/ to /,"Previously we deployed documentation on Read the Docs. By default, Read the Docs would show the master version of documentation on ""/en/latest/"". Many links with that endpoint may already exist. We should configure Fastly to redirect such paths to ""/""."
1,LSST the Docs Fastly Courtesy Redirects for directory paths,Currently if a user browses {{example.lsst.io/v/main/some-directory}} instead of {{example.lsst.io/v/main/some-directory}} they will receive an error.    We should develop a scheme where Fastly can detect that such a path is a directory and redirect to the directory's index.html page.
0,Robustify coadd,"In running the processing from the Twinkles data challenge in DESC we found that it was very easy to use the wrong skymap when making a coadd.  Since the coadd code doesn't even make a cursory check that the coordinate system it is using is the same as that of the coaddTempExps, it is very possible to mess this up.    Adding a check that the coadd WCS is that of each input tempCoaddExp is would solve this."
0,Python EUPS package can use $PYTHON,The {{python}} eups package has a script that checks that the python being used is version 2.7. This script can optionally check {{$PYTHON}} rather than the python in the path but I am confused as to what that test is going to do for us. The problem is that {{sconsUtils}} uses {{python}} and most of the shebangs use {{/bin/env python}} (although shebang rewriting on all platforms could help with that). I think the check script should have the {{$PYTHON}} support removed due to excessive confusion.    It would also help if the check script worked with python 3 so that the wrong python could be caught.
0,Create psutil EUPS package,Add the python {{psutil}} package to the stack as {{python_psutil}}.
1,LTD Keeper: More Robust Edition purges,LTD Keeper needs to purge Fastly when an Edition is rebuilt. Currently the surrogate-key for the build is also used to cover editions. This means that the key needed to purge an edition is the same as that for an build. Hence purging an edition means that the system needs to purge the surrogate key of the previous build.    We're seeing situations where the surrogate key that Keeper is purging is not the one that needs to be purged. A more robust configuration would be for each edition to have a stable surrogate-key that can be unambiguously purged.    This ticket covers the following work:    # Diagnose the issue.  # Enable Alembic migrations for Flask (Flask-Migrate)  # Add a surrogate-key column to the Edition model  # Change the S3 copy rebuild code to change the surrogate-key header  # Change the rebuild code to purge based on the edition's surrogate-key.
1,Finish technical note on galaxy shear experiments,"In the review of DM-5447 we decided it made sense for [~jbosch] to take over finishing the technote, in particular providing an introduction and concluion with more context."
0,Create focus script,"In DM-3368, we stripped out the focus calculation since it's not camera-generic, and the scatter/gather isn't necessary for general processing.  We need to reinstate the focus calculation in its own scatter/gather script."
1,Refactor DipoleFitPlugin classification into separate Classification plugin,"Currently the new DipoleFitPlugin runs measurement and then classification from a single measurement method. The classification should be moved out to a separate plugin. This will require more information be stored in the measRecord, in order to do the classification separately. Given that complication, evaluate whether this is even worthwhile."
1,Remove qmeta::QueryId and use global qserv::QueryId,Remove the qmeta::QueryId and use the typedef of QueryId in global/intTypes.h instead. Also try to verify that QueryId is used instead of uint64_t where applicable.
2,Replace the heap in ScanScheduler with a list.,"ScanScheduler is using heaps to order Tasks by chunkId. This makes it difficult to add Tasks to actively running chunks, which causes a significant delay to the start of query execution. Using a list of buckets of Tasks where each bucket is for one chunkId will make it easy to add Tasks to chunks being. Within each bucket it will still be necessary to order Tasks by tables used in the query."
2,Alter the worker thread pool to allow threads to leave the pool and continue.,There are times when it is desirable for a thread to continue but effectively leave the thread pool and be considered finished by the scheduler. util::ThreadPool needs to modified to do this.
1,"After the first result set is returned, have the thread leave the pool.","When large results are returned from the worker to the czar, the thread should leave the thread pool and the Task should indicate to the scheduler that it is done.   "
2,Add code to the czar to throttle incoming large results.,The czar needs code to limit the number of Tasks sending back large results at any given time.
0,Fix circular references in Mapper objects,"Whilst running tests with pytest and the new file descriptor leak checker it became clear that Mapper objects were not freeing their resources when they were deleted. In particular, the registry objects remained and the associated sqlite database files were opened. This led to pytest running out of file descriptors when large test suites were being executed.    The problem turns out to be the dynamically created map functions. These are created as functions (not bound methods) attached to an instance. Since they are not bound methods the instance object (self) has to be passed in to closure. This leads to self containing a reference to a function that contains a reference to self and this prevents the Mapper from ever being garbage collected (leading to all the resources being retained).    A short term fix is pass the mappers into the closures using {{weakref}}.    Eventually it would be nice to consistently make the {{map_}} items bound methods rather than attaching them as functions but that is beyond the scope of this ticket."
1,"Add ""everything"" scan",Add a low priority ScanScheduler to the worker to handle very slow scans or scans that do not  work well on the other schedulers.
2,Add ability for workers to switch slow queries to the everything scan.,Give the workers the ability to move user queries to the everything scan if they are taking too long to complete a Task or several Tasks.
1,Document planned implementation of toy model of Lupton(ZOGY),"Develop a better understanding of the planned implementation of ZOGY in real space by implementing the kernel correction in k-space and investigating its characteristics when transformed back into real space. Do this either symbolically (if possible) or numerically in an ipython notebook. First in 1-D, then in 2-D, both assuming a constant kernel. Include documentation in the ipython notebook describing the current understanding of how this will be implemented"
1,Decide how to rework afw:Wcs guts with AST,"Following the to-be-written recommendation for DM-4157, we plan to rework the guts of afw:Wcs to use AST. We need to decide how afw:Wcs will use AST, whether as a wrapper or as a complete replacement with AST.    The product is a design"
2,Decide how to rework XYTransform/GTransfo guts with AST,"We want to better connect our other transforms with the WCS system, which means reworking the guts of XYTransform/GTransfo to work with AST. This could involve making one or both of them a wrapper, complete replacement, or writing a converter that turns our transform object into an AST map or FrameSet.    The product is a design"
3,Design an API for the new Wcs and Transform system,"We need to design a new API for the WCS/Transform system. This is somewhat independent of the question of how the low-level code is used: we want a clean and simple API that lets the components of the stack create, manipulate, use, and persist the necessary transformations. Related to this question is whether we will still need skyToPixel/pixelToSky or whether the necessary operations with those can be subsumed into some Frame-to-Frame transformation (e.g. pixel-to-pixel or tan-to-tan).    The product is an RFC"
1,What transforms do we currently need?,"In order to use AST in the stack, we may need to add mappings to it. We also need to be able to describe our transforms at a high level so that we know how to create them.    We need a list of the currently necessary transformations (e.g. from afw:wcs, XYTransform, GTransfo and any other relevant stack packages), and some concrete ideas about the kinds of transforms we may need in the future. These should be described in a high-level mathematical manner, independent of our wcs/transform system.    This can be informed by DMTN-005 and the requirements section of DMTN-010"
1,Describe our composite mappings and transformation endpoints (Frames),"To use AST in the stack, we need to be clear what our different transformations (AST:Mappings) and endpoints (AST:Frames) are going to be so we can create the chain of transformations (AST:FrameSets) that will be used throughout the stack. This applies to both images and CameraGeom. We may want to produce similar descriptions for other stack objects.    This ticket is the high-level Frames equivalent to the mathematical Mapping description in DM-5918.    This will help us determine how we can put our current input/output image frames into the new system."
1,Create DCR metric using new dipole measurement,"In order to evaluate DCR correction algorithms we need a metric that defines the severity of DCR in a residual image. This ticket is to run the new dipole measurement task on simulated difference images affected by DCR, and to define a useful metric. The result will be a brief technical note defining the process and the metric, with a few examples."
0,Clarify how to work with ci_hsc's astrometry_net_data,"ci_hsc's {{README.rst}} contains [a note|https://github.com/lsst/ci_hsc/blob/87b6ecb1cc0157cac8dafb356520f49f971bb1ec/README.rst#reference-catalog] on declaring & setting up the included reference catalogue data.    I believe this was rendered obsolete by DM-5135, which automatically sets up the reference catalogue when ci_hsc itself is set up. Attempting to follow the documentation therefore produces confusing warning messages, and may break things.    Please check if my understanding is correct and, if so, fix the documentation."
1,Rework camera geometry to use the replacement for XYTransform,"As part of overhauling XYTransform we will likely need to replace the way we describe the transformations supported by camera geometry and {{Detector}}. This is likely to include a new way of describing the coordinate frames (e.g. {{PIXEL}}. {{FOCAL_PLANE}} and {{PUPIL}}).    If we adopt AST (as seems likely) then these frames will be AST {{Frames}}, the transforms will be AST {{Mappings}} and the collection described by {{Camera}} and {{Detector}} will be one or more AST {{FrameSets}}.    An RFC for the redesigned API for camera geometry will be required and this ticket is to implement the resulting design."
0,Support arbitrary sky rotation angles in StarFast,"Currently, if a region of sky is simulated in StarFast the stars must always have the same x,y coordinates (before DCR effects). This ticket is to support arbitrary rotations and offsets of the simulated stars to mimic realistic repeated observations of the same field."
1,Improve overscan correction,"Overscan correction can be improved.  Specifically, some systems have sharp discontinuities in the bias section."
1,Implement fringe correction in ISR,There is an initial implementation of fringe correction in the obs_subaru package.  It should be ported and generalized.
0,networking in strange state for newly created instances,"When starting a new instance, occasionally something strange seems to happen with the  network setup.  The instance will come up but is inaccessible (icmp, ssh). When this happens, the console log shows that a DHCP address was obtained and cloud-init injected ssh-keys, so it isn't a total network setup failure.    I have seen this happen a few times in the last couple of weeks but I can't reliably reproduce it.  I'm wondering if neutron is logging anything interesting when this happens.    This failure mode happened  again a few minutes ago with 7adffa82-7221-454c-acfe-5f21cdd34ea8.  Which I killed and recreated as instance b6f64981-099b-46e5-a27e-e3694372f447 with the same private IP address.   The new instance is accessible as expected."
0,API errors when trying to start up multiple instances,"I am attempting to start up 20 {{m1.medium}} instances without floating IPs to take available of the new instance cap from DM-5840.  This consistently fails after starting a few instances with an HTTP 403.    {code:java}  Error creating OpenStack server: Expected HTTP response code [201 202] when accessing [POST http://nebula.ncsa.illinois.edu:8774/v2/8c1ba1e0b84d486fbe7a665c30030113/servers], but got 403 instead  {""forbidden"": {""message"": ""Maximum number of ports exceeded"", ""code"": 403}}  {code}    Of the instances that do manage to start, most end up in an error state with.      {code:java}  (openstack) server show 134b69dc-56fc-4249-b92f-e958e561ae3b  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  | Field                                | Value                                                                                                                                               |  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                                                                                              |  | OS-EXT-AZ:availability_zone          | nova                                                                                                                                                |  | OS-EXT-STS:power_state               | 0                                                                                                                                                   |  | OS-EXT-STS:task_state                | None                                                                                                                                                |  | OS-EXT-STS:vm_state                  | error                                                                                                                                               |  | OS-SRV-USG:launched_at               | None                                                                                                                                                |  | OS-SRV-USG:terminated_at             | None                                                                                                                                                |  | accessIPv4                           |                                                                                                                                                     |  | accessIPv6                           |                                                                                                                                                     |  | addresses                            |                                                                                                                                                     |  | config_drive                         |                                                                                                                                                     |  | created                              | 2016-05-02T20:29:54Z                                                                                                                                |  | fault                                | {'code': 500, 'message': 'No valid host was found. Exceeded max scheduling attempts 3 for instance 134b69dc-56fc-4249-b92f-e958e561ae3b. Last       |  |                                      | exception: [u\'Traceback (most recent call last):\\n\', u\'  File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 2235, in _do',   |  |                                      | 'created': '2016-05-02T20:29:57Z'}                                                                                                                  |  | flavor                               | m1.medium (3)                                                                                                                                       |  | hostId                               | b383eddb06f7a1cc5929e5fa8b6982cc523f5ac1cbe3c9c40120a700                                                                                            |  | id                                   | 134b69dc-56fc-4249-b92f-e958e561ae3b                                                                                                                |  | image                                | centos-7-slurm-20160422210744 (7364ada7-263e-4fb0-a9f4-219ab19e0be0)                                                                                |  | key_name                             | jhoblitt-slurm                                                                                                                                      |  | name                                 | slurm-slave4                                                                                                                                        |  | os-extended-volumes:volumes_attached | []                                                                                                                                                  |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                                                                                                    |  | properties                           | slurm_node_type='slave'                                                                                                                             |  | status                               | ERROR                                                                                                                                               |  | updated                              | 2016-05-02T20:29:57Z                                                                                                                                |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                                                                                                    |  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  {code}      "
1,April 2016 LAAIM work,"Drafted documentation for Web SSO capabilities: https://confluence.lsstcorp.org/display/LAAIM/Web+SSO  Began testing new NCSA IAM capabilities (group management, user self-registration).  Registered NCSA with InCommon as a sub-org of UIUC to ease future IdP/SP registrations.  Attended local NCSA LSST coordination meetings."
3,April Work for ConOps,"Work on developing, editing and providing feedback for various ConOps.  Converted existing ConOps to new format."
1,Replace exiting DipoleMeasurementTask with DipoleFitTask,"The goal of this ticket is to replace the existing DipoleMeasurementTask with the new  DipoleFitTask, subsequent to ticket DM-5413.    TBD: does this include completely removing all remnants of DipoleMeasurementTask code?"
1,Test planned implementation of Lupton(ZOGY) algorithm in real space,Develop a (1-D?) simple toy model and test the effects of the correction for varying I1 and I2 noise levels and different image PSFs and matching kernel(s). This will be done first in an ipython notebook.    See DM-5914.
3,Trial implementation of Lupton(ZOGY) in stack,The Lupton reinterpretation of the ZOGY algorithm in real-space is essentially a post-convolution that implements noise whitening (or decorrelation) of the image difference. We will make a first-pass at implementing this in the existing diffim codebase in order to perform future evaluations on real data.
0,Replace jointcal.StarSelector with meas_algorithms.starSelector,jointcal has its own custom star selector. This should be removed and replaced with a star selector based on meas_algorithms.starSelector. A good choice might be meas_algorithms.objectSizeStarSelector.
0,Update developer guide with Astropy guidance,Once RFC-178 is adopted the developer guide has to be updated to include guidance as to how Astropy can be used in the stack (similar to how Boost is documented).
0,Package Astropy for the stack,Once RFC-178 is adopted Astropy needs to be packaged in an EUPS container. Given the complexity of Astropy dependencies the packaging will be done as for {{numpy}} and {{scipy}} by checking that Astropy is available (v1.1 will be the minimum version).
0,Make afw rgb unit test PEP440 compliant for matplotlib check,"If a user has a version of matplotlib installed from a git clone, the afw rgb unit test fails at the matplotlib version check. The versioning scheme for this type of install is determined by pep 440. Make the unit test properly handle this type of version comparison."
0,April work for middleware,Participated in requirements definition
0,Redirect non HTTPS requests on LSST the Docs to TLS,See https://docs.fastly.com/guides/securing-communications/allowing-only-tls-connections-to-your-site
0,Pre-release versions of matplotlib 2.0 break afw unit tests,"In the afw rgb unit test, testWriteStarsLegacyAPI checks to make sure that a file name with an unknown extension raises a value error. In current version of matplotlib, saving a file with an unknown extension causes this error:  {code}  *** ValueError: Format ""unknown"" is not supported.  Supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff.  {code}    In matplotlib 2.0 prerelease the file is saved as a png when an unknown extension is specified. Since the write call success the unit test fails as it is expecting a failure.     If nothing depends on this behavior, the unit test should probably be removed."
1,Create new build based on the converted firefly code.,#NAME?
0,Private network not available across all instances,"I'm setting up an ELK system. Part of that is an Elasticsearch system. When I bring up the system the private network is bisected. I attempted creating a security group, in case that was a problem but it didn't help. Note that the work around is to create security groups or use a firewall and use floating ips. This is far from ideal. I think the right solution is to use the private network.    Example:    First section {{p-es-1}} {{p-es-3}} {{p-es-k}}    {code:bash}  vagrant@es-1:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:47:28:a7            inet addr:10.0.42.30  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe47:28a7/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:363265 errors:0 dropped:0 overruns:0 frame:0            TX packets:304215 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:95396177 (95.3 MB)  TX bytes:238466304 (238.4 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:850 errors:0 dropped:0 overruns:0 frame:0            TX packets:850 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:138411 (138.4 KB)  TX bytes:138411 (138.4 KB)    vagrant@es-1:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  64 bytes from 10.0.42.32: icmp_seq=1 ttl=64 time=0.284 ms  64 bytes from 10.0.42.32: icmp_seq=2 ttl=64 time=0.266 ms  64 bytes from 10.0.42.32: icmp_seq=3 ttl=64 time=0.265 ms  64 bytes from 10.0.42.32: icmp_seq=4 ttl=64 time=0.302 ms  ^C  --- 10.0.42.32 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.265/0.279/0.302/0.019 ms  vagrant@es-1:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  64 bytes from 10.0.42.34: icmp_seq=1 ttl=64 time=0.333 ms  64 bytes from 10.0.42.34: icmp_seq=2 ttl=64 time=0.325 ms  64 bytes from 10.0.42.34: icmp_seq=3 ttl=64 time=0.322 ms  64 bytes from 10.0.42.34: icmp_seq=4 ttl=64 time=0.319 ms  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.319/0.324/0.333/0.022 ms  vagrant@es-1:~$ ping 10.0.42.31  PING 10.0.42.31 (10.0.42.31) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.31 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3017ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.33 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3008ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.35 ping statistics ---  5 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3999ms  pipe 3  {code}    Second section {{p-es-2}} {{p-es-4}} {{p-lfr}}    {code:bash}  vagrant@es-2:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:6f:30:2c            inet addr:10.0.42.31  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe6f:302c/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:196344 errors:0 dropped:0 overruns:0 frame:0            TX packets:160561 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:47667399 (47.6 MB)  TX bytes:7135345 (7.1 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:97268 errors:0 dropped:0 overruns:0 frame:0            TX packets:97268 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:8558096 (8.5 MB)  TX bytes:8558096 (8.5 MB)    vagrant@es-2:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  64 bytes from 10.0.42.33: icmp_seq=1 ttl=64 time=0.311 ms  64 bytes from 10.0.42.33: icmp_seq=2 ttl=64 time=0.309 ms  64 bytes from 10.0.42.33: icmp_seq=3 ttl=64 time=0.300 ms  ^C  --- 10.0.42.33 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 2000ms  rtt min/avg/max/mdev = 0.300/0.306/0.311/0.020 ms  vagrant@es-2:~$ ping 10.0.42.30  PING 10.0.42.30 (10.0.42.30) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  ^C  --- 10.0.42.30 ping statistics ---  5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4014ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  From 10.0.42.31 icmp_seq=5 Destination Host Unreachable  ^C  --- 10.0.42.32 ping statistics ---  5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 4023ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3006ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  64 bytes from 10.0.42.35: icmp_seq=1 ttl=64 time=0.387 ms  64 bytes from 10.0.42.35: icmp_seq=2 ttl=64 time=0.278 ms  64 bytes from 10.0.42.35: icmp_seq=3 ttl=64 time=0.288 ms  ^C  --- 10.0.42.35 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 1998ms  rtt min/avg/max/mdev = 0.278/0.317/0.387/0.053 ms  {code}    This can be reproduced by sourcing your OpenStack credentials and running this [{{Vagrantfile}}|https://gist.github.com/jmatt/7b6eb6a042c4e63531d40d1a68069f33]. Use {{vagrant ssh p-es-1}} to connect to the {{p-es-1}} instance.  "
0,Public elasticsearch configuration using SSL/TLS and basic auth,Create a (relatively) secure way to use Elasticsearch from outside of Nebula using Ansible. See:    https://www.elastic.co/blog/playing-http-tricks-nginx    Note that this will not allow another Elasticsearch to join but will allow the usual admin and client queries using basic auth.
1,Add Git refs to Jobs Table of QA Dashboard,The Level 0 QA DB should know what Stack Git refs correspond to each job. This will enable plots to filter jobs based on development ticket so that a developer can understand how a branch compares to master.    This ticket will add jobs to the schema (http://sqr-009.lsst.io/en/latest/#level-0-qa) and create the necessary migration script.
0,Implement validate_drp plot in Bokeh as proof-of-concept for QA Dashboard,This ticket will implement a plot from validate_drp in the QA Dashboard as a proof-of-concept for how existing matplotlib plots can be re-implemented in Bokeh with data from the QA database.    Stretch goals (maybe for a future ticket) will be to overplot the validate_drp output of one job against another’s to understand performance changes.
3,SUIT design ,"Finish SUIT design, produce a design document"
3,Jupyter widget using Firefly,"Jupyter widget using Firefly visualization components.   To better support the user community in using Jupyter notebook with Python packages, we want to start exploring the process of creating Jupyter widget, using Firefly visualization capabilities. "
3,Workspace design,workspace design   * 
3,Testing framework setup and more unit test,Testing framework setup and more unit test code   
1,Make obs_subaru PEP8 (pyflakes) compliment,"Running pyflakes on obs_subaru revels many places where the code is not (LSST specific) PEP8 compliment. Actual coding bugs reviled by pyflakes were fixed in DM-5474, however many of the formatting issues need to be fixed. Once DM-4740 and DM-4668 are done, all remaining code should be brought into coding standard compliance."
1,Initial draft(s) of the Data Backbone ConOps,"Initial drafts of the data backbone concept of operations. Versions 0.1, 0.2. Produced document that is ready for friendly, internal review although document is not complete."
0,Internal review of Data Backbone ConOps and revision(s),NULL
1,Watch Boot Camp materials,Videos from the [DM Boot Camp|https://community.lsst.org/t/dm-boot-camp-announcement/249] cover a lot of topics a newbie like me is interested in.
1,Build LSST Software Stack from the source,Create a virtual machine with functioning LSST Software Stack to have an environment where I can see and play with its code.
3,Integration Environment: top level design,"Working with SUI/QServ teams to create a plan that includes deployment schedules, levels of support, discussions of administrative requirements in integration environment, detailed documentation before procurement, security reviews, reviews with deployment team."
2,Integration Environment: Procurement,Discussions with vendors. Quote selection. Budget tracking. Quote submission to finance. GCO follow up questions. OBFS follow up questions. Finance follow up questions. Overall tracking of purchase progression.
3,Node delivery to racking,"Node delivery acceptance, unpacking, inventory, rack builds, pdu placement, node racking, bios updates, power connections, "
1,Networking,"Unpacking networking equipment, inventory, ordering/procure/unpack cabling, equipment racking, cabling, config creation and management."
3,OpenStack deployment,"This activity is outside of LSST project; this story is a placeholder for demonstrating progression toward completion of the epic.    Some activities that are likely to occur:  Software installation, OS installation/updates, RAID configurations, software configuration, monitoring and notification system(s) installation and configuration, integration testing and friendly uses evaluation. "
2,Compute Upgrade,Discussions and planning for the creation of an LSST service-only tenant for better isolation (hence protection) from ad hoc services. Further discussions with LSST DM interested stakeholders about this feature. Policy development for use and monitoring of use of this feature.
1,Object Storage Installation,Discussions and planning for allocating the 1PB storage increase within Nebula between object storage and block storage. Further discussions with LSST DM interested stakeholders about this feature. Policy development for use and monitoring of use of this feature.
0,Remove use of Boost smart pointers in meas extensions,"Removal of boost smart pointers in DM-5879 missed some meas extensions which are not built as part of {{lsst_distrib}}. Namely: {{meas_extensions_shapeHSM}}, {{meas_extensions_simpleShape}} and {{meas_extensions_photometryKron}}.  Update these too."
2,Provide docker swarm POC for Qserv containers orchestration,NULL
1,Split secondary index loading from qserv_data_loader.py to separate unit test,"To isolate development and validation of secondary index loading strategies, encapsulate loading of ""pure"" secondary index data via qserv_data_loader.py, without using entire datasets."
1,Deploy any secondary index loader mods into qserv_data_loader.py,"Following completion of DM-5968, any modifications to the top-level data loading procedure for the secondary index need to be deployed back to the main qserv_data_loader.py driver."
3,Slurm deployment preparation,Installation and familiarization with the supported level of service of slurm installation. Includes rolling that configuration into puppet modules/manifests. Also includes documentation of the installation and configuration requirements.    This is month-of-April work; a new story continues into May.
3,Inventory to racking,"Inventory, unpacking, bios updating, racking, rack building, pdu installation, power cabling, OS installation, RAID configuration, xCAT deployment planning, Puppet deployment planning, puppet module/manifest creation, puppet config versioning, cabling strategies discussions, additional cable purchases (minor)."
1,Update developer guide with pytest guidance,Now that DM-5561 explains how to migrate to pytest compatibility the developer guide must be updated to state how to use pytest in unittests.
0,Change SubtractBackgroundConfig.isNanSafe default to True,[~price] suggests that the default value for {{SubtractBackgroundConfig.isNanSafe}} be changed from False to True.
1,Create and deploy Beats for Logstash and Elasticsearch.,Create and deploy Beats for Logstash and Elasticsearch. These beats are used to transport logs and monitoring data to ELK.    See: https://www.elastic.co/products/beats
1,Miscellaneous Nebula service items for x16,"This story is for miscellaneous Nebula service items that do not have individual LSSTDM JIRA issues (often handled in the RT ticket system)  including account creation requests, reporting on hanging/errant processes for cleanup, response & communiques on security incidents, etc. "
0, tests in testArgumentParser.py fail Jenkins run-rebuild on nfs,"(1) {{testOutputs}} fails because paths are compared literally  Jenkins run-rebuild #139 failed with pipe_base  https://ci.lsst.codes/job/run-rebuild/139//console  {code:java}  FAIL: testOutputs (__main__.ArgumentParserTestCase)  Test output directories, specified in different ways  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testArgumentParser.py"", line 497, in testOutputs      self.assertEqual(args.input, DataPath)  AssertionError: '/nfs/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input' != '/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input'  {code}    Please make the comparison more robust.     (2) File descriptor leaks  Jenkins run-rebuild #138 failed with pipe_base  https://ci.lsst.codes/job/run-rebuild/138//console  {code:java}  FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/utils/2016_01.0-2-g97a6e33/python/lsst/utils/tests.py"", line 133, in testFileDescriptorLeaks      self.fail(""Failed to close %d files"" % len(diff))  AssertionError: Failed to close 1 files  {code}    {code:java}  File open: /nfs/home/lsstsw/build/pipe_base/.nfs000000000a20a3f700005679  {code}  This test passes on local disk.  "
2,update jenkins to 2.x,NULL
0,Stop cleanly MySQL if configuration step fails,Next scripts doesn't stop cleanly MySQL if configuration step fails:    {code:bash}  admin/templates/configuration/tmp/configure/scisql.sh  admin/templates/configuration/tmp/configure/tools/sql-loader.sh  {code}
1,Use RO MySQL account in qserv_testdata,"It seems integration tests datasets are now loaded with Loader. So using MySQL root account is no more required in integration tests, an account with SELECT access on test databases (like qsmaster), should be enough.     Furthermore, all code related to MySQL writes can be removed from   {code:bash}  python/lsst/qserv/tests/sql/cmd.py  python/lsst/qserv/tests/sql/connection.py  {code}"
1,Add unicode support for Qserv password,Qserv password must be encoded in ASCII for now in qserv-meta.conf. Unicode passwords should be supported.
1,Use sagas in place of side-effects in chart-related controllers ,"Replace side-effects with saga and clean-up chart related controllers (TableStats, XYPlot and Histogram)."
1,Support Monocam reduction,"Monocam is being used on a telescope, and we want to reduce the data obtained.  This is made difficult by the fact that the camera and the telescope are not talking to each other so the usual header keywords are in separate files from the data."
1,A look at the overall performance of the application,Investigate the overall performance of the application and improve it where possible.  It is pointed out that triview is especially slow compare to expanded.  Need to investigate.
2,Reception and Placement,"Receive, unbox, inventory, inspect, build racks, rack nodes and power."
2,Networking Configuration,"Unbox, inspect, rack, power networking equipment, cables ordered, alternate cable purchase options tested, initial switch configuration(s), cabliing."
3,Provisioning,"OS + updates installation, imaging building, stateless/stateful node provisioning, test of image, security configurations, networking bandwidth tuning, file system tuning, file system installation and tuning."
3,Disaster Recovery Implementation,Testing/practicing recovery of node/image/software after various types of faults.
1,Documentation,Document each component sufficient enough for transfer of knowledge and system recovery as needed.
2,Capability Validation,Review that design was implemented successfully including recover and supporting documentation exists.
1,Security Vetting,Review of capability by site security team
0,Acceptance by Stakeholders,"Review with stakeholders (target users, release manager, others as necessary) to confirm that capability fulfills original requirements. "
1,Acceptance by Stakeholders,"Review of services (compute, storage, networking) by LSST project before considering work final."
1,Procurement,NULL
1,Reception and Placement,NULL
2,Networking Configuration,NULL
3,Provisioning,NULL
3,Disaster Recovery Implementation,NULL
2,Documentation,NULL
2,"Capability Validation	",NULL
2,Security Vetting,NULL
3,Acceptance by Stakeholders,NULL
0,Reception and Placement,NULL
1,Networking Configuration,NULL
2,Provisioning,NULL
1,Disaster Recovery Implementation,NULL
1,Documentation,NULL
1,Capability Validation,NULL
1,Security Vetting,NULL
1,Acceptance by Stakeholders,NULL
1,Lazy load related chart data on table data update,"When new table data received, the related chart data should be updated only for the components on display. Hidden components' data should be lazily updated when a component becomes visible."
0,ingest.py throwing away errors,"Line 118 of ingest.py has a problem try block which is currently just throwing away errors, which has made for some confusing/frustrating debugging.    This should be changed to either warn or raise, but not silently dispose of errors."
0,Make it possible to distinguish TABLE_NEW_LOADED actions triggered by sort,"It would be beneficial to have in the TABLE_NEW_LOADED payload a  trigger field, which would differentiate actions triggered by sort (where  data do not change, only their order) or filter from other loads. We don't  need to reload table statistics or histogram on sort. But we do need to to  reload them on filter.      created TABLE_SORT action to distinguish sorting from filtering.  sorting should not reload xyplot nor catalog overlay.    Also:  - disable history when in api mode.  - ensure tableMeta.source reflects the file on the server.  - fix TablePanelOptions not resetting columns selection.  - remove 'Fits Data' tab when no images available.  - fix 'Coverage' appearing when it should."
0,Validation is not performed on unchanged fields,"Currently, validation is performed only if a field has changed. We need to be able to validate all fields on form submit.    The issue is not limited to initial (ex. empty) value being invalid. The invalid message is lost when a field is unmounted/re-mounted.    You can test the following way:  - http://localhost:8080/firefly/;a=layout.showDropDown?view=AnyDataSetSearch  - Open chart settings, enter 1000 into X/Y ratio - the field is shown as invalid  - Switch to histogram and back, the invalid message is gone, the field appears to be valid    Another test case is Example Dialog tab 'X 3', 'X 3'  tab test field initial value 88 is invalid (it should be between 22 and 23), but it appears valid. "
0,Error message is not shown,The error message is not showing consistently when mouse is over tha exclamation icon.
2,Investigate possibilty of cosmic ray muons (etc) for precision gain calibration,"In the era of CBPs, we care about absolute system throughput, and thus need to accurately know the gain of amplifiers in the CCDs.    Initially, this can be done by lab-based Fe55 characterisation (modulo the non-linearity, though that itself will need to be need to be characterised and corrected for), but changes in the relative gains of the various amplifiers need to be monitored, and this must be done in a way that is not degenerate with the optical transmission in any way.    Theoretically it should be possible to use cosmic ray muon tracks, and tracks from radioisotope contamination of the glass/dewar, to measure the (change in the) relative gains of the amplifiers.    Early work has shown that this does work in principle, but this ticket is for some further effort to see whether this method can provide the necessary accuracy given the amount of data available remains to be seen.    This ticket would normally need to be significantly more points, but as it builds on earlier work, it can, at least for initial results, be done quite cheaply.    Initial investigation will inform further work.    1st order: histogram all pixels in dark images after careful bias subtraction. Look at shape of spectrum. Fit some arbitrary function, and correlate with Fe55 gain measurements.    2rd order: Separate muon tracks from soft electron tracks/nuclear recoil events. Can cutting one of more of these types out improve the correlation? Or can treating them separately improve the resolution?    3rd order: Recalculate the histogram in terms of the dE/dx for the muon tracks, i.e. taking into account their track lengths and thus angle through the silicon."
1,Create documentation for bright object masks,"The bright object mask code ported from hsc bought the ability to mask regions, during coaddition, by providing mask files. How to create these files, and where they should be placed in the file system is documented on and HSC ticket (https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1351) but not on the LSST side. The information on how to create and use bright object masks should be put into LSST documentation."
0,Support sphinxcontrib-bibtex in technotes,Allow bibtex-based references in technotes using [sphinxcontrib-bibtex|https://github.com/mcmtroffaes/sphinxcontrib-bibtex].    DMTN-010 will be used as a pilot case.
0,Produce and ingest master calibs for USNO monocam data.,"Use the construct*.py scripts added to pipe_drivers to produce temporally relevant master biases, darks, flats (and fringe frames?) for the recent USNO observing with monocam.    A small amount of hacking will be required due to the fact that the current ingestion model assumes that each CCD frame has a USNO counterpart which tells about the telescope pointing etc, but the bias frames do not have these.    Once the master calibs are produced, get them ingested."
1,Reduce sky data from USNO monocam run,"Using the master calibs produced in DM-6036, push all the monocam data through processCcd.    Others will run sanity checks on the output (initial astrometry & photometry). From there I believe people will look at using the data to test jointcal & sim_astrom etc, but this ticket just related to the initial reduction.    As more data comes in from the 2nd telescope, a little further hacking may be necessary to keep everything running. Some of this will likely be hacky or need one-off solutions/header modification, hence the higher-than-normal number of story points assigned to what one might expect to be an hour-long job."
0,Monocam bias structure analysis,"Estimates of the noise in the bias frames coming from the USNO monocam run around ~20e- RMS. This is higher than with the same readout configuration in the lab, and could be due to several things.    This ticket is to take a look at a few bias frames and investigate the structure of the noise. If it is periodic and at a constant phase then using master biases will significantly improve the SNR in the calexps produced, but if it is not, then whether or not bias frames should be used at all should be considered."
0,Download temporally relevant raw calibs for CTIO DECam data,"Go to the NOAO portal and download a sufficient number of darks, biases and flats (in each band) from around the time of the CTIO trip to produce master calibs."
0,Create and ingest master calibs for DECam CBP reduction,"Having collected data in DM-6039, push all this through the master calib creation scripts and ingest master calibs into registry.    This is a necessary but not-necessarily-sufficient ingredient for making progress on DM-5465."
0,Functional use cases for tools,"Work with Vandana Desai (IRSA) to tabulate science use cases for tools. Then transform the science use cases to functional use cases (""this is how we want the tool/interface to behave"").    This work is  to identify common functional and scientific use cases between PTF/ZTF and LSST to inform LSST on how the SUIT web portal might be organized for user interaction with LSST data. "
1,Add viewer launching API,"Add ability to launch the viewer and load images, xyplots, and tables from the api. We use to call this firefly.getExternalViewer()  Also, we have this concept of 'root path' through out the code. The api use can set a root path so he can use his when we are cross site. Need to implement.  "
2,Change mouse readout to use supports MouseReadoutCntlr & add an API readout,"Change mouse readout to use MouseReadoutCntlr.     * Use {{VerySimpleMouseReadout}} as a reference.   * Change to use {{MouseReadoutCntlr}} for options instread of ImagePlotCnltr  * Add a second (vertical) Mouse readout to be used in the api mode. The readout should be set into {{ApiUtilImage.jsx}}, {{initAutoReadout}}.  It should replace {{VerySimpleMouseReadout}}       & add an API readout"
0,Bundle up more HSC data for validate_drp,We would like to include a larger set of HSC data for validation.  I tested this while in Tucson.  My working dir was {{/tigress/pprice/frossie}}.  The raw and processed data should be stuffed into validation_data_hsc
0,Table caching optimizations,"We need to avoid duplicate requests which result from minor differences in TableRequest parameters, which are not used to get data.  For example, loading catalog table, which triggers table statistics, and then getting an XY plot, results in 3 requests, returning identical data.    1. RequestClass=ServerRequest; *tbl_id=tbl_id-1;* UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; catalog=wise_allwise_p3as_psd; RequestedDataSet=wise_allwise_p3as_psd; radius=200; use=catalog_overlay; catalogProject=WISE    2. RequestClass=ServerRequest;RequestedDataSet=wise_allwise_p3as_psd; catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; catalogProject=WISE; radius=200; SearchMethod=Cone    3. RequestClass=ServerRequest; *tbl_id=xyplot-tbl_id-1;* catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; RequestedDataSet=wise_allwise_p3as_psd; catalogProject=WISE; radius=200; *decimate=decimate=ra,dec,10000,1,,,,*    The difference between 1 and 2 is tbl_id parameter. The difference between 2 and 3 is tbl_id and decimate parameters. As well as the order of the parameters. None of which change the catalog search result.    Test Case: Test Searches, Test catalog, AllWISE Source, radius=200"
0,Add extendedness vs. star selector test to single-visit validation in ci_hsc,"ci_hsc has a test that verifies that extendedness as measured on coadds broadly agrees with the star selection done for PSF estimation on individual frames.  This tests a bunch of stuff, including aperture corrections on the coadds and propagation of flags from visits to coadds.    It doesn't test that aperture correction vs. extendedness logic is correct in processCcd.py, but just copying this test to the appropriate validation function in ci_hsc should do the trick.  This is currently broken, but should be fixed in DM-5877."
1,Improve password management for Qserv MySQL accounts,"Qserv passwords management for MySQL account (i.e. root, monitor, qsmaster) should be improved. See wmgr password management to have a good example. Furthermore qsmaster use currently empty password, this must be fixed."
1,Allow use of other MySQL account thant 'qsmaster',"'qsmaster' value can't be changed in qserv-meta.conf, this must be fixed    {code}  diff --git a/admin/templates/installation/qserv-meta.conf b/admin/templates/installation/qserv-meta.conf  index 81b6ca9..203ebda 100644  --- a/admin/templates/installation/qserv-meta.conf  +++ b/admin/templates/installation/qserv-meta.conf  @@ -103,7 +103,7 @@ user_monitor = monitor   password_monitor = CHANGEMETOO      # Used to access Qserv data and metadata (like indexes)  -user_qserv = qsmaster  +user_qserv = qservdata  {code}    Above change leads to next error in integration tests:    {code:bash}  154 [0x7f1beacf9700] ERROR lsst.qserv.sql.SqlConnection null - connectToDb failed to connect!  154 [0x7f1beacf9700] ERROR lsst.qserv.sql.SqlConnection null - runQuery failed connectToDb: START TRANSACTION  2016-05-10 14:55:09,684 - root - CRITICAL - Exception occured: Error from mysql: (-999) Error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=XXXXXX, db=qservCssData, socket=]  Traceback (most recent call last):    File ""/home/dev/src/qserv/bin/qserv-data-loader.py"", line 274, in <module>      loader = Loader()    File ""/home/dev/src/qserv/bin/qserv-data-loader.py"", line 225, in __init__      css_inst = css.CssAccess.createFromConfig(config, """")  CssError: Error from mysql: (-999) Error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=XXXXXX, db=qservCssData, socket=]  2016-05-10 14:55:09,810 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/dev/qserv-run/git/etc/wmgr.secret --delete-tables --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.cfg --css-remove --skip-partition --chunks-dir=/home/dev/qserv-run/git/tmp/qservTest_case05/chunks/Object --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.cfg --empty-chunks=/home/dev/qserv-run/git/var/lib/qserv/empty_qservTest_case05_qserv.txt qservTest_case05_qserv Object /qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.sql /home/dev/qserv-run/git/tmp/qservTest_case05/chunks/Object/Object.txt   ERROR  {code}  "
0,Minor updates in suptertask from following DMTN-002,Some examples in the DMTN-002 seem slightly out of date.    Update supertask documentation and code to catch up with some recent developments in the stack. 
2,v12.0 [Winter 2016 / Extra 2016] release,This is the ticket for the v12.0 release prep.     Edit: Release announcement at https://community.lsst.org/t/lsst-stack-version-12-0-winter-2016-extra-2016-release/874  
2,Launch integration tests using Docker+Openstack,vagrant and packer will be replaced by openstack python API and cloud-init which are more flexible.    - A pseudo-DNS will be provided thanks to avahi/mdns  - Only one public/floating IP willl be used (in order to allow booting large clusters laters)
0,Fix how aperture correction is applied,[~lauren] committed a fix Jan 15 to how aperture correction is applied that I accidentally lost when refactoring in DM-4692. https://github.com/lsst/pipe_tasks/commit/d904e3d188698b4f57bf3dad1516b0bf201078f5 Restore the fix.    The need for this fix suggests a design flaw in measurement that will be fixed as part of DM-5877
3,"Define, design, and RFC repository refactor.",Includes support for   * butler manages input & output repos  * repo tagging  
1,design work for butler storage & format factorization,NULL
2,Camera team visualization support (F16),Support the camera team to use Firefly for their visualization needs.
3,L1 Messaging path status,"All principle entities for L1 are in place and the messaging is working as intended. The message dictionary includes some message types for prototyping and will likely double in size as the interface between OCS is firmed up in the coming weeks. The implementation thus far is a 'ready, set, go' set of states. Test files from 10 Forwarders are sent to 10 distributors thru a WAN emulation device, and the result can be timed. The DMCS component is a simple CLI to initiate messages as currently written. This component will be expanded as requirements are determined.    Still under development is a component layer between the Condor controller and the NCSA Foreman entity so that resource availability can be queried and provide a communication link for ancillary information as needed."
3,Li prototype code and the Wan Emulator,"L1 Forwarder components and Distributor components are located on opposite sides of the Emulator (the Long Haul network component) and move files across the path when configured and  given a 'go' signal via messaging. Forwarder/Distributor pairs are set up dynamically for each file transfer (similar to a readout event). Results are temporarily forwarded to a status queue sink, where messages are processed for the publishing of results."
1,Pass background to NoiseReplacerTask,"Implement RFC-180:    `NoiseReplacerTask` wants some statistics about the background that was subtracted from the exposure, but it gets these in a fragile and roundabout fashion: it expects the code that measures the background to put the mean and variance into the exposure's metadata, using special keys. It is difficult to enforce correctness because background is measured several times while processing an exposure.    To solve this, pass the background directly to `NoiseReplacerTask`. This will require passing the background through the various measurement tasks, which will require small changes to code that calls the measurement tasks.    In addition, remove computation of background statistics from the background fitting code."
0,Add RegistryField support to Task.makeSubtask,As part of implementing RFC-183 add support for tasks specified in {{lsst.pex.config.RegistryField}} to {{lsst.pipe.base.Task.makeSubtask}}  
0,Document the need for abstract base tasks for tasks,"As part of RFC-183 document the fact that variant tasks should have a common abstract base class that defines the API. If we add future tasks that we feel are likely to have variants, then we should create an abstract base class.    Candidates include star selectors, PSF determiners and ISR tasks.    Note that this applies to tasks LSST provides in its stack, not to variants users produce and other obscure one-off code.    Also document the desire that tasks with anticipated many variants, such as star selectors, and PSF determiners should be in registries. This explicitly excludes tasks such as ISR where only one task is likely to be useful for a given set of data.  "
1,Create a registry for star selectors,Create a registry for star selectors and use the registry instead of ConfigurableField in tasks that call a star selector.    Update config overrides in obs_* packages and unit tests accordingly.
0,Change PSF determiners into tasks,"PSF determiners are already configurables, and some benefit from having a log. Take the logical next step and make them instances of {{lsst.pipe.base.Task}}."
0,Aperture correction fails to measure a correction for the final plugin in the list and reports misleading errors,"Since the refactoring of DM-4692, runs of *processCcd.py* detail the following in their logs:    {code:title=With base_PsfFlux and base_GaussianFlux plugins registered}  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measurement: Measuring 65 sources (65 parents, 0 children)   processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 1 flux fields  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  ...  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find base_GaussianFlux_flux or base_GaussianFlux_fluxSigma in apCorrMap  {code}    {code:title=With base_PsfFlux, base_GaussianFlux, and ext_photometryKron_KronFlux plugins registered}  processCcd.charImage.detectAndMeasure.measureApCorr: Measuring aperture corrections for 2 flux fields  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_GaussianFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  ...  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 3 flux fields  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find ext_photometryKron_KronFlux_flux or ext_photometryKron_KronFlux_fluxSigma in apCorrMap  {code}    I can confirm that for the latter, running HSC data with the fix on DM-6063, the aperture corrections are being measured and applied for the PsfFlux and GaussianFlux measurements, but NOT for the KronFlux measurements.      Looking at the output from the current ""expected"" values for the {{lsst_dm_stack_demo}} we see that there is an offset in the Psf-Gaussian fluxes, implying the Gaussian fluxes are not being measured (and hence not applied):  !demo_current.png|width=500!    From this I conclude that the aperture corrections are indeed being measured for all but the final entry in the plugin list.  This implies that the report of ""Only 0 sources for calculation of aperture correction for 'xxx_xxFlux'; setting to 1.0"" is incorrect for all but the final plugin measurement.    The demo previously successfully calculated aperture corrections and, after the logic fix of DM-4836, applied them in the correct order:  !demo_previous.png|width=500!    The sources of these issues and fixes for them are the goal of this issue."
0,description of archive in a box,"Please put in more detailed description of the ""Archive in a box"" concept"
0,Add Sublime Text configuration tips to Developer Documentation,"[~rowen] and [~Parejkoj] have some good tips about setting up Sublime Text.  [~jsick] suggested that we add these configuration tips to the Developer Documentation.    http://developer.lsst.io/en/latest/#part-tools    Both want to include info about recommended packages, but also the linter configurations to support the DM styles.    I paste in here various helpful parts from the HipChat Software Development room discussion of this.  Both verbatim, and summarized.    1. Install {{Package Control}}    2. Packages:  {{Git}}, {{GitGutter}}, {{SideBarEnhancements}}, {{SublimeLinter}}, {{SublimeLinter-flake8}}, {{SublimeLinter-html-tidy}}, {{SumNumbers}}, {{Gist}}, {{BracketHighlighter}}, {{TrailingSpaces}}, {{Trimmer}}, {{OmniMarkupPreviewer}}, {{ReStructuredTextImproved}}, {{MarkDown Editing}}, {{Colorsublime}}    3. Themes:  {{Sunburst}} color scheme    * VIM users:  {{Vintageous}}  + Mac OS X configuration:  {{defaults write com.sublimetext.3 ApplePressAndHoldEnabled -bool false}}  so that holding down 'j' moves downward.  Note that {{Vintageous}} is not a complete implementation of {{vim}}, but it at least allows enough basics so that one doesn't go crazy switching back and forth.    link the {{subl}} command to {{/usr/local/bin}}     Quick Tips:  ""option-select (to select blocks) and select something then cmd-D are both extremely useful for modifying lots of things at once.""    ""Similarly, ctrl-shift-up/down arrow.""    ""cmd-click on multiple lines to have multiple synchronized cursors""    Configurations:  1. [~rowen]'s SublimeText Preferences file: https://jira.lsstcorp.org/secure/attachment/27846/Preferences.sublime-settings  2. Configuration {{flake8}} so that it works in the linting can take a bit of work if {{flake8}} isn't in your default path.  See SublimeLinter.sublime-settings attachment for [~rowen]'s configuration: https://jira.lsstcorp.org/secure/attachment/27845/SublimeLinter.sublime-settings    The above are useful, but we'll need someone to detail the linter stuff more."
0,Enable websocket client to pickup channel parameter from url,send websocket channel information via url.  keep channel information on browser reload.    This is needed for Firefly Python API and external (when Firefly viewer is invoked trough URL) API.  
1,JSON Schema for metric data from validate_drp to be ingested by the QA Dashboard app,"A well-defined JSON schema is needed for {{validate_drp}}’s JSON output so that it can be easily, and consistently ingested into the QA Database. The schema will also make the JSON more self-describing, and potentially useful for other tools to build upon as well.    The schema is being drafted in a thread at https://community.lsst.org/t/json-schema-for-squash/777?u=jsick. Once an informal consensus is reached the schema will be implemented in {{validate_drp}} on this ticket."
1,jenkins job to execute validate_drp and push results to qa dashboard,"This is the initial jenkins job that ""ties"" all the components together.    It needs to:    * execute validate_drp  * push metadata about the jenkins build to qa dashboard  * push the validate_drp metrics to qa dashboard"
0,Use fixed width integer types from std instead of boost,The following fixed width integer types are used in the stack:    * {{boost::int16_t}}  * {{boost::int32_t}}  * {{boost::int64_t}}  * {{boost::int8_t}}  * {{boost::uint16_t}}  * {{boost::uint32_t}}  * {{boost::uint64_t}}  * {{boost::uint8_t}}    This ticket aims to replace them with their equivalents from {{cstdint}}.
1,draw a diagram of DRP data flow,"Study Jim Bosch's diagrams and descriptions (Parallelization in Data Release Production, Data Release Production Top-Level Overview), consider inputs/outputs of high level pipelines and parallelization of the DRP, draw a diagram to illustrate the data flow. "
1,Improve afw.table Astropy view support,"DM-5641 completed the first version of Astropy view support, but there is still room for improvement:   - Make {{Footprint}} s in {{SourceCatalog}} s available as a {{dtype=object}} column.  Same for {{Psf}} , {{Wcs}} , {{Calib}} in {{ExposureCatalog}}.   - Use Astropy's coordinate classes for Coord fields (may require an RFC to determine how much we want to use Astropy's coordinate classes).  "
0,afw/tests/rgb.py fails due to .ttf files,"afw/tests/rgb.py fails for me with the below error. We likely shouldn't be trying to track system resources like fonts, as we don't have any control over them.    {code}  [2016-05-12T19:46:12.528961Z] Failed test output:  [2016-05-12T19:46:12.536029Z] tests/rgb.py  [2016-05-12T19:46:12.536057Z]  [2016-05-12T19:46:12.536070Z] ...s......ss...F.  [2016-05-12T19:46:12.536106Z] ======================================================================  [2016-05-12T19:46:12.536138Z] FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase)  [2016-05-12T19:46:12.536173Z] ----------------------------------------------------------------------  [2016-05-12T19:46:12.536192Z] Traceback (most recent call last):  [2016-05-12T19:46:12.536261Z]   File ""/Users/parejkoj/lsst/lsstsw/stack/DarwinX86/utils/2016_01.0-4-g52f464f/python/lsst/utils/tests.py"", line 134, in testFileDescriptorLeaks  [2016-05-12T19:46:12.536330Z]     self.fail(""Failed to close %d file%s"" % (len(diff), ""s"" if len(diff) != 1 else """"))  [2016-05-12T19:46:12.536352Z] AssertionError: Failed to close 2 files  [2016-05-12T19:46:12.536356Z]  [2016-05-12T19:46:12.536391Z] ----------------------------------------------------------------------  [2016-05-12T19:46:12.536404Z] Ran 17 tests in 3.451s  [2016-05-12T19:46:12.536407Z]  [2016-05-12T19:46:12.536424Z] FAILED (failures=1, skipped=3)  [2016-05-12T19:46:12.536445Z] File open: /Library/Fonts/NISC18030.ttf  [2016-05-12T19:46:12.536479Z] File open: /System/Library/Fonts/Apple Color Emoji.ttf  [2016-05-12T19:46:12.536495Z] The following tests failed:  [2016-05-12T19:46:12.539928Z] /Users/parejkoj/lsst/lsstsw/build/afw/tests/.tests/rgb.py.failed  [2016-05-12T19:46:12.540060Z] 1 tests failed  {code}"
1,implement basic oauth2 authentication for qa-dashboard,"Per discussion at the SQRE co-working session on Thursday, we agreed to implement minimal authentication for the MVP version of the qa dashboard as an external reverse proxy, such as https://github.com/bitly/oauth2_proxy."
1,Implement the post_save mechanism to update bokeh sessions when new data is available,In tickets/DM-5750 the bokeh python library was integrated in the squash django project. In order to exemplify its use the KPM CI chart is showing only hardcoded values for now.    In this ticket we will implement methods to read the data from the database and the post_save mechanism to update the bokeh session when new data is available.   
1,color map in visualization,"the four new colormaps introduced in matplotlib last year  http://bids.github.io/colormap/    d3js cmap: http://bl.ocks.org/mbostock/3289530    D3 supports CIELAB (Lab) and CIELCH (HCL) color spaces, which are designed for humans rather than computers. http://bl.ocks.org/mbostock/3014589    "
3,Firefly performance profiling and code refactoring if needed,We need to dedicate some effort in each cycle to do the performance profiling and code refactoring needed to improve performance
1,More work on firefly viewer layout control,"More work needs to be done on the triview layout controlling:    * When there is a table with image meta data is loaded, the images need to show with the meta data tab selected  * When any data is pushed then drop downs needs to close  * when a table a catalog table is loaded and there are no plots then then the tri-view should be up with the coverage tab selected. When there is plots then the coverage tab should not be selected.  * we need a way to remove load a table and then only see the table, same with xy-plots  * catalog and image meta data are determined by looking at the data.  However, we might need this logic in a single function  * when a table is loaded and we cannot determine what type it is then the table and the xyplots only should some up.  * When all data is deleted the default tab should open.  (in IRSAViewer case the is the select image panel)"
0,Browsers should cache editions for a shorter time period than Fastly,"Currently we set {{Cache-Control: max-age=31536000}} so that Fastly caches uploads from LTD Mason for a year on its POPs. This has the side-effect of also having browsers potentially cache documentation on the client for up to a year. In practice, browsers churn through their cache space more quickly, but I've noticed that Safari has no cap on its cache space, and therefore can hold onto pages for a long time.    The solution is to set a {{Surrogate-Control}} max age to 1 year, and have {{Cache-Control: max-age=0, private, must-revalidate}}. This will be done on LTD Keeper during the copy phase of a build into an edition (since it is reasonable for a client to cache a build forever), but then give us the flexibility to update an edition instantly.    In the future we may want a more nuanced solution where CSS and JavaScript, for example, are cached longer on the browser."
0,Provide minimal documentation for meas_extensions_photometryKron,"Please provide a minimal level of documentation for meas_extensions_photometryKron, to include:  * A doc directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary).  "
3,Calibration Products Pipeline work during F17,"This will need to be properly fleshed out before scheduling: for now, it's a bucket for stories that [~mfisherlevine] is unlikely to complete in X16. [~rhl], [~mfisherlevine] & [~swinbank] to provide further definition."
1,The color stretch dialog box does not work properly,"There are a few issues in the Color Stretch dialog box:  # When the asinh or gamma algorithm is selected,  the asinh parameters and gamma parameters are always reset to the default.  The user-entered values can not be kept and used.   # When there are two or more images, when the lower/upper range in one of the image is set, the lower/upper range in all the rest images are set to the same lower/upper range.    # The rangeValues are always reset each time when the Color Stretch dialog is open.  "
0,"update ""newinstall.sh"" nebula images & docker containers - w_2016_20",NULL
1,Remove old DipoleMeasurementAlgorithm from imageDifference.py,"Currently the new algorithm is run alongside the old. This ticket will deprecate the old algorithm, making the new one the default. This will be done after the new algorithm is vetted on real data (DM-5412).  It may also be blocked by the pending SFM overhaul so that it can be implemented as a standard registered plugin."
1,Build SFM housing for PSF approximation using ngmix code,"Build a measurement plugin which allows PSF approximation to be done using ngmix.  After consulting with Erin, it was decided that this would make use of the EM code and would produce as its output some variable number of Gaussians.  These will be turned into MultiShapeletFunction outputs.    A suitable set of configuration options and output failure flags will also be provided."
1,Testing ngmix Psf plugin with CModel,Test that the ngmix PSF approx plugin works correctly in our measurement framework by testing it with CModel and comparing results with those produced with ShapeletPsfApprox.
1,Do robustness tests of ngmix PSF approx plugin,"Run tests on the ngmix PSF approx plugin similar to those which were run on ShapeletPsfApprox.  We will test both for how long the plugin takes to run, and how often it fails.    Note previous report on CModel and SPA was DM-4368"
0,LSST's version of Astrometry.net doesn't build on Ubuntu 16.04,"Reproduced building on Ubuntu 16.04.    https://groups.google.com/forum/#!topic/astrometry/aDCjhfMYhpE    The current version (0.67) does build successfully standalone.    These two patches fix 0.5.0:  https://github.com/dstndstn/astrometry.net/commit/7ded70917d7cf1efa1d3af6d0da8b336ebbf9d92.diff and https://github.com/dstndstn/astrometry.net/commit/7c65b3cefc4f33c59af90c1a40b5f246002cdf28.diff  Though only the first one is needed, I believe the second one is part of the build already."
0,ngmix has no license,"ngmix does not have a license, which means we shouldn't distribute it. Work with Erin Sheldon to see if he is willing to add one."
0,Expanded view not doing fit/fill consistently ,"Expanded view not doing fit/fill consistently. Sometimes is seems to fit/fill and resize it correctly, other times it stays at the zoom level.  It should always fit/fill and change zoom level with resize when in expanded mode. (unless zoom type is FORCE_STANDARD)."
0,Fix docker git script: providing both -R and QSERV_DIR make it fails,NULL
0,mpi4py does not compile under Yosemite due to hardcoded MACOSX_DEPLOYMENT_TARGET,"{{mpi4py}} build on Yosemite (Mac OS X 10.10) fails with   {code}  _build.log:[2016-05-17T16:51:55.847161Z] error: $MACOSX_DEPLOYMENT_TARGET mismatch: now ""10.9"" but ""10.10"" during configure  {code}    For details see attached build log.    The {{MACOSX_DEPLOYMENT_TARGET}} is being set in {{ups/eupspkg.cfg.sh}}    {code}  [serenity mpi4py] cat ups/eupspkg.cfg.sh  # If MACOSX_DEPLOYMENT_TARGET is not set, we force it to be at least 10.9  # (Mavericks). This is the earliest version of OS X expected to work with  # release 11 of the LSST stack.  # This works around DM-5409, wherein mpi4py was attempting to use an OS X 10.5  # SDK, based on querying Anaconda, and failing.  export MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET:-10.9}  {code}    What is it that is supposed to be setting {{MACOSX_DEPLOYMENT_TARGET}}?  And why is it not set at the time when {{ups/eupspkg.cfg.sh}} is run, but is set to 10.10 by the time the actually compilation is done?   "
1,Fix style of catalog panel , finish up DM-5388 ticket by fixing the UI style of the panel
3,Migrate VO search panel,VO search panel is a tab part of the catalog drop down panel that should be migrated. This was an outlier of DM-5388 ticket.
1,Review and connect validation part to the input area field component,Catalog panel (DM-5388) needed an input area for polygon input search but it needs a review and connect the validation reducer to it.
2,Add input based on catalog DD table,"Add a panel to use DD catalog information to allow user to input catalog constraints as editable table component.    This is to get a table with one or more extra column which are input field. The extra column needs  a different default cell renderer (TextCell) - see TableRender.js.  This table component needs to be hooked up to the fieldgroup or some way so it can be used in the catalog search panel.   The catalog search panel will need to be adapted to display this table constraints to fully complete the search query options.    The implementation:  It contains the redesigned panel based on IRSA current OPS catalog search. A table with input constraints and sql area input are added.   There is a lot in that PR. In particular, usage of table renderers and 'fieldgroup' together with a changed BasicTable component to be able to get the value from input field custom column 'constraints'.   The panel will be reviewed also by IRSA and some input requirements were left out for now (mainly aesthetic details). Another left-out is the text are component and the validation of it. That should be addressed in the other ticket DM-6136. "
0,Change Fields groups to handle other actions better,"The fields group can be out of sync with actions if they are trying to use store data when that actual value is changes.  This is a classic side-effect issue.  It can be solved with sagas.    Our current example.  The color panels updating from the plot when the activePlotId changes.    More to do:  * field groups need a sega to more effective respond to out side actions  * the dispatchChangeFieldGroup needs better, more documented parameters  * update multiple fields at the same time.  * should we have the field group support reset to init state? probably not, but look into it.  * change init values?  * Check example dialog and see if the large/smaller example is validating correctly."
0,Change server side hardcopy code to work better with the non-GWT call,The server hard to make a hard copy now takes a StaticDrawInfo object.  We want to use only a region array.  Change the server side to support this.
1,Produce tech note describing detailed project management procedures,Write a technical note describing the detailed project management procedures derived by the pmp-wg. Source material is [~jbecla]'s document at https://github.com/lsst/ldm-pmt/.
1,Drawing layer improvement to handle mouse selection,"Drawing layers are not handling and sharing the mouse quite  right.  Also the mechanism to determine to is priority for the mouse needs work as well. This is all necessary to make markers work correctly, since every marker is an individual drawing layer.     Also, the draw layer utilities are all in PlotViewUtil.js.  They need to be moved to something closer to the draw layers."
1,Client side Hardcopy support for png with drawing layer overlay,Add all hard copy support so we can create a png with all the overlays.
0,jenkins/qa terraform destroy fails if there is an existing rds final snapshot,AWS appears to prevent the overwrite of an existing final rds snapshot.  This scenario may arise when creating/destroying a dev env multiple times.  This can be avoided by disabling the final snapshot when destroying an rds instance.  One way to resolve this would be to add a terraform var to signal this is his is a development env.
1,Evaluate performance of dipole fitting in crowded regions,"There are legitimate concerns about performance of the new dipole fitting algorithm in crowded fields. This will be evaluated (on real data? if no existing data, then realistic simulated data) and contrasted with other possible alternatives. This is in response to Zejlko's concern and suggestion that DipoleFitTask should constrain only positions using pre-subtraction images, and only fit fluxes using the diffim."
1,Set SUSPECT mask in ISR task and make saturation a double,"Implement RFC-190 and mask suspect pixels:    Add {{selectLevel}} to {{lsst.afw.cameraGeom.AmpInfoCatalog}}, as a double, and add support for it to {{lsst.ip.isr.IsrTask}}, analogously to masking saturation: iif {{suspectLevel}} is not {{nan}} then set the {{SUSPECT}} flag for pixels above the suspect level.    Also change the type of {{saturation}} in the {{AmpInfoCatalog}} from {{int}} to {{double}}, so that the existing test for {{nan}} actually works"
0,Reduce memory utilization in mysql proxy,Jon is trying to run tests with large result which kills proxy/czar because it runs out of virtual memory. Would be nice to reduce memory use and find a way not to keep query result in memory.
0,Failure to fail when fallbackFilterName is None,"When no {{fallbackFilterName}} is set, we can get a confusing error message when failing to load a calib:  {code}  RuntimeError: Unable to retrieve dark for {'filter': 'U', 'date': '2016-05-12T02:58:56.591', 'ccd': 0, 'basename': '2016-05-12skyflats_02', 'object': 'FLAT', 'visit': 883, 'expTime': 20.0006, 'channel': 16} and no fallback filter specified: Unknown value type for filter: <type 'NoneType'>  {code}  This is unrelated to the calib load failure, and merely reflects the fact that {{fallbackFilterName=None}}."
3,long term re-plan,"We need to make the long term plan for FY2017 - FY2019, ready for ComCam; and more FY2020- FY2022, ready for operation."
2,New features in histogram ,". 1D histogram, maybe a special case of density plot  . new way to calculate the bin size?  "
1,Attend SciPi WG meeting,Attend the Science Pipelines Working Group meeting in Seattle.
1,Attend SciPi WG phonecon,Attend the Science Pipelines working group meeting by video con.
1,Flesh out MOPS work,Work with Lynne Jones and Colin Slater to flesh out the high level MOPS design to a point where we can plane the risk associated with each component.
1,Attend SciPi WG F2F in Tucson,Attend the Science Pipelines working group face to face meeting in Tucson.
2,Flesh out the Level 1 processing diagram,Andy and I need to make sure we understand the Level 1 processing.
1,Adding an int to the end of CzarConfig causes a segfault error.,Adding and int to the private members of CzarConfig causes a segfault when Czar::Czar() calls LOG_CONFIG(logConfig);. gdb shows logConfig is the correct string value but somewhere in log4cxx something is corrupted and causes a segfault.    Adding an int to the end of Czar (the class where CzarConfig is instantiated) does not cause the issue. 
1,Investigate how the diffim decorrelation correction works for the case of non-uniform PSFs and noise,"It is not clear whether, or how, the L(ZOGY) post-convolution kernel (PCK; see DM-5914) will work for non-uniform PSFs or noise/variance. This will be investigated using the simple implementation from DM-5914.     Tasks:  1. Determine whether variation in the PCK across the field is significant enough to matter for typical LSST images  2. If it does matter, investigate options for performing interpolation of the PCK across the field, or via calculating the PCK across the field from the spatially-varying matching kernel. "
2,Extend the capabilities of the StarFast simulator beyond the minimum needed for DCR algorithm testing,During development of the StarFast simulator for DCR correction algorithm testing several addition features were identified that would make it more general and useful. These capabilities will enable a wider range of testing of new image differencing algorithms and give greater confidence in the accuracy of the results for DCR simulations. 
1,Time AST and compare to our WCS code,"Time TAN-SIP for our code and for AST, in order to get a sense of the performance impact of switching to AST for our WCS implementation."
0,Create DMBP project in jira,"Create a new project in JIRA (DM Baseline Plan), spec provided here: https://confluence.lsstcorp.org/display/DM/ProjMgmtWG%3A+The+New+DLP  I am sure we will fine tune it, but it is a (hopefully good) start."
3,Wrap afw using pybind11,"Experiment with using pybind11 (rather than Swig) to expose afw, and the packages it depends on,  to Python.    The concrete result of this epic is an assessment of the utility gained by wrapping the rest of the stack in pybind11 and an estimate of the time that would be required to carry out that work. If those goals are reached without completing the work on afw, we can claim success. In particular, if it becomes clear early in the epic that there is no long term utility here, we should abort the rest of the work."
3,Participate in DM replanning process,"Participate in the ongoing DM replanning process. This includes contributions to both the scipi-wg and pmp-wg, including such documentation writing or other tasks as the chairs of the those groups request, as well as resource loading and delivering the complete plan."
2,PSF fitting study,"Investigate the [Piff|https://github.com/rmjarvis/Piff/] PSF modelling code. Experiment with applying it to realistic LSST (or precursor) data. Discuss whether it is an improvement over existing techniques, and a recommendation as to whether it should be adopted in the stack. (NB writing the code to incorporate it into the stack is not a requirement of this epic, but may be a desirable side-effect).    If Piff is still too immature for this to be useful, investigate Kendrick Smith's [hscpsf|https://github.com/lsst-dm/meas_extensions_hscpsf/] code."
3,Service technical debt accumulated in earlier cycles,Service technical debt accumulated in earlier cycles.
3,F16 DRP emergent work,Handle emergent work during F16.
3,Serve as chair of the IVOA Time Domain Interest Group,[~swinbank] will serve as chair of the IVOA Time Domain Interest Group through May 2017. This epic captures work associated with that activity in F16.
3,Prepare for and participate in SBAG meeting,"By request of [~zivezic], [~nlust] will participate in the June 2016 meeting of the [Small Bodies Assessment Group|http://www.lpi.usra.edu/sbag/meetings/]. This epic captures work associated with preparation for that meeting."
3,Visualization tools for Science Pipelines,[~nlust] will collaborate with the SUIT group on development of appropriate visualization tools in support of the work of the Science Pipelines group during F16.    [~xiuqin] will provide further specification of success criteria.
3,Optimal coaddition,"Experiment with building ""optimal"" coadds, as defined by [DMTN-015|http://dmtn-015.lsst.io/en/latest/].    The aim here is to be able to generate coadds for experimentation with measurement algorithms. The expected output is appropriate mathematical formalism and prototype code. Polished integration of this facility with the LSST stack is not a requirement (but may be a useful by-product) of this work."
0,Increase memory locked amount in container,"In order to lock memory, the memory locking limit within the container for the qserv worker needs to be raised. My understanding is the container uses whatever is the host setting so the limit has to be set for the container user and whatever the user is inside the container. The particular limits is:    memorylocked 64 kbytes    notice that by default it's 64K. That needs to be raised to say 75% of the real machine size. I wouldn't make it unlimited as a memlock mistake may crash the whole machine. The limits are specified in ""/etc/security/limits.conf"". You will know that you are successul when you ssh into the container as the qserv worker user and the ""limit"" command tell you have can lock lots of memory.    We would also set the CAP_IPC_LOCK privilege but setting the soft/hard limit above should be good enough. So, let's start with that. "
0,Add eups version for Qserv for stack package version,"For stack packaged Qserv version, version needs to be retrieved and added to monitor.yaml using next command:    dev@clrinfopc04:~/src/qserv$ eups list qserv -s -V  LOCAL:/home/dev/src/qserv    Indeed, pkgautorversion doesn't work in this case, I.e. with no git repos"
3,Support Python 3 migration,"Support the migration of the DM code to Python 3. This includes writing transition documentation, integration of a new scons, migrating a handful of low-level packages and liaising with the teams on their packages.    The final outcome of this epic is that everything would be in place for the migration at the August All Hands meeting."
3,Update LSE-61 requirements and traceability,"With the updates to DPDD and LDM-151 in the early part of F16, there is a need to update LSE-61 (DMSR) such that it can directly trace requirements from OSS+DPDD through LSE-61 and down to implementation LDM documents.    This will require substantial rewrites of many of the existing requirements and possible addition of new requirements. It may also be necessary to add annotations to DPDD and other LDM documents to provide traceability anchors for DMSR.    The outcome of this epic is a new baselined DMSR approved by CCB."
0,reST roles for JIRA References,"Add {{:jira:`DM-1234`}}-type roles to documenteer so that JIRA tickets, epics and RFCs can be referenced easily from all of our Sphinx-based projects."
0,sourceSelector needs a schema in ImageDifferenceTask,imageDifference.py crashes with a vague error on initializing the sourceSelector task. The problem turns out to be that sourceSelector needs a schema passed in.
2,Add a python 3 Jenkins instance,We need a Jenkins instance where the default python in the PATH is python3 (where version >= 3.4 with 3.5 preferred). The underlying OS does not matter.    A prerequisite of this is a modification to the {{lsstsw}} {{bin/deploy}} script to allow Python3 to be installed ({{miniconda3}} EUPS package?) rather than python2.    Modifying the EUPS {{scons}} and {{python}} packages is outside the scope of this ticket. A build of a third-party EUPS package is sufficient demonstration of the capability.
3,Get jointcal running on minimum data,"It is very important for other teams to have a version of jointcal running to remove the sensitivity on the errors in astrometric reference catalogs.  The suggestion is to get jointcal running with CFHT, HSC, DECam and lsstSim."
3,Provide input for the update of LDM-151,We need to update the Level 1 portions of LDM-151 to be both more descriptive and close to what we actually plan to deliver.  This will involve breaking down things to a finer level of planning as well as delivering content for the document.
1,"First draft of overview (""vision"") document",See https://dmtn-016.lsst.io
2,Complete update to LDM-230,"Complete an update to LDM-230 and submit to TCT for re-baselining.  Along the way, contribute to and review other documents needed by DPS-WG."
2,Update sizing/cost model,"Contribute to the updating of the sizing/cost model, including fixing known bugs, synchronizing the inputs and models to fit the current baseline, and investigating changing the modeling technology."
2,Refine SuperTask design document,Deliver a refined SuperTask design document including reslicing to accommodate changes in the axis of parallelization between SuperTasks making up a composite SuperTask.
1,Update LSE-75 DM-TCS ICD,Submit an LCR to update LSE-75 to reflect current thinking on telemetry feedback from DM to the TCS.
1,Update LSE-72 DM-OCS ICD,Submit an LCR to update LSE-72 to reflect changes discovered by work at NCSA to support early integration tests.
1,Update LSE-68 DM-Camera DAQ ICD,"Submit an LCR to update LSE-68 to reflect understandings developed between Mike Huffer and NCSA about the interface, including the image deletion policy for the camera data buffer."
1,Provide input to Commissioning Plan,Provide input based on understanding of the DM interfaces to the Commissioning Plan being developed by Chuck Claver.
3,SQuaSH capability extension: multiple testdata service,"This epic covers work to deliver the following improvements to the SQuaSH prototype stood up in X16:    - drilldown 1 level (time series->histogram)    - multiple testdata options (requires jenkins, backend, dashboard extension)    - processccd + validate_drp pseudo-workflow    - pseudo-provenance (track manifest.txt - real LSST provenance system will be swapped in for extensive functionality when available)   "
0,Update LSE-76 Summit ICD,Submit an LCR to update LSE-76 based on Summit rack and power needs obtained from Ron Lambert.
2,Ad-hoc Docs & Comms requests,Timeboxed epic for in-cycle ad-hoc developer or management requests. In the first half of FY16 most of these are likely to be deveoper-guide related. 
3,Stack API documentation ,Stack API Doc generation -> pipelines.lsst.io
1,Resource load F16 part II,Resource load for second half of F16    (SP estimate from first half)
3,"SQuare Requirement, Design, & Review Docs for DM","This is an epic to track the work required on for DM baselined documentation from SQuaRE staff, including any associated with Working Group / replan etc    [FE: 45% MWV: 45% DN: 10%]    "
3,Releases and Release Engineering Improvements,  [50% FE 50% MJP]
3,Build/CI/Deploy improvements requested by the DAX/Qserv team,  Requests for Build/CI/Deploy improvements initiated by the DAX/Qserv team prioritised on request from the DM Project Manager.     
1,Build/CI/Deploy improvements requested by the Architecture Team,  Build/CI/Deploy improvements requested by the Architecture Team prioritised by request from the DM System Architect.     They cover predominantly support for the Python3 support. 
1,CI Improvements: Jenkins 2 upgrade etc,"  This epic covers a timeboxed maintainance of the Jenkins-based CI system, including the Jenkins 2 upgrade as well as the required updates to the Jenkins-puppet module. It also may include work done as part of DM-6204 brought over to the apps CI service. "
1,CI/Build/Deploy improvements for Sims,This is a timeboxed effort to prioritise support requests from the Sims group
1,SQuaRE services disaster recovery,This is a timeboxed effort to test and improve backups and disaster recovery for SQuaRE services. It is unlikely to be sufficient in itself. 
1,Ad-hoc developer requests,"This is a bucket epic for ad-hoc developer requests that cannot be postponed till the next planning cycle. In the event that it is underutilised for this purpose, it will be assigned to technical debt DM-5850"
3,Improve OSX support,NULL
1,Gitlfs maintenance - protocol upgrade etc,NULL
1,Slack migration,NULL
3,Conda binary distribution improvements,NULL
1,logging.lsst.codes improvements,NULL
3,Verification dataset exploratory work,[DN 50% AF 50%]
2,F16 DAX Services Containers & Ops,NULL
2,F16 DAX Services Improvements,NULL
2,F16 NCSA Dax Services Deploy,NULL
3,F16 Replan,NULL
3,F16 Support SUIT for Prototype DAC,NULL
3,F16 L1 DB Prototype I,NULL
3,F16 NCSA Stripe 82 Image Ingest,NULL
3,F16 Butler Repository Refactor,"Per KT, the parent/peer repository relationship scheme was not an exact fit for what we need. We discussed and decided that butler should manage its own input and output repositories. Also discussed with KT and Gregory was the ability to select inputs by 'tagging' repositories. The design discussion with the larger group is captured in RFC-184."
3,F16 Butler Storage & Format Refactor,"We want a pluggable architecture that allows code that uses butler to be able to define the the storage format and location from configuration and/or run time code.  (maybe it's implicit in this epic, but we need to define, design, RFC, and implement this feature.)"
3,F16 Butler Composite Dataset Design,"Do design, RFC, and some prototype code for loading and saving ""composite datasets"" via butler.    Composite Dataset definition: a python objects loaded by butler from file/database/etc that is persisted in more than one physical location (e.g. more than one file on disk). Those objects should also be able to be written to more than one physical location - the design should support this but the initial implementation may not be required to have this. "
3,F16 Butler Repo of Repos Design,NULL
2,F16 VO Standards Investigation,NULL
3,F16 Qserv Loader Improvements,NULL
3,F16 Qserv Containers and Ops,NULL
3,F16 QServ Improvements,NULL
3,F16 NCSA Qserv Deploy,NULL
3,F16 NCSA Stripe 82 Catalog Ingest,NULL
0,"Take part in LDM-151 Progress Meeting, 2016-05-27",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-05-27",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-05-27",NULL
1,Familiarization with RHL calibration documentation,NULL
0,The grid labels are not placed in the right position when the coordinate is Ecliptic coordianates,The algorithm to calculate the label position does not work well for the Ecliptic coordinate system.  The algorithm needs to be modified to work for all the coordinates.
2,Support API interaction with Regions,"We now need more fine grain controls over regions:    From API, user can:    * load region file  * delete region layer  * add a region entry to a layer  * delete a region entry from a layer    When this ticket is complete, region conversion should be completed."
3,Implement the ZOGY extension to the A&L algorithm in the stack,"DM-5422 provided a test implementation of the real space extension to the A&L algorithm for a correction kernel motivated by the ZOGY paper.  This epic is to take that test algorithm and incorporate it so that it can be used by the diffim tasks.    The first step will be to incorporate it using a static PSF t compute the correction kernel.  The second will be to evaluate how that affects the resultant difference image.  This should also include an estimate of the overall performance relative to the base A&L algorithm by examining runtime, false positive rate, and accuracy of noise estimation in both the detection threshold and the reported measurement SNR."
3,Study spatial variability of ZOGY correction,DM-6241 looked at how the correction term to the A&L algorithm performs under the simplifying assumption that the science image PSF is spatially invariant (though the matching kernel is spatially varying).  This epic will focus on how to extend the correction to include spatially varying terms.
1,Study the impact of having a spatially invariant decorrelation correction factor to A&L,The initial implementation of the A&L + noise whitening correction term assumes a single matching kernel and variance value(s) for the image(s) in the correction kernel.  We should assess how well that assumption performs in simulated and real images.  One test would be the variance and covariance in the noise as a function of position in a set of typical images.
1,Assess performance of the decorrelation correction to A&L,"Study the performance when using the (currently, spatially invariant) correction term to the base A&L algorithm in terms of runtime, detection threshold, reported measurement noise, and false positive rate for similarly tuned versions of both the base algorithm and that with the correction applied."
3,Compare competing algorithms for correcting DCR in template images,DM-5455 provides an implementation of a correction algorithm that depends on a matrix inversion approach to correct for DCR.  This should provide another approach for comparison (potentially a more forward modeling based approach).    Compare the algorithms in a simplified system in 2-D where DCR is along one axis.  The algorithms should be extended to arbitrary rotations.  The bakeoff will be repeated in the case of arbitrary rotations.    The result should be a recommendation as to the algorithm to use for DCR correction in template images.
0,Vertical overscan off by one again,"In DM-5524 [~price] fixed the vertical overscan by directly editing the amp info catalogs, but didn't mark the camera generating code as bad. In DM-6147 I regenerated the files, reintroducing the problem. The problem seems to be a subtle bug in the camera generating code. Rather than try to fix it, I'll convert the fixed catalogs directly and mark the generating code as broken. [~price] will issue an RFC that suggests a better way to handle generating amp info and once that is dealt with we can come up with a more permanent fix (e.g. delete the generating code or fix it)."
0,DRP Outline for LDM-151,"Write outline for Data Release Production section of LDM-151, using the DRP Data Flow diagram as the organizing principle."
0,"DRP Top-Level Diagram and Descriptions, Draft 1","Insert the content from the DRP Data Flow diagram on Confluence into LDM-151, adjusting it to the outline developed on DM-6247."
2,Implement competing algorithm,Implement a competing (potentially a forward modeling approach) algorithm for correcting DCR in templates.
1,Extend competing algorithm to arbitrary rotation angles.,NULL
0,Convert DRP Top-Level Diagram to standard conventions,"DM-6248 adds a large, complex diagram that will need to be cleaned up and converted to use the same conventions and colors as other diagrams in LDM-151."
1,Do bakeoff between the two algorithms in simplified case,"The original matrix inversion technique and the competing technique will likely have different sensitivities.  This should be a comparison of the algorithms, likely based on numbers of dipoles, along with performance (memory and runtime) considerations.    This will be done on 2-D images with DCR along one axis."
1,Bakeoff between algorithms extended to arbitrary rotation.,Redo bakeoff in the case of arbitrary rotation in DCR effect.
1,Develop standard conventions and colors for LDM-151 diagrams,"We want diagrams in LDM-151 to have consistent notation and colors, and to be produced using the same tool.  Someone needs to look at the diagrams produced so far to gather requirements, decide on and document these conventions, and select the tool we'll use to produce them."
1,Improve detail for for DRP imchar/jointcal in LDM-151,Write more detailed descriptions and possibly draw a rough diagram for the single-frame processing and simultaneous calibration components of Data Release Production.    Does not necessarily involve turning this section into prose.
1,"Improve detail for DRP background matching, coaddition, and diffim in LDM-151",NULL
0,Improve detail for DRP coadd processing in LDM-151,NULL
1,Improve detail for DRP object characterization in LDM-151,"Includes coadd measurement, multifit, and forced photometry.    Could be faster to write than other sections because we can lift from ""blended-measurement"" document that already exists in LDM-151 repo; could be harder because that document has already exposed a number of unresolved questions that may need to be addressed (by at least getting agreement among pundits on the best-bet approaches) before we can plan."
0,Improve detail for DRP afterburners and level-3 gathering in LDM-151,NULL
0,Cleanup and standardize DRP imchar/jointcal diagrams,"DM-6255 will produce some rough, draft-level diagrams that will need cleanup and standardization."
0,"Cleanup and standardize DRP background matching, coaddition, and diffim diagrams","DM-6256 will produce rough diagrams that will require cleanup and standardization.    [~ctslater] has made some suggestions for the current diagram that I'll implement on this issue, so I'm assigning it back to me.  I'll also go ahead and integrate his updated DRP overview diagram (currently on Confluence) into LDM-151 here.  "
0,"Cleanup and standardize DRP detection, association, and deblending diagrams","DM-6257 will produce rough, draft-level diagrams that will require cleanup and standardization."
0,Cleanup and standardize DRP object characterization diagrams,"DM-6258 will produce rough, draft-level diagrams that will require cleanup and standardization."
0,Cleanup and standardize DRP afterburners and level-3 gathering diagrams,"DM-6259 will produce rough, draft-level diagrams that will require cleanup and standardization."
0,Audit DRP LDM-151 for correct handling of chromaticity,"Correctly handling wavelength-dependent photometric and PSF effects is one of the biggest qualitative differences between the current state-of-the-art and what we have in mind for LSST, and that makes it easy to get wrong.  We need to make sure all steps that produce high-quality fluxes or rely on high-quality PSFs have access to object colors and a reasonable approach to using them.  "
3,Upgrade cfitsio and deal with long keyword handling,"To implement RFC-105, we need to figure out how we are handling long FITS header keywords, before we can upgrade to cfitsio 3.38 or newer. There may be other FITS-related idiosyncrasies in the stack that may be brought to light while upgrading, as 3.38 has changed how it handles some of the non-standard conventions.    See some of the notes in DM-4115 for problems encountered while attempting to upgrade to the 3.38 beta."
3,Attend HTCondor Week,Attend HTCondor Week with [~gdaues] to learn about condor and pegasus  http://research.cs.wisc.edu/htcondor/HTCondorWeek2016/index.html
3,Review of Workflow Systems,"Review different workflows and write a final comparison report. The plan is to look at up to 8 workflows. Current list of workflows:  - pegasus, HTCondor  - panda  - swift  - ...    Each review should take around 3 days. The goal is to review the workflows systems with:  - longevity, how long has the system been around, what is the funding?  - use cases, who is using it?  - scale, how large of a workflow has it used?  - code, is the code open, how is the developer community, does it have python bindings?  - GUI, does it have some easy way to monitor the workflow?  - can we generate workflows programmatically?  - what clusters are supported?    Ideally we should also try and get it up and running and maybe even generate a dummy workflow. Discussion could be how we can prototype DRP and use that as a use case.    Deliverable: Evaluation report  Staff: Rob Kooper, Hsin-Fang Chiang, Matias Carrasco Kind, Mikolaj Kowalik, Steve Pietrowicz  Effort: 25 days  Planned Start: 6/1/2016  Planned End: 6/30/2016"
0,Audit DRP LDM-151 for correct handling of crowded fields,"[~jbosch]'s background is in extragalactic science on high-latitude fields, and he frequently forgets to think about how algorithms will perform in crowded stellar fields.  When the first draft is complete, we should have someone experienced in that area read closely to check that he hasn't made any incorrect algorithmic assumptions as a result."
1,Reception and Placement,NULL
2,Statement of Work ,NULL
2,Propose extension of SuperTask functionality for workflow package,"Should see what is needed to add to supertask so we can use it with workflows. After this is decided we should create a RFC for implementation.    Deliverable: RFC to extend SuperTask  Staff: Rob Kooper, Matias Carrasco Kind, Mikolaj Kowalik  Effort: 5 days  Planned Start: 6/1/2016  Planned End: 6/22/2016"
3,Implementation of Supertask RFC,"This should implement the RFC written in DM-6274.  Note that this activity is independent of the work to complete the supertask and activator prototype in DM-6418.    Deliverable: Deliverables based on outcome of RFC  Staff: Matias Carrasco Kind, Mikolaj Kowalik  Effort: 15 days  Planned Start: 7/1/2016  Planned End: 7/31/2016"
3,ConOps for Workflow/Middleware,"Create a conops for workflow, this will depend on some decisions made about L2 conops.    Deliverable: ConOps document for Workflow  Staff: Rob Kooper, Hsin-Fang Chiang, Matias Carrasco Kind, Steve Pietrowicz, Jason Alt, Margaret Johnson  Effort: 15 days  Planned Start: 7/1/2016  Planned End: 7/31/2016"
3,Proof of Concept Implementation of Workflow System,"This should start the implementation of the workflow system. This will be a proof of concept only.    Deliverable: Proof of concept workflow implementation code  Staff: Hsin-Fang Chiang, Mikolaj Kowalik, Rob Kooper, Steve Pietrowicz  Effort: 20 days  Planned Start: 8/1/2016  Planned End: 8/31/2016"
0,Investigate proper precision for afw::image::Image pixel transforms,"The various pixel based transforms in {{afw/src/image/Image.cc}} were converted from using {{boost::lambda}} to C++11 lambda per DM-6091.    At many places the previous implementation contained implicit casts (through {{boost::ret}}) of intermediate results to {{PixelT}} (e.g. {{float}}).  In particular this affects opperations such as {{result = l + c*r}} where {{l}} is the left hand side image, {{r}} is the right hand side image and {{c}} a {{double}} constant.  When calculated at double precision (e.g. without the casts, which are not needed with C++11 lambdas) the result is slightly different and this causes {{tests/testProcessCcd.py}} to fail on {{self.assertAlmostEqual(psfIyy, 2.17386182921239, places=7)}} which is only equal up to the fifth place.    In order to not break existing behaviour I added explicit casts to {{PixelT}} for intermediate results. But this approach is questionable as the end result will be less accurate then possible. The aim of this ticket is to decide which approach is best:    1. Calculate at full precision and modify the test case.  2. Cast intermediate results to final precision (as it is done now).  3. Do something else?"
0,Fix possible logic error in pex_policy dictionary,Investigate and fix the following warning in {{pex_policy}}.    {code}  src/Dictionary.cc:312:9: warning: logical not is only applied to the left hand side of this comparison [-Wlogical-not-parentheses]      if (!getType() == Policy::POLICY) // should have checked this at a higher level          ^          ~~  src/Dictionary.cc:312:9: note: add parentheses after the '!' to evaluate the comparison first      if (!getType() == Policy::POLICY) // should have checked this at a higher level          ^           (                          )  src/Dictionary.cc:312:9: note: add parentheses around left hand side expression to silence this warning      if (!getType() == Policy::POLICY) // should have checked this at a higher level          ^          (         )  src/Dictionary.cc:312:20: warning: comparison of constant 'POLICY' (5) with expression of type 'bool' is always false [-Wtautological-constant-out-of-range-compare]      if (!getType() == Policy::POLICY) // should have checked this at a higher level  {code}
0,The labels in HMS formate are wrong in WebGrid,The labels in HMS format no longer show hh:mm:ss anymore.  The porting introduced the bug.  
0,Fix mismatched-tags warnings in meas_modelfit,The following warnings are produced in {{meas_modelfit}}. Fix them.    {code}  include/lsst/meas/modelfit/UnitSystem.h:90:1: warning: 'LocalUnitTransform' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct LocalUnitTransform {  ^  include/lsst/meas/modelfit/Model.h:41:1: note: did you mean struct here?  class LocalUnitTransform;  ^~~~~  struct  {code}
0,Remove swig special casing for obsolete boost features,In {{utils}} the file {{python/lsst/p_lsstSwig.i}} defines special cases for boost features that are removed as part of DM-5880. This ticket removes the special cases.
1,Chart API: external API and the API for histogram,need to support external API and the API for histogram    for now only addXYPlot and showPlot are implemented
2,"Charts (XY plot, histogram) Container",Need to be able to view multiple charts simultaneously in the chart area    User would like to see multiple XYplots from the same data displayed at the same time. One example will be to display different color-color plots using all 4 bands data from WISE catalog.  It is also possible we want to display histogram data at the same time as color-color plots.
1,Charts refactoring,"I'd like to do some cleanup, which would facilitate further development. This includes:  - moving chart related code to a separate package (now it is in visualize)  - converting components created with React.createClass to es6 classes  - reorganize store and controllers to have all charts related things under 'charts'. Now we have 'charts' for charts ui, xyplot for xyplot charts, histogram for histogram charts, and tblstats for table statistics.      Fixed bugs    * missing chart mount action, when a chart is removed and then recreated on the same table    Steps to reproduce: load a table (default scatter plot created), create histogram, delete scatter, create new scatter.       The last scatter did not produce mount action, and the plot was not tracking table changes, like filter.    * undefined shows as a label when no server call is necessary    Steps to reproduce: load table (default scatter created), clear options and choose the same columns , click apply.    ""undefined"" are shown as axis labels"
1,Chart options display,"Make chart options ""in-place"" popup, similar to table options for consistent look. It will also alleviate resizing, because the chart size won't need to change when options are open."
1,Chart options reset and clear,Need to support reset and clear for plot and histogram options.    Should be no-brainer after DM-6138 (update multiple fields at the same time)
1,Attend SBAG Meeting,"Meeting runs Tues 28 to Thurs 30 June; that means we'll likely lose Nate for the whole week, given travel."
1,Read materials related to SBAG prep and attend telecon,"Read up material to prepare for SBAG, and discuss readings with Mario, Lynne, and Zeljko."
1,Fix error in cmodel related to computing LinearTransforms,"When running cmodel in ci_hsc, the cmodel plugin throws the error:  {code}  processCcd.charImage.detectAndMeasure.measurement WARNING: Error in modelfit_CModel.measure on record 775961510756221246:     File ""src/geom/LinearTransform.cc"", line 66, in const lsst::afw::geom::LinearTransform lsst::afw::geom::LinearTransform::invert() const      Could not compute LinearTransform inverse {0}  lsst::afw::geom::SingularTransformException: 'Could not compute LinearTransform inverse'  {code}    This seems to be causing some aperture corrections to fail, as there are no sources to compute the corrections from. Investigate why this error is being thrown. If it is a bug, fix it, if the code is not handling situations it should then make the algorithm more robust."
1,Add support for pybind11 to build system,Add pybind11 as third party package to the stack. Update sconsUtils to support building with pybind11. Use daf_base DateTime to demonstrate that this works.
1,Unit test for coadds in pipe tasks detects too many sources,"The unit test for pipe tasks creates a dozen stars, to use in coaddition testing. However the results of running the test show over a hundred sources found. Investigate why the extra sources are being detected, and fix to increase the robustness of the test. If this relates to other sections of the codebase (deblender) investigate if it is appropriate to make changes to those components to make them more robust instead of creating a simple hack in the unit test."
1,Wrap afw::geom with pybind11,"The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete."
1,Wrap afw::detection with pybind11,"The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete."
1,Wrap afw::math with pybind11,"The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete."
2,Extend galaxy shear fitting results to cover ngmix,NULL
1,Write example meas_base plugin in Python,"During X16, new functionality was exposed to Python plugins in meas_base. Write a complete pedagogical example. It should go beyond our current pure Python plugins to demonstrate use of:    * FlagHandler;  * SafeCentroidExtractor;  * Other relevant, undocumented functionality.    This should be added to the package level documentation for meas_base, so it appears in some extended version of https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html."
1,Wrap pex_exceptions with pybind11,"While wrapping these packages, pay particular attention to exception translation (see Jim's bullet point 3: https://jira.lsstcorp.org/browse/RFC-182?focusedCommentId=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48644)."
2,Wrap ndarray with pybind11,Note particularly Jim's second bullet point at https://jira.lsstcorp.org/browse/RFC-182?focusedCommentId=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48644.
2,Wrap afw dependencies in pybind11,"Everything that isn't base, utils, pex_exceptions (DM-6302), ndarray (DM-6303)."
0,Executable test in utils needs to test an executable,In DM-4036 all the test binaries were removed as no longer being needed. This had the unfortunate side effect that the {{testExecutables.py}} test no longer tests anything. This ticket will be used for adding a test file.
1,input outlines to LDM-151 for AP,Write outlines into the LDM-151 document for the alert production pipelines.
0,Upload JSON from validate_drp to SQuaSH REST API on Jenkins,"This ticket covers work to build a Python package/script whose role is to take JSON output from validate_drp (DM-6086), shim it into the JSON schema currently expected by the SQuaSH REST API (http://sqr-009.lsst.io), and post the data to the API's {{/jobs}} endpoint.    This tool also adds additional metadata to the ‘Job’ document, including the build ID and versions of packages as run by validate_drp (see DM-5943)."
0,Update LDM-151 with SDQA Skeleton and Outline,1. Update the LDM-151 draft with the SDQA Skeleton from the DMLT + SciPipelines working group discussions of May 16-20.  Implement as bullet points in a semi-coherent list. (/)    2. Clean up list. (/)
0,Transform SDQA bullets to prose,NULL
0,Create 1st-level block diagrams for SDQA,NULL
1,Update Scons to v3.0,Scons v3 is the scons version that supports both Python 3 and Python 2.7. This ticket is for updating Scons and ensuring that the Python 2.7 stack still builds.    This work depends on the Scons developers delivering a new Scons by mid July. Whilst work is ongoing it may be necessary to help out with the port if we wish to meet our Python 3 target.
0,Create miniconda3 EUPS package,{{newinstall.sh}} currently installs miniconda via EUPS. To replicate that functionality in Python3 we need to create a {{miniconda3}} package. This package should be almost identical to {{miniconda2}}.    Requires that {{lsstsw}} first be updated to support python 3.
1,Port lsstsw to Python 3,Get {{lsstsw}} working with Python 3:  * Update the {{deploy}} script to allow a Python 3 python to be installed and modify the version checking code.  * Demonstrate that {{lsstsw}} {{rebuild}} will successfully build and install a third-party non-Scons package.
2,Write Python 3 porting guide,Porting the stack to Python 3 is not as simple as blindly running {{futurize}}. A guide has to be written explaining the issues and providing guidance on when to accept {{futurize}} suggestions and when to ignore them. This guide will be written as a Tech Note.
0,Update newinstall.sh to support Python 3,{{newinstall.sh}} currently insists on installing and checking for python 2.7. This needs to be changed to allow Python 3.    Requires {{sconsUtils}} works with Python 3 as the {{lsst}} EUPS package is installed as part of {{newinstall.sh}}.
0,Update developer guide to include Python 3,Update the developer guide to indicate that Python 3 must be supported and that code must run on Python 2.7 and 3.    This ticket will reference the tech note delivered as part of DM-6315. Writing extensive user documentation on the {{future}} package is beyond the scope of this ticket.
1,Port sconsUtils to Python 3,{{sconsUtils}} has to be modified to ensure it works with Python 3. Additionally SWIG calls must be changed to trigger Python 3 mode.
0,Port utils to Python 3,Ensure that the {{utils}} package will work with Python 3.
0,Port base package to Python 3,Ensure that {{base}} works with Python 3.
1,Lead Python 3 migration at All Hands Meeting,* Prepare for all hands meeting.  * Present plan to developers.  * Advise developers doing migration.  * Contribute fixes as required.
0,Replace BOOST_STATIC_ASSERT with static_assert,Replace BOOST_STATIC_ASSERT with static_assert from C++11.
0,reST roles for mock code references,"Add mock code reference roles so that authors can add semantics to their writing without attempting to make actual references to API documentation that does not _yet_ exist. Covers all roles in the Python domain, and supports tilde syntax for collapsing the namespace."
0,Shifting F16 milestones to S16,"Per [~jbecla]'s request, provide Kevin with a list of milestones which we will not address in F16. Reschedule them to S16 in JIRA."
0,Update LDM-151 introduction to reflect new structure,"Some proposals made on DM-6247 to change the structure of LDM-151 (add Algorithmic Components section, move overview into production-specific sections, add notation section to introduction) were accepted at the live meeting on 6/27.  This issue rewrites the introduction accordingly."
0,Firefly Python API scope and decision,Python API to use Firefly visualization components and other functions.   This story is to come up with a good plan for the rest of the development work to related to Python API. 
1,User installation and operation instructions for conda ,Create documentation for the Stack conda binaries created in DM-5415 as part of the Science Pipelines documentation
0,Add FlagDecorator to support FlagHandler in Python,"DM-4009 added the C++ and swig changes needed to allow the FlagHandler to be used from Python.  During review, Nate suggested that a decorator class could be used to improve the use of this code in Python.  This ticket will be to review Nate's decorator and confirm that it is the correct model for Python-only plugins.    We will also modify the unit test in DM-4009 and the EmPsfApprox plugin in DM-6123 to use the decorator."
3,Write Calibration Products Production section of LDM-151,Write photometric calibration pipeline section of LDM-151
2,Replace cameraGeom PAF files,"PAF files have long been deprecated, but continue to be used for describing the camera geometry.  We need to replace the PAF cameraGeom files used for CFHT-MegaCam, DECam, LSSTSim and SDSS, and the scripts used to convert these files to FITS files for reading by the Mappers.  They might be replaced by a configuration like YAML, or pure python."
1,Generate camera description at build time,"Camera geometry used to be defined using PAF (policy) files, which are now deprecated. As part of the transition to the refactored camera geometry scheme, scripts were introduced to convert from the PAF files to the new camera geometry configuration scheme which uses FITS files and a python file to describe the camera. These scripts are still part of the obs_* packages, and some people rely on them for making changes to the camera description. On the other hand, the generated FITS files and python file are also first-class members of the obs_* packages. This means that we have two sources of the same information, which is dangerous.    For obs_lsstSim, obs_decam, obs_cfht and obs_sdss, we want these scripts to be the primary source of information.  This means we should delete the generated files, and create them at build time.  We should also standardise the name of the script used to generate these."
2,Add skeleton words to LDM-151 for AP,We need to flesh out the algorithmic components and narrative sections to the point of having ~1 sentence per paragraph in the finished document.
1,Use the HTM based reference catalogs in tests,"In order to move A.net out of meas_astrom to make it a true dependency, we need to replace its use in tests."
1,Add linearity correction to obs_decam,Add linearity correction from DM-5462 to obs_decam using the standard linearity tables.  
0,"Take part in LDM-151 Progress Meeting, 2016-06-03",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-03",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-03",NULL
0,"Update ""Using Boost"" section in DM Developer Guide to prefer standard library by default","Implement RFC-185 by updating the ""Using Boost"" section in DM Developer Guide to prefer standard library by default."
2,Replan (June),NULL
2,Replan (July),NULL
2,Replan (August),NULL
1,Design DAX containers,NULL
2,L1 DB Prototype (June),Placeholder for L1 database prototyping in June 16 -- to be replaced with actual stories of same sp total
0,Adjust version check of EUPS python package to allow v3,To enable Python 3 support of the stack the EUPS {{python}} stub package needs to allow Python 3.    
1,Test DIA simulation script with Postgres,I will be useful to compare MySQL and Postgres performance for use in L1. After DM-6918 is complete (means works with MySQL) verify that it can also run against Postgres. 
2,L1 DB Prototype (August),Placeholder story for work in this epic in August -- replace with detailed stories at same sp load
0,Improve skeleton for LDM-151 Algorithmic Components,For all subsections in  Algorithms Components owned by [~jbosch]:   - Provide enough bullet points to capture scope.   - Add bullet points for subtly difficult aspects of components.   - Add extra level subsubsubsection level for Measurement.   - Create matrix of measurement algorithms and contexts.  
3,New image visualization functions (F16),"TO support the pipeline QA and build the first web portal, there will be new functions need to be developed.  This epic is to collect those functions."
1,Implement DAX containers,Implement containerized DAX services
2,SPIE conference 2016,Activity related to attending the SPIE meeting in June 2016. Deliverable is a report on the conference.
0,Persist output of simple DCR correction,DM-5695 will create transfer matrices stored as numpy arrays. This ticket extends that work to determine a useful format and write functionality to persist those arrays.
1,Investigate CAOM,"Investigate observation model interfaces and storage, and applicability of CAOM"
2,Attend SPIE conference,This ticket relates to attendance at the SPIE meeting in Edinburgh the last week of June.
1,ADQL support in dbserv,Work on understanding coordinate systems in ADQL and implement the ADQL->qserv rewriter
1,Generate template DCR images,DM-5695 will create transfer matrices that can be used to create template images of a field at arbitrary airmass. This ticket is to write the code to generate and persist those template images.
0,Use template DCR images for image differencing,"DM-6382 creates template images of a field at arbitrary airmasses, which can be used to match the template airmass to the science image precisely to mitigate Differential Chromatic Refraction in image differencing. This ticket is to determine the best method to supply the new templates to image differencing, which may be simply to create a new exposure and ingest/process the template as though it were a real observation."
3,Create and deploy Conda binaries for v12.0 release build,Deploy Mac OS X and CentOS 5 conda binaries for v12 to a conda repository (http://conda.lsst.codes/stack/current).
0,Create CLI tool to add mac users.,Create a CLI script to add and delete Mac OS X users. Somehow this is a many step process on Mac OS X.
1,Deploy Conda repository to S3,Deploy conda binaries to s3 using their static website feature. http://conda.lsst.codes/stack/current.
1,Create Ansible automation to run the conda build,Create an Ansible deploy to automate Conda binary builds. Target Mac OS X and CentOS5.
0,Create CentOS5 Conda binaries,Create CentOS5 conda binary builds using docker then push them to the S3 static website.
0,Text on variability characterization for LDM-151,Expand the variability characterization algorithmic section of LDM-151.
3,DM Replanning: ConOps Development,"Development of Concept of Operations documents for various DM services, including Data Backbone, AA system, L3 Hosting, and Batch Processing for the commissioning phase.    Deliverable: ConOps documents  Staff: Don Petravick, Margaret Johnson, Jason Alt, Steve Petrowicz, Hsin-Fang Chiang, Jagadeesh Yedetore, Jim Basney, Alex Withers, Robert Gruendl (roughly 0.5 effort each)  Effort: 33 days  Planned Start: 6/6/2016  Planned End: 8/31/2016"
1,Data Backbone conops iteration 1: create raw draft (internal),Write a raw draft of the concept of operations for data backbone services. In this iteration the document is developed in Google docs following the ConOps template.
0,Data Backbone conops iteration 2: group review to produce first draft,"Review raw draft of concept of operations for the data backbone services to work through underdeveloped areas, clear up uncertainties, and make readable."
1,Data Backbone conops iteration 3: larger review to produce second draft,"Review first draft of data backbone services conops within Data Processing Architecture working group, bringing in relevant experts.    Input from review is incorporated into a second draft."
0,Data Backbone conops formatting: convert second draft to reStructuredText,"When the data backbone services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process."
2,DM replanning: NCSA WBS broken down to Level 3 WBS elements,"Break down NCSA restructured WBS to level 3 (02C.07.xx.xx) sufficient for integration into PMCS.    Deliverable: Draft WBS for project-level change processes to accommodate.  Staff: Don Petravick, Margaret Johnson, Jagadeesh Yedetore, Santanu Chaudhuri (roughly 0.5 each)  Effort: 5 days  Planned Start: 6/1/2016  Planned End: 6/30/2016"
3,DM replanning: phased WBS elements for minimal data archiving of camera data and minimal transport via data backbone to NCSA,"Phased breakdown of work activities to construct minimal archiving of camera data and transport via data backbone to NCSA. Includes release of claim on camera buffer.    Deliverable: Phased WBS for minimal data archiving of camera data and minimal transport via data backbone to NCSA.  Staff: Don Petravick, Margaret Johnson, James Parsons, Steve Petrowicz, Jagadeesh Yedetore, Felipe Menanteau  Effort: 20 days  Planned Start: 7/1/2016  Planned End: 7/31/2016"
3,DM replanning: instantiating Project Reporting Group and planning & reporting process,"Instantiating NCSA's Project Reporting Group and planning and reporting processes.    Deliverable: Project reporting normalized to new methods.  Staff: Don Petravick, Margaret Johnson, Santanu Chaudhuri, Jagadeesh Yedetore  Effort: 10 days  Planned Start: 6/6/2016  Planned End: 6/30/2016"
1,DM replanning: participation in PM working group,"Participation in Project Management working group for DM replanning.    Deliverable: Deliverables to PM working group.  Staff: Santanu Chaudhuri, Don Petravick, Margaret Johnson  Effort: 3 days  Planned Start: 6/1/2016  Planned End: 6/30/2016"
3,Operations Planning in LOPT and TOWG,"Participation in LOPT and TOWG for LSST operations planning to get to ConOps.    Deliverable: Deliverables to get to ConOps  Staff: Don Petravick, Margaret Johnson  Effort: 12 days  Planned Start: 6/6/2016  Planned End: 8/31/2016"
3,Verification Planning with Systems Engineering,"Extend concepts of operations to include adequate verification. Will involve coordination with LSST Systems Engineering team.    Deliverable: verification plan descriptions in ConOps documents  Staff: Don Petravick, Margaret Johnson, Jason Alt, Paul Wefel, Steve Petrowicz, Jagadeesh Yedetore  Effort: 27 days  Planned Start: 6/6/2016  Planned End: 8/31/2016"
1,Install and configure GNU/Linux on my office desktop.,NULL
3,Main prototype for all L1 entities (F16 part 1),Main program prototypes for all entities in the L1 Prompt Processing system. This epic continues work started in x16 and covers work in the first part of the F16 cycle.    Deliverable: Major component hierarchy and all 'has a' objects list.  Staff: Jim Parsons + 2 summer students (at 50% each)  Effort: 15 days  Planned Start: 6/1/2016  Planned End: 7/31/2016
3,L1 Startup Scaffolding,Scaffolding to remotely start and stop all L1 Prompt Processing system entities.    Deliverable: Initial scripts starting and stopping all entities remotely  Staff: Jim Parsons + 2 students (50% each)  Effort: 8 days  Planned Start: 7/1/2016  Planned End: 7/31/2016
3,L1 System Status Message Dictionary,Message formats and contents between all L1 Prompt Processing System entities. This epic continues work from x16.    Deliverable: Enumeration of message formats and contents as needed.  Staff: Jim Parsons + 2 students (50% each)  Effort: 10 days  Planned Start: 6/1/2016  Planned End: 6/30/2016
2,Learn about design principles and usage of data processing tasks.,"As we are working on designing a new workflow framework I should read available documentation and look through the existing code base regarding data processing tasks to learn more how they are written, work, and interact among themselves."
3,Message interaction between all L1 entities (F16),"Basic messaging and interactions, including data dictionary and message patterns.    Deliverable: All communication, data, and reporting paths specified and implemented  Staff: Jim Parsons + 2 students (50% each)  Effort: 15 days  Planned Start: 6/1/2016  Planned End: 8/31/2016"
3,Basic Framework for L1 System Health and Status,"Basic framework for L1 prompt processing and archiving system health and status display, including status event recorder.    Deliverable: First draft of health checks via message; plan for remote diagnostics  Staff: Jim Parsons + 2 students (at 50% each)  Effort: 10 days  Planned Start: 7/1/2016  Planned End: 8/31/2016"
2,ConOps/Planning Document for the OCS-DM interface,Detailed concept and technical implementation plan for DM interface with OCS.    Deliverable: ConOps document for OCS  Staff: Jim Parsons  Effort: 7 days  Planned Start: 8/1/2016  Planned End: 8/31/2016
2,ConOps/Planning Document for the Camera-DM interface,Detailed concept and technical implementation plan for DM interface with the main Camera.    Deliverable: ConOps document for Camera  Staff: Jim Parsons  Effort: 7 days  Planned Start: 8/1/2016  Planned End: 8/31/2016
2,ConOps/Planning Document for Base Archiving API,Detailed concept and technical implementation plan for Base Archiving API.    Deliverable: ConOps document for Base Archiving API  Staff: Jim Parsons  Effort: 5 days  Planned Start: 8/1/2016  Planned End: 8/31/2016
3,Specification for Comfort Dashboard and Alarms,"Detailed specifications for comfort dashboard and alarms, including interactions with human operations, and some technology prototyping.    Deliverable: Detailed specifications  Staff: Jagadeesh Yedetore, Santanu Chaudhuri, TBH  Effort: 21 days  Planned Start: 6/13/2016  Planned End: 8/31/2016"
2,Final version of Supertask and Activator prototype implementation,"Final version of Supertask and Activator prototype implementation. This is not the final version of SuperTask, just what was initially designed.     Deliverable: Final prototype  Staff: Matias Carrasco-Kind, Mikolaj Kowalik  Effort: 6 days  Planned Start: 6/6/2016  Planned End: 7/15/2016"
2,Propose and discuss workflow selection,"Propose and discuss workflow selection.    Deliverable: RFC for workflow selection  Staff: Hsin-Fang Chiang, Mikolaj Kowalik, Rob Kooper, Steve Pietrowicz  Effort: 5 days  Planned Start: 7/1/2016  Planned End: 7/15/2016"
3,"Investigate Shifter, HTCondor, preemption, and file cleanup",Evaluate use of containers to configure HTCondor slots for DRP tasks. Investigating Shifter on Blue Waters.    Deliverable: investigation report  Staff: Greg Daues  Effort: 10 days  Planned Start: 6/1/2016  Planned End: 8/31/2016
2,Evaluate use of dynamic slot mechanism in HTCondor for DRP tasks,Evaluate use of dynamic slot mechanism in HTCondor for DRP tasks.    Deliverable: Evaluation report  Staff: Steve Pietrowicz  Effort: 5 days  Planned Start: 6/1/2016  Planned End: 6/30/2016
0,Python 3 migration work,"Migrate existing packages in the LSST stack to support Python 3 compatibility. Some work will occur during the All-Hands meeting.    Deliverable: Migration of existing packages for Python 3 compatibility  Staff: Steve Petrowicz, Jim Parsons, Matias Carrasco-Kind, Mikolaj Kowalik, Hsin-Fang Chiang  Effort: 1 days  Planned Start: 8/1/2016  Planned End: 8/31/2016"
3,Data Backbone: explore overheads and costs of staging model,Explore overheads and costs of staging model from Data Backbone into data caches.    Deliverable: assessment and characterization of staging component of orchestration  Staff: Steve Pietrowicz  Effort: 15 days  Planned Start: 8/1/2016  Planned End: 8/31/2016
1,Analyze existing implementation of Supertask,Look through the code base of current prototype of Supertask and Activator to understand better limits of existing design of data processing task and how they are being addressed.
0,Understand how EUPS works,Read Developer's Guide tutorial and official documentation of EUPS to understand how it works to manage Stack's packages easily.
1,Data Backbone: produce abstract API to ingest data into L1 archive ,Implement an abstract API to ingest data into the L1 archive in the Data Backbone.    Deliverable: abstract API in github  Staff: Steve Pietrowicz  Effort: 4 days  Planned Start: 6/1/2016  Planned End: 6/30/2016
3,Data Backbone: initial analysis of OpenStack and Ceph object store APIs,Initial analysis of OpenStack and Ceph object store APIs in Data Backbone.    Deliverable: analysis report  Staff: Steve Pietrowicz  Effort: 10 days  Planned Start: 6/1/2016  Planned End: 6/30/2016
3,Data Backbone: initial analysis of DDN WOS object store API,Initial analysis of DDN WOS object store API in Data Backbone.    Deliverable: analysis report  Staff: Steve Pietrowicz  Effort: 10 days  Planned Start: 7/1/2016  Planned End: 7/31/2016
3,Object Store and WAN Data Interchange Evaluation,"Evaluate data interchange over the WAN with object store technology.    Deliverable: evaluation report  Staff: Jason Alt, TBD SET hire  Effort: 25 days  Planned Start: 6/6/2016  Planned End: 8/31/2016"
2,Commission the evaluation framework on one machine at NCSA,Commission the evaluation framework on one machine at NCSA.
3,Evaluate at additional sites and/or by simulation using WAN emulation techniques,Additional sites and/or simulation using WAN emulation techniques.
1,Produce evaluation report,WAN data interchange evaluation report.
3,Cost Model Updates for FY17,Semi-annual updates of costing forecast in sizing model.    Deliverable: RFC for proposed cost updates  Staff: Jason Alt  Effort: 10 days  Planned Start: 6/6/2016  Planned End: 6/30/2016
2,Service Management for F16 (part 1),"Provide service management for LSST development resources at NCSA, including Nebula openstack cluster, lsst-dev, and relevant FY16 procurements. Interface with functional groups to provide support for DM services.    Deliverable: services  Staff: Greg Daues  Effort: 6 days  Planned Start: 6/1/2016  Planned End: 8/31/2016"
3,Deploy FY16 Integration Environment,"Deploy infrastructure for FY16 SUI/Qserv integration environment (a.k.a., prototype DAC).    Deliverable: secure SUI/Qserv integration environment  Staff: 3 NCSA ICI engineers (networking, storage, systems)  Effort: 65 days  Planned Start: 6/1/2016  Planned End: 6/30/2016"
1,Convert GWT projection and Coorindate Conversion routine to JavaScript,convert Booth's projection code and Judy Bennet's coordinate conversion routines to pure javascript  
2,Test the new JS convertion and projection routines against the java versions,"Now that the gwt algorithmic code has been converted it needs to be validated. Set up the test for both the projections routines and the coordinate conversion routines.    h3. Task Details  * Unit test should be run on the Java and JavaScript side  * A unit test should exist for all 10 projections. More might be necessary since there are variation within a projection type.  * We need to bring Booth in for details of each projection and each variation.  * Booth has example file somewhere.  * Xiuqin ran coordinate conversion test in to past.  She (and maybe Booth) have the best understand of what that test should be.   * The same input file should be used for the Java and JavaScript side.  * The same output (if possible) should be produced.      h3. Status so far    *Projections*    * GNOMONIC - somewhat tested  * ORTHOGRAPHIC -somewhat tested  * AITOFF - somewhat tested  * SFL - somewhat tested  * PLATE - somewhat tested  * LINEAR  * ARC  * CAR  * NCP  * CEA      *Coordinate conversion.*    It appears to work when run in Firefly, however the code is not fully covered.    h3. Switching between GWT and pure JavaScript    To switch between GWT and JS edit {{VisUtil.js}} and change the following line:  {{export const USE_GWT= false;}}    true uses the old code, false used the new code.    h3. Entry Points and Directories      * Coordinate Conversion:  _Dir_: firefly/src/firefly/js/astro/conv, _Entry Point:_ VisUtil.js, convert(), line 104, also see line 26  * Projection: _Dir_: firefly/src/firefly/js/visualize/projection, _Entry Point:_ WebPlot.js, makeWebPlot(), line 124  "
1,Remove GWT from build,After the code is tested remove the GWT from the build. Should check with [~roby] to make sure the boolean to enable GWT has been removed from VisUtil.js and WebPlot.js
1,May 2016 LAAIM work,Gave input on IAM design for FY16 Integration Environment.  Discussed IAM replication requirements with stakeholders.  Attended local NCSA LSST coordination meeting.
1,Create Ansible automation to run the conda build,Complete Ansible implementation started in DM-6388.
3,Measure photometric and astrometric precision for DECam COSMOS dataset,Measure the photometric and astrometric precision for the DECam COSMOS dataset and determine the sources of extra systematic scatter.
0,"For the 3_build-git-image.sh, pass -j$(nproc) to scons to speed up the build process",NULL
1,Verification CoDR preparation,Just capturing FE's SPs  towards this. 
0,Remove boost dependencies where possible,"In X16/DM-5580, we removed Boost from a number of packages. However, we may not have rigorously updated their dependency lists to indicate where Boost is no longer required. Please do so."
2,Revise and improve DMTN-020,"An initial version of DMTN-020, describing project management practices, was produce in DM-6140. Revise and update that based on feedback from the DM Project Manager, DM Project Controls Specialist, DM technical managers, and others."
3,Deploy FY16 Storage Expansion (part 2),"Deploy infrastructure for FY16 storage expansion. This epic covers follow-on work to DM-3830.    Deliverable: storage expansion  Staff: 5 NCSA ICI engineers (networking, storage, systems)  Effort: 45 days  Planned Start: 6/1/2016  Planned End: 7/31/2016  "
3,Deploy FY16 Nebula Expansion (part 2),"Deploy infrastructure for FY16 Nebula expansion. This epic covers follow-on work to DM-3832.    Deliverable: expanded services  Staff: 3 NCSA ICI engineers (networking, storage, systems)  Effort: 10 days  Planned Start: 6/1/2016  Planned End: 7/31/2016"
3,Deploy FY16 Cluster Services (part 2),"Deploy infrastructure for FY16 cluster services. This epic covers follow-on work to DM-5624.    Deliverable: cluster services deployed  Staff: 3 NCSA ICI engineers (networking, storage, systems)  Effort: 10 days  Planned Start: 6/1/2016  Planned End: 6/30/2016  "
3,Deploy FY16 Verification Cluster (part 2),"Deploy infrastructure for FY16 Verification Cluster. This epic covers follow-on work to DM-5626.    Deliverable: verification cluster  Staff: 5 NCSA ICI engineers (networking, storage, systems)  Effort: 65 days  Planned Start: 6/1/2016  Planned End: 6/30/2016  "
2,L1 System Mock 1: Butler component,Implement a mock program that receives the Level 1 processing system data stream to simulate Butler integration.    Deliverable: mock program  Staff: Felipe Menanteau  Effort: 5 days  Planned Start: 7/1/2016  Planned End: 7/31/2016
2,L1 System Mock 2: Archive component,Integrate mock API to ingest data into Archive that organize data spatially on tape.    Deliverable: mock API  Staff: Felipe Menanteau  Effort: 5 days  Planned Start: 8/1/2016  Planned End: 8/31/2016
3,"Investigate the feasibilty of hosting an extracted, transformed and loaded database at archive site instead of a full EFD","Investigate the feasibilty of hosting an extracted, transformed and loaded (ETL) database at archive site instead of a full EFD. Gather sufficient details to support change request to eliminate full EFD at archive site. Address evident concerns relating to the increased data volume in the reformatted EFD and better integration into operational context, including the data backbone.     Deliverable: sufficient details to support a change request  Staff: Steve Peckins  Effort: 16 days  Planned Start: 7/1/2016  Planned End: 8/31/2016    "
3,Investigation of workflow and interface tools in the OpenStack environment,"Investigation of workflow and interface tools in the OpenStack environment. This is learning to support the eventual toolkits anticipated in the SUI for production deployment at the DAC at NCSA.    Deliverable: investigation report  Staff: Matias Carrasco-Kind, 2 students (at 50% each)  Effort: 20 days  Planned Start: 6/15/2016  Planned End: 8/15/2016"
1,create description of features in storage APIs,"The APIs for the storage brokers we're looking into are similar, but don't have a 1-1 correspondence.  Write up the features offered by the APIs, and see where there is overlap."
3,Design and RFC for Repository Refactor,Drive the RFC for Repo Refactor to completion (this includes a lot of design work)
3,Expand skeleton in LDM-151,We need to flesh out the skeleton text to contain full descriptions of the algorithms and pipelines we expect the baseline design to use.
3,"productize ""Repository Refactor""","After RFC-184 is closed: implement, unit tests, review, document, submit.    When this story closes, I think RFC-184 status is supposed to be changed from Adopted to Implemented."
3,Ramp-up adminstrative capability of qserv for deployment of SUI prototype system,"New staff will be on-boarded who has no prior experience with qserv. The goal is to ramp up to provide administration for the deployment of the prototype DAC, and to foster development of documentation within the qserv project that facilitates administration and usability of the product as a component in the LSST systems.    Deliverable: administration of qserv for SUI prototype system  Staff: Steve Peckins  Effort: 15 days  Planned Start: 6/1/2016  Planned End: 8/31/2016"
3,App logging framework migration work,"App logging framework migration work. Enhance the lsst::log package, prepare and do a RFC, migrate codes to use lsst::log, deprecate pex_logging.    Deliverable: framework migration  Staff: Hsin-Fang Chiang  Effort: 20 days  Planned Start: 7/1/2016  Planned End: 8/31/2016"
3,"Emergent middleware work (F16, part 1)","Reserve of effort to handle MINOR middleware-related work that emerges during the F16 cycle, June-August.    Deliverable: TBD middleware fixes  Staff: Steve Pietrowicz, Hsin-Fang Chiang, Mikolaj Kowalik, Matias Carrasco-Kind, Rob Kooper  Effort: 10 days  Planned Start: 6/6/2016  Planned End: 8/31/2016"
0,Please provide how-to-reproduce instructions for LSST/HSC comparison epics,"For all the stories describing comparisons between HSC and LSST results (notably DM-5301 and DM-5827), please provide instructions describing the steps to reproduce the comparison. In particular, include:    * A list of any tweaks that had to be applied to the code;  * Non-default configuration options;  * Exactly which comparisons were made."
1,Compare CModel results from LSST and HSC,"Demonstrate that the HSC and LSST stacks produce consistent results for CModel measurement. Account for (and fix, where relevant) differences."
1,Compare Kron results from LSST and HSC,"Demonstrate that the HSC and LSST stacks produce consistent results for Kron measurement. Account for (and fix, if relevant) any differences."
1,Compare meas_mosaic-ed HSC and LSST coadds,"The previous comparison of coadds on HSC and LSST was performed without an operable LSST-based meas_mosaic. When one becomes available, demonstrate that mosaicking is consistent between HSC and LSST; describe, account for, and (where possible) correct differences."
1,Account for noise replacement differences between LSST and HSC,"In DM-5827, [~rearmstr] wrote:  {quote}  In most of these plots you can see some scatter at relatively bright magnitudes... These are likely getting different pixel values when we replace objects with noise which is causing these changes.  {quote}  Check that this is the case, and, if so, explain why the noise replacement is different."
0,convert jenkins-ebs-snapshot job to use credentials for aws keys,"At present, this job is being templated by puppet to inject the keys in plain text which are they converted by jenkins to stored secrets if/when the job is edited and resaved via the jenkins UI.  This means that the credentials may be leaked."
0,Conda eups packages don't work if eups is already configured,"When [~tjenness] attempted to use the Conda repository he ran into a problem installing and using the packages because he already had an active EUPS_DIR and EUPS_PATH.    When the eups package is installed and linked, it should warn users when these environment variables are set.    I'm open to another solution but would prefer it doesn't change the eups package behavior. Changing behavior goes against the best practices for Conda and more generally packaging."
1,Possible image related issues in firefly viewer,"Image Meta Data tab  * images cannot be remove, but in expanded mode, it can.  * selecting image no longer highlight table.  the reverse works fine.  * visualize/saga/ImageMetaDataWatcher.js:272 returns -1.  * when a non-meta table is selected, images are shown, but not the toolbar.  * after table is removed, images are still there.    image external api does not mix well with firefly viewer.  * firefly viewer uses 'triViewImages’ viewer_id while api has no viewer_id.  as a result, images loaded by api will be lost once table or other searched data are returned.    catalog overlay are drawn outside of the images.    more issues:    - It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled.  "
0,Restore star selector registry,"Restore the registry for star selectors that was lost in DM-5532, now that tasks in registries can be used as subtasks.    Also use the registry where appropriate."
0,Install conda psutil instead of LSST's version,[~tjenness] requests that conda-lsst uses Conda's psutil. Currently we use our own version.
0,Report and work around conda repository change,This needs to be worked around by either using a different version of conda-build or addressing the changes to the conda recipe structure. I also want to comment and/or create an issue on conda-build so they know that such changes are affecting users.    See:    https://github.com/conda/conda-build/issues/1003  https://github.com/conda/conda-build/pull/1004    https://github.com/conda/conda-build/commit/b4ec0e0659d8f376042d4fc391616bf235996cf5    [Mario fixed it|https://github.com/mjuric/conda-lsst/commit/6a552b6f9cada2530681cfdc4a9f67add261ff99] but that fix will be broken as soon as conda-build #1004 is merged.
1,Form validation regression issues,"Recent changes in 'dev' made some of the components stop working.  We need to click through firefly viewer to identify the problems and fix it.  Below are a few that I've spotted.    Data Sets menu:  Form fail validation when they should not.  filters and upload file should be nullable.    Catalogs Classic menu:  form fail validation without any visual indications.  valid parameter is false when passed into validUpdate in CompleteButton.    xyPlot(Scatter Plot) options:  Beside X and Y, everything else should be optional(nullable).  Even after entering a valid value into all of the fields, 'OK' still fail validation similar to above where 'valid' parameter is false when passed into CompleteButton.    There may be more, please do a quick search to make sure all usage of CompleteButton is good.   "
1,Investigate calibration zeropoint offset between HSC vs. LSST processCcd.py runs,"As reported in DM-4730, while the scatter between single frame processing measurements of the same dataset on the HSC vs. LSST stacks is quite good (rms = 0.009 mag between Gaussian fluxes, for example, in the figure shown on that ticket), there is a clear offset (0.0166 mag in the figure shown) in the zeropoint between the two stacks (it is systematic, i.e. no trend with magnitude).  The cause may well be due to slight differences in the reference stars selected for calibration.  We also speculated about differences in slot definitions used in the calibrations steps (e.g. for aperture corrections, psfex, etc...), so I have rerun visit 1322 through both stacks having forced all apertures used in calibration to be the same, namely a circular aperture of 12 pixels measured using the sinc algorithm (as opposed to ""naive"").  I have attached the *processCcd.py* config files for the two runs so my settings can be reproduced.    Also of note, I am using a {{meas_algorithms}} branch on the HSC stack with the following commit:    {code}  commit 173ad0b32ed4f4ab074f1a942d2d3f758e189917  Author: Lauren MacArthur <lauren@astro.princeton.edu>  Date:   Wed Jan 13 16:35:59 2016 -0500        Hack to allow flux.aperture to be used in apCorr            Since it does not seem possible to access the nth element of a      schema element that is an array in the context of setting a config      override, this allows for flux.aperture to be set as      calibrate.measureApCorr.reference and it sets it to index 4 (which      corresponds to a radius of 12 pixels) in the __init__.  This was      selected to match the current LSST default.    diff --git a/python/lsst/meas/algorithms/measureApCorr.py b/python/lsst/meas/algorithms/measureApCorr.py  index 9f6c599..f1fa99d 100644  --- a/python/lsst/meas/algorithms/measureApCorr.py  +++ b/python/lsst/meas/algorithms/measureApCorr.py  @@ -81,6 +81,9 @@ class MeasureApCorrTask(lsst.pipe.base.Task):       def __init__(self, schema, **kwds):           lsst.pipe.base.Task.__init__(self, **kwds)           self.reference = KeyTuple(self.config.reference, schema)  +        if self.config.reference == 'flux.aperture':  +            print ""NOTE: setting aperture correction flux to flux.aperture[4] ==> radius = 12 pixels""  +            self.reference.flux = self.reference.flux[4]           self.toCorrect = {}  {code}    I attach some of the figures comparing the PSF fluxes from these runs which compare the output of the two stacks having matched the two src catalogs.  There are two sets: 1) having adjusted the flux for each source to the zeropoint calculated in the calibration and stored as *FLUXMAG0* 2) having adjusted the flux for all sources to a common zeropoint (zp=33.0, chosen to roughly match the calibrated zp).  Note that my figures do include aperture corrections (in DM-5301, many of the plots show fluxes pre-aperture correction).  I have also included plots that directly compare the aperture corrections applied (difference in mag units).  Finally, I also include plots comparing the 12 pixel circular aperture mags (i.e. to which no apCorr is added).    Clearly, the zeropoint determined in the calibration of the two stacks differs between the two stacks and, in particular, there seem to be some very problematic CCDs where the differences are particularly significant (~0.05 mag, and not always in the same direction).  Please investigate the source of this discrepancy."
1,Investigate offset in baseline zeropoint between LSST vs. HSC stack reductions for some HSC visits,"DM-6490 reports on an offset between the calibration zeropoints between HSC vs. LSST *processCcd.py* runs.  Here we report another, additional, offset seen in certain HSC visits.  It is not seen in the figures shown in DM-6490 for visit 1322.  However, here attach the same figures for visit 19696, run with identical setups/configs for both stacks as in DM-6490, where we see an additional offset in the ""common ZP"" figures (i.e. all fluxes have been scaled the same zp=33.0 for comparison).    A best guess at present is that the calibration frames are different between the HSC and LSST stacks for the timeframe of this visit; e.g. were the inputs ingested exactly the same for both sets?  Did the bug in regards to flagging on the flats noted in DM-5124:  {quote}  I found a difference in the codes doing the statistics: the HSC code uses a hard-coded mask ignore list of DETECTED only, while the LSST code uses a configurable mask ignore list that defaults to DETECTED,BAD (and the default isn't overridden). This produces a large difference on CCDs with bad amps (e.g., ccd=9).  There's a smaller difference on ccd=49 because the number of BAD pixels is smaller. Also note that the scaling of one CCD (like ccd=9) can affect others because we force the normalisations to correspond to that which we get from solving the system of M exposures of N CCDs.  {quote}  have a greater impact on these calibs?    Please investigate the cause of this offset."
0,Review LTS-210,NULL
0,Better error messages from the camera mapper when a template cannot be formatted,"The CameraMapper produces a very unhelpful traceback if it cannot format a template string with the provided data ID dict. For example:  {code}  Traceback (most recent call last):    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_tasks/12.0.rc1-3-gb785bf9/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 479, in parse_args      self._processDataIds(namespace)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 577, in _processDataIds      dataIdContainer.makeDataRefList(namespace)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 126, in makeDataRefList      dataRef=dr)]    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 935, in dataExists      return butler.datasetExists(datasetType = datasetType, dataId = dataRef.dataId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/butler.py"", line 288, in datasetExists      locations = self.repository.map(datasetType, dataId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 392, in map      return self.doParents(Repository.doMap, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 325, in doParents      res = func(parent, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 405, in doMap      loc = self._mapper.map(*args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/mapper.py"", line 169, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 284, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/mapping.py"", line 123, in map      path = mapper._mapActualToPath(self.template, actualId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 732, in _mapActualToPath      return template % self._transformId(actualId)  TypeError: %d format: a number is required, not NoneType  {code}    It is unclear what string was being formatted with what data, making the problem difficult to diagnose and correct.    I suggest changing line 732 of CameraMapper.py from:  {code}  return template % self._transformId(actualId)  {code}  to something like the following:  {code}  try:      transformedId = self._transformId(actualId)      return template % transformedId  except Exception as e:      raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e))  {code}    Here are the last few lines of the same traceback after applying this change:  {code}      path = mapper._mapActualToPath(self.template, actualId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 735, in _mapActualToPath      raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e))  RuntimeError: Failed to format '%(date)s/%(filter)s/decam%(visit)07d.fits.fz[%(hdu)d]' with data {'date': '2013-02-10', 'ccdnum': 10, 'hdu': None, 'visit': 176837, 'filter': 'z'}: %d format: a number is required, not NoneType  {code}    A bit wordy, but it is much easier to figure out what went wrong.    I have stumbled across this problem twice in the last few weeks, so I consider this change fairly important. The first time it was caused by a defective format string in a paf file. This time I'm not yet sure what is causing it, but at least I have something to go on."
1,Assist IN2P3 engineer in loading DC2013 data sample,"Bogdan Vulpescu, IN2P3 engineer, tried to load DC2013 data sample. Fabrice help was required to install Qserv in multi-nodes and understand data-loading system.    Some issues have been found and will be reported in future tickets:    - a script to publish loaded data (i.e. insert db name in qservw_worker.Dbs) would be useful  - mysql client might break proxy if option are not provided correctly (a bug report will be available soon)"
1,Assist IN2P3 student in using Openstack and following LSST coding standards,"[~oachbal] has written a code to automate Qserv cluster boot on Openstack cloud. Soma support was required to understand and solve cloud-init and openstack issues, Qserv container deployment and LSST coding standards."
2,convert irsaviewer to react.js,"Convert irsaviewer to use the new firefly library built on react/flux.    also made these changes:  remove irsa footer from fireflyviewer  filter by selected rows  auto-correct table’s filter input.  multiple columns sort via sortByCols  fix menu item not showing selected  ife automatically builds firefly.    To test, make sure you pull ife repos as well.  same branch name on ife."
1,Regrid needed to for WebGrid,"When compute the points for the grid lines, there is no guarantee that the number of points will all be the same.  However, the points can be regrided to ensure all the lines have the same number of points."
0,setup test framework,"Need to decide what to check so we have a consistent testing, requirement is opensource  - language  - license  - maturity  - funding  - ease of install  -- dependencies  - OS requirements  - ease to create a workflow  - ability to execute on clusters/laptop  - test with simple LSST workflow  - willingness to meet and answer our questions  -- open bug reporting site  -- speed at which bugs are resolved  -- size of community, external collaborators  - how big a graph can it support?  - is shared filesystem required for data, or can it take care of data transfer  - does it support MPI other parallel code?  - smart wrt to data available on node (optional)  "
1,pegasus,Review Pegasus Workflow Management System (https://pegasus.isi.edu) against criteria defined in the epic.
1,Swift,Review [Swift|http://swift-lang.org/main/index.php] scripting language against criteria defined in the epic.
1,final report,NULL
1,panda,NULL
1,Verification Plan Systems Engineering Status Review,"This follows on from  [DM-5315] and covers collating comments from DMLT, submitting the status report and document to Systems Engineering and dealing with the comments.     "
1,"Remove unsused ""version.h"" file and associated code","This code seems obsolete and unused:    {code:bash}  qserv@clrinfopc04:~/src/qserv (tickets/DM-5967)$ grep -r ""version.h"" *  admin/tools/docker/git/src/qserv/site_scons/genversion.py:# genversion.py : declare a builder for global version headers.  admin/tools/docker/git/src/qserv/site_scons/genversion.py:    """"""Construct a version header from git-describe output and store  admin/tools/docker/git/src/qserv/core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader)  core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader)  {code}"
0,Minor fixes to linearization,"DM-5462 added linearization to {{IsrTask}} but had a few loose ends which this ticket aims to correct:  - I intended to enable linearization by default, but somehow lost that change.  - I intended to update obs_test to use null linearization, but I forgot and the previous item meant I didn't catch the omission  - It turns out that the butler data proxy object will not work with functors (attempting to call the retrieved item results in an error, rather than resolving the proxy). This is easily worked around by using immediate=True when retrieving linearizers. This didn't show up until DM-6356 because obs_decam is the only camera that uses linearization lookup tables, and obs_subaru avoids the problem by not returning a proxy.  "
3,Convert footprint support,Convert the footprint support from the GWT code
1,Fix scheduler delays caused by mlock call in memman.,"Locking tables in memory with mmap and mlock greatly increases scan query speeds but makes the worker scheduler unresponsive to interactive queries. This also tends to have only one scheduler (fast, slow, medium) running at a given time."
0,Temp local background broken,The temp local background feature has been broken and needs to be fixed.
2,Prepare an RFC about logging migration,"Summarize RFC-29, evaluate technical details, prepare working examples, re-raise RFC-29 or file a new RFC before the migration.  Some implementation may be done before the new RFC.  "
1,Enhance lsst.log by having a Log object and Python interface ,"Based on branch u/ktlim/getLogger in {{log}} and requests from DM-3532, implement a lsst::log Python interface through Log objects, and allow controllability of logger names and levels in Python.  "
1,Capture ProjMgmt WG Long Term Planning conclusions in DMTN-020,The ProjMgmt WG is going to agree on a strategy for long term planning. Make sure it's captured in DMTN-020.
2,Statement of Work ,NULL
1,Networking Configuration,NULL
1,Investigate single frame processing astrometry failures/poor solutions on some HSC chip/visits.,"The astrometric solution of some visit/ccd combinations for HSC data are failing or finding very poor solutions.  This typically occurs for the outermost (highly fringed) ccds (e.g. 100..103, 95).  I provide some sample output below.    {code:title=LSST bad fit: visit=19696 ccd=100}  processCcd.calibrate.astrometry.refObjLoader: Loaded 71 reference objects  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 71 stars  processCcd.calibrate.astrometry.matcher: Purged 4436 unusable sources, leaving 288 usable sources  processCcd.calibrate.astrometry.matcher: Matched 6 sources  processCcd.calibrate.astrometry.matcher WARNING: Number of matches is smaller than request  processCcd.calibrate.astrometry: Matched and fit WCS in 1 iterations; found 6 matches with scatter = 0.000 +- 0.000 arcsec  {code}    {code:title=HSC fit: visit=19696 ccd=100}  2016-06-10T17:13:54: processCcd.calibrate.astrometry: Found 80 catalog sources  2016-06-10T17:13:54: processCcd.calibrate.astrometry: Matching to 119/148 good input sources  2016-06-10T17:13:55: processCcd.calibrate.astrometry: Matched 20 sources  2016-06-10T17:13:55: processCcd.calibrate.astrometry WARNING: Number of matches is smaller than request  2016-06-10T17:13:55: processCcd.calibrate.astrometry: 20 astrometric matches for 100, 0_31  2016-06-10T17:13:55: processCcd.calibrate.astrometry: Refitting WCS  2016-06-10T17:13:55: processCcd.calibrate.astrometry: Astrometric scatter: 0.038076 arcsec (with non-linear terms, 20 matches, 0 rejected)  {code}    {code:title=LSST failed fit: visit=19696 ccd=103}  processCcd.calibrate.astrometry.refObjLoader: Loaded 68 reference objects  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 68 stars  processCcd.calibrate.astrometry.matcher: Purged 2206 unusable sources, leaving 225 usable sources  processCcd.calibrate.astrometry.matcher: Matched 4 sources  processCcd.calibrate.astrometry.matcher WARNING: Number of matches is smaller than request  processCcd FATAL: Failed on dataId={'taiObs': '2015-01-21', 'pointing': 1116, 'visit': 19696, 'dateObs': '2015-01-21', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 103, 'expTime': 300.0}:     File ""src/sip/CreateWcsWithSip.cc"", line 142, in lsst::meas::astrom::sip::CreateWcsWithSip<MatchT>::CreateWcsWithSip(const std::vector<_RealType>&, const lsst::afw::image::Wcs&, int, const lsst::afw::geom::Box2I&, int) [with MatchT = lsst::afw::table::Match<lsst::afw::table::SimpleRecord, lsst::afw::table::SourceRecord>]      Number of matches less than requested sip order {0}  lsst::pex::exceptions::LengthError: 'Number of matches less than requested sip order'  {code}    {code:title=HSC fit: visit=19696 ccd=103}  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Found 84 catalog sources  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Matching to 137/162 good input sources  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Matched 19 sources  2016-06-10T17:20:11: processCcd.calibrate.astrometry WARNING: Number of matches is smaller than request  2016-06-10T17:20:11: processCcd.calibrate.astrometry: 19 astrometric matches for 103, 1_31  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Refitting WCS  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Astrometric scatter: 0.086131 arcsec (with non-linear terms, 18 matches, 1 rejected)  {code}    Other failed visit/ccd combos:  visit=19684 ccd=101  visit=30488 ccd=95: RuntimeError: Unable to match sources    This may simply be due to some threshold in the configs that is rejecting more stars on the LSST side, but this is not confirmed.  Please investigate the cause of these failures."
0,LDM-151 adjustments,"Adding text to LDM-151 where appropriate, working around the structure defined by Jim et al."
2,Write DMTN describing Lupton diffim decorrelation,"Write a technote describing the analysis and implementation of the Lupton(ZOGY) difference image decorrelation correction.    A new technote has been set up, it will be: http://dmtn-021.lsst.io"
2,Propose track to improve container infrastructure,"Qserv uses Docker for deployment, this ticket will add track on how to improve container deployment and management."
0,lsst-dev shared stack should provide release builds,"The shared stack on {{lsst-dev}} (etc) currently only provides tagged weekly builds of the LSST stack. Releases, RCs, etc are not included. Please update the build script so that they are.    NB simply including these builds is easy enough by changing the {{VERSION_GLOB}} regular expression in {{shared_stack.py}}. However, the {{current}} version is selected by a lexicographic sort of available versions. That works well enough for weekly builds ({{w_2016_XX}} is less than {{w_2016_XX+1}}), but fails with other tags. Better use a sort based on the date the tag was created on the HTTP server instead, perhaps."
1,Prevent external viewer from popup blockers.,"Currently, when external viewer launched, it is blocked by pop-up blockers. Need to change polling logic to a pushed solution so the 'launch' action can happen immediately. "
1,Release Note integration for 12_0 Stack Release,Transcribe v12_0 release notes prepared by development teams on Confluence into the Pipelines documentation sphinx project.    Pipelines documentation is published with LSST the Docs to https://pipelines.lsst.io.
1,Setup mononode test environment for initial learning about installations,How to load data and perform queries.  Investigate DAX interface to qserv.
0,Add queryId to messages at start and end of user queries.,"The queryId, ""QI=xxx:"", needs to be added to log messages that are useful for analysis. of primary interest are messages that indicate that a query has begun or finished, such as ""Discarded UserQuerySelect""."
0,Capture proposed epic review procedure in DMTN-020,See notes at https://confluence.lsstcorp.org/display/DM/ProjMgmt+Meeting+2016-06-14
0,Capture release policy in DMTN-020,"Capture the policy for releases (all work to be done 2 weeks before end of cycle, release at the cycle changeover) in DMTN-020."
1,Attend SBAG prep meeting at UW,[~nlust] will travel to UW for preparatory discussions in advance of this month's SBAG meeting.
0,"Take part in LDM-151 Progress Meeting, 2016-06-13",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-13",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-13",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-20",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-20",NULL
0,"Take part in LDM-151 Progress Meeting, 2016-06-20",NULL
0,Fix order of flags in Kron photometry,The flags are not added to the flag handler in the correct order for Kron photometry.
0,Clean-up rerun documentation,"Following DM-4443, there are a few ambiguities in the new {{--rerun}} documentation. Fix them."
0,Make updateSourceCoords and updateRefCentroids more visible,Implement RFC-197 to make updateSourceCoords and updateRefCentroids more visible
1,"Further prep for SBAG meeting, attend video telecon with Heidi et al.",Read back ground materials on LSST moving object simulations. This will be used to prepare for both the SBAG meeting and to come up with questions that need clarification in the preparatory telecons.
0,Remove the extra init method from the SourceDetectionTask,"SourceDetectionTask defines both {{init(self, schema=None, **kwds)}} and {{\_\_init\_\_(self, schema=None, **kwds)}}. The first exists purely because of a Doxygen bug that makes {{\copydoc \_\_init\_\_}} fail. However,   {code}  copydoc \_\_init\_\_  {code}  works. Remove the non-dunder init method and update the documentation with  {code}  \copydoc \_\_init\_\_  {code}."
0,Refactor Known Issues and Metrics pages in Pipelines Docs,Make Known Issues and Metric Report both top-level pages. Link to installation issues from installation page.    See https://pipelines.lsst.io/v/DM-6575/index.html
1,Convert jointcalTask unittest into a validation measure,"Now that jointcal has some basic unittests that check whether the relative and absolute astrometry are less than some value, we should convert those tests into validation measures a la [validate_drp|https://github.com/lsst/validate_drp]. This would help us track whether we are actually improving things as we tweak the algorithm and the mappings that we fit."
1,Initial tests running HTCondor jobs utilizing Shifter,"We start with initial tests of Shifter, with the first goal to  submit PBS jobs on the Blue Waters test system utilizing Shifter that start HTCondor master/startd daemons on compute nodes.  These daemons will communicate to a remote HTCondor central manager (e.g., running on the Nebula OpenStack) and glide-in to join a working pool.  The setup will then be tested with simple payload jobs (these  submitted from a Nebula instance running the schedd)  that verify access to the LSST stack within the UDI (User Defined Image)."
1,Understand and ensure variance plane compliance with diffim decorrelation,"Understand how the variance plane should be adjusted in the decorrelation (ZOGY) correction, and ensure it is being done correctly."
0,Decrease warning messages in dipoleFitTask,The dipoleFitTask was spitting out too many warnings. Change many of those to debug statements and remove the `lmfit` UserWarnings.
1,Design a metadata system for LSST code and documentation repositories (technote),"This ticket involves the research and design of a metadata system for describing LSST code and documentation repositories. Such metadata would be leveraged by DocHub and LSST the Docs (see [SQR-011|https://sqr-011.lsst.io]) and would reside as a YAML/JSON file in a resource’s GitHub repository.    [JSON-LD|http://json-ld.org] is of particular interest. I’m also consulting with GitHub, ADS, Zenodo, and CfA Library on making a sustainable system.    *Note: this story should be moved to a DocHub epic.*"
1,Adapt qa analysis script for LSST vs. HSC coadd processing comparison,The analysis script was adapted for single visit processing comparisons in DM-4393 and DM-4730.  Do the same here for coadd processing comparisons.
0,Fill out Software Primitives section of LDM-151,NULL
1,Implement exception translators in upstream pybind11,Pybind11 does not currently support translation of custom exceptions. This ticket tracks work done on upstream pybind11 (internal fork https://github.com/lsst-dm/pybind11-1) to implement this functionality. It should support functionality equivalent to (but not necessarily with the same API) as Boost Python exception translators (http://www.boost.org/doc/libs/1_61_0/libs/python/doc/html/reference/high_level_components/boost_python_exception_translato.html).
1,firefly api related issues due to irsa integration.,"* firefly_loader.js mistakenly uses relative path to load dependencies when it should resolve it via location of the loading script.  * TablePanel should render html content as html by default.  * paging bar style does not show correctly in irsa html  * row height does not resize to the icon size, the old api did. and the default row selectable is set to false in the old api.  * the help button needs to be added on top of the table panel.  * the expand button does not function as expected (open a full table panel)."
0,Write command-line driver tutorial for LSST@Europe2 meeting,"This will be done as DMTN-023 so the results are preserved for posterity.    This may be somewhat redundant with the work Mandeep Gill is doing in translating HSC docs, but I need it now; we can merge later."
0,Prepare presentation for SPIE,Write the presentation for the SPIE conference. Date of presentation: 26th June.
0,Clean up naming of multiband tasks and scripts,"Several of the multiband processing tasks and files in pipe_tasks and pipe_drivers have inconsistent names:   - Some task names do not agree with the script names.   - Words like ""Coadd"" and ""Merged"" are not consistently used.     Actually making these changes is trivial, but the work also requires creating and shepherding an RFC."
0,Port change to EXP-ID handling,"From [HSC-1409|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1409]:  {quote}  Due to an operational reason (to meet the requirement of Subaru FITS dictionary), the definition of EXP-ID is soon to be changed in the data acquisition side.  In the new definition, EXP-ID is set to 'HSCE%08d' where the letter 'E' is fixed as requested in the dictionary, and the number part corresponds to exactly the same number as our familiar 'visit'.  obs_subaru:ingest.py needs to be updated to include this rule.  The data taken with this change so far are:  HSCA07441200--HSCA07441757  HSCA90925200--HSCA90929557  {quote}    The change made as part of HSC-1409 introduces a new code path for the updated data, while old data continue to be supported with the old code path."
1,Reporting improvements,read and critique Jacek's  LPM document and the more manual oriented work from John Swinbank.  Phone con with Kevin w.r.t. reporting channel for equipment expenses in Jira (as opposed to the now clear separate distinct financial channel).  Worked out checklist and principals for revised WBS. 
0,Install packstack to test OpenStack Object Storage API,NULL
0,Finalize v12 Pipelines release documentation,Add the release announcement and finalize other documentation details in pipelines.lsst.io for the v12 release.
1,Review LDM-135 (LSST Database Design),NULL
3,Further refine alert generation pipelines sections,"There is much more information in the document, but the pipelines sections need to be refined.  We also need to give the software primitives a go over."
0,Update X16/W16 release notes for qserv and dax services,NULL
0,Make HSC processing without bright object catalogs easier,"obs_subaru enables bright object masks by default, as that's desirable for HSC production runs.      However, when HSC data is processed without bright object masks available (as will happen in most GO observations and development use), multiBandDriver.py will fail because the BRIGHT_OBJECT mask plane is not present but the base_PixelFlags algorithm is configured to make use of it. This is confusing, and it also requires the definition of a configuration file to fix the problem because base_PixelFlags cannot be configured directly on the command-line.    Some possibilities for fixing this:   - Add the BRIGHT_OBJECT mask plane in AssembleCoadd if doMaskBrightObjects is True but the external catalog is not found.  This will make the PixelFlags operation a silent no-op.   - Allow configuration options to allow PixelFlags algorithm to silently skip some flags if the appropriate masks are not available.    I am sure there are other options as well.  "
0,Include Kron parameters in algorithm metadata,"The Kron code doesn't set the algorithm metadata.  E.g.  {code}  algMetadata.set(""ext_photometryKron_KronFlux_nRadiusForFlux"",                  config.plugins[""ext_photometryKron_KronFlux""].nRadiusForFlux)  {code}  "
0,"update ""newinstall.sh"" nebula images & docker containers - v12_0",NULL
0,Cannot instantiate LoadAstrometryNetObjectsTask without Config object,"One should be able to create a LoadAstrometryNetObjectsTask without passing a Config object, if one only wants the default configuration. Currently it raises TypeError:    {code}  Traceback (most recent call last):    File ""testJointcal.py"", line 79, in setUp      refLoader = LoadAstrometryNetObjectsTask()  TypeError: __init__() takes at least 2 arguments (1 given)  {code}    If the config object really is a kwarg, it should default None and create a default config, so that one doesn't have to do, e.g.:    {code}  LoadAstrometryNetObjectsTask(LoadAstrometryNetObjectsConfig())  {code}"
1,cleanup non-survey-generic python in jointcal,"jointcal.py current does things like:    {code}  for dataRef in dataRefs:      if dataRef.dataId[""visit""] == int(visit) and dataRef.dataId[""ccd""] == int(ccd):          ...  {code}    This is not survey generic, and is probably not the best way to identify data blocks anyway. This, and other non-generic things in jointcal.py should be cleaned up so they work across surveys."
2,make jointcal integration/validation test for hsc,"We need an integration/validation test for jointcal on hsc data, to show that jointcal can run safely on hsc data processed through the stack."
2,make jointcal integration/validation test for cfht,"We need an integration/validation test for jointcal on cfht data, to show that jointcal can run safely on cfht data processed through the stack."
2,make jointcal integration/validation test for DECam,"We need an integration/validation test for jointcal on DECam data, to show that jointcal can run safely on DECam data processed through the stack."
2,make jointcal integration/validation test for lsstSim,"We need an integration/validation test for jointcal on lsstSim data, to show that jointcal can run safely on lsstSim data processed through the stack."
0,Fix base_* stuff in CcdImage.cc,"CcdImage.cc currently has hard-coded a bunch of {{getSchema().find(""base_blah"").key}} things. These should either be replaced with ""slot_*"", config.blahName, or dealt with at a higher level (e.g. not loading all those values directly inside of ccdImage::LoadCatalog).    Once this is done, we should delete the comments at the top of the file."
1,Reimplement diffim decorrelation as task,Reimplement the image decorrelation as a subtask rather than a direct call to a function.
3,validate_drp: design and implement an API for metric measurements and serializations,"{{validate_drp}} computes metrics and generates JSON that, through the [post-qa|https://github.com/lsst/post-qa] tool, is submitted to the SQuaSH REST API for persistence and display in a web app.    A previous ticket, DM-6086, we bolted on a JSON serialization scheme compatible with SQuaSH. However, this approach was not well integrated with {{validate_drp}}. We want a framework/API where serialization is handled consistently and integrally with metric computations. This includes the semantic serialization of computational parameters and reduced datasets.    This API can be applied beyond {{validate_drp}} as a means for metrics and integration tests to be submitted to SQuaSH as well."
0,Support ingesting reference catalogs from FITS files,Support a means of ingesting index reference catalogs from FITS tables (e.g. SDSS catalogs).
1,Single-frame processing tasks are no longer usable without a Butler,"Adding a butler argument to the constructor signatures for {{CharacterizeImageTask}}, {{CalibrateTask}}, and {{ProcessCcdTask}} makes these tasks difficult to use without a butler.    The fix is to make the butler argument optional (with a default of None), while adding another argument that allows a fully-constructed reference object loader to be provided directly instead.    This is closely related to DM-6597, which has the opposite problem: pipe_drivers' {{SingleFrameDriverTask}} doesn't take a butler argument, but it needs to in order to provide one to {{ProcessCcdTask}}.    I have a fix for this just about ready, but I'd like to add some unit tests that verify we can run all of these tasks both from the command-line and directly before calling it complete."
1,Make match and flag propagation more reusable,"We have two bits of code for doing spatial matches and propagating flags:   - {{PropagateVistFlagsTask}}: propagates flags from individual visit catalogs to coadd catalogs, and depends on a butler to do so (reasonably; it includes the smarts to load the appropriate catalogs, so it has to do I/O).   - {{CalibrateTask.copyIcSourceFields}}: propagates fields from icSrc to src, but is only usable as part of {{CalibrateTask}}.    Both of these should delegate at least some of their work to new class (possibly a Task) that manages the Schemas, SchemaMappers, and cross-matching necessary to do this work.  This new class should be reusable without a butler and without constructing any higher-level tasks."
0,"HSC ISR configuration file is applied to ProcessCcdTask, not IsrTask","{{obs_subaru/config/hsc/isr.py}} has its config options specified relative to {{ProcessCcdTask}}'s config hierarchy, not {{IsrTask}}'s.  This allows the ISR task to be retargeted in this file, but it will prevent {{IsrTask}} from being run as a {{CmdLineTask}} directly.    ISR Task retargeting should be moved to {{config/processCcd.py}}, allowing the {{config/isr.py}} level to be moved to the appropriate level."
0,Add JIRA-wrangling howto to DMTN-020,Expand https://dmtn-020.lsst.io/v/DM-6447/#jira-maintenance to describe best practices for T/CAMs working with JIRA. Include:    * Appropriate labels;  * Teams;  * ... other things?
0,LTD Keeper: Auto slug for edition paths deals with underscores,"Had a bug where {{utils.auto_slugify_edition}} did not replace underscores with a dash, and therefore failed {{utils.validate_path_slug}}. This created a silent breaked where a branch like {{u/rowen/r12_patch1}} did not get an edition created for it.    This ticket adds this replacement code and adds a test for such a case."
0,IsrTask is not a valid CmdLineTask,"IsrTask is a command-line task, but its run method does not take a dataRef (it instead has a {{runDataRef}} method.  This is inconsistent with other {{CmdLineTasks}} and more importantly breaks {{parseAndRun}}.    I'm committing a small workaround on DM-6631 to get {{parseAndRun}} working, but the ultimately method names should be made consistent across CmdLineTasks.  That will require an API change and hence an RFC."
0,Make list of elements for consideration for planning packages,Create a detailed checklist for developing the planning packages for the replan and WBS restructuring.
1,RADICAL-Pilot,Review [RADICAL-Pilot|http://radicalpilot.readthedocs.io/en/latest/index.html] against criteria defined in the epic.
1,Makeflow,Review Makeflow against criteria defined in the epic.   http://ccl.cse.nd.edu/software/makeflow/
1,pinball,Review [pinball|https://github.com/pinterest/pinball] workflow management system.
1,CloudSlang,Review CloudSlang against criteria defined in the epic.   http://cloudslang-docs.readthedocs.io/en/v0.9.60/index.html
0,Adapt qa analysis script to apply corrections measured by meas_mosaic,DM-2674 involves getting HSC's {{meas_mosaic}} working with the LSST stack.  This issue consists of adapting the analysis.py script of DM-4393 & DM-4730 to (optionally) apply the astrometric and photometric solutions derived running {{meas_mosaic}} to the individual visits before comparison.  This is useful in general and is specifically useful in comparing the {{meas_mosaic}} results between the HSC and LSST stacks.
0,Management level review of two products of the Management process working group,"Reviewed https://github.com/lsst/LDM-PMT/blob/integration/index.rst and https://dmtn-020.lsst.io/v/DM-6447/    Made extensive markup of LDM-PMT,  delivered to Mario Juric.  Assessed DM-6447,  which show promise of an actual workable manual, though not complete."
0,Move new reference loader so meas_astrom can use it and perform some cleanup,"The new reference object loader code lives in pipe_tasks, which means it cannot be directly used by code in meas_astrom. This will hamper separating astrometry.net out of meas_astrom, because unit tests need reference catalogs and meas_astrom cannot depend on pipe_tasks.    Also, I'd like to take a cleanup pass on the module names, so the new code is easier to find, and improve the unit tests."
0,Remove database hack,"DM-5988 introduced a hack in reading the raw files: we use a database to cache metadata from the shutter files and update the camera files at read time.  The camera files have now been ""sanitised"" (updated with the appropriate metadata), and it's time to remove the hack.    [~mfisherlevine] writes:  {quote}  Data is on lsst-dev in:    /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/1m3    Raw calibs are in:    /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/calibs    Regarding what I want: everything to be the same, but with a normal ingest, i.e. no splicing, just taking everything that is needed from one set of files. Some points to note:    * should be able to ingest all the raws and calibs files, and register their OBJECT types to allow processing with these as ids (inc. pipe_drivers scripts)  * pipe_drivers master calib scripts should still run (and their outputs still be ingestable)  * processCcd should run  {quote}"
1,implement the active target,"When a dialog such as catalog search is displayed, it should be able to pick up the active target or the coordinates from a highlighted row in a table. Please, implement the mechanism that will automatically pick up those coordinates and pre-fill the search form for you."
1,ffApi image related issues found by irsa integration,"*-for external image viewer, the default RangeValues causes problem, i.e. other defaults not set-. (FIXED)  *-global default does not always apply to external image viewer-(FIXED DM-7016)  The Gator implementation related to coverage map   (1) default symbol size, shape, color setting is different from that of original map   (2) cannot specify the shape, size, and color through API;   (3) cannot specify the shape, size, and color of a search center through API;   (4) -does not display any image and source when the table has only one ra,dec values, for example:  one table with one position value or one table with many records but has the same ra,dec values.- MOVED to [DM-7001]   (5) -the sources on coverage map are not clickable. However, on table and plot are clickable and work fine.- (FIXED)  "
1,ffApi XYplot related issues found by irsa integration,"* default  symbol size, shape,and color setting is different from that of original version.  * no XY Plot Options pop-out windows  *  the plot displays non-ascii characters on the panel (for example: Â FitÂ Â )  * miss Filter Dialog on the plot panel comparing with the original version.  *  does not accept default column names for the plot."
0,CR finder does not care about XY0 of input image,"Port of [HSC-1391|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1391]:  {quote}  The current version of CR finder does not care about XY0 of the input image and when I try to run CR finder on warped (difference) image, PSF cannot be properly extracted.  {quote}  and:  {quote}  I have noticed that the center of warped image is a gap between CCDs and PSF estimation there will fail. So get PSF without specifying the position is good enough. PSF class will select the best position.  {quote}"
0,"ConfigDictField says ""Inequality in keys for..."" even if I give 2 same configurations","From [HSC-1401|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1401]:  {quote}  config.py:  {code:python}  from lsst.meas.photocal.colorterms import ColortermGroupConfig    for key in ['i', 'i2', 'y', 'r', 'N1', 'N2', 'N3', 'z']:      root.calibrate.photocal.colorterms.library[key] = ColortermGroupConfig.fromValues({}){code}    This comamnd line  {code:bash}  rm -fr output ; for i in {1..2} ; do processCcd.py ./HSC --output output -C config.py  ; done  {code}  raises following error  {noformat}  2016-06-01T02:43:45: processCcd FATAL: Comparing configuration: Inequality in keys for calibrate.photocal.colorterms.library: ['z', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'N3'] != ['N3', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'z']  2016-06-01T02:43:45: processCcd FATAL: Failed in task initialization: Config does match existing config on disk for this task; tasks configurations must be consistent within the same output repo (override with --clobber-config)  {noformat}  {quote}"
0,Study iPlant as a potential candidate for workspace implementation,NULL
0,Investigate why afw.table.IdFactory doesn't allow reserved=0,"Setting reserved=0 when constructing a source ID factory (as would be logical when there is no exposure ID to reserve bits for) strangely doesn't work; it seems to be necessary to reserve at least one bit.  This may be a signedness problem (we use signed 64-bit integers for IDs to appease FITS, which is unfortunate), but we should be careful just reducing the number of available bits, as this could break code that expect to read IDs already written to disk.    Note that any change to this code in afw.table may require changes to code in daf.butlerUtils.ExposureIdInfo as well."
1,set up unit test for projection in Java,"While working on DM-6438 (set up unit test for projection in JavaScript), we realized we should have a parallel unit test system set up for Java code, to keep the two systems in sync. "
0,Data Backbone conops iteration 4: submit to TCT,"Submit the document for TCT change control. Process is TBD.     If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system."
1,Data Backbone conops: develop engineering considerations for BOE for work package,"Based on the data backbone services conops, develop a list of engineering considerations for making a BOE for the data backbone planning package."
1,Authentication & Authorization conops iteration 1: create raw draft (internal),Write a raw draft of the concept of operations for authentication and authorization services. In this iteration the document is developed in Google docs following the ConOps template.
0,Authentication & Authorization conops iteration 2: group review to produce first draft,"Review raw draft of concept of operations for the AA services to work through underdeveloped areas, clear up uncertainties, and make readable."
1,Authentication & Authorization conops iteration 3: larger review to produce second draft,"Review first draft of AA services conops within Data Processing Architecture working group, bringing in relevant experts.    Input from review is incorporated into a second draft.  "
0,Authentication & Authorization conops formatting: convert second draft to reStructuredText,"When the AA services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process."
0,Authentication & Authorization conops iteration 4: submit to Systems Engineering,"Submit the document for TCT change control. Process is TBD.    If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system."
1,Authentication & Authorization conops: develop engineering considerations for BOE for work package,"Based on the AA services conops, develop a list of engineering considerations for making a BOE for the AA planning package.  "
1,Level 3 Hosting conops iteration 1: create raw draft (internal),Write a raw draft of the concept of operations for Level 3 Hosting services. In this iteration the document is developed in Google docs following the ConOps template.
0,Level 3 Hosting conops iteration 2: group review to produce first draft,"Review raw draft of concept of operations for the L3 Hosting services to work through underdeveloped areas, clear up uncertainties, and make readable."
1,Level 3 Hosting conops iteration 3: larger review to produce second draft,"Review first draft of Level 3 Hosting services conops within Data Processing Architecture working group, bringing in relevant experts.    Input from review is incorporated into a second draft.  "
0,Level 3 Hosting conops formatting: convert second draft to reStructuredText,"When the L3 Hosting services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process."
0,Level 3 Hosting conops iteration 4: submit to TCT,"Submit the document for TCT change control. Process is TBD.    If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system."
1,Level 3 Hosting conops: develop engineering considerations for BOE for work package,"Based on the L3 Hosting services conops, develop a list of engineering considerations for making a BOE for the L3 Hosting planning package."
1,Batch Processing for commissioning conops iteration 1: create raw draft (internal),Write a raw draft of the concept of operations for batch processing services for the commissioning phase. In this iteration the document is developed in Google docs following the ConOps template.
0,Batch Processing for commissioning conops iteration 2: group review to produce first draft,"Review raw draft of concept of operations for the batch processing for commissioning services to work through underdeveloped areas, clear up uncertainties, and make readable."
0,Batch Processing for commissioning conops formatting: convert first draft to reStructuredText in a technical note,"When the batch production for commissioning services conops is in a solid state, convert the Google doc to a DM technical note in reStructuredText."
1,Batch Processing for commissioning conops: develop engineering considerations for BOE for work package,"Based on the batch services for commissioning services conops, develop a list of engineering considerations for making a BOE for the batch services planning package."
0,"Planning package for Management, Engineering and Integration with engineering judgement BOE based on RACI diagram","Following list of elements for consideration (DM-6642), estimate planning packages for Management, Engineering and Integration WBS element.    BOE is derived from RACI document, which list roles and responsibilities of line management, reporting group, steering group, and area technical leads."
0,Planning package for L1 Services with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Level 1 Services WBS element.    BOE is derived from detailed plan for prompt processing and archiving services created in February and engineering judgement based on conops documents."
0,Planning package for Batch Production Services with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Batch Production Services WBS element.    BOE is derived from engineering judgement based on conops documents.  "
0,Planning package for Data Backbone Services with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Data Backbone Services WBS element.    BOE is derived from engineering judgement based on conops documents.  "
0,Planning package for Data Access Hosting Services with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Data Access Hosting Services WBS element.    BOE is derived from engineering judgement based on conops documents."
0,Planning package for Common Workflow/Middleware with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Common Workflow/Middleware WBS element.    BOE is derived from engineering judgement based on conops documents."
0,Planning package for Misc. Services with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Miscellaneous Services WBS element. An example is the Authentication and Authorization services.    BOE is derived from engineering judgement based on conops documents."
0,Planning package for Development Support Services with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Development Support Services WBS element.    BOE is derived from engineering judgement."
0,Planning package for ITC and Fabric Provisioning and Operation with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for ITC Fabric Provisioning and Operation WBS element.    BOE is derived from engineering judgement.  "
0,Planning package for Service Management with engineering judgement BOE,"Following list of elements for consideration (DM-6642), estimate planning packages for Service Management WBS element.    BOE is derived from engineering judgement based on ITIL methodology."
0,Submit change request,Submit formal change request to restructure NCSA WBS in PMCS.
1,Revise Level 1 ConOps    ,Revise the Level 1 conops to incorporate the minimal required functionality of Level 1 Services: minimal data archiving of camera data and minimal transport via data backbone to NCSA.
3,Build draft design,Produce functional design breakdown from the revised conops (DM-6696).
2,Articulate the design in format needed for planning,Articulate the design created in (DM-6697) into the format needed for planning.
2,Produce revised WBS,"Based on articulated design, revise WBS to incorporate phase."
1,Discuss elements of RFC,Discuss elements of RFC (technical details and scope).
0,Produce RFC,Write up and submit RFC.
0,Respond to RFC comments and update RFC as needed,Respond to RFC comments and update RFC as needed.
0,Select workflow based on conops and review of workflow systems,"Based on use cases/requirements gathered in DM-6270 and evaluation reports completed in DM-6276, select workflow system."
1,Discuss elements of RFC,"Discuss elements of workflow RFC (technical details, scope, requirements)."
0,Produce RFC,Write up and submit RFC.
0,Respond to RFC comments and update RFC as needed,Respond to RFC comments and update RFC as needed.
1,Pull down and install OCS SAL code in prep for ConOps development,"At the Camera Workshop in Mid-June, OCS Team members suggested that DM pull down their Service Abstraction Layer software and gain familiarity with it. The User manual is being studied before compiling the software and running it with DM software as a means of simulating planned Telescope & Site processes and how DM will interact with them.  Most of the work for this epic will be conducted in August. This story captures our prep work."
0,Monitoring plan for Startup procedure,Identify startup processes to be monitored for health and to provide notification for startup failure.
3,L1 entity prototypes,This story addresses the need to separate the processes that connect to the DAQ and retrieve the image data from the processes that forward the image data to NCSA. Requirements for this component and the component that formats the image data into a file which includes associated metadata were discussed at length during the Camera Workshop this month. Prototypes for these component processes are underway.
1,Message Dictionary additions,"Message types for system bookkeeping acknowledgements as well as report messages were added to the existing dictionary and the means for acting upon these message types are being added to component prototype code.  In addition, needed changes were made to the existing dictionary so all reporting entities write more complete details to their report message queues."
1,Amendments to message interaction,Proper acknowledgements began being added to the messaging system this month.
1,Camera Workshop attendance,Work on preliminary specific additions to the camera interaction ConOps took place this month during attendance at the Camera Workshop
1,Use Shifter+HTCondor  in processing Stripe82 ref data at modest scale,"To test out processing at modest scales (~ 100 -- 1000 cores)  utilizing Shifter+HTCondor on machines like BW, organize processing (processCcd of obs_sdss) of stripe82 data similar to that used in the lsst_dm_stack_demo (run=4192 field=300)."
1,Add tests for order of flags to all measurment plugins,"In the meas_base framework, we independently define an enumeration of available flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/include/lsst/meas/extensions/photometryKron.h#L82] and a set of table fields for storing flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/src/KronPhotometry.cc#L422]. We implicitly assume that these are declared in the same order, but do not, in general, enforce this.    In DM-6561, these were found *not* to be in the same order in meas_extensions_photometryKron. Setting a flag based on a bad result would therefore set the wrong flag in the output table.    In the DM-6561 solution, we introduced a test for this which is specific to the photometryKron codebase. However, the basic structure of the test would be easily extended to cover all meas_base plugins to ensure this error can never occur. Do so."
2,"Message refinement , in light of development #1",NULL
1,Default chart and other optimizations,"These are the changes to support defalt chart and single chart type (as for IRSA release)  - Remove chart selection from chart area  - Use dropdown for chart selection (can be omitted if single chart type is used)  - Populate current values in chart options  - Support Clear and Reset in chart options  - For tables, connected to charts, if no default parameters are specified, default chart is an XY plot with CATALOG_COORD_COLS (used to produce an overlay) for catalogs or two first numeric columns for other tables.  - Label, matching column expression, and unit, matching table model, are default parameters for both app and api now.  "
2,Message refinement #2,NULL
1,Camera workshop attendance,"Attend camera workshop, Meeting did not fully address the need.  Travel to SLAC."
0,Summarize meeting results into Concepts of Operation,NULL
3,ConOps for Comfort Console and System Monitor,"This is the first definition of the Concept of Operation of the Comfort Console and System Monitor piece of the DM system. This application will play a key role in fault detection and correction as well as monitor actively the state (and sub-state) of all the components in the DM system. Based on the role of the operator, he / she will be able to dig down into the faults and take corrective action. An action dashboard will provide a hierarchical view of the state of the system and its components at any point in time."
3,Definitions of Alarms and Actions,This is a task to define all the failure cases and alarms that they should generate. It will also define the action that will be taken in the event of an alarm / fault and who will be taking the action.
0,"Process reference data on ""lsstdev pool"" for reference",NULL
1,Reference processing on NERSC Cori Shifter implementation,NULL
0,Write test jobs and submit files,NULL
1,Investigate HTCondor configuration wrt dropping of nodes in backfill scenario,NULL
0,Run test jobs and evaluate,NULL
0,Write final report,NULL
2,Make SuperTask data-aware,NULL
2,Add workflow features,NULL
0,Select existing tasks for prototyping conversion to workflow supertask,NULL
2,Convert selected tasks,Convert a {{CmdLineTask}} into {{SuperTask}}
1,Finish gathering input from DM representatives,NULL
0,Compile input,NULL
0,Review conops template,NULL
1,Iteration 1: Write raw draft based on input gathered from DM representatives,NULL
1,Iteration 2: group review to produce first draft,NULL
1,Iteration 3: larger review to produce second draft,NULL
1,Iteration 4: final draft and convert to reStructuredText to produce tech note,NULL
1,Service Management for F16 June,Dividing F16 Service Management  ~ monthly.
1,Service Management for F16 July, Dividing F16 Service Management  ~ monthly.  
1,Service Management for F16 August, Dividing F16 Service Management  ~ monthly.
0,Write test programs to exercise Swift API with OpenStack ,NULL
0,Write test programs to exercise Swift API with Ceph,NULL
1,Benchmark Swift command line tool for objects less than 5GB,NULL
1,Benchmark Swift command line tool for objects greater than 5GB,NULL
1,Benchmark Swift custom tool for objects less than 5GB,NULL
1,Benchmark Swift custom tool for objects greater than 5GB,NULL
1,Analyze results and write report,NULL
0,Find and read documentation for OpenStack Swift API,NULL
0,Find and read documentation for Ceph API,NULL
1,Write abstract API,NULL
0,Research existing API,NULL
0,Find and read documentation for DDN WOS API,NULL
0,Create description of features in storage APIs,NULL
0,Write test programs to exercise Swift API with DDN WOS,NULL
0,Benchmark Swift command line tool for objects less than 5GB,NULL
0,Benchmark Swift command line tool for objects greater than 5GB,NULL
0,Benchmark Swift custom tool for objects less than 5GB,NULL
0,Benchmark Swift custom tool for objects greater than 5GB,NULL
1,Analyze results and write report ,NULL
1,Write test programs to exercise object stores,NULL
0,Benchmark Swift command line tool for objects less than 5GB,NULL
0,Benchmark Swift command line tool for objects greater than 5GB ,NULL
0,Benchmark Swift custom tool for objects less than 5GB,NULL
0,Benchmark Swift custom tool for objects greater than 5GB ,NULL
1,Write report ,NULL
0,remove SizeMagnitudeStarSelector,"The sizeMagnitudeStarSelector is still in meas_algorithms, but it is unused and likely no longer works. We should either remove it, or update it to be fully supported.    The same holds true for any other C++-based star selectors we still have lying around."
1,Add support for deriving from Python exception types to pybind11,DM-6302 adds support for custom exception translators to pybind11. However exceptions mapped do not inherit from Python {{BaseException}} or higher. This prevents exceptions from being raised and caught with {{except Exception as e}} in Python. This behaviour also occurs with Boost Python and Swig (we hack around it with a pure Python wrapper).    This ticket aims to solve the problem by adding support for inheritance from Python exception types to pybind11.
1,Port meas_extensions_convolved from HSC,HSC has a new measurement extension: meas_extensions_convolved.  This performs aperture photometry with the PSF degraded to nominated seeings (similar to how galaxy photometry is commonly done these days).    Relevant HSC tickets are [HSC-1395|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1395] and [HSC-1408|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1408].
1,Port parent/child measurement from HSC,"The deblender sometimes gets into trouble with cluster galaxies, and the deblended fluxes aren't accurate.  In that case it helps to have measurements on the image without any deblending having been performed.  This is a feature used in HSC's mid-2016 production run afterburner, ticket [HSC-1400|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1400].  This feature should be ported for use in LSST."
0,Document meas_extensions_ngmix,"meas_extensions_ngmix has no useful documentation, not even a {{doc}} directory. Add some.    This should include at least an overview of the package contents, a description of its capabilities, and instructions on enabling it within the meas_base framework. The package should have a README."
2,Provisioning,NULL
2,Product Acceptance,NULL
3,Disaster Recovery Implementation,NULL
1,Documentation,NULL
1,Capability Validation,NULL
1,Security Vetting,NULL
2,Acceptance by Stakeholders,NULL
2,Capability Design,NULL
2,Gathering product pricing,NULL
1,Updating LDM-143,NULL
1,Acceptance into baseline,NULL
2,Design,NULL
2,Implementation,NULL
2,Capability Design,NULL
2,Procurement,NULL
2,Reception and Placement,NULL
2,Networking Configuration,NULL
3,Provisioning,NULL
2,Disaster Recovery Implementation,NULL
2,Documentation,NULL
3,Capability Validation,NULL
2,Security Vetting,NULL
2,Acceptance by Stakeholders,NULL
2,Qserv container crashes on Openstack using up to date CentOS/docker setup,"  {code:bash}  [qserv@lsst-fabricejammes-qserv-0 ~]$ docker run -it --net=host -e ""QSERV_MASTER=lsst-fabricejammes-qserv-0"" qserv/qserv:dev_master bash    qserv@lsst-fabricejammes-qserv-0:/qserv$ /qserv/run/bin/qserv-start.sh   INFO: Qserv execution directory : /qserv/run  Starting MySQL  [FAIL.] Manager of pid-file quit without updating file. ... failed!  [FAILing xrootd.[....] : Manager of pid-file quit without updating file. ... failed!   failed!  See startup logfiles : /qserv/run/var/log/xrootd-console.log, /qserv/run/var/log/worker/xrootd.log  [FAILing cmsd.[....] : Manager of pid-file quit without updating file. ... failed!   failed!  See startup logfiles : /qserv/run/var/log/xrootd-console.log, /qserv/run/var/log/worker/cmsd.log  [ ok ing mysql-proxy..  [FAILing qserv-watcher failed!  See startup logfile : /qserv/run/var/log/qserv-watcher.log  [ ok ing qserv-wmgr.    # Here error log can be different sometimes...  qserv@lsst-fabricejammes-qserv-0:/qserv$ cat /qserv/run/var/log/mysqld.log  ...  2016-06-27 23:50:00 140703880873792 [Note] InnoDB: Waiting for purge to start  2016-06-27 23:50:00 140703880873792 [Note] InnoDB: 5.6.27 started; log sequence number 1661735  2016-06-27 23:50:00 140703880873792 [Note] Plugin 'FEEDBACK' is disabled.  2016-06-27 23:50:00 140703105521408 [Note] InnoDB: Dumping buffer pool(s) not yet started  2016-06-27 23:50:00 140703880873792 [Note] Server socket created on IP: '::'.  2016-06-27 23:50:00 140703880873792 [Note] /qserv/stack/Linux64/mariadb/10.1.11.lsst2/bin/mysqld: ready for connections.  Version: '10.1.11-MariaDB'  socket: '/qserv/run/var/lib/mysql/mysql.sock'  port: 13306  Source distribution  2016-06-27 23:50:04 140703665879808 [Note] /qserv/stack/Linux64/mariadb/10.1.11.lsst2/bin/mysqld: Normal shutdown    2016-06-27 23:50:04 140703665879808 [Note] Event Scheduler: Purging the queue. 0 events  2016-06-27 23:50:04 140703088736000 [Note] InnoDB: FTS optimize thread exiting.  2016-06-27 23:50:04 140703665879808 [Note] InnoDB: Starting shutdown...  2016-06-27 23:50:06 140703665879808 [Note] InnoDB: Shutdown completed; log sequence number 4432991  2016-06-27 23:50:06 140703665879808 [Note] /qserv/stack/Linux64/mariadb/10.1.11.lsst2/bin/mysqld: Shutdown complete    160627 23:50:06 mysqld_safe mysqld from pid file /qserv/run/var/run/mysqld/mysqld.pid ended  160629 19:36:11 mysqld_safe Starting mysqld daemon with databases from /qserv/data/mysql  2016-06-29 19:36:11 139694065747776 [Note] /qserv/stack/Linux64/mariadb/10.1.11.lsst2/bin/mysqld (mysqld 10.1.11-MariaDB) starting as process 143 ...  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Using mutexes to ref count buffer pool pages  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: The InnoDB memory heap is disabled  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Memory barrier is not used  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Compressed tables use zlib 1.2.8  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Using SSE crc32 instructions  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Initializing buffer pool, size = 128.0M  2016-06-29 19:36:12 139694065747776 [Note] InnoDB: Completed initialization of buffer pool  2016-06-29 19:36:12 139694065747776 [ERROR] InnoDB: ./ibdata1 can't be opened in read-write mode  2016-06-29 19:36:12 139694065747776 [ERROR] InnoDB: The system tablespace must be writable!  2016-06-29 19:36:12 139694065747776 [ERROR] Plugin 'InnoDB' init function returned error.  2016-06-29 19:36:12 139694065747776 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.  2016-06-29 19:36:12 139694065747776 [Note] Plugin 'FEEDBACK' is disabled.  2016-06-29 19:36:12 139694065747776 [ERROR] Unknown/unsupported storage engine: InnoDB  2016-06-29 19:36:12 139694065747776 [ERROR] Aborting  {code}"
2,Track statistics about user queries and tasks running on chunks,NULL
1,Move queries to different scheduler if too slow,NULL
2,Update LSST full-stack processing configuration to match best practice from HSC,"In preparation for running an end-to-end comparison of large scale processing with the HSC and LSST stacks, we need to update the configuration to reflect currently understood best practice.    In general, we expect the default HSC configuration to be better understood and ""battle-tested"" given that it has been used for science-grade data releases.    Audit the default configuration of the full LSST stack (from ProcessCcdTask through multiband coadd processing). Where LSST defaults differ from HSC, update the LSST configuration to match the HSC equivalent unless there's a clear reason why LSST's default should be different. When it's not appropriate to update the LSST configuration, add an override to obs_subaru.    In some cases, the LSST and HSC stacks have diverged so that a direct transfer of configuration options isn't possible. Where an equivalent can be found, take advantage of it. Otherwise, stick with existing LSST defaults."
1,"Process HSC ""RC"" dataset through the LSST stack","Process the ""RC"" dataset used to verify HSC data releases through the LSST stack using the configuration specified by DM-6815."
1,Compare HSC and LSST processing of RC dataset,"Using the script enhanced in DM-6588, compare HSC and LSST (DM-6816) processing of the RC dataset."
1,Quality check LSST processing of RC dataset,Perform a quality analysis on the LSST processing of the RC dataset (DM-6816) in the same way as would be performed before an HSC data release.
1,Resolve CModel issues with aperture corrections,While working on DM-4202 it became apparent that the aperture corrections calculated and applied by CModel were too large. This ticket is intended to trace down where the failure is occurring and correct it.
3,Develop resource loaded plan for executing DRP sections of LDM-151,NULL
2,Deliver DRP slide deck for LSST Director's Review,Required by 2016-07-12.
0,Add meas_extensions_ngmix to lsst_distrib,Primarily so it can enjoy the benefits of regular CI runs.
0,Use meas.algorithms.astrometrySourceSelector in measOptimisticB,"Now that there is a working astrometrySourceSelector (just merged in meas_algorithms from DM-5933), we should get matchOptimisticB working with it. This would entail replacing matchOptimisticB.SourceInfo with AstrometrySourceSelectorTask and tweaking the latter to do whatever matchOptimisticB needs, and removing SourceInfo."
1,Deliver sections for  Operations Use Case Report,"For each diagram covering a key use case, provide a narrative interpretation of the key concepts being conveyed, including significant operational implications from the concepts being presented.    Fill in the table for the assigned use case areas.    Key use cases/concepts include: L1 production, L2 production, ITC incident response, ITC problem management."
1,Deliver sections for Concept of Operations,"Contribute to Concept of Operations sections about Chilean, NCSA, and CC-IN2P3 facilities. Describing the ""nuts and bolts"" basics and summarize each facility's role in the LSST operational system."
1,Investigate effects of turning on the Brighter-Fatter correction for single-frame processing of HSC data,"In the process of comparing HSC vs. LSST stack single-frame processing runs, we have been running with the Brighter-Fatter correction (BFC) turned off.  The reason for this to begin with was that is was not yet available on the LSST stack when we started these comparisons.  We also want to isolate as many features as possible in order to confidently assess their individual effects  The functionality was ported on DM-4837 with a default of *doBrighterFatter=False*.  This issue is to continue the single-visit run comparisons (see e.g. DM-5301, DM-6490, DM-6491) with BFC turned on on both stacks.    In particular, we are finding that slight differences in the reference stars selected for a given CCD can result in significantly different psf models.  Also, it was noted in DM-4960 that LSST seems to select reference stars to a brighter cutoff than HSC.  If a given field has a larger fraction of bright stars considered in the psf modeling, it is conceivable that it will be more significantly influenced by the BF effect, thus causing the large CCD-to-CCD variations seen in, e.g. DM-6490 (https://jira.lsstcorp.org/secure/attachment/28213/compareVisit-v1322-diff_base_PsfFlux-skyZp.png)."
0,Wrap base with pybind11,Split off from DM-6302.
0,Wrap utils with pybind11,Split off from DM-6302.
0,add 'placeholder' attribute to the input element,An attribute called placholder is available in html element <input> to give a hint to the user of what can be entered. The placeholder text must not contain carriage returns or line-feeds.      Add it as proptype to <inputfield> component.
1,Write report on SPIE conference,Write a report on my visit to the SPIE conference in Edinburgh.
1,Learning about Openstack,Sahand progress on learning about Openstack
1,Create a python interface to access OpenStack,Sahand progress on getting the interface to access the Openstack interface using Nova Client
0,Data Backbone Conops  iteration 1 prep:  Create a list of service endpoints,"Create a for list of service endpoints, with service considerations, and deliver to the Development file tree. Del with new ambiguities from the camera meeting at SLAC by listing the ""summit data services""  for both main camera and spectrograph as service endpoints,  since this may increase the functionality required, and it seems prudent to flow any of these requirement into further processes, since they seem likely."
1,Learning about Spark,Sahand progress on getting familiar with Spark and use of the interface to  create a small Spark cluster in OpenStack
1,Learning about Docker,Sahand progress on learning Docker containers and potential automatic deploy in OpenStack
0,Set up and install Spark,Sahand progress on getting Spark installed 
0,Learning about Kubernetes,Sahand progress on learning about automatic deploy and scalability of Docker containers using Kubernetes 
1,Deal with emergent related requests  affecting operations planning in June,"There emergent request for comment emerged in June.     1) The interim project manager,  directed that the project begin an investigation into Amazon Wen Service due to contacts he developed at a Data base orient workshop he sponsors.  Formulating  a response required a review of the service offered by AWS, and inquiring about the validity of pursing an evaluation of just one vendor in a marketplace that has many vendors, and a deciding that an appropriate amount of work was to send additional NCSA staff to an AWS workshop to gain a similar appreciation of AWS as was gained at the database meeting at SLAC.  (Authority of interim project manager to insist on immediate action was also sorted out)    2) Request to understand computing capabilities at alternate site from the Deputy director.  Support for for alternate site capabilities are documented in the  the emerging L2 Batch concept of operations a copy of which was shared (though draft status noted)     3) Processed a summary of the Camera meeting which occurred at SLAC. Did not find  conclusions that related to a concept of operations.    IN particular we could not understand it there was a call for computing and a summit data service to support disconnected operations,  or if this was a mere optimization in the system to relocate the acquisition and forwarding infrstructure to the summit, with no other changes."
1,Learning about Swift and HDFS,Sahand progress on storage objects to be used in OpenStack
1,Learning about Openstack and Jupyter,Di progress on learning these web technologies
0,Installing JS9 in Openstack server,NULL
1,Learning about SocketIO and HTML REST API,Di progress on Communication technologies for the web
1,Integrating Jupyter and JS9 for FITS visualization,Di progress in getting JS9 to work in Jupyter notebook
1,Write wrapper API for JS9 and Jupyter,Di progress in writing a wrapper to interact between JS9 within Jupyter
2,Understand the installation and administrative processes,"Review and gain administrative insight using the processes encoded in test scripts and other relevant features based on investigations in the prototype installations. For example, we may observe steps in test scripts to gain understanding of capabilities behind the scripts.    Provide comments on documentation where deemed helpful."
2,Liaison with deployment effort,Interact with qserv developers supporting deployment and NCSA's service provisioning environment. Learn and investigate aspects of qserv administration present in test deployment but not previously covered.
1,Setup multinode test environment for initial learning about installations ,Setup up one master node and one worker node.
1,Update Activator to reflect recent changes in CmdLineTask,NULL
0,Discussion regarding  'quanta' definition in SuperTask,NULL
1,Finalize documentation and current issues of prototype,"After updating some latest changes, need to update documentation to explain the extend of this supertask and activator initial implementation."
1,TBD related emergent work in July,NULL
1,TBD related emergent work in August,NULL
0,Document that the catalog returned from star selectors is a view,"Star selectors return a catalog whose records are shallow copies of the input catalog records. Document the shallow copy aspect. This is important for two reasons:  - The user should know  - Implementers must be told this, because if the records are deep copies then the code that sets a flag for stars will not set a flag in the input catalog, which loses most of the point of setting that flag."
0,Mapper tests require modification when new datasets are added,"[~price] [recommends|https://community.lsst.org/t/centrally-defined-butler-datasets/841] a new way to define datasets common to all cameras in daf_butlerUtils, but modifying these yaml files require explicit lists of datasets to be modified in tests/cameraMapper.py.    If these tests are still useful, they need to depend on a minimal set of dataset definitions instead of the real ones."
1,Participation according to direction from interim project management,"Given directions from interim project management, participation consisted of direct conversations with Kevin and Jacek plus background work talking to staff related to assembling a plan."
1,Refine simple 1D DCR correction,"DM-5695 created a functional implementation of a simple DCR correction algorithm. While it appears to successfully create template images with airmass and DCR matched to science images, it is computationally inefficient and appears to introduce new artifacts to the template image. This ticket is to enhance the simple algorithm in several ways:  * Convert to sparse matrices where possible  * use variance weighting of the images  * propagate masked pixels correctly  * Refine the algorithm to mitigate the new artifacts"
0,Understand how to render conops documents in Sphinx,Learn how to render conops documents in reStructuredText. Prototype conops template and for delivery into Technical Control Team Sphinx engineering environment.
1,Raw draft of System Monitor and Comfort Display,Produce raw draft of conops for review by steering committee. Includes operational components and connectivity for the system monitoring services that will monitor devices from the summit to NCSA.
1,Add verification feature to L1 conops,NULL
1,Add verification test to L1 plan,NULL
1,Add verification test to L1 design,NULL
2,Add verification feature to Data Backbone conops,NULL
1,Add verification feature to L2 conops,NULL
2,Add verification feature to Authentication & Authorization conops,NULL
1,Liaison with Systems Engineering,NULL
1,Appreciate amount of effort needed to run preliminary planning exercise,Run planning process with local staff to appreciate amount of effort needed.
0,Review evaluation criteria with CC-IN2P3,Review evaluation criteria with Fabio during his visit from CC-IN2P3 to NCSA.   https://drive.google.com/open?id=1Xhj6kaFEnNhCyRPskB6BCXYxgfs_9-cl3BRX9wCh1sE
1,Create evaluation plan from evaluation criteria,Turn criteria into tabular comparison chart and respect test implementation constraints.
0,Estimate amount of effort needed to run detailed planning exercise,"Run through process of detailing activities down to story size requested by the LSST EVM system.     Do detailed estimation of conops development and a sample of technical areas, and extrapolated based on number of epics, size of staff, and complexity of mission. Total = 100 hours for 3 months of activities for current staff size."
0,Design framework for integrating procurement activities with invoices,Respond to request to relate equipment charges to acquisition strategy document procurement activities.
1,Design framework for reporting and steering meetings,"Run the process with staff to assess and supervise technical status, the appropriateness of work compared to architectural vision, consistency with NCSA general acumen, and status vs. plan."
1,TBD processes coordinated with impending hire,"Design and implement critical processes defined in the RACI document, coordinated with impending hire."
1,Address concerns with source side (Dave Mills),"Work with Dave Mills and others to understand architecture and use of ""source"" EFD. The goal is to understand the amount of volume of data that would be in reformatted EFD that otherwise would not have been, should we proceed with the proposed change."
1,Address concerns with target side (SLAC),"Understand permissions and protections that would be in the reformatted EFD that otherwise would not have been, should we proceed with the proposed change."
1,Address internal concerns,Understand whether the file annex should be kept in the same cluster as EFD as opposed to general files in the data backbone.
1,Incorporate into ConOps and any draft design notes,"Incorporate concerns, solutions and agreements into ConOps and any draft design notes."
0,Address additional emergent concerns ,Address TBD additional emergent concerns 
1,Rework MemMan to be inline with the qserv worker Scheduler.,Split the memory mapping function from the memory locking function to allow the scheduler to initiate locking without blocking. Add additional memory tracking improvements in line with current thinking. Reduce lock contention. Add logging.
0,forcedPhotCoadd.py fails on CFHT data due to a CModel bug,"Hello,    forcedPhotCoadd fails while running on CFHT data due to a CModel bug. Here is an example on the error message that we get:    {code}  python: src/CModel.cc:1368: void lsst::meas::modelfit::CModelAlgorithm::measure(lsst::afw::table::SourceRecord&, const lsst::afw::image::Exposure<float>&, const lsst::afw::table::SourceRecord&) const: Assertion `measRecord.getFootprint()->getArea()' failed.  Aborted  {code}    Adding the following lines in cmodel.py (in CModelForcedPlugin.measure, before the call to self.algorithm.measure) allows to go around the problem for the time being, which seems to arise for null value of the number of pixel in a given footprint:    {code}  if not measRecord.getFootprint().getArea():      raise ValueError(""measRecord.getFootprint().getArea(): 0. No pixel in this footprint."")  {code}"
0,deploy jenkins python env support,NULL
0,Access to system with LSST stack,Secure access to machine(s) with the LSST stack. This includes installation on local desktop/laptop.
1,Controlled Test of LMSimpleShape using high SNR objects,"Some issues came up during DM-6300 which indicated that a more controlled set of tests would be required than the random Great3Sims tests to understand the behavior of NGMIX LMSimpleShape.      LMSimpleShape appears to fail computing moments on low SNR objects.  It also shows pretty wide variation in shear bias which did not show up with CModel.    The needed tests with would include controlled profiles (Gauss, Dev, and Exp), controlled SNR, and controlled q, theta, and flux.  This should separate out the causes of failure and shear variation which we have seen."
1,Ensure DipoleFitTask uses correct PSF(s) in case when Decorrelation is turned on,"Diffim A&L decorrelation (DM-6241) modifies the diffim PSF, but leaves the ""pre-subtraction"" images used by DipoleFitTask as they were. Ensure that the correct PSFs are being used for dipole fitting when decorrelation is turned on (and actually, in all cases)."
0,Get data stream from socket into a fits file,Get data stream (module?) from Jim into a fits file than can be loaded subsequently to the butler.
0,Load known image data format into the Butler,Use some type of known data (image) to load and test into the buttler. Data types might include DECam MEF images of single-plain image files from simulations.
1,Assemble data stream from socket to lsst-stack pipeline,Connect all of the parts together.
0,ci_hsc failure: insufficient PSF sources classified as stars,"Since [ci_hsc#396|https://ci.lsst.codes/job/ci_hsc/396/], the regular ci_hsc build has been failing with:  {code}  [2016-07-05T23:59:53.929169Z]  FATAL: At least 95% of sources used to build the PSF are classified as stars (49 > 50): FAIL  [2016-07-05T23:59:53.929201Z] Traceback (most recent call last):  [2016-07-05T23:59:53.929238Z]   File ""/home/build0/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in <module>  [2016-07-05T23:59:53.929249Z]     main()  [2016-07-05T23:59:53.929317Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [2016-07-05T23:59:53.929334Z]     validator.run(dataId)  [2016-07-05T23:59:53.929375Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 163, in run  [2016-07-05T23:59:53.929394Z]     self.validateSources(dataId)  [2016-07-05T23:59:53.929436Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 201, in validateSources  [2016-07-05T23:59:53.929451Z]     0.95*psfStars.sum()  [2016-07-05T23:59:53.929510Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 91, in assertGreater  [2016-07-05T23:59:53.929547Z]     self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [2016-07-05T23:59:53.929587Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 82, in assertTrue  [2016-07-05T23:59:53.929614Z]     raise AssertionError(""Failed test: %s"" % description)  [2016-07-05T23:59:53.929660Z] AssertionError: Failed test: At least 95% of sources used to build the PSF are classified as stars (49 > 50)  {code}  This error appears to be related to DM-5877: when I reverted to versions of pipe_tasks, meas_base, meas_algorithms, ip_diffim, meas_extensions_photometryKron and obs_subaru predating that ticket landing, the error vanishes.    I note that [~nlust] reports that he does not see these failures on his (OS X) system, but I can reproduce them on Linux: we should investigate if that's just a coincidence, or if there is per-platform variation here."
1,VO search doesn't trigger coverage image nor overlay,"While migrating the VO search panel, i found the follwing problem: once the table gets back, no image coverage or overlay is rendered.  One problem from IRSA simple cone search result is that the VO table doesn't contain the right UCDs expected. The second problem when the VO table does contain the proper UCDs is that the META_INFO is not set.   The edu.caltech.ipac.firefly.server.query.SearchManager#jsonTablePartRequest doesn't set the attributes from the DataGroup object into the TableMeta data as it is done previously in OPS by edu.caltech.ipac.firefly.server.query.SearchManager#getRawDataSet    Please, add the META_INFO object missing to the table and set the proper CATALOG_OVERLAY_TYPE, and CATALOG_COORD_COLS needed so the Coverage image and overlay can be rendered."
0,Add an option to label ccd serial number on the showVisitSkyMap.py plot ,(actual assignee: Samuel Piehl)     Sometimes it is useful to know where the CCDs are on the plot. Add an option to label the CCD numbers. 
1,Create DCR visualization tools,Several visualization tools will be very helpful to fully understand the effect of DCR correction algorithms and their failure modes.   * A function that generates difference images with the sources used for calibration and/or psf fitting marked.  * A visualization that indicates the spectral type of each source in an image. This could be a mask overlay where the color corresponds to the spectral type.  * A visualization of the coarse spectral resolution model built for DCR correction
0,Locate the test dataset for PDAC,Locate and evaluate a dataset of SDSS Stripe82 which is going to be used for testing the prototype DAC.
1,XYPlot density plot with log scale - bin size is not reflected correctly,"XYPlot with the large number of points does not display correctly when log scale is selected. When log scale is selected, binning on the server should be using the logs, so that the bins are the same size on the log scale. "
1,Filter editor on a chart toolbar,Need to add filter editor to the chart toolbar. Filter editor should be without selectable rows.
0,Filtering from expanded mode cancels expanded mode,"When a table is filtered from the expanded mode, the layout is changed back to unexpanded.    It looks like the issue is more general: table actions trigger layout changes, which are not always right. For example, TABLE_REMOVE action while in a dropdown makes the  dropdown to get closed. I've traced it to FireflyLayoutManager.js:layoutManager generator function.    Test sequence in firefly:   - When a table is loaded, open ""Charts"" dropdown, select Col link for X, then select Col link for Y. (At this point the previous table is removed).  - TABLE_REMOVE action on the second click triggers dropdown to go away.   "
0,git-lfs.lsst.codes certificate is expired,"Per reports on hipchat, the tls certifcate on git-lfs.lsst.codes was not upgraded to the new *.lsst.codes cert.    {code:java}  John Swinbank  9:52 AM  @josh @jmatt I'm seeing the following, which I think might be the same as @srp's error above. Any ideas?  Get https://git-lfs.lsst.codes/objects/24874b686b9479a823987dc2bd2700cad5b73e74a43108fb61b91d7f79f0cd99: x509: certificate has expired or is not yet valid  Followed by git lfs failing.  (I assumed it was user error on my part at first, but if so it's coincidence that Steve's git lfs fails at the same time.)  {code}    "
1,"jointcalRunner passing tract to jointcal, which had tract removed from run()","When cleaning up jointcal for testing, I removed tract from jointcal.run(), but did not remove it from the return list of JointcalRunner.getTargetList(). Tract isn't actually used anywhere in jointcal.run(), so we should be able to just remove it from getTargetList's return.    Keeping those two in sync may be a bit tricky without a unittest that compares them."
0,Documenteer seeds Git revision date and branch name if not present in metadata.yaml,"If {{last_revised}} and {{version}} are not present in metadata.yaml, then the Git commit date and branch name should be used while building metadata instead.    Also updates lsst-technote-bootstrap to take advantage of automated metadata for new projects."
1,Write User Guide for new validate_drp metric/measurement API,"DM-6629 provided a new API for consistently specifying metrics, their specification, and reporting results of measurements.    This API can, and should, be used beyond validate_drp for any code that wants to submit metadata to SQUASH. This ticket will provide user documentation on the API base classes to help other developers write new metrics and measurements."
2,Implement script to simulate AP workflow,"To understand better the load on L1 database I need a more or less adequate set of queries running against the databases. AP-generated queries should be a good start so a simple script that simulates what AP does will be very helpful. Sure I don't need any actual image processing or alert production, only the parts which read/write data to the database on per-visit basis."
0,"Please rename ""afterburners""","In DM-4887 we introduced a new measurement post-processing system which we called ""afterburners"".    The term ""afterburner"" is overloaded and applied in multiple contexts. To save confusion, please rename this system to something less ambiguous. Best if we can do this soon, before this usage spreads."
0,Upgrade to new stack install procedure for containers,"LSST stack install has evolved: https://pipelines.lsst.io/install/newinstall.html#  Release container creation script needs to be update.  Latest Docker version will be tested, as [~bvan] reported cmd line options have changed."
0,Apply distortion when searching for astrometric reference objects,"While investigating DM-6529 I found that LSST generally finds fewer reference objects than HSC when doing astrometry.  For the CCDs on the edge of the focal plane the number of stars was typically very low causing frequent failures.  I found that in the HSC code, there is a distortion being applied that shifts the exposure bounding box when getting objects from the reference catalog.  This distortion is not being applied in the LSST code."
1,Resurrect obs_file,obs_file needs to be resurrected.  This is partially due to the reorganization of processCcd.  My take is to try to make the ingest script read the files and ingest them keyed on the filename.  Then the dataId will be just the filename.  Hopefully we can then mock all the other info needed for processing in a general way.  Calibration (astrometric and photometric will be off by default).  
1,star selector and PSF determiner are selecting stars that are not valid point sources,"When turning on CModel a more robust extendedness classifier relieved that many of the stars being used as PSF candidates were being classified as extended as shown in the attached plot. This plot was generated from the output of ci_hsc. Work should be done to determine why these stars are mistakenly being selected and fix the bad behavior. Additionally the [temporary work around in ci_hsc|https://github.com/lsst/ci_hsc/commit/6daf43ca41b6d192b6e1dbedb60cde0bec90b615], where the success criteria for validate sources in validate.py should be reverted from 85% to 95%."
1,HSC backport: Include PSF moments in the output tables,"This is effectively a port of [HSC-110|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-110] though, due to the considerable differences in bookkeeping for the SdssShape code, this will be more of a reimplementation.  "
0,access policy for PDAC,This is a Prototype DAC (PDAC) and the access to it is limited. We need to draft a access policy. 
0,Startup Scaffolding Machine Requirements - Base,"Plan Base machine needs for Startup, including those processes that must run together on the same physical host (such as databases resident in memory needed by other processes)."
0,Plan Base site machine startup pattern,"List all dependencies for specific processes that must be previously up an running. Establish the settings for 'time zero' on the startup timeline, such as purging queues, clearing specific data stores, arranging file system for startup, etc."
0,Prep for and attend Kubernetes meeting,"Kubernetes is a machine provisioning application that has potential to assist LSST Startup scaffolding such as Just In Time personality assignment, persisting config settings, etc."
1,Download and install Kubernetes,Implement a Kubernetes instance running on the Nebula cluster and begin configuration for testing Base site Startup behavior.
1,Evaluation of Kubernetes for Startup Scaffolding,"After Kubernetes is running as a simulated base site start up and provisioning tool, begin evaluation with fault injection such as the need for hot swap machine failover, sudden changes to Network topology and name server entries, etc."
0,Set up proposed start up tools and procedure for NCSA L1 components,"If Kubernetes is the answer for startup provisioning which it is hoped to be, apply it to NCSA L1 machine startup requirements."
0,Final Startup Scaffolding document,"This is expected to be a document specifying the final Startup scaffolding disposition. If specification is not final, it will assess which requirements for this epic were not reached."
1,First round of updates to DRP LDM-151 sections from reviews,"Will address comments from [~swinbank], [~rhl], and probably [~ktl]."
1,Explore and experiment the process of creating a Jupyter widget,Study the Jupyter notebook and understand the concept of Jupyter widget. Try to make a simple Jupyter widget that works with Firefly visualization.
1,Integrate multiple-backgrounds concept into LDM-151,It's recently become apparent that we need to at least consider using different background estimation techniques for different kinds of measurements.  This is will require some thought to work into our current processing plans.
1,Add text to algorithmic components sections in LDM-151,"While [~swinbank] has commented that the outlines are probably good enough for planning work (and I thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [~rhl]'s first review pass, so I think I should flesh that out with text sooner rather than later.    In this issue, I'll stick to sections that no one else has added text for, but eventually I'll also need to work with [~krughoff] and perhaps others to ensure that section has a consistent level of detail and focus."
0,Firefly has problem to render in other browsers than Chrome,"Couple of problem using Firefly in  Safari:  * the components appears blank,    in Firefox:   * image and xyplot are not aligned (Gator).    The alignment can be reproduced in my Chrome and Safari.  Search parameters: ALLWISE source catalog, m81 100arcsec.   "
1,Table problems,"Table component has couple of problems:    #  Scrambled table values after column selection and saving the table, then reset mess up the table. Saving the table and reset makes the table comes back.  #  table display no longer redefines column names in the table based on the column label, e.g. ""Field Size"" instead of ""s_fov"".  #  Downloaded file is not a valid IPAC table.  #  Filtering table does not change image overlay or plot until table is saved. At that point, the filtering works, but the plot symbol changes (happens when result is decimated, datapoints > 5000?)  #  In Edit Table Options, it's unclear what the reset button resets to.  "
2,Image problems are grouped in this ticket,"Image viewer has a couple of issues:    * There is no panner for the image (is it missing from the API or is it a bug in calling it?)  * No readout value from thumbnail image  * -Image does not have toolbar or layers control (Gator) - probably API options to be used or missing?- -> move to DM-7001  * -Markers don't show up in PNG download- -> moved to DM-6980  * The top bar readout doesn't include units for the pixel flux  * Clicking on the expand icon deletes the image (in API only)  * Clicking on the expand icon diabled the expand mode of table and xy plot (in API only)   * Image button ask you for a position and displays that, wiping out whatever image brought you to IRSA Viewer in the first place. It should give another tile of the same field, drawn from the selected data set. (IRSAVIewer only)  * Need mode to draw grid without labels  * RangeValues messing up image display        {noformat}  old:  var external= firefly.getExternalViewer();              external.setDefaultParams({   ""TitleOptions"" : ""FILE_NAME"",                                            ""ColorTable""   : ""1"",                                            ""TitleFilenameModePfx"" : ""cutout"",                                            ""PostTitle""    :  ""\locstr\"",                                            ""OverlayPosition""    :  ""\lon\;\lat\;EQ_J2000"",                                            ""RangeValues""  : firefly.serializeRangeValues(""Sigma"",-2,8,""Linear"")                                        }) ;    new:  xtViewer.setDefaultParams({  ""TitleOptions"" : ""FILE_NAME"",                                            ""ColorTable""   : ""1"",                                            ""TitleFilenameModePfx"" : ""cutout"",                                            ""PostTitle""    :  ""\locstr\"",                                            ""OverlayPosition""    :  ""\lon\;\lat\;EQ_J2000"",                                        }) ;   The ""RangeValues"" were taken out for now   ""RangeValues""  : firefly.util.image.serializeSimpleRangeValues(""Sigma"",-2,8,""Linear"")  {noformat}  "
1,XY plot problems found,"implement the items listed here:  * min/max options are now gone after migration, need to be added.  * Use the expression for X, Y column as the default label, otherwise the read out could be confusing.  * label changes for decimation: X-Bins and Y-Bins:  Number of X-Bins, Number of Y-Bins * shrink the size (to 1/5?)  of the blue dots for data representation.  I do like the circle when the point is highlighted.  * The units on the plot are indicated with a comma, e.g. “dec, deg”, should be ""dec (deg)"" as before   Need to confirm again ([~ejoliet])  * -Making a change to the plot (e.g. ra = ra * -1), then clicking on the gears to close makes the shading legend disappear. It also “quantizes” the plot- (not any more) * What happens now is the plot appears without legend after a search. Then making a change to the plot (e.g. ra = ra * -1 or filtering the table), then applying makes the legend appears/disappears. Expanding the table and collapsing it, make the legend disappears. *- Step to reproduce: Catalog search on 2MASS around m16 with 10' radius. * Greyscale introduced, where different colors represent different numbers of points. After filtering, the points change color to blue.  (This is because the it is not decimated any more) * -Clicking on plot gears makes plot unusably small.- (Not applicable any more since gear now brings up the options in popup) "
1,Message Dictionary Adjustment.,"Audit format of existing messaging and adjust according to 'wants' not task 'needs'...that is, msg body format that exists now is sufficient to fulfill tasks, but destination components must receive a broader description of overall system state. This will allow all components to log a more comprehensive snapshot of current state and is needed for troubleshooting. These additions to the message dictionary will be configurable like a logging priority levels function, and additions to message payload can be turned off for typical nightly operation."
1,'ACK' (Acknowledgement) message formats,"Enumerate ACK messages for all primary message types. Prototype both blocking and non-blocking acknowledgement aggregator that works via timeout, behavioral change, etc. This is related to DM-6411"
1,Adding ACK messages to existing code framework,New entries in message dictionary must be added and tested with the existing messaging code base.
0,Documentation for new message types,Add new ACK message types to existing Dictionary documentation.
0,Messages as objects,"Consider creating a dictionary of code objects to represent messages. Currently message bodys are built on the fly - evaluate pros and cons of switching to a message factory pattern. Message types are not a very extensive list, but switching to object implementation could increase maintainability of code."
1,Overlay health check code,Implement and overlay health check mechanism on existing messaging control code. Prototype and gather timing information to determine optimal frequency of checks and the location in the exposure cycle when these checks should occur.
1,Fault Injection in the form of unsuccessful health checks for components,Build testing mechanism to inject faults into into health framework and stub code to address health check failures
1,Create policy for health check failure,"Document policy regarding action to take when various components are found to be unhealthy. This can vary depending when 1 component type (Forwarder) is offline versus 21 Forwarders offline. In addition, plans must be formulated for addressing the point in the exposure cycle when the health failure occurs."
1,Implement health failure policy,Formalize the prototypical implementation of health checks and associated policy into 'what to do' actions
1,Make a proposal for API support for representation of relationships between table columns,"End users and the SUIT need to be able to determine a variety of relationships between columns in the tabular data products produced by LSST.  The particular example motivating this ticket is the need to answer the question ""where in the table is the uncertainty data for column 'x'?"".    The answer could be:  * ""there isn't any""  * ""a symmetric Gaussian uncertainty is in column 'sigma_x'""  * ""asymmetric Gaussian uncertainties are in columns 'sigmaplus_x' and 'sigmaminus_x'""  * ""'x' is correlated with 'y' and the covariance matrix is in 'covar_xx', 'covar_xy', and 'covar_yy'""     Ideally we would find a way for these relationships to be defined when the Apps code generates its afw.table outputs, discoverable through an API usable in the afw.table context, exportable to the database, and made available to end users and the SUIT.  It should be usable whether the data are delivered to end users as reconstituted afw.table objects or as tables in common Python formats (at least Astropy tables).    It should assist the SUIT in determining how to (automatically, though optionally) display uncertainty data when the primary data are requested.    This ticket expresses the idea that a solution that consists purely of a documented convention about prefixes to the string names of columns is inadequate.  We would like to avoid having to write code implementing that convention in, potentially, hundreds of places, and we would like to avoid requiring that end users know these conventions in order to see proper displays with error bars.  "
1,Flesh out software primitives,Jim has put together a fairly complete software primitives section.  This task is to read it over from the perspective of Alert Production and expand/refine where necessary.
0,Level 2 conops formatting: convert draft to reStructedText,NULL
0,create a shared stack on NFS for use with  the current local condor pool,"It is well known that building, setting up a stack, and interactive devel work with those operations on NFS has performance issues.  Hence the official shared stack on lsstdev uses /ssd .    However,  a shared stack on NFS is useful and adequate for one important  use case --   users need a stack that can be used for small productions on the local condor pool currently available  on lsstdev.   For this use case multiple ""source""/""setups"" on a node/against the file system  can be avoidable by using a script to directly declare the environment.  run_orca /ctrl_orca supports this feature.       While GPFS is coming soon, there is expected to be a transition period of 2-3 months and so the NFS file system and a stack on it can serve users for an interim period.   If building a shared stack on NFS is not a heavy labor, we think it is worth the effort for this interim period, and as such make this request for a shared stack on NFS. "
0,Fixes to LoadIndexedReferenceObjects,Bug fixes for using the new {{LoadIndexedReferenceObjectTask}} and its associated components.
0,Add tests for bindings of Eigen::Array and ndarray::EigenView,"[~pschella] has discovered that we don't have test coverage for converting less-common Eigen types to Python.  This is not urgent, but it should be fixed."
0,Qserv 2016_07 release,#NAME?
0,Fix Qserv install doc and scripts for new newinstall.sh,Update qserv install docs per new info at https://pipelines.lsst.io/install/newinstall.html
0,"Fix metadata date problem in LDM-{463,152,135}",These docs are currently borken in CI -- just need to have the dates reformatted in their metadata.yaml
0,Type of IngestIndexedReferenceTask_config wrong in obs_ paf files,"In DM-6651 I moved the new HTM indexed reference catalog code from pipe_tasks to meas_algorithms, but didn't do a complete job. The type of IngestIndexedReferenceTask_config in obs_ paf files still must be updated."
0,Document release milestone changes in DMTN-020,Please add a note to DMTN-020 describing the changes to release milestones discussed at the [DMLT meeting of 2016-07-18|https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2016-07-18].
0,watch for Highcharts update ,"There is an issue in the density plot for displaying the legends. Highcharts does not support the setting of the symbol size in the legends. So when the symbol size is too small or too large, the legends are not displayed.     We don't want to do too much workaround currently. This ticket is to watch for the Highcharts update. "
2,verification and test of the Bayesian histogram calculation on server side,We need to set up some unit tests of the Bayesian histogram calculation.     First we need to do some verification of our algorithm with scientists.  I think we can find some known data sets and the results with help from scientists.   The unit tests should include several different distribution of input data. 
0,Update qserv for changes in Log interface,DM-6521 improved Log class interface by replacing some static methods with non-static. Qserv is currently using couple of static methods which were retained in Log class for the duration of this migration. Once updated log package is released update qserv code to use new non-static methods and remove static methods from Log class after that.
1,Markers don't show up in PNG download,Markers don't show up in PNG download
0,Fix oversampling settings in psfex,"The current settings in psfex will only turn on oversampling only if the seeing is < 0.5"", even if you have configured it do oversampling. This needs to be changed so that everything is determined by the config parameters.    We have also seen on HSC data that oversampling in general does not work well in psfex.  We need to change the current configuration which does 2x oversampling to just use the native pixel scale."
0,ci_hsc failure: AttributeError: 'Butler' object has no attribute 'repository',"Following [~npease]'s [recent changes to the Butler|https://community.lsst.org/t/im-checking-in-butler-changes-related-to-rfc-184/959], ci_hsc is failing as follows:    {code}  [2016-07-20T07:57:31.954576Z] Traceback (most recent call last):  [2016-07-20T07:57:31.954643Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in <module>  [2016-07-20T07:57:31.954664Z]     main()  [2016-07-20T07:57:31.954732Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [2016-07-20T07:57:31.954756Z]     validator.run(dataId)  [2016-07-20T07:57:31.954825Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 155, in run  [2016-07-20T07:57:31.954851Z]     self.validateDataset(dataId, ds)  [2016-07-20T07:57:31.954923Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in validateDataset  [2016-07-20T07:57:31.954956Z]     mappers = self.butler.repository.mappers()  [2016-07-20T07:57:31.954991Z] AttributeError: 'Butler' object has no attribute 'repository'  [2016-07-20T07:57:32.023212Z] scons: *** [.scons/ingestValidation-903342-100] Error 1  {code}    See e.g. https://ci.lsst.codes/job/stack-os-matrix/13274/compiler=gcc,label=centos-7,python=py2/console.    Please fix it. "
0,Suggest logging migration in daf_persistence and daf_butlerUtils,Use lsst::log instead of pex::logging in daf_persistence and daf_butlerUtils
1,Suggest logging migration in afw,Suggest a changeset with lsst::log instead of pex::logging in afw
1,Suggest logging migration in pipe_tasks and meas packages,Suggest changesets using lsst::log instead of pex::logging
0,Write up a description of Composite Datasets based on input from KT,"Write a description of composite datasets as I understand them based on the email KT sent on May 20 (attached), and on conversation I had with KT and Fritz on July 20."
0,Review Composite Dataset description document with stakeholders,"Review the composite dataset description document with [~jbosch] and [~Parejkoj], and any others who may be interested (e.g. post on community or do an RFD)"
1,ctrl_events/tests/EventAppenderTest.py fails Jenkins run-rebuild,"ctrl_events/tests/EventAppenderTest.py started failing on Jenkins ""run-rebuild"" last night:   https://ci.lsst.codes/job/run-rebuild/354//console    All test cases in EventAppenderTest.py did run and pass, but it failed with a Segmentation fault in the end.     Jenkins ""run-rebuild"" uses a stack on NFS on lsst-dev (/nfs/home/lsstsw).  The same test passes on regular Jenkins (stack-os-matrix).      "
1,Improve testing in SQuaSH prototype,This ticket captures some testing practices from https://ep2013.europython.eu/media/conference/slides/obey-the-testing-goat-rigorous-tdd-for-web-development-with-django-and-selenium.pdf  that we intend to use in the SQuaSH prototype.     - Use selenium to test user interactions  (functional tests)  - Use unittest module for unit tests   - Include some documentation about testing in sqr-009 
1,Add a script to summarize what visits are in what patches,"(Actual assignee: Samuel Piehl)     Have a script to show what visits are in what tracts/patches. This is especially useful for running coadd making and processing (e.g. makeCoaddTempExp, assembleCoadd) with runOrca and HTCondor, as the dataId of the jobs need to be specified. So this script's output will be in the format of a runOrca input file.     "
1,Extend SQuaSH dashboard to work with multiple datasets,"Currently SQuaSH dashboard works only with a fixed dataset. We want to ingest measurements of metrics computed by validate_drp for multiple test data e.g CFHT, DECam and HSC. In order to handle multiple datasets, we need a new model in SQuaSH  and extended the job API to ingest the measurements for different datasets. The user must be able to selected in the interface the dataset to be displayed."
3,produce a draft document of SUIT requirements,"After combing through the current SUIT requirements, we feel that we need to re-organize and re-write the SUIT requirements to be in-line with SUIT vision document.    This story will be producing the first draft of the rewrite. "
1,Problems with MemoryTest ordering,"{{MemoryTestCase}} (or a derivative thereof) must be run as the last of all tests in a module in order to properly catch leaks.    [Our documentation|https://developer.lsst.io/coding/python_testing.html#memory-and-file-descriptor-leak-testing] implies, and [SQR-012 states|https://sqr-012.lsst.io/#memory-test], that this can be achieved by listing it as the last test case in the file.    This works for py.test, but not when using plain old unittest: the latter does not, so far as I can see, guarantee any sort of ordering as a matter of principle, and, in practice, it sorts things lexicographically (it uses whatever order it gets from running {{dir()}} on the test module, and I don't *think* that's guaranteed to be anything in particular).    For example, consider [{{testAstrometrySourceSelector.py}}|https://github.com/lsst/meas_algorithms/blob/master/tests/testAstrometrySourceSelector.py]. I made the following change to introduce a memory leak:    {code}  --- a/tests/testAstrometrySourceSelector.py  +++ b/tests/testAstrometrySourceSelector.py  @@ -70,8 +70,9 @@ class TestAstrometrySourceSelector(lsst.utils.tests.TestCase):           self.sourceSelector = sourceSelector.sourceSelectorRegistry['astrometry']()         def tearDown(self):  -        del self.src  -        del self.sourceSelector  +        pass  +        #del self.src  +        #del self.sourceSelector         def testSelectSources_good(self):           for i in range(5):  {code}    Py.test catches it:  {code}  $ py.test-2.7 testAstrometrySourceSelector.py  [...]  testAstrometrySourceSelector.py .........F  [...]  {code}    But simply running the test suite does not:  {code}  $ python testAstrometrySourceSelector.py  ..........  ----------------------------------------------------------------------  Ran 10 tests in 0.105s  {code}    Rename the test case:  {code}  @@ -144,7 +145,7 @@ def setup_module(module):       lsst.utils.tests.init()      -class MyMemoryTestCase(lsst.utils.tests.MemoryTestCase):  +class xMyMemoryTestCase(lsst.utils.tests.MemoryTestCase):       pass     if __name__ == ""__main__"":  {code}    And boom:  {code}  $ python testAstrometrySourceSelector.py  .........  54 Objects leaked:  {code}    Based on a very quick check, I think [sconsUtils runs tests by simply invoking {{python}}|https://github.com/lsst/sconsUtils/blob/f9763768d999cefa4c26b9f3418c28394dfb38df/python/lsst/sconsUtils/tests.py#L133], and I'm pretty sure that this is hard-wired into the muscle memory of many developers. In these cases, memory tests written following current guidelines won't be being properly executed.    "
1,Use lsst::log in pipe_base and pipe_tasks,"Per RFC-203, switch from using pex.logging to lsst.log in pipe_base and pipe_tasks (stage 2)"
1,Remove pex_logging dependency on pipe_tasks,NULL
1,Gator / Image Vis issue,"Issues with coverage:     * Toolbar icon not showing up  * If only one point that is no coverage image (or one table with many records but has the same ra,dec values)  * CAN’T REPEAT: In expanded mode, magnifier fails when image fills the visible space entirely (seems to affect 'Coverage' image only)  * Missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view. => _move to:_ DM-7068  * CAN’T REPEAT: if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either.  * in laptop screen size, the toolbar is not fully visible, scrolling from left to right only move the background but not the expanded panel.  * CAN”T REPEAT:in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage image, can be reproduced in http://localhost:8080/firefly/demo/ffapi-highlevel-test.html (BTW, it happens in finderchart in OPS on any image in expanded mode ( ? ) )  * Readout is sometimes off the screen  * Expanded then return to normal: zoom is not adjusted correctly    If we find a way to repeat the items marked 'CAN'T REPEAT' they should go into another ticket.  Maybe in DM-7068 if it is still opened.  "
1,Match across filters -- Make color-color diagram,Add the capability to match across filters.    1. Create color-color diagrams  2. Analyze performance metrics as a function of color.
1,Add ellipticity measurement to validate_drp,"Calculate the ellipticity, and the residual ellipticity (moments - PSF).    Add to calculated SRD statistics.    This will involve thinking about things on an image-by-image basis, which is the natural and largely SRD-specified way for considering ellipticity."
0,Show the list of packages that changed from build to build linked to the git url of the latest commit,Motivated from the deviation seen from build 156 to 157 in  https://squash.lsst.codes/AM1 (caused by a commit in meas_algorithms package) we can show the list of packages that changed in the current build with respect to the previous build by comparing the git commit shas and return a list of tuples with the package name and git url.
0,Update squash to use bokeh 0.12.1,"Bokeh 0.12 was just released and some issues are being fixed, before updating the bokeh version used in SQUASH we propose to wait for 0.12.1 release.  "
1,Investigate coverage of S13 databases found so far,Look at databases located at *NCSA* so far to assess if they cover the full survey. The databases to be evaluated are mentioned in [DM-6905].    According to [S13 Testing Plan|https://dev.lsstcorp.org/trac/wiki/Summer2013/ConfigAndStackTestingPlans/Instructions] the S13 DRP dataset was split into two regions with an overalp used for cross-site verification:  * *NCSA*: -40< R.A. < +10  * *IN2P3*: +5 < R.A. < +55    Hence a goal of this task is to identify which previously located candidate databases and files correspond to either or both of these ranges.
0,Check boost.python building with Python 3,We may want to disable boost.python in the build. There are hints that there are problems with python3.5.
0,std::string construction from NULL pointer in ctrl_events,"I was browsing through ctrl_events package and found couple of instances in the headers where std::string instance is constructed from NULL pointer:  https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/Receiver.h#L87  https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/Transmitter.h#L81    I suspect that this code is never executed and those methods are overridden in subclasses because that construct will very likely crash when executed (std::string does not support construction from zero pointer, it will try to read from that pointer). Even if it's not executed it's better to change to return empty string or, if those two classes are never instantiated, make them abstract and make the methods pure virtual.  "
0,Builds should be optimised by default,"By default, our builds are not optimised ({{-O0}}), which requires everyone who doesn't want to wait until the heat death of the universe to set {{SCONSFLAGS=""opt=3""}}, but other packages that are built with scons may not recognise this.  This default is also contrary to the standard practise for open-source software, which is that by default builds are optimised.  I will change the default optimisation level to {{opt=3}} from the current {{opt=0}}.  I will also add support for {{-Og}}.    This change was approved in RFC-202."
0,Run DECam data through proccessCcd.py and imageDifference.py,NULL
0,assign initial responsibilities in LDM-151,Assign first thoughts responsibilities to all software primitives and algorithmic components.  This is my take.  John will have his own take.
0,Memory cache leak in firefly server,The visualization system is not update the memory accounting for the caching system.
1,Analyze segmentation fault in EventAppenderTest,Analyze the bug described in DM-6462
0,Big image not showing working message when the load,"This is a problem with uploads, large image loads, and Atlas.   When a big image is loading the user does  not get feedback.  The problem is the the UI is not creating the ImageViewer soon enough."
1,Firefly JavaScript API documentation to support Camera team,Convert API documentation and code examples to get Camera team started with the converted Firefly FITS viewer. 
0,Firefly distribution build,"We need to support regular Firefly distribution builds (with bundled tomcat server),  similar to the builds we did in lsst firefly repository before the conversion.    This is to get Camera team started with new API."
1,Setup standalone Firefly build using IPAC github,Modify the existing Firefly-Standalone build in Jenkins to use IPAC's github.  Make sure github auto-releases still works.
0,Update pex_exceptions to support Python 3,{{pex_exceptions}} needs to be updated to support Python 3.
1,Package an experimental Firefly widget,The aim is to package an experimental Jupyter widget with limited functionality so that it can be installed like other Jupyter widgets. Only a small set of Python and Javascript code will need to be packaged -- the widget will connect to a Firefly server. The [Jupyter widget cookiecutter|https://github.com/jupyter/widget-cookiecutter] provides a template.  
1,Add more features to JS9 Wrapper ,"Di progress on adding extra features to JS9, including load and saving regions in the local notebook server, same with files. "
1,Investigate the option to use websockets used by jupyter to explore bi-directional communication ,Di progress on understanding the possibility of using websocket locally to communicate with JS9 instances on local server. In this case we wouldn't need an external server and communication can be bi-directional. Now is only in one direction (mostly) when running Js9 locally 
1,Setup up a cluster with kubernetes,Sahand progress on installing and deploying a cluster automatically with Kubernetes. After this is completed we will use a user case example of running in cluster managed by Kubernetes/Spark
0,Port daf_base to Python 3,Changes necessary to get daf_base to work with Python 3.
1,Image Bugs noticed in the API testing,"- Image is coming with one draw layer. (I can delete this draw layer and nothing changes on the image)    - When draw layer is deleted, and no more layers are present, the layers dialog should be closed. (It stays with nothing to display, you have to click x to close it.)    - After selecting an area in one viewer, I select an area in another viewer, then move the mouse to the first one:      147 ImageViewerLayout.jsx:314 Uncaught TypeError: Cannot read property 'x' of null at ImageViewerLayout.jsx:314   Nothing works after that. I have to reload.    - Selection appears with an offset, if the page, which contains the viewer is scrolled. (Load the attached script, press 'Start selection tracking' click to select a point, then scroll page a bit down, then click to select another point - it shows down from where it should be.)    - I have 2 image viewers in separate divs. Selected line in one, then selected line in the other. The line from the first one disappeared, but its label is still there. (See attached image.)    - Selection is working differently from distance. To select in another plot, I need to press selection again. I don't need to press ruler again to select new distance in another plot.    - It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled. _move to_ DM-6473    - The payload.attValue of CHANGE_PLOT_ATTRIBUTE action is using WorldPt for area and point selections (when payload.attKey is 'SELECTION' or 'ACTIVE_POINT'), but ImagePt for line selection (when payload.attKey is 'ACTIVE_DISTANCE') How can I make them all use image coordinates?        "
1,Update xrootd from upstream,NULL
0,Assign initial responsibilities in LDM-151,Assign first thoughts on responsibilities to all software primitives and algorithmic components. This is my take. Simon will have his own take.
1,Estimate resource requirements for Software Primitives,Meet with Jim & Simon. Discuss the Software Primitives section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate.
1,Estimate resource requirements for Software Primitives,Meet with Simon & John. Discuss the Software Primitives section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate.
1,Estimate resource requirements for Algorithmic Components,Meet with Jim & Simon. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate
1,Estimate resource requirements for Algorithmic Components,Meet with Simon & John. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate
0,Port pex_policy to Python 3,Changes needed to make pex_policy work on Python 3
0,Check endianness in ndarray/numpy conversions,"As reported on [community.lsst.org|https://community.lsst.org/t/how-to-run-the-dm-stack-on-simulated-fits-images/892/9], our {{ImageF(array)}} constructor will accept arrays with non-native endianness and interpret them as native.  This probably means the array converters in ndarray aren't including byte order when checking whether a passed array's dtype matches the expected C++ type.  "
1,Setup JSDoc generation for the API portion of Firefly,"We need generate and publish JSDoc for Firefly JavaScript API, both high and low level."
1,Familiarization with Footprint redesign,"Familiarize yourself with the RFC-37 driven Footprint redesign. Start thinking about ideas for how you could implement it and what the transition plan from the current Footprints might be.    A great outcome would be to propose a set of stories which would tackle the new Footprint development effort.    A good outcome would not be to have the stories ready to go, but to be well prepared for a discussion with [~jbosch] & [~swinbank] where we'll come up with some stories as a group."
0,Stars selected by starSelector change when number of cores varies,"Sogo Mineo writes in [HSC-1414|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1414]:  {quote}  See the following lines:    meas_algorithms/HSC-4.0.0/python/lsst/meas/algorithms/objectSizeStarSelector.py:466  in ObjectSizeStarSelector.selectStars():  {code}      if psfCandidate.getWidth() == 0:          psfCandidate.setBorderWidth(self._borderWidth)          psfCandidate.setWidth(self._kernelSize + 2*self._borderWidth)          psfCandidate.setHeight(self._kernelSize + 2*self._borderWidth)  {code}    In reduceFrames, these lines set the width of psfCandidate to be 21  for the first time the execution reaches there.    When the first CCD image has been processed, the worker process  continues to process another CCD image, and the execution reaches  here again.  This time, psfCandidate.getWidth() is 41, because  psfexPsfDeterminer has set it to be 41, and the value has been  retained because the width is a static member.  And so, for the second  CCD image, the width of psfCandidate is not 21 but 41.    Since psfCandidates are widened, stars positioned at edges of images  are rejected.  It results in a smaller number of PSF candidates than expected.    Only CCD images that are initially given to the worker processes  are processed with psfCandidate.getWidth() == 21. The other CCD images are  processed with psfCandidate.getWidth() == 41.  When the number of SMP cores changes, CCD images are processed with different  parameters.    The change in the number of PSF candidates results in different Psf, a different  result of image repair, and different catalogs.  {quote}    The line numbers are different on the LSST side because of refactoring (objectSizeStarSelector.py:466 has moved to starSelector.py:148), but the bug is still present.  The main problem appears to be that the {{PsfCandidate}} elements are {{static}}, are being set in both the star selector and the PSF determiner and one of those is conditional on what the value is.  I will investigate moving the {{static}} class variables to instance variables --- the desired size appears to vary by context, so it shouldn't be a class variable."
1,Additional constraints on reference band selection for multiband,"Reference band selection currently depends on the configured band priority order, with exceptions made for sources with low signal-to-noise in the high priority bands.  [HSC-1411|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1411] points out that some additional qualifications, such as success for major measurements (e.g., CModel and Kron), would be helpful."
0,Prototype python cache for weak_ptr and weakref objects.,"There is a requirement for composite datasets that components of composites be cached and shareable, and we expect to use an object cache for this.   We have identified that we need to be able to cache C++ weak_ptr and python weakref in the same cache in an opaque way. This needs some prototype R&D."
0,Port pex_config to Python 3,Work involved in ensuring that pex_config passes all tests on Python 3 and legacy Python.
0,validate_drp is failing because it's accessing butler internals that have changed,need to change obs_decam's ingest task to use the newer class hierarchy to get the root of the butler's single repository. (longer term there should be a butler API for this or the task should get the value of root from somewhere else)
1,Move patch/tract and config mapping definitions to daf_butlerUtils,"Implement RFC-204 by adding new entries for all patch/tract and config mapping definitions to .yaml files in daf_butlerUtils, and removing any such entries that are identical to the common ones from .paf files in obs* packages.    I think the ""common"" entry can usually be defined by consensus between any two of obs_cfht, obs_decam, and obs_subaru (and frequently all three).  If there are any patch/tract or config datasets for which no two cameras agree, I think we should use obs_subaru's definitions (but I doubt there are any such cases).    Entries that are not identical to the common ones should not be removed on this issue (that should make this change entirely backwards compatible), but should be documented in new per-camera issues for later standardization."
1,LTD Keeper: Use Google Cloud Platform SQL,"Currently LTD Keeper uses a sqlite DB. This ticket will migrate that DB to Google’s Cloud Platform’s managed SQL. This solution provides automatic backups, and provides flexibility to run multiple ltd-keeper pods. Google’s SQL makes sense since LTD Keeper is run on Google Cloud Platform."
0,conda installation from the stack channel brings in astropy 1.2,If you do a conda install of lsst_sims from http://conda.lsst.codes/sims you get astropy-1.1.1.  If you do the same install from http://conda.lsst.codes/stack you get astropy-1.2.1.  This is a problem for the sims stack since the sncosmo package (on which we depend for our simulations of supernova light curves) is sensitive to which version of astropy you are running.    Was it intentional that the two channels deliver different versions of astropy?
3,Assemble a complete database with S13 DRP catalogs,Create a database populated with complete catalogs resulting from processing of the *SDSS Stripe 82* data at both NCSA and IN2P3 sites. The database has to be created at NCSA on the following database server:  * *lsst-db.ncsa.illinois.edu*    The database name will be:  * *gapon_SDRP_Stripe82*    The database will be populated with the contents of the following databases:  * *NCSA* (lsst-db.ncsa.illinois.edu):  {code}  daues_SDRP_Stripe82_ncsa  daues_SDRP_dedupe_byfilter_0  daues_SDRP_dedupe_byfilter_1  daues_SDRP_dedupe_byfilter_2  daues_SDRP_dedupe_byfilter_3  daues_SDRP_dedupe_byfilter_4  {code}  * *IN2P3* (ccdb02.in2p3.fr):  {code}  lsst_prod_DC_2013_2  lsst_prod_dedupe_byfilter_g  lsst_prod_dedupe_byfilter_i  lsst_prod_dedupe_byfilter_r  lsst_prod_dedupe_byfilter_u  lsst_prod_dedupe_byfilter_z  {code}    Additional requirements:  * The duplicate entries (due to the overlap in the RA range) will need to be carefully assessed and eliminated.  * the referential integrity of the resulted database will need to be tested  
0,Kick-off meeting,[~nlust] & [~swinbank] will meet with the SUI/T team on 2016-07-26 and discuss how we can best engage with them.
1,fix miscellaneous table issues,# disable sorting when content is html  # table options: auto-adjust all column width based on content  # table refractoring: exposing more actions to saga.  #* renamed a few actions to better reflect what it's doing.  #* added TABLE_FILTER  #* added document for sequence of actions where applicable.  # update build script to skip buildClient when possible.  # Catalog overlay should not use table id in drawing layer description. (See the attached screenshot.) It should be using MetaConst.CATALOG_OVERLAY_TYPE attribute value
1,Wrap afw::table with pybind11,"Following the same pattern as DM-6926, DM-6297, etc."
2,Complete afw port to pybind11,NULL
0,Plot sources/source density on WCS quiver plots,"We should show the individual sources (size/color scaled by S/N?) that went into the jointcal fit, and/or a density map of the sources (scaled by S/N?) under the quiver plots. This will help distinguish areas with poorly constrained fits or where the TAN-SIP function diverges, from those where the new WCS really is odd."
1,Plot old/new jointcal WCS vs. tangent plane,"To better understand the jointcal WCS vs. the original single frame TAN-SIP, we need quiver and heatmap plots of each WCS (old and new) separately vs. a tangent_pixel or related ""non-distorted"" projection. This will let us compare the original single frame fit with jointcal's fit.    This probably could be done with CameraGeom, but would be easier with the upcoming new WCS/transform system, since it may involve pulling out a new Frame."
1,"Plot ""real"" distortion by comparing with reference catalog","To compare the old WCS and jointcal's fit with the ""real"" distortion, we can use the matched reference catalog to plot a quiver diagram or an interpolated heat map showing how far each star is from its reference star. We may have to think about how to select objects for this plot, since centroiding errors would make it not so useful.    This would probably be most useful for lsstSim, since that has an infinite-precision reference catalog."
1,Support work related to PDAC effort,"This issue captures emergent work to support for example DM-6905 , for which I spent some cycles locating datasets of the 2013  SDRP, staging some files off of BW tape through globus online and unpacking to /nfs/scratch,  etc.    This effort may not fit exactly as 'emergent middleware',  but it was roughly the best fit at this time. "
0,support work for testing shared stack in NFS,"It was realized that the ""shared stack of lsstdev"" was not actually usable on the local condor pool due to /ssd usage.   In response to this,  an effort for a second shared stack on NFS  was initiated in DM-6968.  This issue captures the emergent work of pipeline testing to validate the new stack of that issue. "
1,Extend functionality of experimental Jupyter widgets for Firefly,"A package for experimental Jupyter widgets for Firefly is being developed in  https://github.com/Caltech-IPAC/firefly_widgets . Using the Firefly Javascript API for Images and Tables, add some further useful functionality for demonstration purposes."
0,Port pex_logging to Python 3,Work required to get pex_logging working on python 3. Will also include some package cleanups.
0,Break joincal's link to upstream lsst_france repo,"lsst/jointcal is still linked to the upstream repo at lsst_france. I believe all the relevant changes have been ported. It's time to break that upstream link, so that pull requests can be made in a more obvious fashion."
1,Firefly API bugs 2,"*Issues*    Gator:  * Missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view.    * The Gator Multi-object search seems having problem with the coverage image.    Atlas:    * if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either. Large drawing layers block viewer from becoming active, WFIRST footprint or WFC3/IR cause the problem.  *  -in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage- - not a bug, magnifier is disable when zoom level is above 8x     API:  *  In API, it is not possible to drag dialogs (ex. Drawing Layers).    All Firefly (found by Tatiana):    * The highlight should of catalog or coverage overlay should just change if you are close to the point. For now I am setting it to 20 pixels  * -Catalog overlay should not use table id in drawing layer description. (See the attached screenshot.) It should be using MetaConst.CATALOG_OVERLAY_TYPE attribute value.- _Moved to_ DM-7055    * After using distance tool in one plot, then the other, clicking again in the first plot does not make it active. (You have to click on the border of the plot to make it active, clicking inside does not help.)    This seems to cause strange behavior, when selections do not work as expected. For example: select ruler, select some lines alternating first and second plot. Unselect ruler, select area icon. Selecting in the first plot will still show distance. I had to delete distance tool drawing layer or click on the border for things to start showing area selection.    In general, there is some confusion with active plot, when I have two viewers. Should a plot become active as soon as mouse enters is? Otherwise in readout, compass thumbnail will still show active plot, while readout thumbnail could use another plot.  _note from trey: this is the same problem as the marker/footprint in atlas described aboved_"
0,Port daf_persistence to Python 3,Work relating to getting daf_persistence to run on python 3. Includes some code modernization.
0,Move consts from top of Associations.cc into JointcalConfig,There are three values at the top of Associations.cc under a TODO comment that should be lifted up into JointcalConfig so they can be configured at runtime. It would be good to try to add tests to check different values for them (and possibly just remove usnoMatchCut).
0,Fix Django admin interface ,"Django admin interface is useful to edit db entries in SQUASH if needed, e.g decam measurements that were incidentally pushed to the dashboard during X16.    A bug was found using the admin interface in development mode, due to a bad field returned by the Jobs model.    This ticket is to capture the fix for this bug, this new git ref will be deployed for better control of data in SQUASH database.    "
1,"visit DRP team, June 2016",Travel to Princeton June 13-17 and meet with the DRP team; work and learn about L2 processing; discuss the workflow requirements and use cases. 
1,Install ESXi on lsst-dm-mac.lsst.org,Install and configure ESXi on the Mac Pro server.
0,Install Mac OS X Mountain Lion on ESXi,"Install, configure and snapshot Mac OS X Mountain Lion. Unfortunately this is required to install any other Mac OS VM on ESXi."
0,Install Mac OS X Yosemite on ESXi,"Install, configure and snapshot Mac OS X Yosemite. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading."
0,Install Mac OS X El Capitan on ESXi,"Install, configure and snapshot Mac OS X El Capitan. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading."
0,Install MacOS Sierra on ESXi,"Install, configure and snapshot MacOS Sierra. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading."
0,Firewall and SSH configuration on ESXi,"Figure out and configure the firewall, ssh server and ssh client for ESXi.    This isn't especially well documented since it's part of VMWare vSphere.    This part specifically was time consuming since most users by vSphere."
0,Upgrade panopticon to 5.0.0-alpha4,This upgrade requires moving from Topbeat to Metricbeat which requires some minor rework and upgrading the entire system at once.
0,Doxygen isn't updating,"The current build of our Doxygen documentation, as displayed at https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/index.html, is labelled ""Generated on Mon Jun 27 2016 03:52:22 for LSSTApplications"". At time of writing, that's more than a month ago. Important additions to the documentations made during the last month are missing.  "
1,Airflow,Review [Airflow|https://github.com/apache/incubator-airflow] workflow management system against criteria defined in the epic.
0,deploy django admin interface fix,Test and deploy django admin fix from DM-7071.
0,Install MySQL and PostgreSQL servers on ccqserv124,"Time to run my incomplete L! prototype on real hardware, for that I need MySQL and PostgreSQL servers on a dedicated machine in in2p3 cluster (ccqserv124). Probably start with installing what comes with the system repos before trying latest and greatest stuff.    Both servers need to be configured to allow me create databases/tables, and I only need to enable connections from localhost.  "
0,Astropy views not available on Catalog subclasses,Somehow the {{asAstropy}} isn't being inherited by {{BaseCatalog}} subclasses in Python; it's probably getting messed up by the fact that {{Catalog}} is a template and this is added at the Swig level.
1,Image select panel not yet working correctly with coverage,The image select panel needs to be able to modify the coverage image.
0,"IrsaViewer catalog panel, labels and input fields moved as you type","Catalog search panel in IrsaViewer, the target panel label, feedback, and input box are jumping as input is being typed.  Their position should be fixed."
3,F16 Qserv Release Mgmt,"Developer work to support the monthly and end-of-cycle qserv releases.  Includes compiling release notes, updating package dependencies, updating installation docs, minor fixes in support of new compilers, etc."
0,"Develop Sphinx configuration for Pipelines Documentation, including MVP HTML/CSS Template","This ticket will kick-off a pilot implementation of pipelines documentation in Sphinx. Specific goals are    1. Develop template for sphinx-ready doc/ directories in packages (based on SQR-006)  2. Setup a MVP sphinx template that works well with numpydoc and astropy automodsumm. Simply porting astropy’s sphinx template would be pragmatic.  3. documenteer-driven configuration for sphinx.    These will be MVPs, and iterated upon in later tickets that implement sphinx API docs for stack packages."
1,Run DAX containers at NCSA,"This is an initial step to manually launch the containerized DAX services on the new PDAC cluster.  This is meant to expose container configuration, account setup, privilege, logging, debugging, etc. issues."
1,support PDAC Qserv deploy,"Support John in adapting scripts and methodology as necessary to support qserv deploy on the PDAC cluster at NCSA, as is currently done at IN2P3.  "
0,Qserv 2016_08 release,NULL
2,PDAC Qserv Deploy,"Configure cluster and adapt scripts and methodology as necessary to support qserv deploy on the PDAC cluster at NCSA, as is currently done at IN2P3.  "
1,Deliver revised slides for Joint Status Review,"Deliver a modified version of the slides from the July 2016 Joint Directors (sic) Review, plus service any other requests from Project Management."
1,Provide updated F16 DRP plan for PMCS ingest,"Only the first three months of F16 were concretely resource loaded and ingested into PMCS at the start of the cycle. A provisional plan was loaded for the remaining three months. Check, refine and update than plan as necessary."
0,Document logging framework,"Store-client: Whenever used, creates a c:/ directory and several directories underneath it to store its log files.  Sending logs to STDOUT by default and/or having a documented way to changes this would be good."
1,Allow for simple updating of project version numbers,It should be easy/simple to update the version number of all parts of the baseline for each release.
3,Bulk load: Verify successful DuraCloud ingest of 10TB of BHL content,NULL
1,JPEG2K: Image conversion service,Service wrapper and service to perform conversion of images from TIFF to JPEG2K.
1,JPEG2K: Image server service,Service wrapper and service to serve JPEG2K images.
1,JPEG2K: Image viewer service,Service wrapper and service to display JPEG2K images.
1,Messaging: Add support for all DuraStore methods,Add messaging support for all methods available through the DuraStore REST API. Define an appropriate topic heirarchy.  Example Uses: - allows for a greater degree of replication filtering - allows for replication on update/delete - allows for creation of a provenance service 
1,Replication: handle replication on update/delete ,NULL
1,Replication: bulk from existing store to new/empty store ,Handles the replication of content from an existing store/space into a new store/space. This allows for starting up a new replicated store.
2,Security: Add application security (basic auth),NULL
1,Indexing service for space/content properties/tags (Search) ,"This includes listening for changes to spaces/content and making the calls to have the information included in the search engine index. Note that there needs to be a search engine started and available for this to work.  Prior to working this task, a choice will need to be made as to the indexing tools used. The most likely candidates are AWS CloudSearch (http://aws.amazon.com/cloudsearch) and Apache Solr (http://lucene.apache.org/solr/. Various comparisons (http://harish11g.blogspot.in/2012/12/amazon-cloudsearch-vs-apache-solr.html) can be found to assist in this selection.  This task does not include updates to the UI or REST API to support users querying the index.  "
2,Integrity: check on demand (rely on DuraCloud generated hashes) ,Should allow for after-ingest checksum validation. Test case: use to validate a replicated content store.
0,Allow expected checksum value to be included when adding a content item,Update the DuraStore REST API call for storing a content item to allow for including an MD5 checksum value which is compared with the stored file's checksum to ensure that the file was received correctly.
1,Handle files larger than 5GB,"Amazon S3 limits files size to 5GB, but many of our customers will want to store files larger than this. DuraStore should facilitate this by handling the chunking of large files into smaller files during transfer. Files will need to be stitched back together on download as well. Ideally this will all be transparent to the user, even as far as multiple files appearing as one in the space listing (with the proper total file size.)  "
0,Enable Pax-logging,"As the use of the OSGi container and PAX-tooling increases, it is important that the container enables the configuration of bundle-level logging.  Resources: http://ekkescorner.wordpress.com/2009/09/03/osgi-logging-part-2-logservices-vs-classic-logger/ http://fedora-commons.org/jira/browse/FCREPO-630"
1,Production AMI continuous integration testing,A suite of automated tests should be created to run on a schedule that: - start instances of production AMIs - check the tabs/fields/buttons of duradmin - start/verify each service - verify content actions 
0,Update the development AMI,"Update the tomcat user in the development AMI to allow calling of apt-get via sudo with no password. Steps: - call 'visudo' - add this line to the end of the file: tomcat  ALL=NOPASSWD:/usr/bin/apt-get  Also, add any necessary SystemProperties for enabling servicesutil to initialize $bundle.home  "
0,Create utility for durastore & duraservice initialization,NULL
0,Initialize servicesadmin $bundle.home at runtime,"Currently $bundle.home is a buildtime configuration item. Since the deployment machine will likely have a different setup than any particular build machine, it would be useful to allow $bundle.home to be set at runtime. Note: this has been avoided in the past by building the deployed projects on the deployment machine.  Solutions may potentially be: - initialize from REST call - initialize from ~/file baked into AMI - initialize from System property (most tempting)"
1,Integrate j2k viewer with duradmin content browse,"Although the j2k viewer is currently enabled, it needs to be tied to the duradmin content browse in order to be useful for jpeg2000 content."
0,Modularize project builds,"In an attempt to isolate fundamental aspects of the baseline in terms of building and testing, this task is to create three top-level pom.xml that separate: -storage -services -infrastructure  Bamboo builds should be created to run each."
0,Cannot upload empty content,"While testing Akubra-DuraCloud (vs duracloud trunk r438), I noticed that anytime I attempt to send an empty stream to ContentStore.addContent(..), it behaves as though the request succeeded (no exception is thrown), but the content item never actually gets created in S3.  Here's what I've verified so far: 1) S3 supports uploading empty items (verified via S3Fox) 2) ContentStore supports listing, getting, and deleting empty items (tested after uploading an empty item into my test space using S3Fox) 3) Duradmin reports an error (400) when attempting to upload an empty file, but is able to successfully list, get, and delete empty files.  "
0,IA Filesystem Checksum Verifier,This utility needs to: - walk a directory of downloaded IA books - generate checksums for each file - read IA-manifest of provided checksums (<book>_files.xml) - compare generated to provided checksums
0,content browser: previous link takes the user to the first page.,NULL
0,Create EMC accounts for each pilot partner,In order to allow our partners to use EMC storage during the pilot we need to: 1. Create an EMC account for each partner 2. Add the EMC credentials to the DuraStore initialization for each partner
1,Large-file chunking in durastore,This feature should accept large streams coming in from the durastore-REST-API and chunk them into duracloud.
1,Large-file stitching in durastore,This feature should accept requests for large streams coming in from the durastore-REST-API and stitch the duracloud-chunks into a single response stream.
0,display thumbnails in content browser,NULL
0,Update Rackspace Java library,"A few issues have been resolved in the RackSpace Java library based on submitted support tickets. The most current library needs to be added, and RackspaceStorageProvider needs to be updated to account for the fixes."
1,Large-file stitching as client-side utility,NULL
0,Limit count of space items in space metadata,"The current call to get space metadata performs a count of the number of items in a space. For S3, this requires getting a list of the entire set of content and counting the included items. When there is a large number of item in the space, this can take quite a while (around 4 minutes for 250,000 objects). Rather than performing this count every time the metadata is requested for a space, the count should be capped and a value returned which indicates that there is a large number of objects. As an example, if the cap for counting is 1000 items, a value of ""1000+"" could be returned when the cap is reached.  Along with this cap, there needs to be a way to get the actual content count. This would likely work best as a client-side count, since the client would be able to update its display to indicate that the count is proceeding."
0,Out of memory error on uploading large files through duradmin.,try uploading three files > 20 MB - tomcat should throw an OOM error.
1,Convert to use SLF4J consistently for logging,"There is currently a mix of log4j, commons-logging, and slf4j in the baseline. The log4j and commons-logging dependencies (including transitive dependencies) need to be pulled out in favor of SLF4J. A good reference point is Chris Wilper's completed task doing the same thing in the Fedora baseline: http://fedora-commons.org/jira/browse/FCREPO-630"
0,Conversion service cleanup not always happening,"In the conversion service, if the call to add the converted file to storage fails, neither the converted file nor the original are removed from the work directory. This cleanup should occur regardless of whether conversion or upload succeed.  Additionally, it would be a good idea to add a retry to the upload call."
0,Service Jars should be kept out of the service work directory,"In ServicesUtil, specifically ServiceInstallerImpl.explodeAndInstall(), when a service package Zip is unpacked, any jars which are found to already be deployed are being placed in the service's work directory. Instead the jar count should just be incremented (as is done already) and the jar ignored."
0,Unable to add metadata to a content item when the metadata name includes a space,"When attempting to add metadata to a content item, if the name of the metadata includes a space, the add fails. There is no failure message, but the metadata is not added to the content metadata list.   This issue does not show up when adding metadata to a space.  Update: This issue was resolved for Amazon, but in Rackspace a similar problem still exists. On rackspace, if metadata is added to a content item in which the metadata name includes a space, it appears to work properly, but on refresh, that item will disappear."
0,Updating the MIME type of a content item to an invalid MIME value causes errors,If you update the MIME type of a content item to an invalid value (such as image*jpeg) an exception is thrown: java.lang.IllegalArgumentException: Error parsing media type 'image*jpg'. The item can then not be accessed via the REST API (and consequently the UI). There should be a validation check in DuraStore (and DurAdmin) to make sure that the value provided as a MIME type is acceptable.
0,Progress bar for uploading content items in duradmin,"right now, adding a content item "
3,Create utility to sync to DuraCloud from the filesystem,"This can be used to sync arbitrary file system directories with DuraCloud, storing new and updated files. One use case for this is direct backup of Fedora and DSpace file content to DuraCloud (archive to DuraCloud without needing to talk directly.)  See http://fedora-commons.org/confluence/x/LwAIAQ for more detail"
0,Add option to set color space in Image Conversion service,NYPL images are in proRGB format and need to be converted to sRGB as part of the process of converting TIFF images to JP2. This task is to include the option to indicate the preferred target colorspace in the call to perform the image conversion. This option will need to be included as a config item when deploying the service.
0,Ability for account-admin to add image/mark-up to duradmin home page,The goal of this feature is to allow for DurAdmin to include the branding of the organization using DuraCloud.
0,Handle access control at the DuraCloud level rather than depending on storage providers,"Access control as it exists in version 0.2 of DuraCloud is handled by using the ACL and other access mechanisms of the underlying storage providers to allow or disallow direct access to content items in those provider stores. As it has been decided that DuraCloud should always mediate content access, access control should be managed within DuraCloud. Having an Open space will allow anonymous read access to content through the DuraCloud interfaces, whereas having a Closed space will allow access only to authorized users."
1,Security: Add application support for transport security (ssl/tls),This issue is to enable the baseline to handle both http and https requests.
1,Security: Update PP AMI,NULL
1,Video Streaming Service - Amazon Impl,This service should expose video content through Amazon's Cloud-front streaming server.
0,Multi-thread the Image Conversion Service ,Update the Image Conversion Service to be able to spin up a set of threads to handle the conversion process.
0,Image Conversion Service should write output more frequently,The process for reporting image conversion results should be updated so that the output file is created and continually updated as the conversion activity progresses. This makes it is possible to review the results even while the service is running and makes results available even if the service does not complete properly.
0,Stylize 'logout' button,"With the addition of security on duradmin, a logout button has been added to the base-header.jsp. It needs to be properly formatted/stylized."
0,Create view link for items when jp2000 service is not running.,NULL
0,Display Generic filetype icon in contents view when jpg2000 service not available.,NULL
1,Implement Browser side caching for space and item metadata,NULL
1,Create content retrieval tool,This tool is the complement to the Sync Tool: A command line utility which would be used to pull content from a DuraCloud space to the local file system. Part of that activity is reconstituting any chunked files.
0,Duraservice REST-API: shorten /duraservice/services to /duraservice,NULL
0,Anonymous StoreClient,"The ContentStoreManagerImpl needs to be updated to allow anonymous users to have read access to content.  notes from skype: [12:26:26] Bill Branan:  the primary store is queried so that a ContentStoreImpl can be created with a StorageProviderType and a storeId. we could pass in StorageProviderType.UNKNOWN and storeId as null. the addStoreIdQueryParameter() method would need to check for null before adding the query param. if it's null, the param is just not included, and durastore reads that as ""use the primary store"" [12:27:42] Andrew Woods: where is your entry point? [12:28:47] Bill Branan: a new public method on ContentStoreManagerImpl, getContentStoreAsAnonymous() or something"
0,Add refresh button to flush clientside cache,NULL
0,Investigate sojo dependency,"Hello Danny, I have noticed there have been a couple of build failures due to a new dependency of duradmin that apparently is not always available from its hosting maven repository, namely, net.sf.sojo:sojo-optional:jar:0.5.0. It looks like this is a transitive dependency that was pulled in recently through: net.sf.spring-json:spring-json:jar:1.3.0.  Would you be able to do some testing to see if duradmin actually needs this dependency or not. If we do need it, then we can pull it into our own Duraspace maven repository. Since it probably is a runtime dependency with the AJAX tooling, could you remove it from your local maven repository, place an exclude element in your duradmin pom.xml, and run through the application to see if it is actually being used anywhere? ...and naturally double check you local repository to make sure it did not slip back in.  Thanks, Andrew  p.s. I am planning on putting up a new page/form in duradmin that allows for showing/adding/removing/updating security users. Once the initial page is in the baseline, could you go through and make it usable and presentable? We can talk at length later. "
0,Update EMC libraries and re-introduce to baseline,"The EMC storage provider is currently excluded from tests and use because of very inconsistent performance around the time that EMC was in the process of taking their storage service into production. They have, hopefully, addressed these problems. The client libraries for EMC need to be updated to the most recent version, our code needs to be updated to work with the latest libraries, and our the tests and use of the EMC provider needs to be re-enabled."
1,Application Initialization tool,"As the initialization process for DuraCloud becomes increasingly involved, tooling support would be helpful. This tool should: - read all configuration from a properties files for durastore, duraservice, duradmin, and security - initialize each of the three webapps - set security users on each of the three webapps"
0,REST-ful initialization of duradmin,Currently the only way to initialize duradmin is through the ui: duradmin/init.htm This improvement should allow initialization through an http call.
0,Propagate duradmin security updates to durastore & duraservice,"When updates to security users happens on the duradmin ui, those changes need to be pushed to durastore and duraservice."
1,Handle XML in a consistent way throughout the DuraCloud baseline,"DuraCloud currently uses XML in many places and there are serveral tools and methods used for validating and parsing the XML into an object model. The first step is to pull each bit of XML generation/parsing/validating code into its own class, passing back objects that can be used by the other parts of the code. This will make it easier to re-factor the XML handling into a standard method.  We should consider which tool(s) provides the best set of features and use it consistently. We should also have a standard method for validation, Relax NG should be considered here."
0,Error viewing files in DurAdmin space listing,"To replicate: Add the attached file (which includes a + in the file name) to a space. View the space using DurAdmin. The request appears to fail to get metadata for the new file, so its link is removed and replaced with: Unable to complete request: status (200)  If you click on the link before it disappears, everything appears as it should, the metadata is displayed, and the file can be viewed or downloaded.  Note that this problem appears to occur when any of these characters show up in the contentID: +,"", #, %, &  So a couple of questions here: - What's causing the failure when retrieving metadata? - Why does the link to the item disappear when the metadata can't be retrieved?  As a side note, if you select a content item to view on the space page before the metadata for all content items has loaded, all content items in the list turn into error messages. The content page loads shortly after this, so things get back to normal, but it may appear to a user that they have done something wrong by clicking on a content link."
0,Be able to embed J2K viewer in local hosted application,"Users would like to view their JPEG2000 images in J2K image server, wrapped in their own UI/portal.  Images and image server would be hosted from DuraCloud."
1,Provide users of the Image Conversion service with feedback/status regarding the time that will be required to complete a conversion task,NULL
1,Duraservice: messaging,"This feature is analogous DURACLOUD-25, except it calls for applying publishing of messaging events across duraservice methods."
1,ServicesAdmin: Clear registered services,"This improvement should provide a RESTful means of clearing the ""registered services"" from servicesadmin's cache. This would facilitate bringing duraservice and servicesadmin back into sync."
0,DurAdmin: Warning on overwrite,"When using DurAdmin, users should be notified when adding a content item will overwrite an existing file."
0,Disable unused projects,This issue should disable /duradavwebapp from the build and remove /sunstorageprovider
1,UI Facelift,NULL
1,Release AMI,This tracker item is a container for all items that should be included in the 0.4 AMI.  * redirect / to /duradmin
0,Project Version in batch scripts,"This issue is a follow-on to DURCLOUD-10 which enabled true project version numbers in the DuraCloud baseline. This issue is to enable those version numbers to be ""sourced"" from a common .bat file (ala /resources/scripts/osgi/env.sh & /services/j2kservice/run-deps.sh)"
0,License Boiler Plates,Before going open-source. the baseline needs to be updated to include the proper licensing info.
0,Refactor: StorageProviderFactory & TaskProviderFactory,"In order to be able to leverage Spring for it IoC, AOP, etc, across these classes (and a chain of classes below them that follow a similar pattern) as well as to more easily unit test them by injecting mocks, this jira-item is to refactor these classes from being comprised of static methods and then use the Spring framework to inject them into their respective *Rest.java counterparts."
0,Change Sync Tool default port,"The default port for the Sync Tool is 8080, it should be 443 to allow it to be used by customers without having to set the port to 80 or 443."
0,Provide a way to make use of Amazon's Reduced Redundancy Storage,"Amazon's Reduced Redundancy Storage provides a lower cost cloud storage solution. We should add a task which will set content in a space to use this type of storage (and get it back to standard storage.) We could potentially also add a service to allow users to do this directly, but that should be in another tracker item."
1,Create file-based credential store for unit tests,"Replace the current unit-test-db with a file-based method for storing and retrieving storage provider credentials. This new method will need to be able to handle storage files which are and which are not encrypted.   - Developers should be able to keep a local file (perhaps pointed to by maven settings) which includes all of the account credentials needed for testing. They should be able to choose to encrypt this file or not. Working with an unencrypted file is simpler for local development. Using an encrypted file will allow for more secure storage and for simpler sharing among developers. (The key needed to enc/dec could be stored in maven settings as it is now for the db.) - We may choose to pass an encryped credentials file into bamboo in order to be able to update the test accounts (and keys) without needing to rebuild the AMI.  It seems likely that we can reuse the code in app-config to actually read the credentials file, assuming the same format as an app-config file. We should also be able to use the EncryptionUtil in common to handle the encryption/decryption (in combination with Commons-IO FileUtils.readFileToString()). There should be an assembly which provides an easy CLI to encrypt and decrypt a file."
1,Transition to using S3 Reduced Redundancy Storage for all pilot S3 storage,Transition all current S3 objects to use RRS. Update the S3StorageProvider to use RRS as the default. One way to do this would be to add a task which sets a flag within the S3StorageProvider indicating the preferred storage setting for S3.
0,Add config element for duraservice service-registry store credentials,This item should add a new config element to duraservice for the username/password needed to get services from the dcprod service-registry store. Currently these credentials are coming from the hijacked config elements: duraservice.service-compute.username duraservice.service-compute.password 
1,500 response from durastore: Too many open files,"After uploading some 1-GB files to the WGBH release-0.3 instance, durastore would only respond with a 500 exception. The tomcat log files (attached) indicate ""java.net.SocketException: Too many open files"" The lsof.out shows an excess of open socket connections to the instance itself. It may be related to the jets3t library (?)."
0,Error retrieving content stores,"I'm getting this error when trying to access my spaces through the duradmin (accessing files through the durastore appears unaffected)  An unexpected error occurred... Error retrieving content stores. Response code was 401, expected value was 200. Response Body: <html><head><title>Apache Tomcat/6.0.18 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:16px;} H3 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:14px;} BODY {font-family:Tahoma,Arial,sans-serif;color:black;background-color:white;} B {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;} P {font-family:Tahoma,Arial,sans-serif;background:white;color:black;font-size:12px;}A {color : black;}A.name {color : black;}HR {color : #525D76;}--></style> </head><body><h1>HTTP Status 401 - Bad credentials</h1><HR size=""1"" noshade=""noshade""><p><b>type</b> Status report</p><p><b>message</b> <u>Bad credentials</u></p><p><b>description</b> <u>This request requires HTTP authentication (Bad credentials).</u></p><HR size=""1"" noshade=""noshade""><h3>Apache Tomcat/6.0.18</h3></body></html>Error retrieving content stores. Response code was 401, expected value was 200. Response Body: <html><head><title>Apache Tomcat/6.0.18 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:16px;} H3 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:14px;} BODY {font-family:Tahoma,Arial,sans-serif;color:black;background-color:white;} B {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;} P {font-family:Tahoma,Arial,sans-serif;background:white;color:black;font-size:12px;}A {color : black;}A.name {color : black;}HR {color : #525D76;}--></style> </head><body><h1>HTTP Status 401 - Bad credentials</h1><HR size=""1"" noshade=""noshade""><p><b>type</b> Status report</p><p><b>message</b> <u>Bad credentials</u></p><p><b>description</b> <u>This request requires HTTP authentication (Bad credentials).</u></p><HR size=""1"" noshade=""noshade""><h3>Apache Tomcat/6.0.18</h3></body></html>"
0,sync tool writes log files into working directory,"The sync tools writes a duracloud-common.log file into the current working directory (possibly only an issue on non-windows platforms), writing to the relative file path ""./c:/work/programs/duracloud/logs/duracloud-common.log"", which causes some errors in the log files:  Caused by: org.duracloud.error.ContentStoreException: Error attempting to add content 'c:/work/programs/duracloud/logs/duracloud-common.log' in 'aapp-preservation' due to: Response code was 500, expected value was 201. Response body value: Error attempting to add content 'c:/work/programs/duracloud/logs/duracloud-common.log' in 'aapp-preservation' due to: Content c:/work/programs/duracloud/logs/duracloud-common.log was added to space aapp-preservation but the checksum, either provided or computed enroute, (c2e207b0a44d27e4337d16f93970f1e4) does not match the checksum computed by the storage provider (4f074bb6537ddb42a54f33b401bbd146). This content should be checked or retransmitted.   org.duracloud.client.ContentStoreImpl.addContent(ContentStoreImpl.java:394)   org.duracloud.chunk.writer.DuracloudContentWriter.addContent(DuracloudContentWriter.java:182) 	... 11 more Caused by: org.duracloud.error.ContentStoreException: Response code was 500, expected value was 201. Response body value: Error attempting to add content 'c:/work/programs/duracloud/logs/duracloud-common.log' in 'aapp-preservation' due to: Content c:/work/programs/duracloud/logs/duracloud-common.log was added to space aapp-preservation but the checksum, either provided or computed enroute, (c2e207b0a44d27e4337d16f93970f1e4) does not match the checksum computed by the storage provider (4f074bb6537ddb42a54f33b401bbd146). This content should be checked or retransmitted.   org.duracloud.client.ContentStoreImpl.checkResponse(ContentStoreImpl.java:512)   org.duracloud.client.ContentStoreImpl.addContent(ContentStoreImpl.java:380) 	... 12 more INFO           2010/06/24 16:43:01 [pool-2-thread-3] (SyncWorker.java:61) [retryOnFailure()] - Adding /Volumes/MLAMellonDigLib4/barcode101640/./c:/work/programs/duracloud/logs/duracloud-common.log back to the changed list, another attempt will be made to sync file.  ====  md-20013004:barcode101640 chris_beer$ java -jar ~/Downloads/synctool-0.3-driver.jar -d . -f 2000 -h wgbh.duracloud.org -p 443 -s aapp-preservation -t 4 -u admin -w changeme -b ..  Starting up the Sync Tool ..................... Startup Complete  --------------------------------------  Sync Tool Configuration -------------------------------------- Sync Directories:   /Volumes/MLAMellonDigLib4/barcode101640/. DuraStore Host: wgbh.duracloud.org DuraStore Port: 443 DuraStore Username: admin DuraCloud Space ID: aapp-preservation SyncTool Backup Directory: .. SyncTool Poll Frequency: 2000 SyncTool Threads: 4 SyncTool Max File Size: 1073741824 bytes SyncTool Syncing Deletes: false --------------------------------------   --------------------------------------  Sync Tool Help -------------------------------------- The following commands are available: x - Exits the Sync Tool c - Prints the Sync Tool configuration s - Prints the Sync Tool status l <Level> - Changes the log level to <Level> (may be any of DEBUG, INFO, WARN, ERROR) Location of logs: /Volumes/MLAMellonDigLib4/barcode101640/../logs --------------------------------------  s   --------------------------------------  Sync Tool Status -------------------------------------- Start Time: 2010-06-24 16:23:07.929 Sync Queue Size: 1 Syncs In Process: 2 Successful Syncs: 30 Failed Syncs: 3   /Volumes/MLAMellonDigLib4/barcode101640/./c:/work/programs/duracloud/logs/duracloud-common.log   /Volumes/MLAMellonDigLib4/barcode101640/./c:/work/programs/duracloud/logs/duracloud-common.log   /Volumes/MLAMellonDigLib4/barcode101640/./c:/work/programs/duracloud/logs/duracloud-common.log --------------------------------------  quit  md-20013004:barcode101640 chris_beer$  tail c\:/work/programs/duracloud/logs/duracloud-common.log  /logs/duracloud-common.log back to the changed list, another attempt will be made to sync file. Caused by: org.duracloud.error.ContentStoreException: Error attempting to add content 'c:/work/programs/duracloud/logs/duracloud-common.log' in 'aapp-preservation' due to: Response code was 500, expected value was 201. Response body value: Error attempting to add content 'c:/work/programs/duracloud/logs/duracloud-common.log' in 'aapp-preservation' due to: Content c:/work/programs/duracloud/logs/duracloud-common.log was added to space aapp-preservation but the checksum, either provided or computed enroute, (c2e207b0a44d27e4337d16f93970f1e4) does not match the checksum computed by the storage provider (4f074bb6537ddb42a54f33b401bbd146). This content should be checked or retransmitted.   org.duracloud.client.ContentStoreImpl.addContent(ContentStoreImpl.java:394)   org.duracloud.chunk.writer.DuracloudContentWriter.addContent(DuracloudContentWriter.java:182) 	... 11 more Caused by: org.duracloud.error.ContentStoreException: Response code was 500, expected value was 201. Response body value: Error attempting to add content 'c:/work/programs/duracloud/logs/duracloud-common.log' in 'aapp-preservation' due to: Content c:/work/programs/duracloud/logs/duracloud-common.log was added to space aapp-preservation but the checksum, either provided or computed enroute, (c2e207b0a44d27e4337d16f93970f1e4) does not match the checksum computed by the storage provider (4f074bb6537ddb42a54f33b401bbd146). This content should be checked or retransmitted.   org.duracloud.client.ContentStoreImpl.checkResponse(ContentStoreImpl.java:512)   org.duracloud.client.ContentStoreImpl.addContent(ContentStoreImpl.java:380) 	... 12 more INFO           2010/06/24 16:43:01 [pool-2-thread-3] (SyncWorker.java:61) [retryOnFailure()] - Adding /Volumes/MLAMellonDigLib4/barcode101640/./c:/work/programs/duracloud/logs/duracloud-common.log back to the changed list, another attempt will be made to sync file. "
0,non-interactive content transfer tool,"as a complement to the sync tool, i'd love to see a non-interactive, one-off content transfer tool (akin to `scp` or `rsync`, either per-directory or per-file) that could be easily integrated into existing workflows, e.g.:   $  java -jar duracloud-transfer.jar --file $i -h wgbh.duracloud.org -p 443 -s aapp-preservation -u admin -w changeme  ==> transfers file $i into the aapp-preservation space    "
0,errors loading duradmin spaces,"After syncing 500+ large files, I'm getting some weird intermittent errors in the duradmin when trying to verify the files are there.. The answer is probably ""don't do that"", but reporting it anyway..  For the url https://wgbh.duracloud.org/duradmin/contents.htm?spaceId=aapp-preservation  1) see attached screenshot 2) after refreshing from (1), I the attached stack trace 3) eventually, I get this application error: org.duracloud.duradmin.util.DataRetrievalException: org.duracloud.error.ContentStoreException: Error attempting to get space 'aapp-preservation' due to: Too many open files java.lang.RuntimeException: org.duracloud.duradmin.util.DataRetrievalException: org.duracloud.error.ContentStoreException: Error attempting to get space 'aapp-preservation' due to: Too many open files at org.duracloud.duradmin.contentstore.ContentItemList.getSpace(ContentItemList.java:54) at org.duracloud.duradmin.control.ContentsController.handle(ContentsController.java:71) at org.springframework.web.servlet.mvc.AbstractCommandController.handleRequestInternal(AbstractCommandController.java:84) at [...]     "
0,Break out OSGI tests into a new project,"Move all of the existing service tests which spin up an OSGI container into a new project //services/osgi-test. These tests should run as normal when using ""mvn clean install"", but should be skipped when the -DskipIntTests or -PskipIntTests flags are used."
0,DuraStore REST-API addition: spaceExists & contentExists,"This issue is to add two additional methods to the durastore REST-api: - spaceExists(spaceId) - contentExists(spaceId, contentId)"
1,Trade out jets3t libraries under S3StorageProvider for Amazon-supported libraries,Use the Amazon libraries (http://aws.amazon.com/sdkforjava/) for connecting to S3. They will also be used for connecting to EC2.
0,Set higher file handle limits in DuraCloud AMIs,WGBH has seen issues on their instance due to the tomcat process running out of file handles. To get around this the changes listed at the bottom of the AMI creation wiki page (https://wiki.duraspace.org/display/neworg/DuraCloud+AMI+Setup#DuraCloudAMISetup-AdditionalNotes) were added. These changes need to be included in the DuraCloud AMIs.
0,Sync Tool run to completion,"Add an option to the Sync Tool which would allow it to run until its work queue is empty, then exit. This would be useful for administrators who would like to use the Sync Tool as part of a cron job to sync all of the day's updates at one time rather than have the Sync Tool run continually."
0,Sync Tool retry failed transfers,"Update the Sync Tool to write failed transfers to a file in the backup directory, then add an option to retry those failed transfers on startup. This would need to be a one time retry (otherwise the risk of a loop is too high) which pulls the contents on the failed transfers file into the work queue, then clears that file; any new failures would be written to the file again."
0,Sync Tool start from scratch option,Add an option to the Sync Tool which would clear out its status logs (or just ignore them) in order to start from scratch (attempt to sync all files) rather than restarting. This would remove the need to ever manually clear the backup directory unless wanting to remove log files.
0,change icon to something more appropriate on deploy/undeploy service.,not sure if the lock/unlock icons used for open/closed spaces really applies to service deploy/undeploy (0.6) 
0,"After logging in, redirect the user to the requested URL",NULL
0,provide next and previous  links for navigating content items.,NULL
0,Fix Duradmin UI integration tests.,NULL
3,Update the Image Conversion service to make use of Amazon's Elastic MapReduce service,"In order to handle processing the volume of data being presented by our pilot partners, DuraCloud needs to be able to distribute compute work onto multiple machines.   It appears that the most standardized way of doing this distribution is to make use of Apache Hadoop. Since Amazon provides the Elastic MapReduce service, which makes use of Hadoop, it makes sense to investigate that as the initial step towards being able to take advantage of scalable processing in the cloud. We already have data to convert (NYPL tiff images) and an existing Image Conversion service, this task is to allow the image conversion to run via MapReduce."
0,Full URL of streaming content,"This improvement is to expose the full URL, including the streaming host server, of content that is streamed via the mediastreaming service. Various stream viewers provide the ability to paste in such a URL for testing."
0,Navigating away from an upload in progress causes the upload to fail.,"The quick fix is to provide a warning to the user.  The better solution is probably to make the spaces,services,admin,and dashboard pages all driven off the same ""page"" and making the various sections (spaces,services,etc) addressable after the hashmark."
0,"The uploads dialog gets a little messy when you have >4 files that have been uploaded (close button sits over other stuff, can't see the bottom of the scroll bar, etc)","# The uploads dialog gets a little messy when you have >4 files that have been uploaded (close button sits over other stuff, can't see the bottom of the scroll bar, etc)"
0,"Mark  image files when the space is closed, indicating that the JP2 viewer is only available for open spaces","It would be really useful to have a notice box on image files when the space is closed, indicating that the JP2 viewer is only available for open spaces (then do viewing as if the JP2 service were not deployed)."
0,Make available services more explicit.,"on the services tab, you have to click ""deploy a new service"" to view a list of other available services not currently running. it would be nice if that information was somehow presented on the immediate service page, as it wasn't immediately apparent that there were other services that could be run."
0,DurAdmin: Add Content Item button is available prior to selecting a space,"The ""Add Content Item"" button on the spaces tab of DurAdmin can be selected without choosing a space. The add content dialog box opens and allows you to interact as usual, but when you click the Add button nothing happens. The ""Add Content Item"" button should likely be grayed out until a space is selected."
2,New StorageProvider: MS-Azure,This feature should create a StorageProvider implementation ala   EMCStorageProvider.java   RackspaceStorageProvider.java   S3StorageProvider.java  for MicroSoft's Azure storage offering. http://research.microsoft.com/azure 
2,Server Health and Status Monitoring,"This is a top-level feature that will span from the UI dashboard to the backend status. Exactly what functionality is expected in this feature needs further investigation. Examples may include:  usage - storage requests, bandwidth consumption, uploads, downloads, etc  storage breakdowns - by space, mimetype, storage provider, etc  cost estimating  see the following page for further ideas: https://wiki.duraspace.org/display/DSPINT/DurAdmin+Dashboard"
0,Persisting User Passwords,This improvement should persist system and user credentials to enable the same ones to be used across migrations to a new DuraCloud instance.
0,Service configuration: Exclusion groups,The task is to update the service-config schemas to support the notion of exclusion groups during deployment and configuration of services. This allows the UI to dynamically display the appropriate input elements based on initial selections made by the user.  See DURACLOUD-215
0,Bulk Delete Space,NULL
0,Bulk Delete Content,NULL
0,Bulk Update Content Mime Type,NULL
0,Service Exclusion Groups - UI Modifications,UI changes to support DURACLOUD-207
0,Chrome & IE mis-behavior,"On Wed, Aug 4, 2010 at 12:21 PM, Bryan Beecher <bryan@umich.edu> wrote:     OK, this is really weird.     Chrome throws a tomcat error.     IE hangs.     Firefox works A-OK.     I'll use Firefox. :-)          -- bryan   On Wed, Aug 4, 2010 at 12:18 PM, Bryan Beecher <bryan@umich.edu> wrote:         Hey, Carissa, I logged in, changed the password, and after hitting submit, got a page with this:         {""user"":{""accountNonExpired"":true,""accountNonLocked"":true,""credentialsNonExpired"":true,""enabled"":true,""grantedAuthorities"":[""ROLE_ADMIN"",""ROLE_USER""],""password"":""[omitted]"",""username"":""admin""}}          which contains the new password.  However, whenever I navigate to the home page (URL below), I get a 500 response from tomcat, and a long stack trace (which I can send it if it helpful).         If I cut-n-paste the URL into a different browser, I do get the login screen, but it just hangs when I try to authenticate.              -- bryan"
0,Ability to view the actual number of items in a space when the count exceeds current limit (1000 now),NULL
0,"In Service UI: Config : Exclusions - clarify relationship to a ""mode"" and the user configs that go with it.",NULL
1,Service Report Dashboard,"This task is to create a UI dashboard for displaying and downloading reports on service jobs. Information about both running services and completed services will need to be displayed.  REST API calls should be available from which the service report information can be gathered. A UI will need to be created to be able to display the information in the reports.  The reports will likely be in XML, so XSLT is an option for display."
0,"On session timeout, make sure javascript forces the user to login (instead of failing ungracefully)",NULL
0,Service configuration: schema update for nested modes,This improvement is to update the service-types.xsd to define serviceType as its current sequence of config elements as well as a modeType that itself contains another modeType and the sequence of config elements.
1,Service Report: backend utility for completed services,"This feature provides the backend support for providing service summary details for services that have completed. 'Completed' is defined as either coming to a state of 'success' or 'failure', or in the case of services that just stay running (e.g. Image-Server) coming to the state 'started'. Service summaries should be collected as soon as the service completes and be persisted to a single file containing all services. This single report file will start fresh on a monthly basis."
0,DurAdmin - enter key does not properly submit content detail edit,"When editing the mime-type of a content item in DurAdmin (using the ""edit content item"" dialog), using the enter key to submit does not save the value which was input and returns the user to the main spaces view with no space selected. Using the Save button works just fine. Using the enter key should perform the same function as clicking Save."
0,DurAdmin - next and previous buttons for content list are displayed even when not necessary,The recently added next and previous buttons to allow cycling through large sets of content items seem to show up even where are only a few content items in a space. Those buttons should only be available when there are more content items than can be displayed in the scroll window.
0,"DurAdmin - ""Login"" button on login screen does not respond to Enter key on Chrome","When using the Google Chrome browser and attempting to log in via DurAdmin, the Login button is not triggered by hitting the Enter key. Clicking on the Login button works fine. In Firefox and IE both clicking on the button and hitting the Enter key work fine."
0,Cannot display Spaces in DuraAdmin UI,"After logging into the DuraAdmin web app I can navigate to most tabs successfully, but when selecting the Spaces tab, I get a pop-up that says only ""error:undefined"".  And the sync tool also exhibits a problem trying to access Spaces:  Starting up the Sync Tool ...Exception in thread ""main"" java.lang.RuntimeException: Could not connect to space with ID 'synctest4'.         at org.duracloud.sync.endpoint.DuraStoreSyncEndpoint.ensureSpaceExists(DuraStoreSyncEndpoint.java:82)         at org.duracloud.sync.endpoint.DuraStoreSyncEndpoint.<init>(DuraStoreSyncEndpoint.java:69)         at org.duracloud.sync.endpoint.DuraStoreChunkSyncEndpoint.<init>(DuraStoreChunkSyncEndpoint.java:33)         at org.duracloud.sync.SyncTool.startSyncManager(SyncTool.java:89)         at org.duracloud.sync.SyncTool.runSyncTool(SyncTool.java:185)         at org.duracloud.sync.SyncTool.main(SyncTool.java:240) "
0,Content does not complete if list size >= 1000,"When the number of content items within a space exceeds 1000 and the ""maxResults"" parameter is set to 1000 or greater (1000 is the default), then ContentIterator.java eventually terminates early. This is because the underlying query to the storage provider is maxResults+1 to account for the possibility that the space metadata content object may be in the listing. Since the S3 client will only return a max of 1000 items regardless of the maxResults parameter, the call with 1001 effectively turns into a call for 1000 items. The logic in ContentIterator is terminating if the number of items in the listing returned is less than the maxResults parameter, which is the case when 1000 items are returned and one is removed (i.e. the space metadata object)."
0,Exception thrown on duradmin initialization,"The attached stacktrace has begun appearing in the catalina.out log when running the app-config initialization tool. By downloading various revisions of the baseline, I was able to associate the origins of the stacktrace to svn r[47], on Aug 2nd. http://groups.google.com/group/duracloud-codewatch/browse_thread/thread/4dcd83462d243272?hl=en#  I observe the stacktrace when running   mvn clean install -DskipTests  from the top of the baseline, followed by   java -jar app-config/target/app-config-0.5.0-SNAPSHOT-driver.jar localhost.properties  It is unclear to me if the mods to   trunk/duradmin/src/main/webapp/WEB-INF/config/tiles.xml   trunk/duradmin/src/main/webapp/WEB-INF/jsp/error.jsp  cause the error, or just allow it to be visible in the catalina.out log.   As a note, I also get the same result when running the following curl command:    curl -d duradmin-init.xml -u un:pw http://localhost:8080/duradmin/init"
1,Allow users to select S3 storage class,As of release 0.6: - All content added to S3 is set to the reduced redundancy storage class - There are tasks available which will transition all content in a given space to a particular storage class (either standard or reduced redundancy)  This task is to: - Provide a way for the user to select which storage class they would prefer to be set when new content is added. This selection would be for all content in an account.  Later tasks will need to handle: - Provide selection of storage class at the space level - Provide a way for users to transition their existing content to their preferred storage class. This will likely be a service which leverages the existing tasks.
0,REST-API: Create top-level exception barrier,"This improvement is to create a top-level exception barrier for the durastore and duraservice REST-APIs. Currently, unexpected exceptions appear in the client response as either a stacktrace or a tomcat html error document."
0,Add support for CloudFront invalidation,"Since the media streaming service was created, Amazon has added a feature called Invalidation, which provides an API call to remove content from CloudFront. The media streaming service should be updated to make use of this call when stopping the service, so content will no longer be available for streaming when the service has been turned off."
1,Metadata name prefix mismatch between durastore & duradmin,"The duradmin metadata prefix is an extension of the durastore metadata prefix durastore: x-dura-meta- duradmin: x-dura-meta-ext-  Therefore, metadata that is created through the durastore REST-API may not contain the full prefix that duradmin is expecting. These two prefixes need to be made consistent, and the logic should probably be pushed down from duradmin into storeclient."
1,Test Infrastructure: mocking integration tests,This improvement is to allow developers to run the test suites more often by reducing the amount of time it takes to run a build. There are many integration tests that unnecessarily go all the way to the underlying storageproviders. These tests should be revisited with an eye towards mocking out the network calls where they are overly redundant.
0,Test Infrastructure: reinstate bamboo,NULL
0,DurAdmin: Uploaded file can be displayed in the wrong space,"When uploading a file, when the upload completes, the content Id of the uploaded file is added to the list of items in the currently viewed space, even if that is not the space where the item has been stored. Clicking on the content Id then brings up a ""get item failed: error"" dialog. Clicking on the either space (the one being viewed or the one where the upload landed) allows them to refresh and show the correct listing."
0,DurAdmin: Images in closed spaces should be displayed as if no J2K service exists,"Re: DURACLOUD-196, when a space is closed, the View link still attempts to open it in a J2K viewer. Along with the current notice (and link to open the space), the images should act like the J2K service is not deployed until the space is opened. "
0,Fedora Integration: Basic-Auth for externally referenced datastreams,This feature is the DuraCloud marker for the Fedora issue: https://jira.duraspace.org/browse/FCREPO-748
1,Fixity Service: Amazon Elastic Map Reduce version,NULL
1,Bit Integrity Service: update UI based on new service-config schema,NULL
0,Incorrect redirects from http to https,"Saw this while testing with Poster:  When performing a PUT to add a new content item, if everything is right, except you use http rather than https in the URL you end up with a 404 response.  Using httpfox, I can see that what happens is that a 301 is sent back and the original request is forwarded to the https address, but as a GET rather than a PUT. So the 404 is the result of attempting a GET on a content item which does not exist.  It would appear that the HTTP verb is lost in the redirect."
0,"On some machines, not able to login to Duradmin","Brian Beecher and Andrew have seen occasional problems logging in.  The symptom:  user tries logging in with correct credentials - no failure, no success.  Brian is using Chrome - doesn't see the problem in ""In cognito"" mode.  Seems to be a caching issue."
0,Use Amazon java client for Cloudfront,"DURACLOUD-178 transitioned the majority of the interactions over to using the Amazon-provided java client. Unfortunately, when that work was done, that client did not yet support Cloudfront, so access to Cloudfront continued to be through Jets3t. Once the Amazon library is updated to include Cloudfront, this task is to complete the transition and remove Jets3t from the S3StorageProvider dependencies."
0,Create 0.7 AMI,This production AMI should  1. contain removal of $BUNDLE_HOME/container & /work & /attic on shutdown in the /etc/init.d/duraspace script 2. update /etc/apache2/httpd.conf to include the following line:   RewriteLock /var/lock/apache2/apache-rewrite.lock         {new}   RewriteMap local-ip-map prg:/etc/apache2/bin/local-ip.sh {already present} 3. touch: /var/lock/apache2/apache-rewrite.lock
0,Login failure on some browsers/environments,"Firefox 3.6.10 Chrome 6.0.472.62 Opera 10.62-6438  Occasionally on Firefox and Chrome the first time a username/password is provided and the 'enter' key hit, the fields just clear. But immediately re-inserting the values has always succeeded."
0,Allow storeclient to handle redirects,"Currently, if the storeclient is initialized to use port 80 to connect to durastore, it will fail when making calls because the responses are 301s which redirect to port 443. This task is to allow the storeclient to retry using the redirect path rather than failing.  This update would likely benefit the serviceclient as well."
1,Ensure integrity of input content to hadoop services,This improvement is to revisit the hadoop services: - fixity - bulk image conversion - replication on demand  and ensure the content pulled from S3 is valid with respect to its stored MD5 value.
1,Notification on service completion.,"This task is to provide a notification to the user running a service when that service completes. The notification should include (a link to) the final output report, if one was generated by the service.  Original description: Bryan Beecher suggested that adding some sort of pop-up, email message, or other indicator might be a good idea once the fixity service has finished the job that it is running. It isn't clear how to tell when it is finished beyond polling the Services page."
1,Add a UI to the retrieval utility,NULL
0,"UI Pagination after ""delete all""","The scenario is revealed when wanting to clear the entire contents of a space or the first few hundred items. 1. have a space with many items (+1000, this is commonly the case with the log output from hadoop jobs) 2. select the space and select the check-box that marks all of the items shown on the current page 3. select ""Delete Selected Content Items"" 4. notice all items are deleted 5. select the ""next"" button 6. notice the next page of items is not displayed 7. refresh space (by hitting the space name) and notice the next page of items displays"
0,UI Message when creating space that already exists,"When creating a space that already exists, the following pop-up displays the error: ""error: space is undefined""  Normally I can determine that the error must relate to the fact that the space already exists.  But sometimes in creating a new space, the action fails and the space is not listed on the left-hand side listing of spaces. But despite the failure, the space is actually created (although not shown).  So trying to create it again shows the ""error: space is undefined"" message which does not make it very easy to determine the actual error. The suggestion is to render a more specific error message."
0,Metadata add button label is removed on focus,"When adding metadata to either a space or content, the add metadata button label (""+"") is removed as soon as the button gets focus. This follows the [name] and [value] text being removed from the metadata text fields, but it should not happen on the button."
1,Handle filenames which include spaces,"This issue is to review the current set of DuraCloud services and utilities (including the sync and retrieval tools) to determine which ones do not currently support working with content items which include a space ("" "") character in their content IDs. After the list is identified, each service and utility should be updated to handle this case."
0,DuraAdmin - View images in closed space in the J2K Viewer,Remove the notice that the space needs to be open in order to view the image in the J2K viewer.  Change the link to view closed images in J2K viewer.  Also change viewer url param from http to https.  Make sure to release with issue DURACLOUD-138
0,Update sync tool command line flags to match retrieval tool flags,The following flags need to be updated in the sync tool to match the retrieval tool: 1. password: change to -p 2. port: change to -r 3. backup (work) directory: change to -w 4. sync (content) directory: change to -c 5. sync deletes: change to -d 6. exit on completion: change to -x 7. add: store id: -i
0,Update retrieval tool to skip space dir creation on single space retrieval,"It is not necessary for the retrieval tool to create a directory for the space being retrieved when retrieval is only for a single space. This task is to update the retrieval tool such that when only one space is being retrieved, the files go directly into the content directory. When multiple spaces are retrieved, space directories are created, as is the case currently."
0,"Selecting a single space for deletion displays ""delete multiple spaces?"" warning","In DurAdmin, if you select a single space for deletion, on the far right you'll see a ""Multiple spaces selected"" message.   After clicking ""Delete Selected Spaces"", you'll also get a warning that says ""Are you sure you want to delete multiple spaces?"".  Obviously, this is a little confusing, as it causes you to have to double check that you aren't accidentally about to delete multiple spaces."
1,Bulk Image Transformation failures,"The Bulk Image Transformation service has failed in numerous tests which has been over a set of TIFF images files supplied by Rhodes. This set includes 317 images, ranging in size from 2.35 MB to 265.42 MB. The primary test case has been converting to JP2 format using the source color space. The service has failed when running with 1 instance of each instance size. The service has also failed in a test running an xlarge instance with only 2 mappers.  Watching the memory consumption on a small instance while the service is running shows that a significant amount of memory is consumed by the mogrify step (which is run using a bash script to call ImageMagick) - all swap space was consumed, and less than 10% of memory remained available.  Testing has indicated that the size of images being transformed likely plays a role in the failures. Moving the two largest Rhodes files to another space (all of the rest of the tiffs being under 115 MB) allowed the transformation to complete successfully using 3 xlarge instances. Running the conversion over the same set with only 1 instance (regardless of size) still failed, however. The attached file shows the list of tiff files which were successfully transformed in these tests, as well as the files on which the tests failed. The large instance was run with two mappers, and the x-large instance with 4 mappers."
0,Duraservice: Resolve spaces for all providers,"This improvement is to have ServiceManager.java resolve the spaces for all providers when a service config object specifies a ""ALL_STORE_SPACES"" mode."
1,Bit Integrity Service: update service config for provider spaces,This improvement is to change the Bit Integrity Service to leverage the 'mode' schema element to specify the need for resolving spaces for all providers.
0,Duplicate on Upload: ensure metadata is included,"This improvement should ensure that content that is replicated via the ""Duplicate on Ingest"" service should also duplicate the user-supplied metadata."
0,Integrate Droid for Mimetype detection,NULL
1,Bottleneck in hadoop processing,"Two runs of the Bulk Bit Integrity service over the same 1TB set of BHL content took approximately the same amount of time (a few minutes over 8 hours.) The problem is that the first run used 10 small instances and the second run used 10 x-large instances. The x-large instance should have completed the task much faster, not only because they have higher bandwidth and compute capacities, but also because they are set to use 8 mappers per instance rather than the small instance setting of 2.   This task is to discover and fix the bottleneck which is not allowing the x-large instances to run at full capacity."
0,Allow system services to be deployed along with services which depend on them,"This task will remove the system services (imagemagick, webapp util) from the list of deploy-able services and instead have them be deployed as needed when dependent services are deployed. Likely this will mean bundling the jars for these services into the services for which they are a dependency, and updating them to start when they are deployed into OSGI."
0,Odd characters seemingly replace copyright symbol -- UTF8 issue?,"At the bottom of the DurAdmin UI it says, ""ï¿½2010 DuraSpace.org""."
0,MS-AZURE Space item count is off by one,The number of items listed in an empty AZURE space is -1. The number of items listed in an AZURE space with one content item is 0. etc...
0,Retrieval Tool to retrieve list of specific files,"I think the retrieval tool is missing an option where you can retrieve specific file(s) from a specific space.  So, in the DSpace use case, when doing a partial restore, you may wish to retrieve only a portion of the files from the space (rather than having to pull down all of them).  I would like to be able to request specific files by name (or by a comma separated list of names).  A regular expression could also work, but a list of exact names would seemingly meet the needs for DSpace. So, for DSpace, the AIP files stored in DuraCloud will have very specific names, similar to:  COLLECTION@123456789-10.zip ITEM@123456789-11.zip  Generally the AIP name is a specific format: <DSpace-Object-Type>@<DSpace-Object-identifier>.zip  So, if a DSpace install wanted to restore just a particular object (or small set of objects), it might be nice if they can request *just* those objects by name (rather than having to pull down their entire space).  I just know this is a specific case that's currently missing -- we either need a simple solution from the Retrieval tool, or we need a DSpace-specific tool to support partial content restores (where a user only wants to restore a particular set of items or a single collection)."
0,Create 64-bit dev AMI,NULL
0,Improve performance for retrieve content metadata in AzureStorageProvider,Move the generation of content checksum from getContentMetadata to addContent.  Also waiting for feedback from the Azure team to find out why they do not provide the md5 in the eTag.
0,Create 0.7.1 release,AMI: ami-24788f4d Docs and artifacts have been created and pushed to dcprod and atlas per: https://wiki.duraspace.org/display/DSPINT/Release-Related+Scripts
0,"When using the previous and next buttons in the DuraCloud Administrator interface, the ""Space Detail"" section on the right disappears.","When navigating through the list of content items for a space using the previous and next buttons in the DuraCloud Administrator interface, the ""Space Detail"" section on the right disappears and only reappears once you re-select the space or click on an individual item."
0,Bit Integrity Service: split into two services,The clean of the Bit Integrity service should include: - remove 'salt' from UI - split the service into two services:  -- Bit Integrity Tools (generate & compare) -- Bit Integrity (verify)
0,Cross-Service Clean up,The clean should include: 1. change order of services to be logical (or possibly alphabetical) 2. change name of services in UI to have the same prefix if they are variations on the same service     - example: Bit Integrity Service - Bulk 
1,Provide a clear visual cue in DurAdmin to indicate which storage provider is currently selected,"It has been observed that, when using DurAdmin, it can be easy to forget which storage provider is selected and perform an action on the wrong provider. This task is to provide a visual cue to help distinguish between providers.   One potential way to do this would be for each storage provider to correspond with a different color, and that color be displayed prominently (perhaps even as the spaces area background) when that provider is selected."
0,"Remove ""Location"" selection on service deployment via DurAdmin","Being able to select an alternate deployment location when deploying a service is a feature to come down the road, but in the meantime, the UI should not display this option when none of the services use it."
1,Simplify deployment of bulk services,"Items to simplify for bulk services (Duplicate on Demand, Bulk Image Conversion, Bulk Bit Integrity Checker):  1. Use a default setting for number and type of instances. The number and type will depend on the service and will require testing to determine a reasonable default. (10 large instances should work for Bulk Bit Integrity, testing by the pilot partners should help narrow in on good defaults for Duplicate on Demand and Bulk Image Conversion)  "
0,Update Image Transformer service deployment options,"These changes apply to both the Image Transformer and Bulk Image Transformer services:  - Clarify the ""Destination Color Space"" such that it is not confused with a DuraCloud space - Move file name prefix/suffix options directly under source space"
1,Azure Storage Provider - list space contents contains truncated contentId,The name attribute returned from IBlobContainer.listBlobs sometimes truncates the contentId.  Instead parse the contentId from the uri attribute until Azure4J fixes this issue - https://sourceforge.net/tracker/?func=detail&aid=3116592&group_id=266876&atid=1135911  http://www.windowsazure4j.org/learn/java_docs/org/soyatec/windowsazure/blob/IBlobProperties.html
1,Bit Integrity Service: simplify user options,"This improvement should update the Bit Integrity Service in the following ways: - create a default space for output - create auto-generated output name - provide checkbox for providing input listing name - add a checkbox for ""I do not have an input listing"", which cause an input listing to be generated from metadata "
0,Bulk Bit Integrity Service: simplify user options,"This improvement should include the following updates to the Bulk Bit Integrity Service: - remove config option from ui/xml: ""destination space"" - have service output automatically right to the space ""x-service-out"", with the content prefix ""bitintegrity-bulk/"" - remove config option from ui/xml: ""working space"" - have service automatically set working space to ""x-service-work"" "
1,Audit log service,"This task is to capture and make available to users audit logs which list the activities that have occurred in DuraCloud. There will likely need to be a separate set of logs for storage and services.   Once messaging support has been completed on storage and services, a new service could be created to listen to the messages being sent and store write those to log files which are stored in the user's storage area."
1,Audit logging: Storage,This task is to capture all of the audit information relating to storage events for all users in a single location. This listener will be running at all times for all users and storing the results within DuraCloud storage. 
0,sync tool syncing .ds_store files from mac,The sync tool syncs .ds_store files and other peripheral files that are autogenerated by a mac in a folder but that are not actually content that needs to be synced to DuraCloud. Would be nice if the tool knew to automatically ignore these types of files.
0,ServiceInfo.clone(),"The clone() method needs to be implemented in order for ""deployedServices"" within ServiceManager.java not to have their SystemConfig data member become null after deployment when it is currently going out of scope."
0,Security initialization,This improvement should disallow updates to the root user security credentials via the REST API.
1,Bulk Bit Integrity Service: error allocating memory,This issue is to resolve the bug recorded at the following link: https://wiki.duraspace.org/display/duracloudpilot/Reported+Issues From Bryan Beecher (ICPSR) on 12/3  The hadoop logs are attached. A brief scan of the logs indicates that the hadoop servers are running out of memory. 
2,Storage Usage Report Service,"Provide a way to generate a report with at least the following information:   - Number of files for each space - Total size of files stored in each space - Average file size for each space - File types (mimetypes) represented in each space  - Total number of files (all spaces) - Total size of all files (all spaces) - Average size of file (all spaces) - File types (mimetypes) represented (all spaces)  - All of the above information for each storage provider in use  These reports should be available for download and should be stored within DuraCloud so that they can be referenced periodically. At some point we may want to provide a way to schedule the running of such reports (monthly).  This would seem to be a good fit for a service which gathers the needed information, formats it, and stores it in an output space.  A REST-api should be exposed for client access to the reports.  This feature was suggested by Markus Wust (NCSU) during a pilot partner meeting on Dec 13, 2010. He was also interested in seeing how the information included in these reports relates to storage costs."
0,Bit Integrity Service: Incorrect report when item on list does not exist,"From David Arjanik (Columbia) on 12/15 Was unable to successfully complete Use Case 4: Preservation Monitoring Test. Here's an excerpt from my notes regarding 2.c of the use case test plan:   c. Service Test 1 i. Using the initial listing from Test0 above, create a new listing file with   some MD5s altered   some entire line items removed   some new line items  Arjanik: Done. Altered checksums of first three lines, deleted the next three lines, and added two lines at the end for files not in the DuraCloud instance. Uploaded to ""admin"" as usecase4-service-test-1.md5  ii. Run fixity service with this altered listing as input as in the previous test  Arjanik: Reconfigured service to run against this new md5 file. Report ran successfully, generating files fingerprints-service-test-1.csv and integrity-report-service-test-1.csv (names I specified in the reconfiguration).  iii. Verify that erroneous entries were identified in the generated report  Arjanik: Downloaded fingerprints-service-test-1.csv and integrity-report-service-test-1.csv. Arjanik: Checked integrity-report-service-test-1.csv. This file is attached. Only one line of report data: | space-id | content-id | 0:admin/usecase4-service-test-1.md5 | 1:admin/fingerprints-service-test-1.csv | status | | Invalid manifest file: newspace | README.txt | Error: metadata is null | for: newspace/README.txt |  Arjanik:    Deleted the new md5 file and the generated report files for this service test from the ""admin"" space to try again.   Reconfigured and re-deployed the service. Downloaded the results. Same results.   Edited the md5 file and replaced fictitious space ""newspace"" with ""usecase3"".   Deleted the old report files and re-deployed the service.   Downloaded the reports. Renamed the integrity report to integrity-report-service-test-1-2nd-try.csv and attached. Results are basically the same as before.   Opened the fingerprints file. The new lines were correctly identified as referring to files that didn't exist in DuraCloud.  The six files that I've attached:  PERFECT MATCH TEST usecase4.md5 - this is the input file listing correct space contents and correct md5 checksums fingerprints.csv - this is the md5 checksum file generated by the Bit Integrity Checker integrity-report.csv - this is the diff report generated by Bit Integrity Checker This test was successful.  ERRORS TEST: - The checksums of the first three files in the list were changed - The next three files were removed from the list. - The last two lines refer to files that are not in our DuraCloud instance. fingerprints-12-15-2010.csv - this file appears to use incorrect formatting in the last two lines, the lines with files that don't exist in the instance. integrity-report-12-15-2010.csv - this is the malformed integrity report.   Service Configuration:  Service Mode: Verify the integrity of a list of items Get integrity information from...:	The storage provider Salt: empty Store: AMAZON_S3 (0) Space with input listing: admin Input listing name: usecase4-service-test-1.md5 Output space: admin Output listing name: fingerprints-12-15-2010.csv Output report name: integrity-report-12-15-2010.csv     -------------------  "
0,Duplicate on Demand: Service failing,Duplicate on Demand service failed on 12/7  Service Configuration:   Source Space - synctest4   Copy to this store - RACKSPACE (1)   Copy to this space - replicants   Store results file in this space - replicate-out   Working Space - tmp2   Number of Server Instances - 1   Type of Server Instance - Large Instance  Service Final Status:   Job Ended - 2010-12-07 at 04:13:29 PM UTC   Service Status - Job Complete   serviceId - replication-on-demand-service-0.7.1   serviceStatus - STARTED   Job State - FAILED   Job Started - 2010-12-07 at 03:14:37 PM UTC   Job ID - j-9PDVC8SADTPQ  Additional Statistics:   - Run at icpsr.duracloud.org   - synctest4 (source) space has: 74993 items (it is a closed space)   - tmp2 (work) space has: 84 log files (all generated by one run of the service)   - job id j-9PDVC8SADTPQ ran for: 57 minutes before failing   - MapReduce job failed after 57 minutes   - 2 files were successfully duplicated   - In logs: /steps/1/stderr shows 10 task attempts which were all killed after not responding in 10 minutes
0,Duradmin upload failures for multiple large files,"When uploading multiple files through duradmin, failures frequently occur causing the content to not land in durastore. scenario:  1. select a ~100-MB file 2. upload it five times in a row through duradmin using different content-ids, and not waiting for one to complete before uploading them all 3. verify all five files successfully upload  The scenario above consistently fails."
1,Bulk Bit Integrity Service: add comparison,"The bulk bit integrity service should contain the following updates: - add optional config option that takes a user-specified input listing that is used to verify MD5s after the hadoop servers have generated a new listing.  -- if no listing is provided, the service should compare against MD5s stored in storageprovider metadata  -- this post processing should be implemented as a PostJobWorker"
0,reverse upload history order in the upload window,"It would be nice to reverse the order of the upload history in the upload window, so that more recent and/or in-progress uploads appear at the top instead of the bottom of the popup window. Sort of what the Firefox download window does. Putting the recent ones at the bottom requires the user to scroll down to see the progress bars of the recent uploads."
0,Image Viewer Flyout no longer works,"In the past, a content item that had an 'image/*' mimetype would present a link on the image thumbnail (if djatoka service was running) or on the canned picture icon. This link would display a ""flyout"" image viewer.  Now, clicking on the thumbnail or picture icon has no effect."
0,DurAdmin incorrectly displaying 0 for space size,"When using DurAdmin to display spaces which are larger than 1000 items, the space size is often mis-reported as being 0, even though there is a listing of items in the space. Also, clicking on the space no longer refreshes the item listing or metadata of a space (this is also true for content items.)"
0,DurAdmin: Delete button mis-aligned,Hovering over a multi-line content item displays a delete button which is half-way between the current item and the next item in the list. The delete button should be displayed completely within the boundary of the content item which it will delete.
0,Video streaming in duradmin:spaces,This item is to address the issue of duradmin not showing the streaming video when clicking on a content item in a space on which the MediaStreamer has been enabled. Scenario: # create two spaces: 'video' & 'tmp' # add the mp4 file attached to this jira item to the space 'video' # deploy the MediaStreamer server using 'video' as source and 'tmp' as destination # wait until service deployment is complete # click on the mp4 item in the 'video' space # ! the video should be shown in the right-hand column at this point !  Note: If you go directly to  https://<host>/durastore/tmp/singleplayer.html the streaming video will be available.
1,NullPointerException with Duplicate-on-change service on 'delete content',"This item is to address the issue of a NPE being thrown when the Duplicate-On-Change service is running and an item is deleted. Monitoring the duracloud-servicesadmin.log will show that upon the above action, an NPE is thrown in: org.duracloud.services.replication.Replicator.replicateContent(Replicator.java:127)  The service does successfully remove the item from the secondary store, and remains alive in the OSGi container; however, this case should probably be guarded against."
0,Duplicate on Change: space access is not propagated,"This item is to address the issue of the Duplicate-on-demand service not updating access state on the secondary provider when a space at the primary provider is changed from open to closed, or vice versa. Scenario: # deploy the duplicate-on-demand service # create a space in the primary store # verify that the space was also created in the secondary store # verify that both spaces have the same access state # toggle the access state of the primary space # boom!, notice the access state of the secondary space did not change"
0,Changing space access removes the space metadata,When changing the access level of a space the space metadata is removed.  Create a space Add space metadata Change the space access Refresh the space Notice the space metadata is gone
0,Media streamer: files not getting set with proper permissions,"On occasion, when starting the media streamer service, one or several of the files to be streamed is not set to have the proper permissions for streaming. Thus far, this issue has always been resolved by restarting the media streamer. It seems likely that the missing permission is due to a failed call to S3, so a retry would likely fix the problem."
0,Chunker Tool: error pushing files in sub-directories on Windows,"When running the chunker tool on Windows, if the content directory includes sub-directories with files, the tool will attempt to send each file to DuraCloud with a name ""sub-dir-name\file-name"", which causes an error because the ""\"" character cannot be part of a content ID. This does not show up on Linux/Mac systems because the ""/"" is used instead. This task is to update the chunker tool to replace ""\"" characters with ""/"" characters in all generated content IDs."
0,DurAdmin: Errors on space filter,"When attempting to filter by name in a space (used the x-service-work dir as an example, need this features to get logs for a specific bulk run), I tend to get ""next page failed: error"" and ""Get Space failed: error"" pop-ups. The filter works and loads the list of filtered content items correctly, so the errors seem unnecessary.  It appears that a new attempts is made to get the content listing on every change to the filter box, so each letter I type creates a new call.  My guess is that just adding a ""Filter"" button beside the text box, and only making the call when the button is pressed would fix the issue."
0,Duplication on Demand destination space,"When configuring the Duplication on Demand service, if you leave the destination space name blank, the service should automatically create the destination space with the source space's name.  Carissa ran the service leaving the destination space blank (assuming this would occur) and the service ran to completion... service status read: ""complete"" but no duplicated content was to be found."
0,Bulk Image Transformer output to be standard CSV format,"Currently the output for the Bulk Image Transformer is in a modified CSV format, similar to: name1=value1, name2=value2, name3=value3, ...  This task is to update the output to be in standard CSV format: value1, value2, value3  The first line in the file should a header which indicates the names of the values to follow: name1, name2, name3  The Bulk Bit Integrity service produces this format by allowing the hadoop reducer to produce all of the value lines, then using a post processor to insert the name line at the top."
1,Consistency of service output destinations and names,"This improvement is to ensure that all report-like outputs from services are created in the x-service-out space. Also, the outputs should have a consistent naming convention: <service-identifier>/<report-name> where <service-identfier> = all lowercase with '-' between words if necessary"
1,Refactor integration tests that regularly fail,"This task is to determine the integration tests that regularly fail due to ""eventual consistency"" effects or other issues related to the underlying storage provider implementations.  Eventually these tests should either be refactored to be more resistant to underlying storage issues, or should be refactored into unit tests that do not require network connectivity. However, this refactoring work will be addressed in a separate task. https://jira.duraspace.org/browse/DURACLOUD-237"
1,Hashed user credentials,"This improvement is to change from the current pattern of storing user credentials in-memory as plain text to, storing the hash of the credentials in-memory and having authN perform the same hashing (SHA256?). See Spring Security for adding a ""password-encoder"" to authentication providers in both 2.x and 3.x versions."
0,Bulk service status message,"This improvement is to more accurately display the service status state of bulk services. Currently, bulk image conversion and bulk bit integrity checking are processed in two steps: 1. hadoop processing 2. post processing on duracloud instance  The service status shows conflicting messages. The ""Job State"" is ""COMPLETE"" when the hadoop step is done, but the ""service status"" shows ""Post Job Processing"" for bit integrity, and nothing for image conversion.  More accurate and consistent status messages need to be put in place across the services."
0,Bit Integrity Service: iterating over http stream,"This issue is to change the ListingIterator found in HashFinderWorkload.java to iterate over a cached file inputstream rather than the content inputstream from an http GET. Although in most scenarios this is not an issue, when the content stream is large (30,000+ lines) the http stream sometimes breaks, causing the iteration to stop."
0,Allow build-time specification of dependency osgi bundle version,This improvement is to allow services built within a particular version of the baseline (say 1.4.0) to specify compatibility with dependency DuraCloud-osgi bundles from another version (say 1.3.0).
0,Bit Integrity Service: Support comparison for space manifests of up to 1 million items,This improvement is to ensure that spaces that contain up to 1 million items can successfully perform a bit-integrity manifest verification.
0,Bulk Bit Integrity Service: shutdown of post-processor,"This item is to address the issue of shutting down not only the Bulk Bit Integrity Service on ""undeploy"", but also shutting down the post-processor.  To reproduce this error, start the BBIS over a space containing more than 1000 items. Once the ""Job State"" indicates ""COMPLETE"", but while the ""Service Status"" still indicates ""Post Job Processing..."", undeploy the service. Undeploy fails due to the inability to remove the fixityservice OSGi bundle from the services container, as it is still processing. At this point, the BBIS remains in duraservice, but is no longer available via servicesadmin."
0,Bulk Image Conversion: failure due to Hadoop AMI upgrade,This item is to address the breakage caused by amazon elasticmapreduce changing their default AMI for hadoop jobs. This effects the bulk image conversion service which is faulting in the bootstrap script due to a missing dependency in the apt-get initialization. 
0,Media Streamer: Allow streaming of multiple spaces,This task is to allow the user to select multiple spaces for the media streaming service to stream.
1,Synctool: selective sync-ing,"This improvement should allow the user to selectively sync specific files based on subdirectories, prefixes, and/or suffixes using the synctool."
1,Media Streamer: Stream files added to a space,"This task is to set up a listener for new files which are added to spaces on which streaming is already enabled, and enable those files for streaming as well."
1,Revisit build from source,It appears that an oc4j dependency used by pax-construct is no longer available in a public m2 repository. This task should revisit pax to see if an upgrade is in order and to ensure that the build works from scratch.
1,Single Use Access URLs,"This task is to allow for the creation of single use access URLs. The user would need to specify the file which they would like to provide access to, then DuraCloud would provide a link by which that file can be accessed a single time. After the URL is used once, it no longer allows access to the file."
1,service reports contain another differentiator,Service reports that appear in the service-output space need to have an additional differentiator so that services that are run multiple times in one day do not overwrite one another.
1,Bulk Bit Integrity Service: post-processing in hadoop,"This improvement is to change the bulk bit integrity service to run its MD5 metadata collection post-processor in hadoop instead of with a single process on the main duracloud instance. This enables the service to run within hours instead of days for spaces exceeding 50,000 items."
0,Notification/Email utility,This feature is to expose a utility for sending emails. It should consist of both a general interface project and a corresponding implementation project.
1,Remove interim bulk-bit-integrity manifests,This improvement is to remove from the service output space the interim manifests that the bulk-bit-integrity service creates once the final report has been created. These manifests are: bit-integrity-metadata-results.csv bit-integrity-results.csv
1,Service Report: backend utility for currently running services,Provide a way to generate a report containing: - details from running services  The details should include at least the following information:  - tbd  The reports should also be available for retrieval through a REST-api
1,Storage Report Dashboard,"This task is to create a UI dashboard for displaying and downloading reports on storage.  REST API calls should be available from which the storage report information can be gathered. A UI will need to be created to be able to display the information in the reports.  The reports will likely be in XML, so XSLT is an option for display."
0,Limit time that can be spent spinning on a mapper task within hadoop,"Currently, the service code which runs in hadoop kicks off a thread to ensure that the mapper has enough time to complete. These threads are not time limited, so if the mapper process hangs, the hadoop job could potentially live forever. This task is to revisit these threads for each of the bulk jobs to set a limit for the amount of time the threads give to a mapper activity."
0,Remove user management from Duradmin,"This task is to remove the user-management capabilities (""Adminstration"" tab) from Duradmin, since this functionality is being moved up the the account-management-app."
0,Run-away service monitor,This task is to create a script/application that monitors DuraCloud accounts for long-running bulk services and sends an email alert upon detection.
0,Azure Storage Provider - adding content and specifying a checksum,"When a checksum is provided upon adding content to Azure, ChecksumInputStream was incorrectly trying to calculate the checksum from the provided stream.  The provided checksum should be used instead of calculating it from the stream."
1,DuraCloud instance losing initialization information,"1. Restart a Duracloud instance via the Amazon Management Console 2. Wait 5 minutes 3. Run app-config to initialize the instance 4. Check the instance, everything looks good 5. Wait between 7-10 min, check again - this time none of the users that were in the init are in Duradmin or Duraservice  After all of these steps, if you log in to DurAdmin as root, you will see on the Admin tab that no users are listed, and going to the Services tab brings up a ""get available services failed: error"" dialog. You will also not be able to log in to DurAdmin as any user besides root. Interestingly, everything seems normal on the Spaces tab, and rest calls to durastore work fine.  As a secondary issue, if on step 2 you don't wait 5 minutes, but go ahead when each of duradmin/durastore/duraservice are available (less than 2 min after restart), then in a short amount of time after you init, (less than 2 min) the durastore and duraservice init go away. But in this case all of the users are still present."
0,Create Example ServiceClient,This task is to create a sample ServiceClient analogous to the Sample StoreClient.
0,Bit Integrity Service: support retries of interrupted content downloads,This task is to add retries to the bit-integrity-service when a content item that is downloaded to calculate its MD5 does not match the expected MD5 from metadata.
0,Bit Integrity Service: Pass into comparison phase content-id of csv input,"The comparison phase of an 'all-in-one-for-space' run of the bit-integrity-service tries to guess the content-id of the input csv files. With date/timestamps appended to the content-ids, this becomes difficult. Therefore, the input content-ids should be passed into the comparison phase."
0,Image Viewing URL Stability,"This task is to investigate the stability of image viewing URLs across upgrades of DuraCloud instances. As each DuraCloud instance has an allocated static I.P. address, the behavior is unexpected. If there are specific uses cases for demonstrating this issue, please add them as comments below."
1,Error on large space delete,"This item is to resolve issues seen when trying to delete a large space  On the RackSpace storage provider: In the bhl.duracloud.org instance, there is a space ""master-rack"" that initially contained ~26,000 items. Deleting single items or a page of 200 items succeeded; however, when trying to delete the entire space a pop-up pops after a few minutes indicating that the operation failed. After one attempt, the number of items dropped to: 25,295 After another attempt, before failing the number of items dropped to: 24,727 In looking at the logs on the instance, it appears that two different types of errors were occurring, both of which were coming from RackSpace: - Could not connect to Rackspace due to error: Read timed out - Object was not found reportofscientif03scotrich/reportofscientif03scotrich_metasource.xml (other content-items also reported as unfound)  See attached catalina.out for details.  On the S3 storage provider: In the bill.duracloud.org instance, there was a space ""num-files-test-work-3"" that initially contained 36,845 items Clicking on ""Delete Space"" in DurAdmin started it spinning for about 5 minutes, it then came back with a dialog saying ""Failed to Delete Space"". Checking on the space size after the dialog popped up showed it at ~31,000 items However, even with DurAdmin out of the loop, the size of the space continued to drop and the space was eventually deleted, after about 35 minutes There were no errors in the logs indicating a failure."
0,Service Registry Naming,This update is needed to support creating a service-config.xml document that is named the same as the allowable service registry name. The old convention was: duracloud-0.9.0-service-repo The new convention is: duracloud-0-9-0-service-repo 
0,Security over UserDetailsService.getUsers(),This improvement is to add method security over UserDetailsService.getUsers() to limit access to this call to ROLE_ADMIN.
0,Create installation package for release,"Such an installation package should include: - documentation - the three wars (duradmin, duraservice, durastore) - zip of service registry - zip of osgi container"
0,Page load latency,"The Duradmin pages are built using jQuery. When the browser loads, the scripts pull data while building the page, creating a perceptible delay in rendering. A recommended solution may be to pull some of the jQuery back and not try to build the screen using as much scripting.  Browser versions and usage scenarios need to be added to this jira item."
0,Metadata editor allows for blank values,"Metadata editing via DurAdmin (using both the regular editor and the ""Edit Metadata"" dialog) allows you to add empty name/value pairs to the metadata list. Metadata names should be required. Although the UI appears to allow empty metadata name/values, if the name is empty, the metadata does not persist."
0,UI shortcuts which are not useful should be disabled,Holding shift + one of the arrow keys does odd things to the display. These shortcuts should be disabled.
1,Allow user to select which spaces to include when running Duplicate on Change,"This task is to provide the option for users to select a list of spaces when configuring the Duplicate on Change service, then only perform duplications for items in those spaces. One reason for this feature is to allow users to choose to not duplicate x-service-out and x-service-work spaces."
0,DuraStore REST API: text/plain response from getSpaces,"Just an inconsistency I noticed while looking at the REST API. This particular method gives a response Content-Type header of ""text/plain"". Most other methods that return XML declare their Content-Type as text/xml.  For example:  https://demo.duracloud.org/durastore/spaces?storeID=0  ...gives text/plain, but:  https://demo.duracloud.org/durastore/cwilper-test?storeID=0  ...gives text/xml."
0,Use application/xml as response Content-Type for XML-returning REST API methods,"Most of the REST API methods that return XML today use text/xml, but application/xml is a actually a better choice due to character encoding issues:  According to the specification of the text/* media types (RFC2046), if a charset parameter is not specified in the declaration of mime type (at the end of the Content-Type value, ala ""text/xml; charset=utf-8"", it must be assumed that the body is encoded in US-ASCII.  The ""encoding"" declared in the XML declaration is supposed to be ignored.  Here are a couple online posts that further explain the problem:  *  http://annevankesteren.nl/2005/03/text-xml *  http://www.grauw.nl/blog/entry/489  It's clearly the intent that DuraCloud REST API methods that return XML are encoded in UTF-8 (because the xml declaration says so), but according to the standards, because text/xml is used instead of application/xml, and the Content-Type is not declared with a ""charset=utf-8"" MIME parameter, the xml declaration is supposed to be ignored by clients and they should assume US-ASCII."
0,Allow for local service repository,It is currently required that the service repository for a DuraCloud instance be hosted on another DuraCloud instance. This task is to allow the service repository to be hosted within the same instance. This will both simplify the deployment of stand-alone DuraCloud instances (for users of the open source platform) and lead the way toward allowing multiple service repositories to be associated with a single instance.
0,Improve binary installation package,"In the 0.9 release the first binary installation package was created. This allows system admins to deploy an instance of DuraCloud without needing to build from source. This was a good first step, but there are a few pieces which could make the process smoother:  - Include wars with names that do not include version. When the jars are dropped into tomcat with the release version included in the file name, they are not deployed to the expected context path (i.e. rather than a context path of /durastore it is instead /durastore-<version>. The installation package zip file includes the version number, this should be sufficient. - Include the app-config jar, as this is needed to configure the instance. - Include an example properties file to be used with app-config.jar for instance initialization. - Include a default/example logback.xml file which can be used within the service bundle home. - Update included run.bat file to properly pick up environment variables in a Windows environment - Allow for the use of a local service repository (handled by DURACLOUD-402) - Update the included readme.txt file to have complete instructions for standing up a DuraCloud instance using the files provided with the installation package. Parts of this document can refer to the wiki for further explanation, but ideally, the entire process would be well enough defined in the readme such that the links to the wiki would be optional reading."
0,Duraservice: clean re-initialization,"Currently, when re-initializing duraservice, due to caching, some elements do not take on the new re-initialize values. This improvement is to determine and implement what is required to make duraservice re-initialization work as if it were being initialized from scratch."
0,"Service.xml: generate on build, not on test",This improvement is to have the services.xml service-repo manifest created during the mvn 'package' phase instead of during 'test'.
0,Service.xml: generate various renditions,This feature is to have the services.xml service-repo manifest generated for each of the various DuraCloud account package types: - archiving / preservation - media access - professional - trial
1,Host DuraCloud maven artifacts with Sonatype,This item includes the following tasks:  - open ticket with sonatype for org.duracloud - check if deps are outside of central - check dep licenses - make sure our poms are of good quality (sonatype oss)  The following plugin will be useful. mvn dependency:tree  https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide 
3,Failed to copy all files when running a large Duplicate-on-Demand service,"A small percentage of files are not copied during the execution of a large Duplicate on Demand service execution.  This can result in either a missing file or a zero length file in the target space.  This has bug has been demonstrated repeatedly both between different providers and copies back to the same provider.  However, it has only been observed in large file sets (circa 70,000)."
1,Use Amazon multi-part upload for large files,"This task is to update the S3StorageProvider to make use of Amazon's multi-part upload option for S3 when content is large enough to require chunking. This would mean that rather than storing our chunked file parts (and associated manifest) in S3, we would use the multi-part tools to push up the pieces and assemble them again in S3.  Since the MD5 for the entire chunked file is not available from S3, we will need to calculate it on transfer and store it as metadata."
1,Implement a content cache to mitigate eventual consistency issues,"In order to make the issues of eventual consistency an issue that does not concern the users of DuraCloud, DuraStore should be updated to provide a cache for the content which is stored. This could mean either storing the file locally first, then making the call to push it to the provider, or wrapping the stream in order to write the file locally while it is being stored at the provider. These local files would be used to fulfill GET requests that come in shortly after the file has been stored. The files in the cache would be removed once there is a greater degree of certainty that the file will be available on all following requests to the provider (perhaps just after a certain time period has elapsed.)  Note that one benefit to storing the files locally prior to pushing them to the provider would be that we could know the file size and generate the checksum locally before making the provider request, meaning that we could ensure that the file reached DuraStore correctly before passing it along to the next location. This may also become useful for handling large files that we intend to pass to the provider using a multi-part upload."
0,Create 1.0.0 AMI,"Updates to the 1.0.0 AMI should include: - trim down apache rules to only expect j2k to be running on port 18080  - add UTF-8 support to tomcat connectors <Connector port=""8080"" protocol=""HTTP/1.1"" connectionTimeout=""20000"" redirectPort=""8443"" URIEncoding=""UTF-8"" /> <Connector port=""8009"" protocol=""AJP/1.3"" redirectPort=""8443"" URIEncoding=""UTF-8"" />  - add debug tomcat startup script "
0,Filter display of service properties,"This feature is to omit the rendering of service properties that have names prefixed with ""System "". These are service properties that are useful for monitoring and scripting, but are probably confusing for end-users of Duradmin."
1,Service completion state must be accurate: non-bulk services,"This improvement is to reflect the completion state of non-bulk services as ""success"" or ""failure"". Currently, the service state remains ""started"".  Affected services: bit integrity checker bit integrity checker tools image transformer  Additionally, the end-time of the service should be captured in the service properties."
0,Service completion state must be accurate: bulk services ,"This improvement is to reflect the completion state of bulk services as ""success"" or ""failure"". Currently, the service state remains ""started"".  Affected services: bit integrity checker bulk image transformer bulk duplicate on demand  Additionally, the end-time of the service should be captured in the service properties."
1,WayBack Machine Service,"For DuraCloud users that are storing web archive content (warcs), a WayBack machine for search and access would be valuable. This feature is to create such a service.  reference: https://webarchive.jira.com/wiki/display/wayback/Wayback+Installation+and+Configuration+Guide"
1,iRODS/Chronopolis Storage Adapter,"This feature is to add a new storage adapter to connect to iRODS/Chronopolis, ala S3, Rackspace, & Azure. The base code has already been provided by Mike Smorul."
1,Bulk upload client tool with UI,"This task is to create a client-side tool which provides a nice user interface for managing bulk upload tasks.  This tool should provide a UI that allows files to be uploaded via the following methods: - file/directory selection dialog  A progress bar should be provided which indicates the status of the upload, specifying which file is being uploaded and the progress of the transfer.  It may make sense to use the SyncTool in the background to manage file transfers."
0,Duradmin mimetype editor: 'application/atom+xml' regex,"When attempting to set the mimetype of a content item to 'application/atom+xml', the error ""Invalid Mimetype"" is displayed. This prohibits the setting of the desired mimetype."
0,file name collisions not detected in Google Chrome,"When using DurAdmin with Google Chrome to upload a file, there is no file name collision detection or pop-up warning. The upload simply replaces the file. I am using Google Chrome on my mac os x. Note that the file name detection and pop-up did work on Firefox."
0,Duradmin service status,"When a service is deployed via duradmin, an entry is created detailing the: - service - hostname - status  The 'status' is apparently populated when the service is deployed and never updated as the actual service status changes through the course of processing. This improvement is to keep the status info in the UI entry up-to-date by pulling the content from the ""Service Status"" property instead of from the Deployment.status The current/accurate status info is also displayed on the right hand side, under the service details."
0,Reinit should clear the cached StorageProviders,Upon reinit the cached StorageProviders should be cleared in order to specify different stores
0,Service registry name and service.xml dependency,"This improvement is to decouple the dependency on the content-id of the service.xml file of a service registry from the space-id of the registry itself. Currently, the content-id is required to be identical to the space-id with the addition of the '.xml' extension. The service.xml content-id should be allowed to be any name."
0,Add service-result to completed service properties,This improvement is to add a new service property across all services that have a completion (success or failure) that notes the content-id of the service result file. This property should be set upon service completion.
0,Enforce space id rules for services,Enforce space id rules when entering a space name for Duplicate on Demand and any other service
1,Handle all unicode characters for storing and retrieving content,"This task is to ensure that a content item can be added and retrieved from both DuraStore and DurAdmin using any characters that can be expressed via unicode.  Generally, it seems like it's possible to add content items with any unicode content ID, but the problems come in when attempting to access/update/delete those items. These problems are not consistent between DuraStore and DurAdmin. Occationally DuraStore allows for retrieving files that DurAdmin does not.  A few Ascii content IDs which currently fail this test are below. This list is not exhaustive, just a examples.  test#item   - duradmin cannot load content   - error when accessing via durastore  test%item   - duradmin cannot load content   - bad request response from durastore  test&item   - duradmin cannot load content   - works fine in durastore  test+item   - duradmin cannot even bring up metadata   - works fine in durastore  Of course, content IDs with non-ascii characters also have problems:   1. blankHélènåJör.txt 2. blankπ × { } © 佈 б ' ' —.txt  #1 seems to be only a problem in DurAdmin, the file can be retrieved and deleted via DuraStore with no problem. #2 is a problem for both DurAdmin and DuraStore, the file cannot be retrieved or deleted via either. "
0,ServiceManager and ServiceManager Client to share Interface,This improvement is for the following two classes to implement the same interface: org.duracloud.client.ServicesManager.java org.duracloud.duraservice.mgmt.ServiceManager.java
0,"Change ""metadata"" to ""properties""","This task is to transition away from the use of the term ""metadata"" in regard to name-value pairs that can be associated with DuraCloud spaces and content. Instead, the term ""properties"" will be used. The work involved will be in transitioning between the two terms throughout the baseline."
0,Service Report: create schema and serializer for ServiceReport.getCompletedServicesReportList(),This task is to create a schema and serializer for the list of content-ids returned from ServiceReport.getCompletedServicesReportList(). The expected xml should look something like this: <service-report-ids>     <service-report-id>Service-Report-2011-06-27.xml</service-report-id> <service-report-ids>
1,Cumulative storage report ,"This task is to add an additional storage report that is a cumulative on-going report which is updated each time a new storage report completes. The information in this report will be an entry for each completed storage report that provides the top level report information. Mime type information would be left out, and other information in the detailed report may be excluded as well. The goal of this report is to be able to create graphs that show the change in content stored over time.  This report could work like the completed services report in that it could be cycled to start a new file monthly.  Information should be included in the cumulative report  that allows for the creation of links to the detailed reports as well as cross-links to the detailed report charts. This could just be the detailed report content ID."
1,StorageProvider support for object copy/rename,"This improvement is to provide the StorageProvider interface support to: 1. copy an object stored within DuraStore to another space within the same storage provider 2. rename an object stored within DuraStore to a new content-id within its existing space  All three storage providers (Amazon, Rackspace, Azure) provider REST-API support for 'copy', but it appears that only the Amazon and Azure java clients expose this functionality.  http://docs.rackspacecloud.com/files/api/v1/cf-devguide-20110629.pdf http://docs.amazonwebservices.com/AmazonS3/latest/API/ http://msdn.microsoft.com/en-us/library/dd894037.aspx "
0,Service Reports: Comma-delimited content-ids,"This bug is that service reports become invalid if a content-id contains a ',' since currently service reports are output as comma-delimited. A potential solution is to create tab-delimited service reports."
1,Update DuraReport storage reporting XML to be generated using JAXB,"The purpose of this task is to swap out the current XStream implementation of the DuraReport storage report generation to use JAXB. The main reason for this is to be able to provide an XML schema for the data that is being generated, so that the information can be more easily retrieved and used."
0,Link Service Report in service properties display,"This improvement is to add a link to the services ""Report"" property to display (possibly as a flyout?) a tabular form of the service results."
0,UI: Use different icons for each service,"The goal of this task is to include a different icon beside each service, both in the deployed services listing and the available services listing."
0,UI: Remove the provider drop-down when there is only one provider,This task is to remove the storage provider drop-down when only one provider is enabled.
0,UI: Use different icons for the content item Download and View links,"This task is to replace the button icon for the content item View link, as it is currently the same as the download link. Perhaps a eye icon or something similar would work."
0,UI: Adjust the service deployment box to remove scroll bars,This task is to re-adjust the pop-up dialog used for deploying services so that scrolling is not necessary and no scroll bars are displayed.
0,UI: Display service status correctly,"Currently, the table which lists all deployed services has a column called Status which always displays STARTED, regardless of the actual status of the services. This value is being read from ""deployment status"" which is meaningless. This task is to read in the actual status of each service from the service properties and display that in the table."
0,"UI: Administration tab always contains text: ""No users defined""","This task is to only display the ""No users defined"" text on the Administration tab when there are actually no users defined."
0,Service Monitoring on reconfigure,"This improvement is to ensure that a monitor thread is in place when a service is reconfigured.  A service can be reconfigured while it is either still running, or after it has completed. Since a reconfigured service retains the same deployment-id, a service that is still running will continue to be monitored by the original monitoring thread. However, a service that is complete needs to have a new monitor thread started since the service is effectively restarted via a reconfiguration."
0,Space Delete: Indicate to users when a space is in the process of being deleted,"Currently, when deleting a space of reasonable size there is a dialog that spins for a while, then the space is removed from the spaces list. But if the list of spaces is refreshed, the ""deleted"" space shows up again. In the background a process is still running to delete the space, but there is no way for the user to know that this is happening.  This task is to make it clear to users when a space is in the process of being deleted.  "
1,Provide informative error messages for special characters in space and content properties,"Special characters included in metadata names or values which are added to spaces or content show up immediately in the list of metadata/tags, but when those special characters aren't valid http header values, the information is not actually added to the space or content item in the provider. This task is to to check for valid characters and pass back errors to users so they know the allowable characters."
0,"UI: Inform users that the characters '\', '?', and ';' cannot be part of a Content ID","Three characters are not allowed in content Ids: '\', '?', and ';', as noted in IdUtil and here: https://wiki.duraspace.org/display/DSPINT/Space+and+Content+Name+Restrictions. Errors are returned from DuraStore when trying to add an item with a content Id including '\' or '?' characters, but no useful information is presented to users in DurAdmin. No error is returned with the use of the ';' character, as the character doesn't get passed through to DuraStore. There should be a validator in DurAdmin to catch these characters, and users should be provided a useful error message indicating the problem."
0,UI: XML files viewed in Chrome are appended with javascript code,"When using DurAdmin via the Chrome browser, selecting a file of type XML, and choosing to View the file, the XML is rendered correctly, but is followed by a block of javascript code, starting with ""try{function lpshowmenudiv(id)""."
0,Handle semicolon character,"When content whose ID contains a semicolon (;) character is added on a DuraCloud instance, the characters after the semicolon are dropped. The file is available, but the ID is wrong."
0,DuraCloud Constants,"This task is to create a new maven project that simply holds constant values required across the other projects. Likely, these constants should be implemented as enums. Examples include: - DELIM = '\t' - ENCODING = ""UTF-8""  Other candidates include service constants (see: org.duracloud.services.ComputeService.java) and other common, constant, classes that have no other dependencies such as: org.duracloud.common.util.DateUtil.DateFormat.java"
0,UI: Using Enter key on Add Space selects Closed space access,"Using the Enter key on the Add Space dialog to submit the request to add a space both selects the Closed access button and submits the request. The space is created correctly, but the access button indicates the default Open state before the Enter key is hit, then the space ends up with Closed access after it is created.  Tested on: Windows 7; Firefox 5.0; Chrome 12.0.742.122"
0,UI: Add message to indicate when no other services are available to deploy,"When all services are deployed, and you click on Available Services, there is no message that there are no services available"
0,UI: Display the selected service on the service configuration dialog,"The name of the service which was selected needs to be displayed on the screen used to configure the service. Currently there is just text that reads ""Configure the Service"", which isn't very helpful."
0,Document Installation Process,"This task is to update the trunk/resources/readme.txt file to have complete instructions for standing up a DuraCloud instance using the files provided with the installation package. Parts of this document can refer to the wiki for further explanation, but ideally, the entire process would be well enough defined in the readme such that the links to the wiki would be optional reading.   The initial outline of the process should be documented by one of the developers, then the detailed steps should be flushed out by someone who is not as familiar with the process."
1,Service Errors with removal of expected spaces,"This task is to analyse the exact impact of removing the spaces:  x-service-out  x-service-work  before and during a run of each service. After the analysis, implement a strategy for allowing services to run successfully to completion. Resolutions could potentially include: - services exiting gracefully - services recreating the expected spaces - or potentially not allowing the spaces to be deleted in the first place"
1,Create consistent css styling for reporting,This improvement is to restyle the Dashboard Storage and Services tabs to have a consistent styling with the rest of the duradmin application.
0,Facilitate integration: CDL Merritt ,"This task is to support the conversations and integration of Merritt with DuraCloud. Likely the software development will all be on the CDL side, but some time and effort will be needed as support."
0,CloudSync Service,This feature is to add a new CloudSync service. The service will likely be a webapp service (ala j2kservice). Note: CloudSync itself should probably by updated to take its working directory as an initialization argument.
0,Re-Add DuraCloud favicon,"This update is to re-add the DuraCloud favicon to the duradmin application. In release 1.0.0, its rendering was somehow dropped."
1,DurAdmin support for object copy/rename,This feature is to add UI support for single content copy/rename operations.
1,StoreClient/REST support for object copy/rename,"This feature is to expose the StorageProvider.copyContent() method through the DuraStore REST API as well as update the storeclient. However, this feature should not be implemented until the patch submitted to Rackspace has been incorporated into the Rackspace java-client baseline. https://github.com/rackspace/java-cloudfiles/pull/18"
1,Duplicate on Change may not successfully duplicate all files,"In testing completed by Harvard, the Duplicate on Change service was deployed, followed by running the sync tool to transfer 33,110 files into a single space. The Duplicate on Change service was configured to watch the primary Amazon provider and copy all files to Rackspace. After all files were uploaded and the Duplicate on Change service was given ample time to complete, only 33,045 files had been moved to the space in Rackspace. So 65 files (0.2%) were not duplicated.  This task is to determine how this small set of files may have slipped through the cracks, and update the code such that: 1. No files are missed in future runs of the service. 2. An output report is generated which indicates all files that have been transferred and captures any errors that occur along the way.  The log files from the harvard instance are attached to this issue for reference."
0,Retrieval Tool: Add the ability to retrieve a space item list,"This task is to add an option to the Retrieval Tool which will pull down the list of items in a given space (or list of spaces), rather than actually retrieving them. This list will be written to a file.  There is an existing tool in duracloud-utils named the space-lister-tool which already provides this capability, so it may be useful to borrow code from that implementation.  This should result in a single new command line option (--list-only perhaps) that, when included causes the space list to be stored in the content directory rather than files."
0,Serve DurAdmin pages as XHTML,This task is to update DurAdmin to serve its pages as strict XHTML. The mime type used should be: application/xhtml+xml. The DOCTYPE header should also be specified as <!DOCTYPE html> which is the recommendation for HTML5.
0,1.1 Production AMI,"This task is to create a new production AMI with the 1.1 tagged codebase. It should include: - /etc/init.d/tomcat: export CATALINA_OPTS=""-Xms512M -Xmx512M -XX:MaxPermSize=512M"""
0,Add version number to output of sync and retrieval tools,This task is to include a version number in the printed output of the sync and retrieval tools so that it's easy to determine which version is being used.
0,StoreClient moveContent(),"This feature is to add an additional method to the storeclient, namely, moveContent(). This method will simply call copyContent() followed by deleteContent()."
0,Duradmin login url,"This issue shows up in the following scenario: - Launch an instance as root user from the Management Console - the login fails (as expected); but typing in the correct root username/password continues to fail - until manually removing the trailing portion of the URL: ""login?error=true""  It would be ideal if an incorrect login using the pattern as in the MC did not produce this URL in the browser."
1,Move Duradmin to Dojo ,https://wiki.duraspace.org/display/DURACLOUDDEV/Notes+on+the+Way+Forward+for+Duradmin
0,ContentStore Duplicate Across Providers,"This feature is to provide users a way to copy a single item from one storage provider to another storage provider (or within the same storage provider). The implementation will likely be similar to the ContentStore.copyContent(), but will contain an optional source storeId. This effort should also include exposing the cross-provider copy functionality at the ContentRest level of Durastore. "
0,Remove Image-Conversion Services (temporarily),This task is to remove the two image conversion services: - bulk image conversion - non-bulk image conversion  from the list of available services for the 1.1 release. These services will become available once again when a strategy has been developed to help users manage the error cases.
0,SyncTool does not recognize chunked content,"The issue here is seen in two scenarios: 1. Content that is chunked into DuraCloud during one run of the synctool is not recognized as existing in the space on a subsequent run of the synctool. 2. Content that is chunked into DuraCloud during one run of the synctool is not recognized as existing locally on a subsequent run of the synctool, and if the --sync-deletes flag is set, then the chunks are not deleted from the DuraCloud space."
0,Update EasyMock library to version 3.0+,EasyMock has rolled the class extension project under the base project and deprecated EasyMock ClassExtension. This task is to update the top level pom to use EasyMock 3.0+ and refactor all tests which use org.easymock.classextension.EasyMock to use org.easymock.EasyMock instead.
0,SyncTool does not delete empty subdirectories,"The issue here can be seen in the following scenario: # create an empty DuraStore space # create a local directory with subdirectories, add files # run synctool ** verify space/directories are the same # delete a local file at the top-level or within a subdirectory (but if in a subdirectory, make sure more content remains in that subdirectory) # run synctool with '--sync-deletes' ** verify content is deleted from the space # here is the issue: delete local content in any of the following ways: ** from a subdirectory, leaving that subdirectory empty ** remove everything from the local directory # run synctool with '--sync-deletes' # Error: notice content is not deleted in space "
0,Throw exception on space names which are too long,"This item is to ensure that an error is thrown back to the user when a space to be created has a name which exceeds the maximum name limit. Currently the name is just truncated, and the truncated name isn't displayed until a refresh of the UI.  See S3ProviderUtil line 133 for the spot where the S3StorageProvider likely does the truncation. All current active storage providers (S3, Rackspace, Azure) should be checked for this same behavior.  Example of a space name that is too long (for S3): duracloud-1-1-0-snapshot-service-repo-trial-test "
0,put psd files from charles on the wiki,NULL
0,New service plan: All Services,"This task is to add a new service plan: ""all"", that includes all services."
0,OOM on Re-Initialization of Durareport,"This issue is revealed when performing multiple re-initialization of a DuraCloud instance via the app-config or MC. It would appear that the report collection threads created during initialization are not properly shutdown on an immediate, subsequent re-initialization. After a few re-inits an OOM is seen."
0,Deployed services list is not refreshing,"Once a service has been deployed, the service information is not being displayed in the deployed services list."
0,Add links on the dashboard which allow for downloading DuraCloud utilities,"This task is to add a link or links to the DurAdmin dashboard which allow a user to easily download the DuraCloud utilities (sync tool, retrieval tool, etc). It would also be useful to have links pointing to the documentation for each of the utilities that can be downloaded.  Update the existing ""Help"" link to open a new window with this page: https://wiki.duraspace.org/display/DURACLOUD/DuraCloud+Help+Center Add a tab on the Dashboard beside the Storage and Service report tabs called ""Tools"" with the content listed here: https://wiki.duraspace.org/display/DSPINT/DurAdmin+Tools+tab"
0,REST endpoint: isInitialized() across all webapps,This improvement is to add a new REST method to each of the four existing webapps: - durastore - duraservice - durareport - duradmin  That authenticates against anonymous users and returns a response code of: - 200 = is initialized - 503 = is not initialized
0,Add messaging support for content copy action,"This task is to add messaging support to the new copyContent() method added to the StorageProvider interface. A new topic should be added to carry messages related to copy operations. Once the messages are being sent, the two services which are listeners, Duplicate on Change and Media Streamer, need to be updated to listen for copy operations and react appropriately."
1,Long-running storage tasks,"This task is to make it possible to run, track, modify, and stop long-running storage activities. Since these actions can take a long time, the user needs to be able to discover the current progress and also be able to cancel or change the task while it is in process. Also, these tasks usually need to be re-started in the event of an instance restart.  Examples of these types of activities: - Multi-file delete - Multi-file copy (within and between providers) - Multi-file or multi-space property add/update/delete - Large space delete  For activities which handle multiple content items, there should be a way to provide a known set of content IDs over which the action will occur.  A follow-on task for this item will be to expose these capabilities through the UI."
0,UI: Provide an understanable error message when users attempt to create a space with a reserved name,"This task is to add validation to the space creation action in the UI which does not allow users to attempt to create a space with a reserved name. The list of reserved names can be found here: https://wiki.duraspace.org/display/DURACLOUDDEV/DuraCloud+Administration. Currently, when a user attempts to create a space named, for example, ""task"", a big error dialog with a 405 http response message is displayed."
1,Distinguish service-failures from failed-service-results for Hadoop Services,"This improvement is to have different informational messages (and underlying service properties) for when a service fails versus when the results of a service run contain failures. Examples of service failures are unrecoverable exceptions, OOM, failed hadoop startup, etc. Examples of result failures are bit integrity discrepancies, unsuccessful image conversions, unsuccessful duplications, etc."
1,Distill failed items from a failed service run,This improvement is to extract out the failed items from the output report of unsuccessful service runs in order to make the errors more clear for the user. The implementation will likely require updates in the reporting UI as well as in the underlying service infrastructure. 
1,Filter administrative content from storage reports,"This improvement is to by default only expose the user content in the UI storage reports. The administrative content to be filter is currently found in the following spaces: - x-duracloud-admin - x-service-out - x-service-work  The implementation may be done at the time of report statistics collection or in the UI, toggling admin spaces on/off. "
0,Make parameters consistent among command line utilities,"This task is to make command line options as uniform as possible among the current set of command line utilities: sync tool, retrieval tool, chunker, stitcher. There are currently several inconsistencies that need to be cleaned up:  - Use -c for content directory (as used by sync and retrieval tools). Currently stitcher uses -d and chunker uses a compound -a option - The chunker should support -i (store-id) - The chunker should use -s (space-id) rather than including it as part of -a - The chunker should use -h (host) and -r (port) rather than -c"
0,Service output files to use .tsv extension and text/tab-separated-values mime type,This task is to update the service output files to have mime type of text/tab-separated-values and .tsv extension
0,Undeploy service should have warning,"This issue should address the apparent lack of an ""ok""/""cancel"" warning when undeploying a service. Currently, when the user selects ""Undeploy"", the service is simply undeployed without a warning notice."
0,Service properties blank on deploy,"This issue is to address the situation where a user deploys a service, and when it has successfully deployed, the ""Service Detail"" panel goes blank. The ""Service Detail"" panel should show the properties of the newly deployed service."
0,Sync Tool should ensure configuration for subsequent runs is functionally equivalent before attempting to restart,"When the Sync Tool starts up, it compares the current configuration with the previous configuration to determine if it can perform a restart. A restart allows the Sync Tool to not attempt to re-sync all of the files in the content directory. The problem is that the check to see if a restart is possible (in SyncTool.restartPossible()) only checks the list of content directories. It should actually compare host, spaceId, storeId, and syncDeletes values as well. If any of those values do not match, a clean start should occur."
1,StorageProvider to support add-content-with-metadata,"This improvement is to investigate and potentially implement an update to the StorageProvider.addContent() interface method or an addition of a parallel addContent() method that optionally takes object-properties along with the content-stream.  As an example, the current ContentRest.addContent() method performs two calls to the StorageProvider: addContent() & setContentProperties(). The number of calls made to the underlying storage-provider could be cut in half if these two calls were combined.  The AmazonS3Client interface, the Rackspace FilesClient, and the Azure IBlobContainer all support calls for including metadata along with the call for adding content."
0,Media Streamer: set logging level down from WARN,This improvement is to set the log level of messages in the MediaStreamService to INFO or DEBUG from the current WARN. 
1,StoreClient encryption support,This feature is to provide a means for users to encrypt/decrypt content stored and retrieved through the StoreClient.  The StoreClient should have a constructor that optionally takes a symmetric or asymmetric encryption key.
1,SDSC Storage Provider integration,This feature is to integrate the SDSC Cloud as another underlying StorageProvider.
1,Auto-user application,This feature is to provide the ability to start a DuraCloud instance with a set list of services that will be run on a set schedule. The first example of such an auto-run/scheduled service is Bit-Integrity.
0,UI - Copy multiple content item across storage providers,"This feature is to expose the copy-content-across-storage-providers functionality within Duradmin. This needs to include the option to ""multi-select"" content items to be copied as well as only selecting a single item.  Likely this will be an addition to the existing content copy UI element. The interface should optionally allow the user to indicate the destination storage-provider."
1,Content manifest,"This feature is to create and maintain a manifest (space-id, content-id, md5) of all content held in the primary storage provider. A listener should update the manifest whenever content is added or deleted from Durastore. The format of the manifest should be compatible with the expected input format of the bit-integrity service."
1,Migrate SCM to Git,This task is to migrate the DuraCloud SCM from Subversion to Git.
1,Rename spaces,"This task is to make it possible for users to rename DuraCloud spaces. In order to do this, space names will need to no longer be directly tied to the bucket/container names of the underlying storage provider. One way to do this is for the name of a space to become a property of that space. A side benefit of this is that we should be able to remove many of the naming limitations we currently have on space names."
1,Archive-It Integration,This feature is to provide an integration with Archive-It so that users with Archive-It content may seamlessly push their collection into DuraCloud.
1,UI - Provide hierarchical content view,"DurAdmin should be able to graphically display folder and file hierarchies in a similar fashion that is available on a local machine. Currently, the naming convention to indicate hierarchical files (ex. folder1/folder2/folder3/filename.txt) is not sufficient from a user perspective. It is expected that the interface will allow them to create ""sub-folders"" or spaces within spaces, etc. that will map to the local directories that the content was stored in.  This task is to update the display of content items such that the user sees the commonly expected folder structure, which allows navigation down to files. This should look pretty similar to the display provided for content in S3 by the AWS console."
1,Long-running content actions,"This task is to add support in DuraStore for long-running actions that can be run over content. Some examples of these kinds of tasks are: multi-file delete, multi-file copy (within and between providers), and multi-file property add/update/delete. It may make sense to implement this feature using the existing tasks infrastructure."
0,Integrate working version of CloudSync,This task is to pull in an updated version of CloudSync into the cloudsyncservice that does not have the issue of not being able to connect to a DuraCloud instance.
1,Upload tool web deployment,"This feature is to establish a means of deploying the UploadTool via Duradmin as a signed-applet, webstart app, or other."
0,Automatically insert logo in x-duracloud-admin,"When a DuraCloud instance is deployed, it should automatically include a logo in the x-duracloud-admin space, so that the logs don't get junked up. I have attached a possible logo to use for this purpose."
1,Implement an automated performance testing framework,Assemble a framework for automated performance testing to fit into the DuraCloud software engineering process. This framework is to include tests and test data sets. It will be implemented incrementally in an agile fashion.  The primary goal of performance testing is to ensure that DuraCloud meets its goals for scalability and reliability. It characterizes the performance of the system under various load scenarios. It is also a primary input for bottleneck analysis that can be used to improve system performance. Performance tests often overlap functional tests and are useful for ensuring that DuraCloud is functioning as documented. Performance testing is also used to help size and manage the operating infrastructure in order to avoid unnecessary costs. These costs are part of the information needed to set the price of our services.
0,Update Sync Tool to take advantage of improvements due to the Upload Tool,The creation of the Upload Tool included several back-end changes to the Sync Tool. This task is to make improvements to the front-end of the Sync Tool to: - Display the progress of file transfers when printing status - Allow for individual files to be watched as well as directories
0,Upload tool applet cannot be used at the same time on spaces of the same name on different providers,"If you are using the upload tool via DurAdmin to perform an upload on Amazon in space ""test"", you cannot also perform an upload to a space with the same name in another provider - you are sent to the existing upload window."
1,ACL: Create UI for managing ACL users/groups during space administration,"This feature is to create a UI in duradmin for managing space-level ACLs. It will likely be a button in the righthand pane, similar to the ""edit"" button for space metadata. The functionality should enable Admins to add/remove 'users' and/or 'groups' to the space ACL with permissions of 'read' or 'write' (which implies read). A potentially attractive UI widget would be the one that renders two, side-by-side boxes containing usernames, and arrows allowing the moving of usernames from one box to the other. "
1,ACL: Update StoreClient with setSpaceACL(),"This feature should add the new method to the StoreClient: setSpaceACL() This method should take a spaceId and a map of 'usernames' and/or 'groupnames' and their associated privilege (read or write), and set the ACL for the given space. "
0,ACL: Update DuraStore security to ACL rules,This feature should update the durastore spring security definitions to limit access as defined in: https://wiki.duraspace.org/display/DSPINT/Space+Level+ACLs+-+design
0,Space and content list reload,"Please add a refresh button to the space listing as well as to the content listing within a space. Currently a space is not reloaded after content items are added, and a user must either (a) reload their entire browser window or (b) click on the space tab and renavigate to the appropriate space to see the newly added content items."
0,ACL: Update app-config and security-user bean to include 'group' details,"This update is to modify app-config, SecurityUserBean, and security-users.xsd, as appropriate to support the inclusion of ACL 'groups' during instance initialization."
0,ACL: Add ACLStorageProvider,This feature is to add a new StorageProvider wrapper implementation to facilitate filtering the spaces listed in a getSpaces() request. The primary objectives are to: - provide a way to only return spaces that are either 'open' or are accessible to the calling user - improve performance so that each request for the space listing does not require querying the properties of each space from the underlying storage provider.  Below are some implementation suggestions:  1. in StorageProviderFactoryImpl.getStorageProvider()         * wrap storageProvider with a new ACLStorageProvider()  2. ACLStorageProvider         * overwrite: getSpaces()         * cache spaces and space acl properties 
0,ACL: Enable multi-map content/space properties,"This feature is to enable multi-map header values throughout the application for content and space properties. The capability should be enabled for the entire stack: storageproviders, to REST, to StoreClient, to Duradmin."
0,UI Copy Single Content Item Across Storage Providers,NULL
0,ACL: Update duradmin UI to exclude tabs from USER role,"This update is to not display the ""Services"" tab nor the reports tabs under ""Dashboard"" if the user who is logged into duradmin only has ROLE_USER. Rather, only users with ROLE_ADMIN should be able to see the services and reports tabs."
1,ACL: Represent space access (open/closed) as an ACL,"This improvement is to refactor the implementation of space access from the 'space-access' property to an ACL property (acl-group-public). In addition to cleaning up specialized logic for space access, handling space access as an ACL will allow a single call to the underlying storage providers to be made in determining access rights in Spring Security instead of a call for space access and a separate call for space ACLs. "
0,UploadTool changes ACLs on upload,"This bug is that when running the UploadTool, the existing ACLs of a space are replaced by a single ACL of {user=w}."
0,Deprecate space-access methods,This improvement is to deprecate the setSpaceAccess and getSpaceAccess methods on the StorageProvider and ContentStore interfaces.
1,Provide a REST call to allow administrators to retrieve a listing of all users and groups,"In order to be able to create an ACL for a space, an administrative user needs to list all of the users and groups which should have access to the space. What makes this hard is that there's no way via the REST API for an administrator to get a list of the users and groups which are available. So while the ACL can be created currently, knowing how to fill it out is a challenge.  This task is to add a call to the REST API which allows administrators to retrieve a listing of all users and groups, as well as which users are parts of which group."
0,Sort permissions display,"This task is to sort the listing of items in the Permissions section of the content item display in DurAdmin.  Groups should be displayed above users, and both sets should be sorted alphabetically (A-Z). The ""public"" group, when added, should always be displayed at the top of the list."
0,Handle space properties cache loading on instance initialization,"Currently, the cache for space properties is loaded the first time a user attempts to access a storage provider. If this user is a non-admin user, and there are many spaces in the account, this action can take several minutes. This task is to transition to loading the cache when the instance is first initialized, rather than waiting for a user to access the instance, greatly lowering the likelihood that any user will have to wait while the cache is populated."
0,Show only writable spaces in the copy content dropdown (in the copy dialog),NULL
0,Multi-space property editing:  User should be alerted if they are trying to update one or more readonly spaces.,"Suppose a user is looking at a list of four spaces and suppose he has write access to only three of them. Currently the set of editable tags and properties is gathered from all selected spaces regardless of whether the user has write access. Similarly if a user tries to add properties or tags to a selected space to which he has only read only access, the system won't handle it very beautifully.    The ideal solution would be to disallow selection of unwritable spaces."
0,Create Messaging over setSpaceACLs(),This improvement is to add messaging over the durastore calls to StorageProvider.setSpaceACLs(). The configuration for this is in: durastore/src/main/webapp/WEB-INF/config/aop-config.xml A new advisor (ala SpaceUpdateAdvice) will needed to be created as well.
0,Copying content from Amazon to another provider produces unnecessary properties,"When performing a content copy from Amazon to another provider, the resulting content item includes duplicate copies of ""default"" content properties, such as mimetype and checksum. This task is to filter out these properties when performing a copy."
0,"Add ""Help Desk"" link to footer","Please add the text ""Help Desk"" to the footer before the ""Contact Us"" link. It should link to: http://duracloud.kayako.com."
0,Remove Duradmin's dependency on the Space Access APIs in storeclient,Remove the dependencies on space access by having duradmin use the acls instead.  It is also necessary to make it clear to the user how to enable public access on a space if no public acl has been set. Additionally the public group should be highlighted in the UI to amplify its uniqueness. 
0,Synctool: not re-syncing content deleted from DuraCloud,"This issue is to fix the bug demonstrated in the following scenario: # run synctool pushing one or more files to a space ** observe success # delete one of the content items in DuraCloud newly pushed to the space # re-run the synctool ** observe deleted item is not re-uploaded  The strategy to fix this issue is to introduce a new flag which instructs the sync tool to perform a ""clean start"" rather than a re-start, so that all files in all local directories are considered as part of the sync. This flag would essentially have the same effect as clearing the work directory prior to starting the sync tool. The flag should not be enabled by default."
0,/init resource should be available to ROLE_USER,"This item is to address the bug that only anonymous users can inspect durastore, duraservice, and durareport. ROLE_USER and ROLE_ADMIN should also be able to make such an inspection."
0,Links into the djatoka viewer are broken in duradmin,Links into the djatoka viewer are broken in duradmin. For example the following url doesn't work:  http://174.129.12.28/adore-djatoka-p18080/viewer.html?rft_id=http%3A%2F%2Fdanny.duracloud.org%3A80%2Fdurastore%2Fimage-transformer-dest%2FSunset.png%3FstoreID%3D0  But this one does:   http://174.129.12.28/adore-djatoka-p18080/viewer.html?rft_id=https%3A%2F%2Fdanny.duracloud.org%2Fdurastore%2Fimage-transformer-dest%2FSunset.png%3FstoreID%3D0  The difference between the above urls:  replace the durastore url  protocol with https and remove the port and everything works.     
0,"Content image does not refresh after clicking ""Make space public"" button.","To reproduce:  1. Make sure image service is running. 2. In duradmin browse to an image content item in a non-public space. 3. Click the make space public button. 4. Notice: the image is not visible.  5. Also note that the warning message  is not automatically removed as it should be.  Also note that when making a space non-public, the button for making it public does not reappear.  I think the panel should be refreshed and the image should be immediately available."
0,January 2012 completed reports are not showing up in duradmin dashboard.,"To reproduce, run a service to completion and then observe in the duradmin dashboard that it as well as another other service run after January 1st is not being displayed in the dashboard.  "
0,Sync tool: Create a file transfer log,"This task is to update the sync tool such that it creates and updates a textual transfer log including all files which are added or updated in DuraCloud along with their checksum. This file would only include files which were successfully transferred to DuraCloud.   Ideally the format of this file would follow the format expected by the bit integrity service, so that after doing a fresh transfer of a full dataset into DuraCloud this transfer log could be compared to the space via the bit integrity checker service."
0,Verify that pointing multiple subdomains to a single instance does not cause issues,"This task is to perform tests against a DuraCloud instance where multiple subdomains are pointed. One subdomain will be used to initialize the instance, while others will be used to access the UI and APIs. This is intended to simulate the interaction that users will have with the Community Cloud. Ideally, coming to DuraCloud through each of the different host values will show that: 1. The DuraCloud applications function correctly 2. The user is always shown only their host value via the UI.  Note that the user interaction can be limited to User role actions."
0,Ensure that client tools continue to function across minor outages,"This task is to test the DuraCloud client tools (primarily Sync Tool and Retrieval Tool) and make updates that are necessary to allow the tools to continue to run even if the DuraCloud instance becomes unavailable for a short period of time. The period of time to be tested against should be the time that is required to upgrade a DuraCloud instance to a new version. This will allow DuraCloud users to keep their tools running even across an upgrade cycle.  A likely strategy is: When a transfer fails, check to see if the instance is available before doing a retry, if its not, then wait for ~5 min and try again. When the instance becomes available again, resume file transfers. "
0,Distinguish service-failures from failed-service-results for Non-Hadoop Services,Same as https://jira.duraspace.org/browse/DURACLOUD-512 but for non-hadoop services that operate on lists of items.
0,"When editing multiple space properties,  it is possible for users to attempt to update read only spaces.",To reproduce this problem:  1. select a writeable spaceA and  read-only spaceB (using the checkboxes) 2. click edit properties 3. add a tag or name value pair and save.  4. duradmin displays the error dialog (unauthorized action error) 
0,"When logged in as a non-admin,  the multi-space delete button should not be visible.",To reproduce:  1. log in as a user 2. select a space or two using the checkboxes in the spaces list. 3. confirm that a delete button is visible.
1,Verify the SDSC storage provider integration,"This task is to ensure that the SDSC storage provider works as expected, that content can be stored and retrieved, and that everything looks as it should on the SDSC side. Tests should be conducted to verify that all DuraCloud APIs work as expected when executed against the SDSC provider. Tests should also be conducted to verify that the SDSC provider can handle storing and retrieving large amounts of content.  This task requires both continuing the conversation with the SDSC folks as well as direct testing and validation."
0,Set expiration policies for log spaces,"This task is to use Amazon's management console for each of the current DuraCloud accounts to set an expiration policy on the x-service-work space to remove all content items with the ""logs"" prefix after 30 days."
0,Create and test 64-bit AMI,"This task is to: 1. Test the DuraCloud software on a 64-bit instance, to ensure that it works properly, and fix any problems that are discovered 2. Create a production 64-bit AMI as part of the release cycle"
0,Various DurAdmin updates,"Update the following items in DurAdmin:  - Remove the option to ""add one item"", change the ""add many items"" link to simply be ""add items"" - Update the image on the copy button to appear consistent with the other buttons (currently appears to be lower quality) - In the Content Detail panel, make content size number understandable (at least include a unit - ""bytes"") - In the Content Detail panel, set all buttons to be the same color (light gray perhaps)"
0,DurAdmin updates,"Update the following items in DurAdmin:   - Change ""Space ID"" and ""Content ID"" to ""Space Name"" and ""Content Name"" respectively - In the footer of every DurAdmin instance page, ensure that DuraCloud has the ""c"" capitalized. - In the footer of every DurAdmin instance page, the link to DuraCloud.org should come before DuraSpace.org - In the footer of every DurAdmin instance page, replace the current ""Help Desk"" link with a ""Help Center"" link which points to: https://wiki.duraspace.org/display/DURACLOUD/DuraCloud+Help+Center - The title of every DurAdmin instance page (that appears in the browser tab) should be DuraCloud: Login, DuraCloud: Spaces, etc. or something similar (not ""DurAdmin :: DurAdmin: Login"" or ""DurAdmin: Spaces"") - Change ""DuraCloud Administrator"" to just ""DuraCloud"" - Make the Dashboard tab available only to root users"
0,DurAdmin: restrict access to Administration tab,This task is to limit the Administration tab in DurAdmin to be viewable only by root users.
1,Move storage reports onto spaces tab,This task is to break out the information in the storage reports and display it instead on the main Spaces tab. The space-specific charts should be included on the Space Detail page. The provider-specific charts should be displayed in the detail area that is currently shown as blank when selecting the Spaces tab. The top-level all-providers charts will no longer be displayed. Both the provider-specific and space-specific charts should still provide the option to view historical charts and data.
0,"when uploading jpg files with hashmark in file name, the mime-type is not accurately identified.",To reproduce:  1. Run the bulk uploader in duradmin. 2. Select a jpg with a # mark in the filename such as test#1.jpg. 3. Select a jpg with without a # mark in the filename such as test2.jpg. 4. Upload files. 5. Refresh space in duradmin 6. Verify that the mimetype for test2.jpg is image/jpeg (correct) 7. Verify test#1.jpg is application/octet-stream (less correct) 
0,"When uploading with bulk upload tool, if  file is a jpg with hashmark in file name, the mime-type is not accurately identified.",To reproduce:  1. Run the bulk uploader in duradmin. 2. Select a jpg with a # mark in the filename such as test#1.jpg. 3. Select a jpg with without a # mark in the filename such as test2.jpg. 4. Upload files. 5. Refresh space in duradmin 6. Verify that the mimetype for test2.jpg is image/jpeg (correct) 7. Verify test#1.jpg is application/octet-stream (less correct) 
0,Release 1.3.1,"This task is to run the attached space-acl-tool over all production accounts to convert the space-access properties to space ACLs. Note that when running the utility in ""CONVERT"" mode, it simply creates the ACLs based on the space-access property. If you want to remove the legacy space-access properties, the utility must be run in ""REMOVE"" mode."
0,"In the copy item dialog, there is no validation for missing destination space info.","To reproduce:  1. as admin,  remove write access to all spaces within storageProviderA. 2. as user, login, select a content item, click copy, and select storageProviderA.  3. Verify that the destination space drop down is empty. 4. click ok.   5. verify that the call is made despite the invalid data.  Optimal solution:  add default list item to the space select (""Select a destination space"") and display invalid form message when this item is selected."
0,Make the image thumbnails in duradmin larger and improve the layout.,"When viewing an image in a public space with the image server service running, make the thumbnail larger and center the layout."
0,Provide visual feedback for slow loading thumbnails from the image server,"Image thumbnails are generated on the fly and then cached by the djatoka image server. The latency in thumbnail generation can sometimes leave the user wondering what happened to the image.   Providing a visual cue to show that something is happening, albeit slowly, would improve the experience."
0,Retrieval Tool: Space error feedback,"This improvement is to add user feedback in the case where the space(s) being retrieved is inaccessible. When a space does not exist or when the user does not have access to the target space, the retrievaltool output message is as follows: ========================================= Retrieval Tool processing complete, final status:  --------------------------------------  Retrieval Tool 1.4.0-SNAPSHOT - Status -------------------------------------- Start Time: 2012-01-20 09:34:57.106 Current Time: 2012-01-20 09:35:17.108 Retrievals In Process: 0 Successful Retrievals: 0 No Change Needed: 0 Failed Retrievals: 0 -------------------------------------- =========================================  In order to help the user understand why the retrieval did not work, a more descriptive status message would be helpful."
0,UI issues with prefix + refresh,"Part 1: Following these steps:  1. Select a space which has more than 1000 items 2. Enter a prefix into the ""type prefix"" box, hit enter 3. Click the refresh button on the content item listing  Causes a series of error dialogs to pop up, indicating a NullPointerException while DurAdmin is attempting to count the space size. The stack trace reported in the dialog is captured in the attached file.  Part 2: Following the same steps as above, but selecting a space with less than 1000 items causes the ""Space Detail"" area of the screen to blink continually"
0,Service Report Link in Service Details,"This bug can be seen when running a service that creates an output report (such as bit-integrity-tools: generate integrity details for space) in a storage provider other than the primary. For example, run the above service over a Rackspace space.  Once the service is complete, select the link in the service details to show the results.  Notice the report is not found due to the missing '?storeID=X' param."
0,Create HP Storage Provider,"This task is to create an HP Storage Provider. HP is still in its testing phase, so this will allow us to test the new provider more easily, and determine the extent to which it meets the expectations of the DuraCloud application.  HP uses OpenStack, so the code required for this implementation should be minimal."
1,Integrate with Shibboleth-based systems,This task is to make it possible for Duradmin to integrate with existing Shibboleth-based authentication and authorization systems. This is needed in order to satisfy the requirements of Internet2 and DfR.  See discussions related to this issue here: https://wiki.duraspace.org/display/DFR/Shibboleth+Thread
0,Space and content checkboxes are hard to click on,"When trying to select a set of spaces to work on, I often select several spaces and then accidentally deselect them all accidentally by clicking a few pixels off of another checkbox.   The solution here is to add padding to the checkbox element so that it is not necessary to be so precise with the mouse position."
0,Eventual Consistency on Secondary StorageProviders: CreateSpace,"This improvement is to increase the probability that the space creation method will work on secondary providers. Currently, the S3StorageProvider has a wait cycle in the createSpace() method that retries adding the space-metadata file until the bucket has been created and registered. A similar wait cycle should be added to the createSpace() method (or rather appropriate 'setSpaceProperties' method) of: - RackspaceStorageProvider - AzureStorageProvider"
0,Add Space Dialog,"This task is to address the following bug: When selecting ""Add Space"", the flyout allows for entering the new space name. When hitting the ""enter"" key instead of the ""Add"" button, a pop-up with the following error is displayed: error: space is undefined "
0,Exception on Space Delete,"This task is to fix the bug that causes an exception to be thrown when deleting a space either in duradmin or via the REST api. With any storage-provider, when deleting a space in duradmin, a pop-up with the following message is displayed: failed to delete space!  In looking at the tomcat log, an error is also shown when using the REST api. In both cases, however, the space is actually deleted."
1,Duplicate on Change at Space-Level,"This improvement is to update the Duplicate on Change service to allow configuration at the space-level. Namely, the service should be able to run on a subset of the total spaces of an account. It would be ideal if this improvement also allowed the spaces over which Dup-on-Change is configured to copy from the primary to any of the secondary providers."
0,Display bit integrity results on each space,"Services are being automated and users are no longer required to run the bit integrity service themselves, but they still need to know that their content is being verified. This task is to include on the UI, likely in the space details area, an indication of when the most recent successful bit integrity check was run over the content in that space.  In the event that the most recent bit integrity check resulted in an error, we (DuraCloud staff) will need the opportunity to investigate the issue and correct it if possible before the issue is displayed to the user (this may also be done in an automated fashion.) If it turns out that the error requires user intervention, then it would be nice to be able to have that be indicated in some way on the UI, calling out specifically which files need to be addressed."
0,Refactor RackspaceStorageProvider to OpenStackStorageProvider,"This improvement is to refactor the RackspaceStorageProvider to OpenStackStorageProvider. Since we currently have three storage-providers that all expose an OpenStack API, the RackspaceStorageProvider (which is coded to the OpenStack API) should now be abstracted up as the base class of the three implementations: Rackspace, SDSC, and HP. Each of these inheriting classes should have their own names displayed in error and logging messages."
0,Streaming URL for spaces streamed by the Media Streamer,"Release 2.0 added the ability for users to select a button on a space and turn on media streaming for that space. This task is to ensure that the streaming URL provided by the Media Streamer service is added to the metadata of the space when it is being streamed, so that the user has access to it."
0,Media Streamer: remove Viewer space,"Since services will no longer be run directly by users, there is no longer a need for the Media Streamer service to produce the contents of the Viewer space. Those generated files are not used as part of the service, they are only provided as examples for how a user might integrate the media streamer service with their own website.  This task is to: - Add a copy of the generated files (updated to be more generic) with some surrounding instructions to the DuraCloud documentation - Update the Media Streamer service to no longer generate these example files"
0,Sync and Retrieval Tool encryption support,"After encryption support is added to the StoreClient, the Sync and Retrieval tools need to be updated to allow the user to choose to enable content encryption and to supply the necessary encryption key."
0,Add user-id to content messaging,"This improvement is to include the authenticated user-id of the user performing content actions over the durastore API. At the least, the user-id needs to be included on the ingest event message.  This is in support of the DfR: Object Creation Service."
0,Update Media Streamer Service to not Require Sources,This task is to remove the requirement of the media streamer service to have a source space.
0,show content manifest head in space detail with link to full manifest,"The purpose of this feature is to guide the user to the content manifest for the space as well as enable them to peek at the recent activity.  The ""peek"" might be equivalent to head -10 audit-log."
0,Snippet Generator for Streaming Content,It would be nice to have way to generate a snippet for streaming audio/video  that can be embedded directly into any page. 
0,"Formatting on ""upload-tool"" space","The display on the Spaces tab causes any space named ""upload-tool"" to be formatted incorrectly. This is because the <div id=""upload-tool""> that is created for the space on the page happens to match up with a css entry for #upload-tool (which is meant for a div in upload-tool.jsp). As it turns out, there are several space names which can cause odd UI behavior for the same reason, such as: ""spaces-list-view-resizer"", ""footer-content"", ""logo-ds"" (which is particularly strange looking), etc. There should be a strategy for making sure that these kinds of name collisions don't happen.  The attached file shows how the display reacts to the ""upload-tool"" space."
0,Media Streamer on/off button in the UI,NULL
0,Executor Client,"This task is to create a java client (similar to storeclient, serviceclient, and reportclient) to simplify interactions with the Executor REST API."
0,Add manifest client,This task is to create a Java client for calling the manifest REST methods in DuraBoss.
0,Consolidate DuraBoss Interfaces,"This improvement is to refactor the Executor, ManifestGenerator, and Auditor interfaces to a common project that can be shared between DuraBoss and the various DuraBoss clients. Additionally, these interfaces should be implemented by the utilities themselves as well as the clients."
0,Make charts zoomable,"Space pie charts should be zoomable, displaying a larger graph in an overlay."
0,Make Spaces pie charts navigable,"Space slices in the cross spaces pie charts should be clickable.  On click, the user should navigate to the same time slice but on the space detail history component."
1,Executor: Media Streaming,This task is to implement the Media Streaming Handler in the Executor. Design notes can be found here: https://wiki.duraspace.org/display/DSPINT/DuraBoss+Design
2,Executor: Bit Integrity Checking,This task is to complete the Bit Integrity Handler in the Executor. Design notes can be found here: https://wiki.duraspace.org/display/DSPINT/DuraBoss+Design
0,Create Auditor Client,"This task is to create a java client (similar to storeclient, serviceclient, and executorclient) to simplify interactions with the Auditor REST API."
0,SystemUserCredential Across Webapps,"This improvement is to create the SystemUserCredential in a similar was to how RootUserCredential is created, via a default username/password over-written by system properties. Once this is done, DuradminExecutorImpl.java should be evaluated for exchanging root-user with system-user."
0,Merge common-notification into notification project,"This task is to move the code which currently exists in the common-notificaton project into the notification project. An update should be made to inject AmazonNotificationFactory into EmailNotifier in DuraBoss, so no dependency on notification-amazon is required in the notification project. As part of this transition, the implementation classes should be pushed into the package org.duracloud.notification.impl."
0,Newly uploaded content items do not show up on clicking the space.,To reproduce:   1. Open a space. 2.  Upload content into a space. 3. Click on the space again in the space list. 4. Notice the newly uploaded content is not showing up.
0,Add multiple tags at once to spaces and content,It should be possible to enter multiple tags at once. A tag should not contain spaces. Let us modify the UI so that you can enter a list of space or comma separated words that get entered as separate tags.
0,Tag and Metadata deletes are not being serialized,"Tag and Metadata deletes are not being serialized. If one deletes multiple tags/key value pairs in rapid succession, you'll notice that some items that were deleted reappear upon refresh.   Solution:  use deferreds to serialize calls."
0,Deselecting all content items after having selected one or more does not cause the detail pane to show the space detail as it should.,To reproduce:   1. select a content item using the checkbox. 2. deselect it. 3. notice the space detail pane is not being displayed.
0,The checkbox used to select all items in the content items list should be cleared when selecting another space,"1. select a space. 2. select all content items. 3. select another space. 4. notice the ""select all"" check box is still checked."
0,Refactor revision 914 so that it is not necessary to resolve the primary content store id on every call to get space and content detail info,Probable solution will involve caching the primary content store on the client.
0,Manifest for empty space,"Currently, when a new space is created, then a call to ask for the manifest is made, the response is a 404. Instead, the response should be a 200 and an empty manifest list should be returned.   This could be resolved by creating the space audit log on space creation, rather than on the addition of the first content item."
0,Chart Labels are off on panel resize,To reproduce:  1. Display a space 2. Make the Space Detail pane wider. 3. Note how the line graph labels are skewed. 
0,Secondary Provider Audit Entry Sequencing,"This issue is to resolve the bug occasionally seen in the content manifest when running dup-on-change. Sometimes ingest and delete messaging events lose sequence on the secondary provider.  To create this scenario, an item must already exist in a space, then be deleted and immediately re-ingested. It is not a common production scenario. The attached audit log demonstrates such a case. Note, the actual content in both the primary and secondary stores were actually the same, but the secondary store does not show file-6.txt in its manifest:  space-id        content-id      MD5 manitest        file-0.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-10.txt     b1946ac92492d2347c6235b4d2611184 manitest        file-1.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-2.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-3.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-4.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-5.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-6.txt      b1946ac92492d2347c6235b4d2611184 manitest        file-9.txt      b1946ac92492d2347c6235b4d2611184 "
0,Exceptions being swallowed by javascript  on space retrieval in Duradmin,See line 1080 of spaces-manager.js  Notice only a 404 error is being handled.   
0,Enable Duradmin to run separately on its own instance.,See attached error:  There appears to a problem calling remotely into duraservice via duradmin.
0,Storage pie charts in duradmin/spaces should display file size and count totals,Add 
0,Update DuraBoss security-config.xml,This task is to update the security-config.xml file in DuraBoss to appropriately cover all parts of the DuraBoss REST API
0,Update to the new Service Plans,"Once the new set of service plans have been nailed down, this task is to make any updates that are necessary to make new plans available and remove unneeded plans."
0,Logback Memory Leak,"This improvement is to determine a way to allow logback in the DuraCloud webapps to  - monitor for logback.xml updates without requiring application server cycling, and - not create memory leaks The memory leak can be seen by re-deploying the DuraCloud webapps and noticing that there are warnings in catalina.out and that the old processes are not destroyed. "
1,SDSC as Primary Storage,"This task is to determine the feasibility of using SDSC as a DuraCloud primary store. Beyond function feasibility, the performance differences of an EC2 to SDSC configuration should be compared to the standard EC2 to S3 configuration.  As a side note, it would be interesting to explore SDSC's openness to supporting ""salted"" server-side checksums."
1,Chronopolis REST Server Client,This feature is to create a java client for the Chronopolis REST Server. https://github.com/msmorul/chron-notifier  Further details of the usage of the Chronopolis client can be found here: https://wiki.duraspace.org/display/CHRONO/DuraCloud+Chronopolis+Integration 
1,Chronopolis Utility Integration,This feature is to leverage the Chronopolis REST client to integrate the business tier of requesting: - backups to Chronopolis - restores from Chronpolis - notifications on errors and completions - polling of in-process requests
0,Chronopolis UI Updates,This feature is to integrate the ability to request backups and restores to/from Chronopolis into Duradmin. 
0,"Do not display blank ""Size"" property for spaces","The ""Size"" property for spaces (the number of bytes stored in that space) is always available for spaces in Rackspace, because this information is provided, but is never available for spaces in Amazon and some other providers. This task is to remove the ""Size"" property from the space display when it is empty."
0,Buttons in Space and Content detail should wrap,"Currently, when viewing DurAdmin on a small screen or window, buttons in the Space Detail and Content Detail areas can be unavailable. Since there is no horizontal scroll bar, it's not obvious that those additional buttons even exist. Looking at the attached screen shot, it is not obvious that there are View and Delete buttons which do not show up on the screen.  This task is to allow these buttons to wrap, so that they will remain available even as the screen size diminishes."
0,Label Storage Graph Axes,"This improvement is to add labels to the two y-axes of the ""History"" ""Byte and File Counts Over Time"" storage graphs. Currently it is not exactly clear that the left-hand y-axis represents cumulative file size, and that the right-hand y-axis represents cumulative file counts."
0,Item-level Bit-Integrity Timestamps,"This improvement is to record the timestamp at the item-level for bit-integrity runs. This would allow for providing the ""green bar"" of bit-integrity health for each item. Likely, the timestamp would be persisted during the bit-integrity runs in each content line-item in the service-generated report."
0,Bagit Manifest: Two Spaces,This improvement is to make sure that the ManifestGenerator for Bagit manifests delimits the checksum from the path with two spaces instead of the current single space character.
0,Top-level Default Error Page,"This improvement is to render a default error page at the apache level. For example, https://subdomain.duracloud.org/junk  should display a more user-friendly page."
0,DuraCloud 2.0 UI doesn't load when using Opera or Internet Explorer web browsers,"When accessing DuraCloud from Opera or IE, the UI doesn't load properly and is ""unusable"" (i.e. you cannot do anything).  Both browsers provide the same result.  Here's a summary of what happens: 1. Visit http://[your-site].duracloud.org/ using the Opera or IE 2. Login -- this works fine 3. You are presented with a screen which shows no Spaces or content items. (I.e. it looks as though all your content has disappeared).  At this point, there's nothing you can do except logout.  This occurs on both Mac OSX and Windows 7 when using Opera, and on Windows 7 when using IE."
0,Bulk Upload Applet Doesn't Work or Hangs on Multiple Browsers,"From the last All Staff Retreat, I know this is a ""known"" issue. But, I wanted to track the scenarios that I've discovered thus far.  The Bulk Upload Applet responds in the following ways when loaded in the following environments: * Mac OSX (10.6.8) + Firefox 11.0 - Initially, saw a ""permanent"" hang. I had to kill Firefox to continue. However, after restarting Firefox, on second attempt it loaded almost immediately. * Mac OSX + Chrome 18.0 - Temporary hang (1-2 mins), eventually loads properly. * Mac OSX + Safari 5.1.5 - Loads almost immediately. * Windows 7 + Chrome 18.0 - Loads in about 20-30 seconds * Windows 7 + Firefox 12.0 - Applet never seems to load & I always see a ""blank"" pop-up window (just includes Duracloud header/footer)  UPDATE: Actually the issue was that the Java Plugin was disabled. See first comment below.  Just as a note, Internet Explorer & Opera also likely should be tested once DURACLOUD-694 issue is resolved."
0,Pie charts not visible on history line graph data point click,"When an admin is logged in and looking at the space detail, the pie charts will display below the fold.  In order to see them, the user scrolls down.   However when the user clicks on a line chart data point, the pie graphs will be updated, but the user will not notice because they are not visible.   It would be much better if the history line chart would scroll to the top of its container when the line chart is clicked."
0,Allow admin to change custom logo by dragging and dropping it into place.,The current method of replacing /x-duracloud-admin/logo is not so user friendly.
0,back and forward browser buttons do not work in the spaces manager in IE 9,To reproduce  1) click a couple of spaces. 2) then uses the browser back button. 3) notice it did not go back as expected.
0,Space Names with '--',"Currently, when using either duradmin or the durastore REST API directly, creating a space which includes '--' behaves unexpectedly. For example, create a space named: 'test--x--space'  It shows up in the space listing as expected. Then click the spaces tab, and it turns into: 'test-x-space'  Likewise, when creating similarly named space via the durastore REST API.  In both cases space shows up in the listing as well as in the AWS S3 Console as having a single '-'. It would appear as if the transformation of '--' into '-' is happening at the durastore level.  This task is to allow durastore to permit space names with '--'. "
0,UI: Space count causes bit integrity notice to be removed,"On spaces with a large number of content items, there is an ajax call made to perform the count of items in the background. This number is updated in the space properties as the count continues, and is eventually set to indicate the final number.   When a space has had a successful bit integrity run, a notice with a green background is displayed in the space properties, with a link to the results of the bit integrity run.  In situations where both of these occur (a large number of items, and a successful bit integrity run), the completion of the item count causes the space properties to be reloaded, which removes the notice about the bit integrity run. This task is to ensure that the bit integrity notice persists through the item count update."
0,Delete space button available to user,"When logged in with ""user"" role, space delete button is available/visible. After clicking to delete space, an error pop-up window appears that states ""space delete failed."" Ideally, a person logged in with ""user"" role would be unable to delete the space, but would have an easy way to delete ALL the contents stored within a space. See attached screenshot for error pop-up window."
0,Bit Integrity Tools service: Create report on empty space,"Currently the Bit Integrity Tools service, when run over a space with 0 content items, indicates a link to a report, but does not actually generate the report. This task is to ensure that a report is generated, even when no content items are in the space."
0,UI: Remove image preview,"Currently the image preview section of the UI displays an icon (with mountains) when either the Image Server is not on, or when the space is not publicly viewable. This tends to prompt lots of user questions around why they cannot view an preview. This is especially an issue for multi-tenant accounts, where the user is not able to start the Image Server or make their space publicly viewable. This task is to remove the image preview area of the content item display."
1,Secondary storage providers should be Read-only via the Rest Api,"As the synchronization of content between storage providers becomes automated (DURACLOUD-642), it becomes unnecessary for users to edit content in their secondary provider. In order to maintain an assurance that content remains consistent between providers, the ability for users to make changes at their secondary storage provider should be removed. They would still be able to view and download all content, but no add/update/delete operations would be allowed.  This task is to update the REST API to ensure that users with role user/admin/owner do not have write access to secondary stores."
0,500 on Image Content Selection,This bug can be seen when quickly clicking between content items with image mimetypes ('image/*') within duradmin. A 500 error is presented to the user.
0,UI: Space counting issues,"This item covers two issues with the space count that occurs on the space properties of spaces  with more than 1000 items: 1. The count now often starts at 0+. The count used to consistently start at 1000+ or 1999+. 2. When the count completes, it very often restarts and performs the count a second time, always ending with the same result."
0,SyncTool: Better handle periods of time when the DuraCloud instance is unavailable,"Update the sync tool to be better able to handle periods of time where the instance has become unavailable. This is particularly important to allow the SyncTool to continue to run normally across DuraCloud instance upgrades.   A likely strategy is: When a transfer fails, check to see if the instance is available before doing a retry, if its not, then wait for ~5 min and try again. When the instance becomes available again, resume file transfers."
0,UI: Display of content items names which include #. characters,"If a content item with the name ""#.txt"" exists in a space, then when it is selected in the UI, the content detail will display for a moment, but will then act as if the spaces tab has been selected. It appears that both the hash and period characters need to be in the content ID, but other characters can be added and the behavior remains the same."
0,SDSC Auth Endpoint,"This improvement is to switch the authUrl found in SDSCStorageProvider from:   ""https://cloud.sdsc.edu/auth/v1.0  to:   https://duracloud.auth.cloud.sdsc.edu/auth/v1.0"
1,SyncTool --dryrun,"This improvement is to create a ""dryrun"" flag on the synctool.  This flag would tell the synctool to go through the steps of determining which files should be sync-ed up to DuraCloud (including deleted and modified content), but instead of actually performing the sync, just a report will be produced detailing which content would have been sync-ed."
0,Synctool - Java7: SSLProtocolException,"With Java7, the following exception is thrown when trying to use the Synctool in any standard configuration: Exception in thread ""main"" java.lang.RuntimeException: Could not create connection to DuraStore due to Error retrieving content stores. handshake alert:  unrecognized_name  The issue and workaround are posted here: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7127374"
0,Use /mnt on EC2 Instances,"This improvement is to leverage /mnt on EC2 instances for temp files, at the least. /var/log/tomcat6 should link to /mnt  An analysis should also be performed to determine if /home or the webapps should use /mnt as well."
0,Limit Tomcat logs,"This task is to update the log management settings for the Tomcat server on DuraCloud AMIs. These settings are contained in /etc/logrotate.d/tomcat6.  Log rotation settings should be updated to rotate on size (perhaps 256M) rather than date, and limit the number of rotations which are kept (to perhaps 3).  A full list of options for these settings can be found with ""man logrotate""."
1,Sync and Retrieval Tool metadata support,This task is to update the Sync Tool and Retrieval Tool to be able to handle content tags and metadata. A DuraCloud space should be able to be pulled down to the local file system using the Retrieval Tool and pushed back up to a new DuraCloud space using the Sync Tool without suffering any loss of metadata.
0,Initializing an instance to use locally stored services is not working,"Performing an instance initialization by pointing to the local service repo (on the same host) no longer works. The issue is that the Executor in DuraBoss attempts to make calls to DuraService during initialization, but gets a 401 (unauthorized) error which halts the initialization process."
0,Audit Log on Space Delete,"This issue is that the audit log (i.e. x-duracloud-admin / audit/storage-audit-log-<space>-<time>.tsv) does not detect deleted content when an entire space is deleted.  Scenario: # Create a space # Add content to the space # Check the content manifest ** curl -u user:pass https://host.duracloud.org/duraboss/manifest/<space>?storeID=X ** note, the manifest is only written every 60seconds, so you may need to wait before the content is contained in the manifest # Delete a content item, and recheck the manifest ** the manifest should not show the deleted item # Delete the space # Check the content manifest ** BUG: the manifest still shows the items that were in the space prior to the space deletion "
0,Auditor not passing tests on Windows,"The Auditor project produces many errors and test failures when being built using ""mvn clean install"" on Windows. This is due to the use of colon characters in created file names (which are date based), and file path separator issues."
0,Auditor: Log re-creation on large spaces is failing,The auditor action to re-create the audit logs for all spaces is failing when being run over large sets of content. This has been seen on several instances where content within a space pushes into the hundreds of thousands of items.  One suggest reason for this behavior is that the thread being used to create the log may run out of memory and exit.
0,Move DefaultFilter from duradmin to common.,The motivation for this refactoring is to make the aforementioned useful class available to other web applications such as dfr-sync and mc.
0,Double approval for content item deletes,"When a user chooses to delete a content item in the UI, a pop-up window should confirm the action and then re-confirm the action (indicating that the secondary copy will also be deleted because of this action). Ideally, the double warning will protect from some ""accidental"" deletions of content through the UI. This will not extend to the sync tool or the REST API."
0,Audit log not updated: SDSC Restore,"This issue is that when Chronopolis restores content into an SDSC container, the Audit log is not updated. This is due to the fact that back-dooring the content ingest by-passes Durastore and the messaging that the Auditor depends on."
0,Duplicaton on Demand testing,"Several runs of the Duplication on Demand service have failed while attempting to transfer a large content set (500,000 files) from S3 to SDSC, for Chronopolis testing.  This task is to review the failures, and perform further tests to determine the cause of the issue."
1,Duplicate-on-Demand via Inspector/Executor flow,"As noted in testing described here https://wiki.duraspace.org/x/8YzvAQ, the work of transferring very large numbers of files from one storage provider to another is not particularly well suited to hadoop. This task it to create an alternative content duplication method.  The requirements are as follows: - Must be able to move content from any provider to any other provider - Must be able to be deployed onto multiple server instances in order to share the load and speed up processing  It may be necessary for this to be a multi-step process. Perhaps something like: 1. Run a process with the source space and the number of instances that will be used for the job as parameters, and it splits up the work into an input file for each worker node. 2. Each worker node is spun up and initialized with the proper information to do the work, including their portion of the job, as defined in step 1. 3. After all nodes have completed step 2, a final process verifies the transfer by comparing the source and destination spaces.  The goals of this work: 1. Provide a method which can reliably transfer large numbers of content files from one provider to another. 2. Provide a starting point for the further work of developing a large file processing capability within DuraCloud that does not rely on Amazon EMR."
0,Sync Tool: CPU Utilization,"The Sync Tool, when run, tends to use a very large percentage of the CPU. As noted by SaaS4, running the Sync Tool continuously for 10 days kept the CPU on their server pegged at 100% the entire time.  It seems likely that the high CPU utilization is due to the constant generation of MD5 checksums for each file. This would be a particular issue when the file set contains a large number of very small files.  This task is to investigate the reason behind the heavy CPU usage. Ideally a fix could also be included."
0,Sync Tool: Limit local logging,"The Sync Tool currently generates three logs: complete.log, duracloud.log, and synctool.log. It was noted by the SaaS4 customer that these logs together took up an unnecessarily large amount of hard drive space. They noted that over 10 days, after pushing 500 Gb of data, the logs totaled around 300 Gb. This is clearly too much space consumed by logs.  This task is to limit the log outputs. This could include removing the complete.log, as well as ensuring that logs are cycled properly, so as to not consume more than a set amount of space."
0,Sync Tool: Exclude list,"This task is to allow for passing a list of files to the Sync Tool which should be ignored when performing a sync. The simple case is for the file to include one full path to a file per line. Any file in this list would not be pushed to DuraCloud. Directories in the list would be excluded as well, along with all files and subdirectories in each directory.  A more complex implementation would allow for using wildcards to indicate files that should not be included. The capabilities of rsync should be reviewed in order to determine the most convenient syntax for the exclusions."
0,Loading throbber disappears before content item is loaded.,"Not a huge problem, but is slightly confusing if there is a lot of latency:  1.  click on a space and then click a content item. 2. notice that the ""loading..."" throbber flashes and then disappears before the new content is loaded.  "
1,SyncTool: Preserve local file timestamp information,"Based on feedback from the State of North Carolina, they would like the sync tool to preserve the local timestamp information for individual files (both the creation and modified dates/times). They would like this information passed into DuraCloud via the sync tool, available for viewing within DurAdmin, and then retrieved/restored properly by the retrieval tool when content is downloaded."
1,Java 7 support,"This task is to transition DuraCloud to be built and run using Java 7. There are a few reasons for this transition:  1. Oracle has announced that Java 6 will hit end-of-life in Nov 2012 (http://www.oracle.com/technetwork/java/eol-135779.html) 2. Java 7 has been available for over a year, and is becoming more prevalent 3. Java 7 has new capabilities. One specific need is the capability (in NIO) to read/write file creation, modification, and access timestamps 4. We need to support customer use of Java 7 with our client-side tools (e.g. sync tool)"
0,Timestamp collection utility,"This task is to create a utility which can loop through a set of directories and capture the timestamp information (Creation Date, Modified Date, Access Date) on each of the included files. That timestamp info should be written to a file (so that it can be archived locally) as well as written as metadata to each files's corresponding DuraCloud content item. (The expectation here being that the Sync Tool has already done the work of moving each file to DuraCloud.)  The capability to capture time stamp information was added to the NIO package in Java 7 (http://docs.oracle.com/javase/7/docs/api/java/nio/file/attribute/BasicFileAttributes.html), thus this utility will need to be built and run using Java 7.  This task is in direct response to needs expressed by the State of North Carolina archives and libraries. Everyone in their offices uses Windows (either 7 or XP), so this utility needs to be tested on Windows."
0,Bit integrity comparison reports have incorrect header,"The reports generated by Bit Integrity Checker and Bit Integrity Bulk have incorrect headers. The header value is ""space-id <\t> content-id <\t> MD5"" but should be something like ""space-id <\t> content-id <\t> MD5-file <\t> MD5-provider <\t> comparison-result""."
0,UI: Number of items displayed count not always updated properly,"The ""Showing X of X"" text at the top of the Content Item pane (with a space selected) is not always correct. Examples:  1. Select a space, notice text at top of content list: ""showing 1 - 200 of X"", switch storage provider, notice text is not cleared 2. Select a space, notice the ""showing X of X"" text. Select several content items and delete them. Notice that the ""Showing "" numbers don't change."
0,Sync Tool: Clean up log output,"The Sync Tool currently logs a variety of ""errors"" when it is running normally. Many of these log statements have to do with calls to determine if a file currently exists in DuraCloud. The attached file includes errors showing up in the log (provided by users at UTK.)  This task is to clean up the logging outputs so that only actual errors are logged as such."
0,DuraStore: Skip sending event messages for content in administrative spaces,"When any event occurs in DuraStore (space/content is added/updated/deleted) a message is sent. This task is to update that mechanism to skip sending those messages for the ""administrative"" spaces in DuraCloud (x-duracloud-admin, x-service-out, x-service-work).   There is currently no need for updates on administrative files, and dealing with all of the messages generated by admin file updates wastes effort."
0,Sync Tool: Add option to not overwrite changed files,"This task is to add an option to the Sync Tool which ensures that it does not overwrite any existing DuraCloud content. This means that when a file on the local file system, which has previously been added to DuraCloud, changes, it is not simply uploaded again.  There are a few choices that need to be made for this new feature to be added: 1. When a file already in DuraCloud changes on the file system, is the updated file transferred to DuraCloud or not. If so, the original file will likely need to be renamed to prevent collision. 2. How is the list of updated files communicated to the user. Are they printed to the console (stdout), or written to a file? Is this consistent between interactive mode and run-to-completion mode? 3. How does this feature impact the upload tool?  A comment from the State of North Carolina about how they envision this feature:  ""Our ideal upload tool would allow us to specify a local directory to update in the cloud, and the tool would then identify and upload new files (new files defined by file name and path), and alert us to any files that have changed (same file name and path but new checksum hash) and not upload the files that have changed (or upload them in a way that doesn't overwrite the files in the cloud). We would want to make sure that the process of uploading content is kept totally separate from any risk of editing existing content in storage."""
0,"Dup-on-Change: Add an option for default duplication setting, to handle newly created spaces","With the updates to the Duplicate-on-Change service for 2.1, it no longer duplicates newly created spaces. This is fine for multi-tenant instances, but not for enterprise instances, where users may be adding new spaces frequently."
1,Download Multiple Items as a zip or tar archival file.,"This feature sprung into my mind:  it is not driven by user requests, but it seemed that it would be very handy especially once we have a search index for listing content items.   The idea is that it might be useful to be able to download multiple content items in one zip archive.  This would be especially useful if one needed to extract a large number of small files. You could select items either based on a search query or a list of contentIds.  I was envisioning it would work both at the durastore as well as duradmin levels.  You would issue a request for an archive and immediately receive a handle.  One could optionally specify an email address. The handle would enable you to get the status of the request.  When the request is complete, if an email address had been specified,  the user would receive an email with a link to the download page.  Otherwise the download URI would be available via a status query using the request handle.  Once the archive had been downloaded and verified the user could then make a call to remove download site.  If no call was made after a standard expiration time, the download would be removed automatically.  Regarding limits: Initially the total size limit would be constrained by the available server side disk space.  Obviously this is not going to work for extremely large downloads, ie greater than 100 GBs.   "
0,UI: Large bit integrity reports cannot be displayed,"For spaces with a large amount of content, the UI option to view the service report in a nice table does not work. This option is presented in the UI on the green health check bar, as well as on the services tab. If the ""report"" link is selected for one of these large spaces, the UI brings up the white display box, then the user waits for a rather long time (while DurAdmin is attempting to read the entire report file into memory so that it can be displayed) and eventually an error is shown indicating that not enough memory is available.  To fix this, the size of the report file should be checked. If the size is over some reasonable threshold, the report link should perform a file download (of the raw report) rather than attempting to display the chart."
0,UTF-8 Support in Content Properties,It appears that UTF-8 characters are not supported in Content Properties.  Test: add the following properties to a content item: ĽĔĘŔĴêť ﯔﷲﮘﺉﺊﺴﻅ ᾡὉᾧ―♫♪♫
1,AWS Glacier,This feature is to expose AWS Glacier as an option for content storage. http://aws.amazon.com/glacier/
0,Secondary storage provider should be read only via the UI,"As the synchronization of content between storage providers becomes automated (DURACLOUD-642), it becomes unnecessary for users to edit content in their secondary provider. In order to maintain an assurance that content remains consistent between providers, the ability for users to make changes at their secondary storage provider should be removed. They would still be able to view and download all content, but no add/update/delete operations would be allowed.  This task is to update the DurAdmin UI to ensure that users with role user/admin/owner do not have write access to secondary stores."
0,Message consumers disconnecting,"This issue is that for a few different reasons the JMS connections which are used to listen to messages from the brokers hosted by DuraStore/DuraService can disconnect and cease to receive any messages. This has been recorded in the Duplicate on Change service, as seen in the stack traces in the attached file. This results in the Duplicate on Change service being rendered essentially useless. This is likely also to be the cause of similar issues where the Auditor stops recording changes in the audit log."
0,Content properties are being overwritten when content stream changes.,"When the content of an item changes locally and is resynced to duracloud, the custom metadata (properties and tags) are wiped out.  To reproduce follow these steps:   1. add a file using the upload applet 2. add a custom property and tag to the content item. 3. change the file on your local machine by adding text to the end of it. 4. resync it using the upload applet. 5. verify that the content metadata is no longer there.  Another symptom of this problem:  if you log in as userX on the initial content item creation and then login as userY when uploading the changed content item, notice ""creator"" property changes from userX to userY. "
0,UI: Check box near space name,"In DurAdmin, each space name has to its left a checkbox. Users expect that selecting the check box will perform the same action as selecting the space name text. Instead, selecting the checkbox (of one space or more) brings up a multi-select display.  A user from UCAR/NCAR was recently caught by this issue, and could not understand why the listing of content items was not showing up until they contacted Carissa for assistance.  This task is to discover and implement an alternative to the current UI functionality described above which will be more easily understandable.  From Carissa: Three users within the past month have had the misunderstanding that selecting the checkbox next to a space name would then allow them to view their content. Each time I had to field either a support email or call and explain that they should *not* select the checkbox next to the space name to view the content, add content, edit content, etc. but that they should only click on the name to select/highlight the space to view the content.   Ideally either clicking directly on the checkbox or the space name will select the space (add the checkmark) and both actions will behave identically: they will display the content of the space with all the other associated actions. The two actions that we lose by doing this are the ""delete selected space"" and ""edit properties"" on the space level. I believe we should determine another way for those options to appear."
0,Sync Tool: Hidden password option,"This task is to provide an option for the Sync Tool which will allow users to supply their DuraCloud password in a way other than via the command line options. The reason this is an issue, is because the full command used to run the sync tool remains available in the process list of the machine on which the sync tool is running.  The suggested fix for this concern is to allow users to define an environment variable which contains their DuraCloud password, which would be picked up by the Sync Tool if the -p option is not provided. "
0,Make use of durable subscriptions for messaging connections,"This task is to introduce the use of durable subscriptions in place of standard topic subscriptions in order to have a better assurance that messages are not lost in the event of an interruption of a JMS connection. The failover directive, which is currently in use, allows connections to be re-established, but there is the potential for messages which were sent during the time when the connection was down to be missed.  More information can be found here: http://activemq.apache.org/how-do-durable-queues-and-topics-work.html http://activemq.apache.org/manage-durable-subscribers.html"
0,Auditor not receiving messages,"The auditor, as well as the service reporter (both part of DuraBoss), are not receiving messages in 2.1.1 instances."
0,Notification email address validation needs work,"As reported by Dan Galewsky, the regex being used to validate the From email address in DuraBoss (which is used to send notification emails on events) does not properly handle all valid email addresses. In particular, the example provided is that setting this value in init.properties:  duraboss.notification.0.originator=user@austin.utexas.edu  causes this error  fromAddress not valid notification: user@austin.utexas.edu  The issue is that the regular expression used in AmazonNotificationFactory.java fails on some valid email addresses, like the one above."
0,Ability to easily share links to DuraCloud content,"There should be a simple way to share content that is stored in DuraCloud. Something like a ""share"" button/feature for spaces/content items that are stored in DuraCloud would likely work. Initially, this could just be for content in open spaces. This would keep users from having to manually find the storage URL for a content item and then find a way to share it. Instead, the user could be presented with a pop-up window that could be configured to send an email or, perhaps, generate some embeddable html code.  This feature would make DuraCloud a convenient platform for sharing large files that were otherwise difficult to share, as they are too large to email or pass around via IM.  This feature would be notionally very similar to Dropbox Links (https://www.dropbox.com/help/167/en).  "
0,SyncTool error,"This issue was reported by Bryan Beecher on Dec 20, 2012. He was, at the time, using the latest Sync Tool from trunk:   I'm seeing a seemingly unending series of errors like this:  at java.util.SubList.add(AbstractList.java:652)  written to stdout/stderr in the window where I run synctool.  I don't see anything obvious in any of the synctool log files to account for the error, and I can't read the status display from the synctool to tell if things are working despite the error, or if the train has gone off the tracks.  The errors persist across restarts.  I've tried it a couple of times now, including removing the contents of the log directory.  Reverting back to version 2.0.0 cleared up the problem."
1,Use S3 bucket tags for system space properties and remove feature allowing users to set space properties.,"The new Glacier storage provider brought back to light the missing ability in Amazon S3 to set properties at the bucket level. Up to this point, we have used a file in each bucket to store DuraCloud space properties, but this will not work in the Glacier model, as that file will be archived to Glacier along with all content.  As no current customers are making use of space properties, this issue can be resolved in two steps: 1. Make use of S3 bucket tags to store the properties generated by DuraStore for each space created 2. Remove the SetSpaceProperty feature  Step 2 is necessary because of the limitations of S3 bucket tags. Only 10 tags are allowed, none more than 256 characters. While it would be possible to allow for a very limited number of metadata/tags provided by the user using a limited character set, the fact that this feature is not currently in use suggests that it is simply not needed."
0,Bit Integrity Tools: Service report not created for empty spaces,"When running the Bit Integrity Tools service over a space with 0 content items, it completes successfully with a link to a report file, but that report file does not actually exist.  Note that this is the same problem which was reported and resolved in DURACLOUD-704. It appears to have been re-introduced.  This task is to ensure that bit integrity reports get generated and stored in x-service-out for empty spaces."
0,Remove Space Properties and Tags in Duradmin,"In order to support Bill's strategy for glacier, see DURACLOUD-767"
0,UI: Expose the ability to retrieve content items from Glacier in DurAdmin,DURACLOUD-754 adds the ability to use Glacier as a storage provider. Glacier requires additional steps to be able to retrieve content (as compared to other DuraCloud storage providers). This task is to expose those additional steps via the UI.
0,Glacier: provide a way for users to perform limited file retrievals,"The Glacier storage provider allows for very low cost storage, but retrieval costs can add up very quickly. This task is to limit the ability of users to request content retrievals in such a way as to minimize the risk of high costs."
1,Allow Glacier to be used as a primary storage provider,"This task it to allow Amazon Glacier to be the primary provider in DuraCloud, rather than Amazon S3."
1,Pull the UpSync tool into the DuraCloud baseline,"This task is to copy the UpSync tool from the DfR codebase into the DuraCloud codebase in order to expose it as a DuraCloud feature and in order for ongoing development to occur.   Once transferred, the tool should be tested with DuraCloud by both technical and non-technical staff to ensure that it can be easily installed, it functions properly, and that there are no usability issues."
0,UI: Make client-side tools easily available,"This task is to update DurAdmin to provide very obvious links which allow users to download the client-side tools (Sync Tool, Upload Tool, UpSync, Retreival Tool)."
0,Option for Retrieval Tool to set time stamps on retrieved files,DURACLOUD-737 allows the Sync Tool to preserve local time stamp information in DuraCloud content item properties. This task is to add an option to the Retrieval Tool which will set the time stamps of files that have been retrieved (assuming that those files have time stamp information recorded in DuraCloud).
0,UI: Handle Glacier storage state errors gracefully,"When content is stored in Glacier, attempts to download, view, copy, or edit the properties of that content will result in an error (http status 409). This task is to provide users with useful feedback when they attempt any of these actions on a content item in Glacier.   More specifically, when a call is made on a Glacier content item via the storeclient, a ContentStateException is thrown. (If the content item was recently added or has been retrieved from Glacier, it will simply download as expected) This task is to show the user an appropriate error message indicating that the content is in Glacier. For now, that error message can indicate that the user should contact DuraCloud support if they wish to retrieve that content item."
0,sync tool include top level directory option/flag,"After requests from several trial and customer users, the synchronization tool should provide a configuration option/flag in which the entire directory path is included upon upload. Currently, the top level directory is ignored and not persisted in the naming of the content items that are uploaded via the sync tool. This new flag for the sync tool would enable the entire folder structure to be included in the name of the uploaded content."
0,SyncToolUI advance config,"This task is to add advanced config options to the sync tool UI. In particular, the user should be able to set the connection port as well as have the ability to select a ""Sync Deletes"" option."
0,Update Duracloud to the latest ActiveMQ,"DuraCloud has a dependency in the POM to ActiveMQ 5.2, an old version.  The version does not formally support Java 7, supported in DuraCloud 2.3.  In particular, specialized settings are needed to work with Java security.  DuraCloud 2.3 has been tested using ActiveMQ 5.7 and 5.8 so the update should require little effort and have little impact."
0,SyncToolUI: Allow for removing sync directories in initial config,"Currently, when walking through the initial setup wizard of the SyncTool UI, if you select a sync directory or file accidentally, there is no way to remove it except to cancel and start the wizard over. This task is to allow selected directories or files to be removed, as they can be in the config update area."
0,Inconsistent Glacier Errors for Root Users in Duradmin,"Root users attempting to edit, copy, or edit metadata on a file in Glacier get inconsistent failure errors."
0,SyncToolUI: Notify user that restart is required after change to configuration.,"Configuration tab: When changing the configuration, this does not take effect until there is a stop/start cycle of the tool. This should be indicated to the user, and an option should be provided to perform a stop/start now or to wait. "
0,UI on IE issues,"The following UI issues were discovered while using DurAdmin via IE during testing for the 2.3 release:  1. Attempting to retrieve a Glacier stored content item in IE does not show the expected notice message (that is seen in Firefox and Chrome), but instead displays the message ""This website is too busy to show the webpage."" 2. Filtering on content item names does not work in IE 3. In IE, when a space refresh is performed, the selection of the space is forgotten, and the display responds as if the Spaces tab were selected."
0,SyncToolUI: Allow clearing of errors,"This task is to provide a way, in the SyncToolUI, to clear any errors that have been noted on the errors list."
0,SyncToolUI should expose pause/resume functions of the synctool.,NULL
0,"Synctool UI should stop promptly when the user clicks ""stop""","Currently the tool remains in the ""stopping"" state until the currently active transfers are complete."
0,SyncToolUI: Better feedback for user without write permissions on space,"The SyncToolUI should be updated to provide better feedback to users when they have selected a space to sync to which they do not have permissions to write to. Since there is not a simple way to collect this information up front, this task is to pay attention to exceptions that are coming back from a sync activity to determine if it is likely that they are a result of lacking permissions, then provide information to the user that will allow them to be aware of he problem and a suggested resolution."
0,Shibboleth: Add 403 Access Denied Page,This improvement is dependent on the Shibboleth branch.  A DuraCloud-branded 403 page should be rendered when an authenticated user who is not authorized to access DuraCloud tries to log in.
0,Shibboleth: Logout,"This improvement is dependent on the Shibboleth branch. When a user logs out, he/she should be redirected to a page that specifies that complete logout requires closing the browser (ala the MC)."
0,DuraCloud instance hitting 100% disk utilization,"Initial issue: - After running synctool on a set of 200,000 files, the sync rate slowed from about 3,000 files transferred per 10 minutes to about 300 or less per 10 minutes.  Reason for the slow down: - The reason the rate dropped, was because the instance storage space had become 100% consumed. The major culprit was the catalina.out and catalina.out.1 files in /mnt/log/tomcat6  Reason there isn't much space available: - As it turns out, this EBS instance, as well as all other EBS instances, no longer include ephemeral storage space by default. It was expected that /mnt was mounted to a large ephemeral storage drive, but that is not the case. - Further info: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#using-instance-storage "
1,Use JClouds library for all calls to OpenStack providers,The current OpenStack storage provider implementation uses both the client originally provided by Rackspace (which has now been discontinued) as well as the JClouds library (which is now the client recommended by Rackspace for Java development: http://docs.rackspace.com/sdks/guide/content/java.html).  This task is to update the OpenStack and Rackspace storage providers to use only JClouds.
0,Can only perform a bulk action (e.g. deletion) on a maximum of 2 spaces at a time,"Today I went in to cleanup some old demo/test content in 'dspacetest.duracloud.org'.  When I selected multiple spaces, I found that no matter how many spaces I selected, the total always read ""2 space(s) selected"".   Even worse, I selected a total of 5 spaces and clicked ""Delete Selected Spaces"" button.  Only the first 2 spaces I selected were deleted.  The other 3 were kept.  It seems as though the multi-select option only captures the first two spaces you select...after that, any other selections are ignored.  Carissa has verified that this also seems to occur on demo.duracloud.org."
1,Recreate storage provider connections,"For some reason, on occasion, all calls to an SDSC provider will start to return 401 responses. When this happens, no calls go through to SDSC. This issue can currently be resolved immediately by reinitializing the instance. The issue also appears to resolve itself after some amount of time has passed.  Since it appears that recreating the SDSC connection (which occurs on reinit) resolves the issue, this task is to update DuraStore to be able to recreate those connections. Ideally this would happen when we start to see 401 failures, but another option would be to simply recreate the connections on a set schedule."
1,Space size alerts,It would be useful for users to be notified when their space reaches a configurable size.  This feature would allow the users to configure this information via duradmin.
0,"When starting sync in SyncTool UI throws an error,  the error message is ugly.",A more compact error description should be displayed - the large amount of text coming back in the message is breaking the look and feel.
0,Update GlacierStorageProvider to retrieve content item storage class,"This task is to update the calls for getting content properties in the GlacierStorageProvider to include a call which will retrieve the storage class of each object, and include that information in the content item details. This will make it possible for users to know if a content item is stored in Glacier, or if it has been retrieved back to S3.  Information about the call that needs to be made to retrieve the storage class can be found here: https://forums.aws.amazon.com/message.jspa?messageID=312132"
0,Print URL to set up screen on the command line when the Sync Tool is executed in GUI mode,"Especially on Linux, the automatic display of the Sync Tool setup/admin screen is not reliably displayed in a browser.  Also, when is does display if there are many browser windows open it is hard to find.  It would be helpful to display the URL as a message when the Sync Tools is started from the command line.  It would also be helpful to display the URL as part the Tray icon in some fashion."
1,"UI: Remove upload tool applet, reinstate single file upload","Now that there is a solid UI as part of the Sync Tool, and because of the seemingly constant issues with java applet technology, the multi-file upload applet should be removed from DurAdmin and the single file upload option should be re-instated.  This work should occur alongside DURACLOUD-775, which provides easy access to the Sync Tool."
1,Sync Tool: Provide feedback for Java issues,"There have been a variety of customer questions and issues reported in using the Sync Tool which mostly boil down to the user's system not having the necessary Java version or not having Java installed at all. This task is to provide actionable feedback to users as they attempt to use the Sync Tool, which will allow them to more easily resolve the problems and get the Sync Tool operational."
1,Sync Tool: Ability to restart on system reboot,"This task is to provide a way, on Windows, Mac, and Linux systems for a Sync Tool that is running on shutdown to restart and continue running after a system restart."
0,S3StorageProvider: Remove dependency on a particular access key,"The S3StorageProvider makes use of the AWS access key as a prefix for bucket names, just to guarantee that the names are unique. The listing of buckets is also limited to only those with a matching access key.  Now that AWS does not make the secret key (essentially the password) available, if we were to lose the secret key, we would have to create another credential pair. If we were to do that, though, the S3StorageProvider would show an empty space listing because the new access key will not match the bucket prefix. This also currently makes it impossible to use the AWS IAM service, which is a best practice.  This task is to allow the S3StorageProvider to display and work with buckets regardless of the access key prefix. The prefix should still be used, since uniqueness is still necessary, and should still be stripped before displaying the space names for users. However, rather than requiring a specific access key, any key should work.  Since this match will likely require a regex: According to AWS docs, the Access Key ID is a 20-character, alphanumeric sequence."
0,Retrieval Tool: Use logback for logging,"In DURACLOUD-774 (in particular, the changes in code r1072) changed over the Sync Tool from using log4j to using logback as the underlying logger. This task is to update the Retrieval Tool in the same way."
0,add sync tool prepend source path (include top level directory) flag to synctoolui,NULL
0,Retrieval Tool: Hidden password option,DURACLOUD-759 added a hidden password for the Sync Tool. This task is to make use of the same environment variable to pick up the password for the Retrieval Tool. This will help to keep the functionality of the two tools in sync.
0,Support multiple instances of synctool ui on one machine,"Currently the synctoolui assumes only one user per machine will be using the synctool.  If this assumption is incorrect, we may need to look into adding port negotiation into the installer mix to enable multiple installs of the synctoolui on a single machine."
0,RetrievalTool NullPointerException when using all spaces option (-a),"The retrievaltool throws a NullPointerException when using the ""all-spaces"" or ""-a"" command line option and the ""spaces"" or ""-s"" option is not specified, which the ""spaces"" option shouldn't need to be when using ""all-spaces"".  The culprit is in RetrievalTool#startRetrievalManager at the line: boolean createSpaceDir = retConfig.getSpaces().size() > 1; because RetrievalToolConfig#getSpaces returns null when the ""spaces"" command line option is not used."
0,synctoolui: unchecking options in configuration tab causes 404 error,The problem:  the controller dependends on the presence of property name in query param while uncheck form fields are not sent as empty values.  
0,List MD5 option for space listing function of Retrieval Tool,It would be nice to be able to optionally list all the md5s as well as the raw list of contentIds for a space.
0,"List contents of space from a ""pre-chunked"" point of view (Retrieval Tool)","It would be handy to be able to optionally list a space contents as it  will appear when retrieved.  That is, only list contentIds as they will appear after they are restitched (do not show chunks or manifests).  It's a kind of dry run for the retrieval tool in a way."
0,Retrieval Tool:  Verify chunked manifests when generating prechunked listing,"It would be nice if we could tell the retrieval tool to verify that the manifest points to a complete set of chunks and that the chunk md5s match the manifest md5s.    This will be a handy addition to  https://jira.duraspace.org/browse/DURACLOUD-837 and https://jira.duraspace.org/browse/DURACLOUD-838  In verify mode, it would probably be a good idea to add a couple of columns to the output: verification status (success, failure) and an explanation column where there is a problem. "
0,Image gallery viewer,"In a cloud service strategy meeting on Aug 6, 2013 (https://wiki.duraspace.org/display/DSPINT/2013-08-06+-+Cloud+Service+Strategy) there was a discussion about a need expressed by UNC to have a way to view images in DuraCloud as a gallery, in order to aid in the finding of images in their collection.  I was thinking that this could be accomplished with an html/css/javascript app that is stored in one of the user's spaces, and allows them to select a space (if that's actually necessary, could be hard coded to the one that is their image library) and view a set of images. The javascript would need to make calls to the DuraCloud REST API to get the list of content items and display each one. The images would likely be scaled down a bit, to make browsing easier, and clicking on them would display the (slightly) larger size. The REST API would also be used to allow paging through the content items (and not trying to display too many at once). The option to specify a prefix for filtering would need to be there, and the user would need to be able to see the full content ID of the item they are viewing, since that will be the link back to the full size archival image on their internal system.  There is an expectation that the UNC folks will be generating the necessary low-res images that would be viewed through this gallery viewer, and would load those images into a DuraCloud space."
0,DurAdmin: Uploaded files should include same metadata as generated by Sync Tool,"When adding files, the Sync Tool captures metadata for the files. DurAdmin should capture the same metadata. The metadata items created by the Sync Tool includes: - creator - content-file-last-accessed - content-file-path - content-file-modified - content-file-created"
0,Make use of S3 Server Side Encryption,This task is to update the S3StorageProvider to make use of the Server Side Encryption feature provided by S3 (http://aws.amazon.com/about-aws/whats-new/2011/10/04/amazon-s3-announces-server-side-encryption-support/)  Implementation details: http://docs.aws.amazon.com/AmazonS3/latest/dev/SSEUsingJavaSDK.html  Storing content with encryption should be the default.  Another task will be needed to update existing content in S3 to make use of encryption. This will likely need to be done by performing a copy action on all existing content such that the copied item is encrypted as it is written to S3.
0,Sync Tool: Uninstalling with app running (on Windows) pops up error dialog,"If the sync tool is uninstalled while the application is running, the uninstall works properly, the application is stopped and all files are removed as expected, but an error dialog is popped up stating ""Program ended with an error exit code"".   This popup does not show up if the application is not running when the uninstaller is run.  The dialog is popped up when the command: ""taskkill /f /t /im duracloudsync.exe"" is run by the uninstaller.  In testing for the 2.4 release I found that if the taskkill command is run without the ""/f"" force flag, the error dialog does not pop up and the exe process stops (the icon is removed from the task bar), but the java process continues to run, so the application is still available. If the command above, with the force flag is then run, the java app is stopped without a popup dialog. This is why a second call was added to the uninstaller, to try running without the force flag first. Unfortunately, this did not work, the error dialog remains."
0,Update to the latest JClouds,In DuraCloud release 2.4 an issue with the JClouds library that was used (the latest at the time) was that it would not perform reauthentication when the auth token expired. This required a work around in DuraStore. The issue has now been fixed: https://issues.apache.org/jira/browse/JCLOUDS-178.  This task is to update to the latest JClouds library when possible.
0,SyncTool/StoreClient failing when attempting to connect to a DuraCloud instance with a configured Glacier provider,"From ucar customer, when attempting to run the sync tool (version 2.1.0) against a DuraCloud 2.4 instance with a Glacier storage provider configured, the following error appears:  Starting up the Sync Tool ...Exception in thread ""main"" java.lang.RuntimeException: Could not create connection to DuraStore due to Error retrieving content stores. null at org.duracloud.sync.util.StoreClientUtil.createContentStore(StoreClientUtil.java:40) at org.duracloud.sync.SyncTool.startSyncManager(SyncTool.java:148) at org.duracloud.sync.SyncTool.runSyncTool(SyncTool.java:292) at org.duracloud.sync.SyncTool.main(SyncTool.java:355) Caused by: org.duracloud.error.ContentStoreException: Error retrieving content stores. null at org.duracloud.client.ContentStoreManagerImpl.getStorageAccounts(ContentStoreManagerImpl.java:164) at org.duracloud.client.ContentStoreManagerImpl.getPrimaryContentStore(ContentStoreManagerImpl.java:118) at org.duracloud.sync.util.StoreClientUtil.createContentStore(StoreClientUtil.java:37) ... 3 more Caused by: java.lang.NullPointerException at org.duracloud.storage.domain.StorageAccountManager.initialize(StorageAccountManager.java:50) at org.duracloud.client.ContentStoreManagerImpl.getStorageAccounts(ContentStoreManagerImpl.java:152) ... 5 more"
0,Bulk Select/Delete Tool,"Several customers have requested the need for a tool that will allow them to perform a bulk delete of a subset of content in a space. They need a tool that will allow them to, first, specify which content to delete from a space and then, second, perform a bulk delete for that content."
0,Add CORS headers to apache config in new AMI,Add the CORS headers to the apache config when the new ami is built:  See  https://jira.duraspace.org/browse/DPS-23 for modifications.  This change enables api calls to be made from javascript hosted on other domains.
1,upgrade for spring 4.0.2,NULL
0,Upgrade to tomcat7,NULL
0,Modify SpaceWriteAccessVoter.java so that the primary storage provider is inferred when no storeId is specified in the URL.,NULL
0,Remove all services dependencies from duradmin,NULL
0,Modify duradmin to control streaming service via ContentStore interface and space properties,NULL
1,Remove services system,"This work is to remove all of the services from DuraCloud. This should include removing DuraService, the entire set of existing services, and components (like the executor) which support or depend on services."
0,Remove ActiveMQ,This work is to remove the use of ActiveMQ in DuraCloud. Storage events should be now captured by the AuditStorageProvider.
0,Update media streaming to work without Media Streamer service,The Media Streamer service is being removed. Media streaming still needs to work. This task is to allow the existing S3StorageProvider streaming Tasks handle the work the service used to manage.
1,Create AuditStorageProvider to push AuditTasks to audit queue,"An AuditStorageProvider needs to be created which is a decorator for StorageProvider. This new class will capture read and write events and write them to an SQS queue as AuditTasks, to be processed later.  Note that this item overlaps with DURACHRON-35."
0,Set up local logging to capture read and write storage actions,The AuditStorageProvider already captures read and write events. This work is to have that information written out to local log files
0,Space listing not being displayed in Chrome browser,"When loading the DurAdmin application in the Google Chrome browser, the list of spaces is not displayed. If there are multiple providers configured, selecting a different provider from the dropdown, and then returning to the primary provider allows the spaces listing to show up. If there is only one configured storage provider, there is no clear way to get the spaces listing to display."
1,Upgrade from Maven2 to Maven3,Upgrade from Maven2 to Maven3. Update POMs to be compatible with Maven3.  Fix all build warnings and errors.
0,Sync Tool: add content ID prefix option,"In a discussion with NCDCR, they noted that they would like to be able to break up their data set, and perform syncs at lower levels in their path hierarchy, but are prevented from doing so because they would like the full directory path to be included in the content ID. Currently, the only way to do this is (in a Windows environment) to either run the Sync/Upload tool from a high level directory, or move content around. Moving content around, in their production storage system is not practical.  The new feature requested to resolve this problem: Allow for a prefix option, that would be prepended on all content items being transferred.  To keep this simple, the prefix provided would apply to all files in all specified content directories."
1,SyncToolUI: Option to set threads,The UI for the SyncTool should provide an option to set the number of threads used for processing. This can just be a drop-down box on the configuration screen.  
0,SyncToolUI: Option to include prefix,DURACLOUD-869 added a prefix option to the SyncTool. This task is to provide a way for users of the UI to make use of that new feature.
0,Create 3.0.1 AMI,Updates: - Use 3.0.1 code baseline - Add lumberjack - Update ssh users - Root pw - SSL cert update
1,Provide a way by which users can determine the optimal SyncTool thread setting,"Currently, to determine the ideal number of threads to maximize throughput, users of the SyncTool need to run the SyncRate tool with a variety of settings, record the speed values, then compare them. This is time consuming and difficult. This task is to make determining the thread number for optimal throughput as simple as possible."
0,Update mimetype determination method,"The MimetypeUtil class in common currently uses FileNameMap (http://docs.oracle.com/javase/7/docs/api/java/net/FileNameMap.html) to determine the mimetype of a file based on its file extension. This class is used by the SyncTool to generate mimetype values for files being transferred to DuraCloud. The current method depends on a mapping file found in each individual user's JRE, make it difficult to update.  A better option would be to use MimetypesFileTypeMap (http://docs.oracle.com/javase/7/docs/api/javax/activation/MimetypesFileTypeMap.html) for this purpose, which would allow us to include our own mapping file (and therefore be much more comprehensive)."
0,Media streaming URL not being displayed in space properties,"When media streaming is turned on for a space, a property is added to the space tags called ""streaming-host"" which provides the host value needed to stream the files in the space (this change was made for DURACLOUD-864).   Unfortunately, as of the 3.1.0 release, DurAdmin is not displaying the streaming host value as one of the space properties, so users cannot actually use streaming even after it has been turned on."
0,Cannot configure or turn off logging of Retrier in ContentStoreImpl,"The ContentStoreImpl currently has hardcoded settings for how it logs errors encountered during ""retries"".  Unfortunately, this is not ideal for all use cases of the API.  For example, in the Replication Task Suite, we use ContentStoreImpl.getContentProperties() method as a way to tell whether content already exists in DuraCloud. If that content does not exist, we expect to receive a NotFoundException. This lets us perform logic like: ""If content exists in DuraCloud, see what checksum DuraCloud reports. If DuraCloud checksum differs from local checksum, re-upload the content.""  Unfortunately, however, getContentProperties() logs this same warning message to log4j 4 times in a row (1 + 3 retries) if the content does NOT yet exist in DuraCloud:  2014-07-14 16:46:28,336 WARN  org.duracloud.client.ContentStoreImpl @ Error attempting to get properties 'ITEM@10673-10.zip' in 'dspace-dev-backup' due to: Response code was 404, expected value was 200. Response body value: null  This causes the logs for the Replication Task Suite to actually fill up with unnecessary warnings from the DuraCloud v3.1.1 API. For example, anytime we want to upload NEW content to DuraCloud, we first check if it already exists (which logs that error x 4) before the new content is uploaded. The logged warnings would cause unnecessary worry for anyone looking at the logs -- in reality they are harmless in this scenario, as the content obviously doesn't exist before it is first uploaded.  It looks like the WARN message is generated by ""doGetContentProperties"" here: https://github.com/duracloud/duracloud/blob/master/storeclient/src/main/java/org/duracloud/client/ContentStoreImpl.java#L904  It is then repeated 4 times as a ""log.warn"" in the default execute() method (which is configured to do 3 retries): https://github.com/duracloud/duracloud/blob/master/storeclient/src/main/java/org/duracloud/client/ContentStoreImpl.java#L216  Ideally, there should be some way to configure the logging in the ContentStoreImpl. I like that this same task is retried 3 times. But in many scenarios, I'd rather it just throw me a final Error if all retries failed, rather than logging the same error message multiple times in a row.   If logging cannot be easily configurable, then switching the default execute() method to just do a ""log.debug"" would be better. That way I can see the underlying error if I turn on debugging, but by default my logs don't get filled with the same errors repeated.  NOTE: This was discovered while attempting to upgrade the DSpace Replication Task Suite to use DuraCloud API version 3.1.1 (JARS: storeclient, storeprovider, common). Unfortunately, until logging is cleaned up in some way, it's unlikely I'll release the upgraded Replication Task Suite code."
1,Fixity check green bar not being displayed,The green bar that show the date of the last fixity check is not being displayed in DuraCloud 3.1. This feature needs to be re-instated as the new bit integrity system is put in place to allow users to see the dates of health checks.
1,Better handling for content files added with an incorrect checksum,"Currently when a content item is added to a storage provider via a PUT content REST call, if the checksum included with that content item does not match the checksum computed by the provider, a 500 response is generated, and the file itself remains on the storage provider.  This task is to: 1. Ensure that if content fails the checksum check, it is either not added to or is removed from the space. 2. Return a response with an error code and body text that better describes the issue."
0,Limit storage provider details,"The current DuraCloud REST API call to determine the available storage providers on an instance provides in its response the set of properties from init that are beyond the standard set of StorageProvider values. In the case of the SnapshotStorageProvider, some of these details should not be shared."
1,Restricted Streaming,"- DuraCloud would continue to use CloudFront to stream files, but would add an option to limit access to streamed content. There would be two options when setting a space to be streamed: open streaming (which is how we do streaming now) and restricted streaming.(the new feature). - When using restricted streaming, no files would be directly accessible. Instead, to expose a file, a new DuraCloud REST method would be provided which could be called to retrieve a signed URL that could then be used to stream the file. - The new REST call would require providing the username and password of a DuraCloud user with administrative rights. - The new REST call would require an ending date and time parameter, after which the returned URL is no longer valid. - The new REST call would allow for providing an optional parameter indicating the IP address or range of addresses that can be used to access the content.  The consumer of this service (UNC in this case) will be responsible for: - Verifying that a user of their system should have access to the content that is to be streamed - Making the call to DuraCloud (with appropriate DuraCloud permissions) to request and retrieve the signed URL - Inserting the URL into an RTMP player (e.g. Flowplayer, JWPlayer, etc) which will allow the user to view the content"
1,User IP limits,"- DuraCloud would add a new user attribute which defines an IP address or range (using CIDR notation) - Once this attribute for a user is filled out, all future attempts to log in as that user (both in DuraCloud instances and the Management Console) would require that the user be coming from that IP address or range. - Users would be able to update this attribute in the Management Console. - Note: This feature would need to support multiple IP address/range entries, as there could be 2 or 3 specific IPs where the user needs to be able to log in from."
0,Make Audit and Manifest REST methods consistent with other durastore calls,"Namely, make sure that storeID parameter can be null and if so, the primary storage provider is used."
0,Problem running the latest synctool installer works on osx mavericks,"Michele was having trouble launching the installer 3.1.1 on Mavericks.  At first it seemed that the problem was the lack of java.  Danny had her install java 8.  Still no luck.  Danny was able to reproduce: Symptoms:   After downloading the latest synctool installer (3.1.1), double clicking on the icon appears to do nothing.  The system log is showing this output less than completely helpful output:  Sep 29 15:54:30 panna com.apple.launchd.peruser.501[241] (com.bitrock.appinstaller.81696[42342]): Job failed to exec(3) for weird reason: 13 Sep 29 15:54:30 panna Finder[259]: 8837325: Attempting to SIGCONT to pid #42342 failed, with errno=#3, or the process failed to actually start Sep 29 15:54:30 --- last message repeated 2 times --- "
0,Make org.duracloud.common.DateUtil  threadsafe,Neither parse nor format operations are threadsafe.
0,Synctool fails when the max file size is greater than max java integer value,"Using the synctool, uploads of files greater than Integer.MAX_VALUE fail when the max file size is set appropriately high (ie > 2 GBs).   "
0,Add SyncTool feature which allows for skipping existance checks for new content sets,"This feature allows initial transfers of data sets to proceed without the usual checks to see if the content is already in DuraCloud (since we know that it is not.) Instead, all files will simply be transferred.  The need for this feature comes from GSU. With a large set of small files (4.7 million under 1 MB), the overhead of doing the checks for each file was slowing down the overall transfer considerably. "
0,Add REST API for Accessing Bit Integrity Reports in DuraStore,"The rest api should support a call to get the last bit integrity report for a space.  It should include, along with the body of the latest report, the status of the report (success if all went well, failure if there are errors) as well as the date the report was completed.  The properties should be accessible through a HEAD call, while the content (as well as properties) will be accessed with a GET.   "
0,Spaces no longer loading in Firefox 33.0+ in duradmin,After upgrading to Firefox 33.0 spaces are not longer loading.
0,Clicking content checkbox unexpectedly deselects after loading the content detail ,Steps to reproduce:    1. Load a space in duradmin  2. Make sure no content items are selected.  3. Click on the checkbox next to a content item.  4. Notice that it is initially checked but then unchecks itself while remaining selected.
0,SyncTool: Make sure files restored from change list on re-start are in the current set of content directories,"An issue reported by NCDCR:  The SyncTool performed a restart and picked up an old change list file even though the set of content directories was completely different. This happened because the SyncTool was started (writing the config) then killed before the old change list could be overwritten. On the next run, the config matched, so a restarted was carried out, pulling in change list files which really should have been ignored.    To fix this problem, the proposal is add a check on restart, just after the change list is read in, to remove any files from the change list that don't actually exist in the list of content directories."
0,Clicking on the checkbox next to a space causes the application to hang.,To reproduce the issue:    Open duradmin.  Select a single space by clicking on the checkbox.  Notice that the throbber spins continuously.  
0,Failure with space and %20 in content ID on SDSC,"Several failures have been noted recently in the mill when attempting to perform a duplication of a content item with a content ID that includes ""%20"" in the content ID from Amazon to SDSC. (Note that %20 is the url encoding for a blank space.) The attached file is a set of errors from the mill dead letter queue.    I was able to reproduce the issue by attempting to create a content item in SDSC with the name: ""test-file spaces%20percents.txt"". Note that I did not see an issue with ""test-file spaces.txt"" or ""test-file%20percents.txt"", so it would appear that the combination of the two characters (an actual space, and ""%20"") is necessary to provoke the issue."
0,Copy dialog in UI has multiple progress spinners next to space selection,"As shown in the attached image, when performing a copy via DurAdmin, multiple spinning circles to indicate loading or progress are shown next to the space drop-down selection. This becomes more of an issue when the storage provider selection is used to change the selected provider.    Previously, there was one of these spinners, which was present as the list of spaces was generated and then went away one the list was populated. That was great. Now when I transition between storage providers, a string of these are presented (I've counted at least 9) with overlapping text (making the text impossible to read). Some of the spinners do disappear, but 2 seem to remain even after the space listing is fully populated."
0,Remove extra authentication provider from duradmin security config,"<authentication-provider user-service-ref=""userDetailsSvc""/> "
0,SyncTool UI: Add warning about use of the sync deletes feature,It has been noted several times that the sync deletes feature in the SyncTool can be dangerous. It provides the opportunity for a user to delete their entire corpus of content if it's not used carefully.    This task is to add a warning dialog when anyone selects the option to turn on the sync deletes feature which describes the dangers and asks them if they are sure this is what they would like to do.
0,Secure streaming cannot be turned back on in DurAdmin,"If a space is set to secure streaming it shows up in DurAdmin as having streaming turned on. If, however, you then turn streaming off via DurAdmin and later attempt to turn it back on again, it fails with an error: ""Task enable-streamingis not supported"". This works fine with a space set to open streaming."
0,Storage report display values are inconsistent,"The storage report charts in the DurAdmin display do not show consistent values between the line chart (historical size) and the pie charts. This inconsistency is because one of the two uses GiB (Gibibytes) and the other uses GB (Gigabytes). A GiB is 1024^3 bytes, while a GB is 1000^3 bytes.    This issue should be resolved by updating the storage reports to make use of GB rather than GiB. This will provide greater consistency.    Note that this is also an issue reported in DURACHRON-79, where the inconsistency is between the storage reports and the snapshot."
0,Durastore init xml is incorrect in the documentation.,This chunk is not right.    <storageProviderAccounts>    <storageAcct ownerId='0' isPrimary='true'>...</storageAcct>    <storageAudit>...</storageAudit>    <millDbConfig>...</millDbConfig>  </storageProviderAccounts>    millDbConfig and storageAudit should be siblings of storageProviderAccounts.    See xml document for correct format.
1,Add byte-range option to DuraStore get content REST API request,"Discussions with Ilya Kreymer from webrecorder.io have suggested that there could be an integration with that service to allow playback of WARC files directly from DuraCloud storage, which could be quite beneficial for Archive-It users storing content in DuraCloud.    In order to make this happen, the DuraStore API needs to support range requests, allowing for only portions of a file to be retrieved from storage, rather than the entire file. This is supported in S3 and OpenStack Swift, so should be something we can support across all providers."
0,Instance not initialized call should return something other than a 503,Each of the DuraCloud apps currently responds to a /init REST call with either a 200 if the app is initialized and ready for use or a 503 if the app is available but not yet initialized.    This task is to update this to change the 503 to another code. The reason is that Apache also returns 503 responses as a DuraCloud instance is starting when it is not yet able to make a connection to Tomcat. This results in the Management Console thinking that an instance is ready to be initialized when  it is not yet available.    Note that this change will require a companion change in the MC to ensure that the correct HTTP code is expected on init.
0,Allow user-level users to see bit integrity notice and manifest button,Users with USER role should be able to see the bit integrity notice and should be able to see and use the space manifest button. Currently these are only available to users with ADMIN role or above.
0,Multiple space selection does not work,"In DuraCloud v3.3.0, when multiple spaces are selected, the UI displays as if no spaces were selected. There is no option to delete the selected spaces or edit properties (as there was in 3.2.2 and previous versions)    To reproduce:    1. Select a space.  2. Click on the checkbox of another space.  3. The multiple space selection panel does not show up in the detail area."
0,Turning off space streaming clears space permissions,"On a space with streaming enabled, setting streaming to off (via the UI) will cause the bucket property which maintains space permissions to be removed. This effectively deletes all space permissions (making the space and the contents of the space unavailable to anyone beyond administrators).     Note that this clearing of permissions is not immediately reflected in the UI, as the permissions set is cached."
0,Audit and Manifest links fail in duradmin when space id has a period in it.,"To reproduce  1. Add a space with a period (such as ""test.space1"") and then add a couple of items to it.  2. Wait a minute.    3. Click on manifest or audit links   4. Page will return blank without downloading a file.    Problem may be that spring is parsing everything to the right of the period in order to serve up content based on the apparent file type extension.  Problem does not exist in durastore - in duradmin only.  "
0,Durastore incorrectly reports duplicate space.,"To reproduce the issue:     1. create a space called ""my-space""  2. create a space called ""my.space""  3. the second call should fail and report a duplicate space.    Note that performing the above calls in reverse order succeeds.    Finally,  after performing call in reverse order, notice that deleting the my.space  results in both my.space and my-space being deleted."
0,Relax check for existing content ID on streaming url request,"In order to successfully stream an mp3 file through Cloudfront, the file must be stored with the "".mp3"" extension, but the task call to generate the URL must leave the extension off of the file.    Example: If a file is named ""myAudioFile.mp3"", it should be stored in DuraCloud (S3 provider) with that name. To stream that file, the content ID would be specified as: ""myAudioFile"", without the extension.    Right now there is a check in the task calls to create URLs to verify that the specified content ID actually exists. In the example above, this causes problems because the stored file name and the ID provided for the get URL call differ.    This task is to change the task code to skip the content ID check altogether. This will also allow for generating URLs for content items which have not yet been uploaded (but for which the IDs are known).    Note that I've only tested this with secure streaming, but expect it will work the same with open streaming."
0,Bit Integrity Reports are not showing up in duradmin for secondary providers.,"(from an email from bbranan)  I was looking at some bit integrity results this morning and noticed in all of the accounts I looked at (colum, icpsr, and demo), that there are no green bit integrity bars on any of the spaces in any secondary providers. I spot checked a few, and there are successful bit report entries for those spaces in the mill db, they just aren't being shown in the UI.    When I select a space on SDSC providers in duradmin, it takes a while for the space to load. It appears to be attempting to get the bit integrity report, but it fails to do so. Looking at the logs, I see consistent errors like this:    ERROR  2015/09/11 13:26:40 [ajp-bio-8009-exec-25212] (SpaceController.java:117) [populateBitIntegrityResults()] - failed to populate bit integrity results due to error:Error attempting to get bit integrity report  properties 'carissa-folder-test' due to: Response code was 500, expected value was 200. Response body value: null  org.duracloud.error.ContentStoreException: Error attempting to get bit integrity report properties 'carissa-folder-test' due to: Response code was 500, expected value was 200. Response body value: null          at org.duracloud.client.ContentStoreImpl.getBitIntegrityReportProperties(ContentStoreImpl.java:1214) ~[storeclient-3.3.0.jar:na]          at org.duracloud.duradmin.spaces.controller.SpaceController.populateBitIntegrityResults(SpaceController.java:109) [SpaceController.class:na]          at org.duracloud.duradmin.spaces.controller.SpaceController.get(SpaceController.java:96) [SpaceController.class:na]  "
0,Streaming: Allow streaming URL requests for content IDs not in storage,"In order to stream mp3 files through Cloudfront, the files themselves must have the "".mp3"" file extension, but the streaming URL must be requested without the file extension. Currently, there is a check to make sure that a call to get a streaming URL corresponds to a content ID already in storage. In this case, though, the content ID and the ""stream"" ID don't quite match.    This task is to remove the check to see if a content ID is in place before a streaming URL can be requested. This should solve the problem noted above, and make it possible to pre-generate URLs for content files prior to upload, which may be useful as well."
0,Add support for S3 - Infrequently Accessed storage option,"According to this announcement: https://aws.amazon.com/blogs/aws/aws-storage-update-new-lower-cost-s3-storage-option-glacier-price-reduction/?sc_ichannel=em&sc_icountry=global&sc_icampaigntype=launch&sc_icampaign=em_148542750&sc_idetail=em_2049897831&ref_=pe_430550_148542750_7, Amazon is now offering the option of a lower cost S3 tier for infrequently accessed data. This task is to transition all DuraCloud content to this new option.    The best way to take advantage of S3 - IA is via bucket lifecycle policies. Because of the additional cost of content deletes in the first 30 days, it would be best to set these policies to transition content from S3 Standard storage to S3 - IA storage after the content has been stored for 30 days. This policy should be created when a new space is created (via the S3StorageProvider)    As part of this work, the current S3 task which transitions content to Reduced Redundancy storage should be replaced with a task that adds the bucket policy for S3 - IA to an existing bucket. To complete this transition, a script will need to be written which uses this task to update all current accounts.    The storage class init parameter should also be removed (from both app-config, and from the MC). It no longer makes sense to support reduced redundancy storage."
1,Use CloudWatch rather than DuraBoss to generate storage reports,"Cloudwatch now provides metrics for S3, providing a daily total of files and total size per bucket in an account. We need to use this rather than running DuraBoss for reporting, at least on S3 and Glacier stores. Ideally we can discontinue the use of DuraBoss completely."
0,Beanstalk: Connect DuraStore to MC database,"In order to have all of the information that is needed, DuraStore in beanstalk will need to have a direct connection to the MC database. Much of the code for setting up this connection should be already available from the MC or Mill (share/reuse where possible).    Options for getting the database connection information:  - As system properties specified in JAVA_OPTS (using beanstalk environment variables, note that there is a 256 character limit)  - Use JAVA_OPTS to point to a config file S3 URL, and pull that in on startup.    In order for the DuraStore calls to complete quickly, and to keep the database load down, information from the DB should be cached. This could be done preemptively or lazily, but in either case, there will need to be handling for invalidation, to ensure updates to the database are picked up."
1,Beanstalk: Account determination,"All calls going through DuraStore will need to determine the account for which the call should be processed. By default this should be done using the subdomain of the host used for the call. There should also be an optional query parameter added to each API call which allows specifying the accountID, as this will be necessary for testing and may be needed by other scenarios.    Part of this work will be to ensure that the combination of the account and user credentials still ensures that only users with access to an account are able to make calls that affect that account."
0,Beanstalk: DuraStore calls to accounts,All calls through DuraStore in a beanstalk setup will need to pull account information then perform the expected requests on the appropriate account.
0,Beanstalk: Handling of roots,"DuraStore through beanstalk will no longer have access to a single root user. Instead it will need to allow any user with the root role to have root-level access. This work is to ensure that root user access works properly.    Since DuracloudUser has a root boolean property, we need to take that into account when loading user details managers from the global account repo. Currently we are relying on ROLE_ROOT associations in the account rights."
0,Users with USER role should be able to view streams on open spaces,"When a space is set to open streaming (by turning streaming on via DurAdmin), any user with read access to that space should be able to view media (audio/video) that is streamed through the built-in player. Currently, this works fine for admin users, but users with only USER role just see ""Unable to stream file"" where the player should be.    This is very likely to be caused by the fact that a task call is being made to retrieve the streaming URL, but that task call is limited to admin users. A user with access to a space should be able to make the task request to stream content from that space."
0,Backup thread runs too frequently on synctool.,Currently the synctool's backup thread is running on the same schedule as the directory polling period which is 10 seconds by default.  The back up thread period should not be tied to the polling period.  Instead it should run at least every 5 minutes by default.  Also thread synchronization on on the ChangedList.persist method is causing threads to wait until the backup file is completely written rather than waiting only until the file list is copied in memory.
1,Beanstalk: Set up topic subscription on DuraStore init,"On DuraStore init, an SNS topic subscription is set up. The SNS connection details are stored in the MC DB. When a message comes through that subscription, it picks up what changes were made and updates local cache."
0,Allow for setting chunk size in SyncTool UI,The UI version of the Sync Tool should have a setting (on the configuration tab) which allows for the size of chunks to be set.
0,SyncTool chunk size settings should be multiples of 1000,"In the rest of DuraCloud, GB is defined as a multiple of 1000 (a decimal representation.) The Sync Tool currently uses a multiple of 2 (decimal representation). The Sync Tool should be updated to set the GB value to 1000^3."
1,Duradmin must use new durastore storage stats api,"Duradmin must use new durastore storage stats api rather than pulling, parsing and caching results in memory.      The ContentStore interface will need to be extended.   Duradmin code for parsing and caching storage reports must be removed."
1,Write utility for adding storage report data to space_stats table in mill db,"Currently the storage history of all spaces are stored in xml files found in x-duracloud-admin on the various instances.   We want to extract this data and store it in the mill db space_stats table.      To accomplish this task,  we will need a tool that can convert storage reports for a single account by looping through the list of reports,  extracting the data for each space  and inserting a record in the space_stats table."
0,Remove storage report generation code,"Remove duraboss, reporter, report client, and any dependencies on these modules."
0,Beanstalk: Connect DurAdmin to MC database and allow for calls to any account,DURACLOUD-949 and DURACLOUD-951 updated DuraStore to connect to the MC database and pull the information needed to work for all DuraCloud accounts. This work also involved caching the database information and refreshing that cache on change notificatoin. This task is to carry that work forward into a similar capability for DurAdmin.
0,SyncTool fails to connect when using a password with special characters,"When a DuraCloud user sets their password to a value which includes special characters they are able to log in to the DuraCloud UI just fine, but attempting to connect using the command line SyncTool results in a stacktrace like the following:    Starting up the Sync Tool ...Exception in thread ""main"" java.lang.RuntimeException: Could not create connection to DuraCloud (test.duracloud.org:443/durastore). Cause: invalid credentials. Check your username and password and try again.          at org.duracloud.client.util.StoreClientUtil.createContentStore(StoreClientUtil.java:42)          at org.duracloud.sync.SyncTool.startSyncManager(SyncTool.java:154)          at org.duracloud.sync.SyncTool.runSyncTool(SyncTool.java:294)          at org.duracloud.sync.SyncToolInitializer.runSyncTool(SyncToolInitializer.java:34)          at org.duracloud.sync.SyncToolInitializer.main(SyncToolInitializer.java:23)          at org.duracloud.syncui.SyncSelector.main(SyncSelector.java:33)    An example password which provokes this issue is: H89v$2oRlaD?5Qe\YT3j. This password works fine with the SyncTool if all special characters are removed (H89v2oRlaD5QeYT3j), but fails if any remain.    Once fixed, this needs to be tested in the SyncTool UI, the Retrieval Tool, as well as the chunk, stitch, and sync-optimize tools."
0,Add REST API call to retrieve latest fixity report for a given space,"As discussed in DPS-123, Jim Tuttle from Duke needs a way to easily retrieve bit integrity reports generated by the DuraCloud Mill. This task is to add a new REST API call to the DuraStore API which will allow for retrieving the latest report file."
1,Remove integration-test and unit-test-db modules,"The tests in the integration-test module have been disabled since 4/25/2011. The most important of these, the tests for storage provider connections, have been recreated in the ""integration"" module. Many of the remaining tests are likely far out of date.    This task is to review the test in integration-test, move those which have real value to the integration module, then remove integration-test and its dependent unit-test-db from the codebase."
1,Finish storage report conversion:  relative space byte and item count sizes.,For beanstalk version implement storage stats pie chart view of space size and space item count as of x day to match what is currently available in duracloud 3.6.0.
0,Remove dashboard from duradmin,No longer supported.
0,Bit Integrity Report REST call incorrectly returns 204 code when space is not found.,It should be returning 404
0,OpenstackProvider marker and prefix params are not url encoded on space listing calls,This problem applies to both get spaces and get spaces chunked calls.  It appears that this problem was addresses for get content calls.  So the OpenStackStorageProvider performs a url encode as well as some other encoding magic to get things to work properly.   These issues were identified for  jclouds 1.8.0 (the version we're using) in this jira: https://issues.apache.org/jira/browse/JCLOUDS-217 .  According to the issue details it was resolved in jclouds 2.0.0.   Short term solution is to apply the fix to the OpenStackStorageProvider.  I believe the longer term fix is to upgrade to jclouds 2.0.0.  That later fix will likely be somewhat involved since there were major api changes between 1.x and 2.x.   
0,Durastore:  TaskProviderFactory is not aware of global account repo,"With the recent beanstalk-oriented changes, any durastore instance can serve requests for any account.    However it appears that the TaskProviderFactoryImpl is not aware of these changes.  It is still thinking that it must be initialized externally as opposed to reading from the global account store."
1,Remove classic init from DuraCloud,"As we move towards an Elastic Beanstalk oriented deployment,  we no longer need to support classic two phase init (ie xml posted to the application)."
0,Duradmin must be aware of AccountChangeEvents,NULL
0,Mill:  Handle corner case of missing storage provider,Currently the looping task producers  will enter an infinite loop if a storage provider is missing.  It should retry a few times.  If it continues to fail it should send and an email and move on.
1,Ensure DuraCloud dependencies are version consistent and secure,"This task is to add two new checks to the DuraCloud dependency chain:  1. Add the dependency convergence option to the maven enforcer (https://maven.apache.org/enforcer/enforcer-rules/dependencyConvergence.html). This compares versions in the dependency tree to ensure that only a single version of any given artifact is referenced in the build.  2. Add the enforce-victims-rule to the maven enforcer (https://github.com/victims/victims-enforcer). This checks the dependency tree against a list of dependencies known to have security vulnerabilities.    After adding these requirements, there will be changes needed in the codebase to comply with any issues that are uncovered."
0,Changes to users in management console does not always trigger update in duracloud,The following scenarios appear not to cause a cache update in duracloud:    1. Removing a user from an account  2. Adding a new user to an account  3. Updating a user's rights on an account.    also: double check deleting a user from all accounts works.
0,Setup Bit integrity producer to run automatically,"It would be beneficial to rerun failed bit integrity runs in order to reduce the time spent managing bit integrity failures.     To achieve this end we must  1. Ensure that if an instance of the producer is currently running, do not run.  2.  modify the bit integrity producer to ignore any space that has run successfully within the last 60 days."
0,Looping task producers should ignore Inactive Accounts,Currently inactive accounts are not ignored.  
0,500 error attempting to download file through DurAdmin,"In the UNC account, the file named 20304_VT0010_0001.mp4 in the space sfc20304-open cannot be downloaded using the ""Download"" button. Selecting the download button results in a 500 error, which is captured in the attached ""500-error-trace.txt"" file. The error which is generated on the server side is captured in the attached ""500-error-server-side-catalina-out.txt"" file.    The file in question is 4.5 GB in size. It is in a streamed space, and streaming works properly. The file can be downloaded from S3 and from DuraStore, so the issue seems to be occurring within DurAdmin."
0,Duplicate response headers from REST API,"Calling the REST API for GetContentProperties return some duplicate headers, excerpted below:    $ curl -I -u ""${DURACLOUD_USER}:${DURACLOUD_PASSWORD}"" https://${DURACLOUD_HOST}/durastore/rest-api-testing/foo2    HTTP/1.1 200 OK  Date: Tue, 05 Apr 2016 19:38:14 GMT  ETag: 26dc5f719443fb5ff49b87b3853d5d7c  ETag: 26dc5f719443fb5ff49b87b3853d5d7c  Content-MD5: 26dc5f719443fb5ff49b87b3853d5d7c  Content-MD5: 26dc5f719443fb5ff49b87b3853d5d7c  Last-Modified: 2016-04-05T18:36:52  Last-Modified: 2016-04-05T18:36:52"
1,Beanstalk:  ACLStorageProvider must be reloaded on ACL change,"ACLs are currently cached in the ACLStorageProvider in order to improve performance of space reads.  This works fine when a single instance of ACLStorageProvider can be assumed.  However in beanstalk mode we expect that each node may have a copy of the same ACLStorageProviderCache.  Changes to groups or users that occur in the management console will generate a notification to all of these instances.  However,  changes to the permissions made via Duradmin or the Durastore API must also cause nodes to reload their caches.      One approach to resolve the issue will be to have the ACLStorageProvider generate a notification to the other nodes.    It might be useful to include the source IP address in the notification so that message receivers can opt to ignore messages received from themselves.  "
0,Do a test run of 4.0.0-SNAPSHOT on beanstalk.,NULL
0,Create dev AMI for  DuraCloud 4.x testing.,NULL
0,Improve indexing of management console database,NULL
0,Beanstalk: Load duracloud properties from S3,Currently -Dduracloud.config.file only supports files.  In order to make it possible for us to use a plain vanilla beanstalk ami it would be better for us to load config from S3.     Loading from S3 would make the instances more secure by now storing credentials on the instances themselves.  
0,root URI does not  redirect to /duradmin/ in 4.0.0 on beanstalk,https://<account>.duracloud.org does not redirect to  https://<account>.duracloud.org/duradmin as it does in the current production 3.x line.
0,Remove duracloud user and password from GlobalProperties in Management Console,NULL
0,MC:  global properties form does not give useful feedback when data is invalid,NULL
1,Investigate automate deployment to beanstalk,"As a first step, it would be very helpful if commits to development version were automatically deployed to a test beanstalk environment.  Additionally it would be good if integration tests were run as part of that process.    NB: Since dependencies such as duracloud-db have *-SNAPSHOT versions while each duracloud instance is in development,  it would be good if duracloud-db commits also trigger a rebuild of duracloud and management-console.  "
0,Make MC able to read properties from S3,NULL
0,upgrade spring security in management console,NULL
0,upgrade spring security in snapshot,NULL
0,Non-admin users not able to see snapshots,"On a DuraCloud Vault instance (vault.d.o), I gave read/write permissions to two spaces to a non-admin user. Both of those spaces had snapshots (one complete, one in process). When logged in as the user, both spaces were visible, but no snapshots were displayed.    A call to the get-snapshots REST endpoint as the user shows the two expects snapshots are listed. This is likely an issue in the UI layer."
0,Add restful error handling to the Storage Report REST API,Currently failures are returning 500 errors.
0,Refactor storage report API endpoints to be more consistent with the rest of the DuraCloud API,"Current API endpoints:    # Provides a listing of totals for content in a space over a time range  /storagestats/timeseries ? spaceID & storeID & start & end    # For a single day, provides a list of report details for each space in a store  /storagestats/snapshot-by-day ? storeID & date    Should be changed to:    # Summary of a space, listed by day  /report/space/{spaceID} ? storeID & startDate & endDate    # Summary of a store, listed by day  /report/store ? storeID & startDate & endDate      # Summary of a store (for a single day), listed by space  /report/store/{date} ? storeID    - storeID should be optional, default to primary store    Note: No changes need be made to the functionality, just to the REST endpoints."
0,SyncTool chunking error,I have an issue chunking files larger than the max size:     ERROR 2016/05/07 02:46:37 [pool-3-thread-2] (SyncWorker.java:68) [run()] - Exception syncing file /srv/perkins/duracloud/spaces/repository.duke.edu-dev/BINARIES/d4/a6/22/d4a622b5a2a64e750e3e01abaf687eceebfe06d5 was MaxChunkSize must be multiple of 1024: 1000000000   org.duracloud.common.error.DuraCloudRuntimeException: MaxChunkSize must be multiple of 1024: 1000000000           at org.duracloud.chunk.ChunkableContent.calculateBufferSize(ChunkableContent.java:93) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.chunk.ChunkableContent.<init>(ChunkableContent.java:61) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.chunk.ChunkableContent.<init>(ChunkableContent.java:53) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.chunk.FileChunker.doAddContent(FileChunker.java:226) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.chunk.FileChunker.addContent(FileChunker.java:152) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.sync.endpoint.DuraStoreChunkSyncEndpoint.addUpdateContent(DuraStoreChunkSyncEndpoint.java:164) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.sync.endpoint.DuraStoreSyncEndpoint.addUpdateContent(DuraStoreSyncEndpoint.java:304) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.sync.endpoint.DuraStoreSyncEndpoint.doAddContent(DuraStoreSyncEndpoint.java:262) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.sync.endpoint.DuraStoreSyncEndpoint.syncFileAndReturnDetailedResult(DuraStoreSyncEndpoint.java:229) ~[duracloudsync-3.7.3.jar:na]           at org.duracloud.sync.mgmt.SyncWorker.run(SyncWorker.java:65) ~[duracloudsync-3.7.3.jar:na]           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]           at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
0,Management Console: Deleting a user from an account is failing,HTTP Status 403 - Invalid CSRF Token 'null' was found on the request parameter '_csrf' or header 'X-CSRF-TOKEN'.    Probably is due to the fact that  CSRF tokens are on by default in the new spring security.
0,Management console: Logout link results in 404,NULL
0,Move Bit Integrity Producer on to the Sentinel,In order to better utilize our AWS resources we should do the following:      1. Retire the Bit Integrity Producer  Autoscale group and configuration.  2. Move the bit integrity producer process onto the sentinel and have it run continuously.  3. Upgrade the sentinel to an m3.medium.   4. Ensure that the bit report autoscale group is scaling down when the bit report queue has been empty for more than 3 hours.
0,Mill: Dup Task Processor should perform delete on destination only if the source does not contain item.,"In order to cover the case of content disappearing (due to provider failure) from the source provider we must make sure to delete destination store content only if the content is no longer in the source manifest.  Currently if a dup task comes to dup content from a source to a dest provider, if the item doesn't exist in the primary we delete it in the destination.   However  the looping dup producer adds dup messages for items not in the source that are in the destination in order to synchronize any missed deletes.   Thus a file that disappears from the source unexpectedly will be deleted resulting in data loss.    So to fix this problem: if dup task targets a content item that does not exist in the source provider,  ensure that it is not in the source manifest before deleting from destination."
0,"In the CleanUpSnapshotTask,  read all items before generating delete messages","Currently the CleanupSnapshot task streams manifest items using a streaming iterator.  For each item, a  delete audit message is generated.  Under the covers the manifest item streamer pulls items in 10K item pages.   If the manifest processor in the mill begins working the list before all pages have been pulled,  the manifest streamer will miss some records.    To resolve,  write entire list of content Ids to file before generating audit tasks."
1,Add Chronopolis storage provider,"This will be a subclass of the snapshot provider, to provide the ability to push to Chronopolis, not DPN."
0,"Update Bridge dependencies to rely on latest duracloud,spring, etc.","Update dependencies for duracloud, spring, hibernate, etc.  Resolve any issues the upgrade creates."
0,Chart data calls are returning 500 error from DuraStore,NULL
0,Add CORS headers to Elastic Beanstalk config,We need to perform the same config on 4.x that we did on 3.x 
0,Use Manifest to Count items and Bytes for Glacier for Storage Stats,NULL
0,Error message on Chronopolis provider says DPN,"If a communication or other error occurs within the Chronopolis provider, an error dialog is shown that includes the text: ""DuraCloud is not currently able to connect to DPN"".     This error continues to make sense for the DPN provider, but not for the Chronopolis provider. Making the error more generic, so that it could apply to either provider, would be sufficient."
0,Tags for USER role with read-only permission are presented with no spaces,"When a user with USER role is given read access to a space, they are able to see the content, but when they select the content in the UI, the list of tags are listed with no space between the tags, so they all appear as if they are a single word.    Example: If the tags on a file are ""test"", ""popcorn"", and ""bicycle"", then they are displayed as ""testpopcornbicycle""  "
1,Update Duplication Monitor to work with DuraCloud 4.x,1. Remove obsolete dependencies on database  2. Retrieve space counts from storage stats rather than making a content item count by means of looping through all the content ids.  3. Ensure monitor username and password is encrypted  
0,Ensure that management console uses beanstalk settings to require https,It should work just like the duracloud elastic beanstalk.
0,"Management Console:  Changing a root user's profile (password, etc) does not reset all caches ",Resetting a root user's profile should reset all user data caches.   Currently a USERS_CHANGED message is being sent with no accountId.  The result is that instances are not registering the change.
1,As a JIRA Administrator I would like to be able to change the trigger of the night service,NULL
1,As a JIRA Administrator I would like to be able to change the trigger of the night service,NULL
1,Generic webwork aliases may clash with other plugins,"Some web work actions have commands that have very generic aliases, such as ""Update"", ""Move"" and ""Synch"" etc. These aliases have a good chance to clash with the action name or aliases used by other plugins.  These aliases should be given a name specific to the plugin, such as ""UpdateGreenHopperDropBoard""... Alternatively, the bang syntax can be used in place of command aliases, such as ""DropBoardAction!update"". In this case, there is no need to define aliases for commands. (However, still need to define alias as an empty String; otherwise, a NPE will be thrown when parsing the xml.)"
1,Generic webwork aliases may clash with other plugins,"Some web work actions have commands that have very generic aliases, such as ""Update"", ""Move"" and ""Synch"" etc. These aliases have a good chance to clash with the action name or aliases used by other plugins.  These aliases should be given a name specific to the plugin, such as ""UpdateGreenHopperDropBoard""... Alternatively, the bang syntax can be used in place of command aliases, such as ""DropBoardAction!update"". In this case, there is no need to define aliases for commands. (However, still need to define alias as an empty String; otherwise, a NPE will be thrown when parsing the xml.)"
0,"Add text to the Agile Gadget ""Invalid Project"" message","When the error is ""Invalid Project"" add details that the project is not configured for use with GreenHopper - ""Invalid Project - [Not configured for use with GreenHopper|http://confluence.atlassian.com/display/GH/Configuring+your+GreenHopper+Global+Settings#ConfiguringyourGreenHopperGlobalSettings-context]"""
0,"Add text to the Agile Gadget ""Invalid Project"" message","When the error is ""Invalid Project"" add details that the project is not configured for use with GreenHopper - ""Invalid Project - [Not configured for use with GreenHopper|http://confluence.atlassian.com/display/GH/Configuring+your+GreenHopper+Global+Settings#ConfiguringyourGreenHopperGlobalSettings-context]"""
3,Greenhopper ranking field is not displayed correctly if using Confluence Jira Issues macro,"When using Greenhopper ranking field in conjunction with the jiraissues macro, the actual database field value is being reported in xml rather than a sanitized version."
3,Greenhopper ranking field is not displayed correctly if using Confluence Jira Issues macro,"When using Greenhopper ranking field in conjunction with the jiraissues macro, the actual database field value is being reported in xml rather than a sanitized version."
1,"Version can be set in the create issue screen in JIRA, but not in GreenHopper","Costumer has a project in which some users don't have schedule permission which means that they cannot manage the project backlog. However, they need the ability to raise new defects inside a specific version, e.g. when testing that sprint/version. They can do this via the JIRA create new issue screen. But they cannot do this via the GreenHopper user interface (when using the button ""New card"", the version is fixed as ""Unscheduled""). In this case, the user should have the ability to set the version in both JIRA (create issue) and GreenHopper (new card) screens."
1,"Version can be set in the create issue screen in JIRA, but not in GreenHopper","Costumer has a project in which some users don't have schedule permission which means that they cannot manage the project backlog. However, they need the ability to raise new defects inside a specific version, e.g. when testing that sprint/version. They can do this via the JIRA create new issue screen. But they cannot do this via the GreenHopper user interface (when using the button ""New card"", the version is fixed as ""Unscheduled""). In this case, the user should have the ability to set the version in both JIRA (create issue) and GreenHopper (new card) screens."
2,As a user I would like the ability to see a horizontal swimlane on my Rapid Board,"COS: - Based on Issue Priority Blocker - Only show swimlane when necessary - Warning if swimlane exceeds max amount - JQL longer term   ---- In order to implement Kanban concepts like service class, etc. we need to be able to create horizontal swimlanes with limited WIP for each. For example, I might want to have:  - Standard (Limit of 6) - Expedite (Limit of 1) - Fixed Delivery Date (Limit of 2)  LeanKit Kanban has this feature (actually, it is far more advanced than what I am describing)."
2,As a user I would like the ability to see a horizontal swimlane on my Rapid Board,"COS: - Based on Issue Priority Blocker - Only show swimlane when necessary - Warning if swimlane exceeds max amount - JQL longer term   ---- In order to implement Kanban concepts like service class, etc. we need to be able to create horizontal swimlanes with limited WIP for each. For example, I might want to have:  - Standard (Limit of 6) - Expedite (Limit of 1) - Fixed Delivery Date (Limit of 2)  LeanKit Kanban has this feature (actually, it is far more advanced than what I am describing)."
2,As a GH user I would like to be able to view the cumulative flow diagram using story points,NULL
1,"GreenHopper appends Project IDs to URL when adding to list of Enabled Projects, causes HTTP 413 error.","Per ticket here: https://support.atlassian.com/browse/GHS-2112  When adding select projects to the ""Allowed Projects"" listing (under Administration > GreenHopper > Enabled Projects,) adding a project to the allowed list creates a request URL like this: http://localhost:8080/secure//secure/SetAdmissibleProjects.jspa?decorator=none&listValue=10010%2C10000%2C  Where the response is a ""302 Temporarily Moved,"" a POST content size of zero, and a listValue entry that keeps getting longer.  Eventually, the list gets so long that the page will not load due to an ""HTTP 413 - Request entity too large"" error.  (Screenshots + Dev console in Chrome attached.)"
1,"GreenHopper appends Project IDs to URL when adding to list of Enabled Projects, causes HTTP 413 error.","Per ticket here: https://support.atlassian.com/browse/GHS-2112  When adding select projects to the ""Allowed Projects"" listing (under Administration > GreenHopper > Enabled Projects,) adding a project to the allowed list creates a request URL like this: http://localhost:8080/secure//secure/SetAdmissibleProjects.jspa?decorator=none&listValue=10010%2C10000%2C  Where the response is a ""302 Temporarily Moved,"" a POST content size of zero, and a listValue entry that keeps getting longer.  Eventually, the list gets so long that the page will not load due to an ""HTTP 413 - Request entity too large"" error.  (Screenshots + Dev console in Chrome attached.)"
0,"Investigate: Log work operation of a task in the task board redirect the page to ""ALL Assignee"".",*Steps to reproduce:*  1. Click on Agile Tab  2. Navigate to Chart Board > Task Board > Choose Assignee  3. This will display all cards for which the user is the Assignee  4. Log Work    *Expected Result:*  The work is succesfully logged  The card should make the Assignee unchanged (that is maintaining same Assignee)    *Actual Result:*  work logged successfully  The card is set to All Assignee      
0,"Investigate: Log work operation of a task in the task board redirect the page to ""ALL Assignee"".",*Steps to reproduce:*  1. Click on Agile Tab  2. Navigate to Chart Board > Task Board > Choose Assignee  3. This will display all cards for which the user is the Assignee  4. Log Work    *Expected Result:*  The work is succesfully logged  The card should make the Assignee unchanged (that is maintaining same Assignee)    *Actual Result:*  work logged successfully  The card is set to All Assignee      
1,Changing the fixVersion of a card to a fixVersion outside the Parent results in a page refresh and a change of the fixVersion filter (unexpected and inconsistent behaviour),"From https://studio.atlassian.com/browse/JST-3848    {quote}  This bug appeared on 14.2.2010, after the update on 13.2.2010.    Go to Task Board, select version X and developer D.  Change the version of a task to version Y.  The filter changes to version Y and no developer is selected (filter to All Assignees).    Was working fine until at least 11.2.2010, where the behaviour was that the flter stayed in place an a little message said something that ""you can't view the message you were working on because it is not in the filter"".    The current behavior is really annoying because you have to get back to the version you were working on.  Using the planning board is not an option because you cannot filter on the developer.  {quote}"
1,Changing the fixVersion of a card to a fixVersion outside the Parent results in a page refresh and a change of the fixVersion filter (unexpected and inconsistent behaviour),"From https://studio.atlassian.com/browse/JST-3848    {quote}  This bug appeared on 14.2.2010, after the update on 13.2.2010.    Go to Task Board, select version X and developer D.  Change the version of a task to version Y.  The filter changes to version Y and no developer is selected (filter to All Assignees).    Was working fine until at least 11.2.2010, where the behaviour was that the flter stayed in place an a little message said something that ""you can't view the message you were working on because it is not in the filter"".    The current behavior is really annoying because you have to get back to the version you were working on.  Using the planning board is not an option because you cannot filter on the developer.  {quote}"
1,"As a user I would like to be able to quickly toggle the display of columns on and off on A Board, similar to the quick filter buttons",NULL
1,"As a user I would like to be able to quickly toggle the display of columns on and off on A Board, similar to the quick filter buttons",NULL
0,"When creating a new card with security level on the card, it does not have 'None' in the select for security level.",NULL
0,"When creating a new card with security level on the card, it does not have 'None' in the select for security level.",NULL
1,As a GH User I would like the Assignee and Reporter fields on the New Card dialog to be the new shiny autocomplete to improve performance,NULL
1,As a GH User I would like the Assignee and Reporter fields on the New Card dialog to be the new shiny autocomplete to improve performance,NULL
0,WorklogHistoryCacheImpl doesn't remove cache registration on disposal,From [JST-4291|https://studio.atlassian.com/browse/JST-4291?focusedCommentId=39798&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-39798]  It's not possible to disable and re-enable the GH plugin in UPM atm. The problem is that the _com.atlassian.greenhopper.service.timetracking.WorklogHistoryCacheImpl_ doesn't clean up its registered cache on disposal. It should implement _org.springframework.beans.factory.DisposableBean_ and call _cacheManager.removeCache(CACHE_NAME)_ (watch out for ISE) or check for _net.sf.ehcache.CacheManager.cacheExists(String)_ on _afterPropertiesSet_.
0,WorklogHistoryCacheImpl doesn't remove cache registration on disposal,From [JST-4291|https://studio.atlassian.com/browse/JST-4291?focusedCommentId=39798&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-39798]  It's not possible to disable and re-enable the GH plugin in UPM atm. The problem is that the _com.atlassian.greenhopper.service.timetracking.WorklogHistoryCacheImpl_ doesn't clean up its registered cache on disposal. It should implement _org.springframework.beans.factory.DisposableBean_ and call _cacheManager.removeCache(CACHE_NAME)_ (watch out for ISE) or check for _net.sf.ehcache.CacheManager.cacheExists(String)_ on _afterPropertiesSet_.
1,As a user I would like the ability to edit the remaining estimate without using the JIRA Log Work dialog,Acceptance Criteria:  - Clicking on the Remaining Estimate field will allow inline edit  - Similar behaviour to the new inline text edit behaviour
1,As a user I would like the ability to edit the remaining estimate without using the JIRA Log Work dialog,Acceptance Criteria:  - Clicking on the Remaining Estimate field will allow inline edit  - Similar behaviour to the new inline text edit behaviour
0,Ability to print on standard index cards,"We are in the process of migrating from another system to Jira. We currently using a bubblejet printer to print stories directly onto 3x5 index cards.  With the current Greenhopper printing functionality, we can't print one card per page and therefore can't print directly onto cards.  We would like the ability print one card per page with the card layout sized to fit well on a 3x5 index card"
0,Ability to print on standard index cards,"We are in the process of migrating from another system to Jira. We currently using a bubblejet printer to print stories directly onto 3x5 index cards.  With the current Greenhopper printing functionality, we can't print one card per page and therefore can't print directly onto cards.  We would like the ability print one card per page with the card layout sized to fit well on a 3x5 index card"
1,"As a user I would like the Days Remaining Gadget to cascade down and select a release date that does exist, even if it is within a child version","For instance, at present it selects the parent version which has no release date set (cause we don't yet know when we release). Instead, it should select the child version that has a release date."
1,"As a user I would like the Days Remaining Gadget to cascade down and select a release date that does exist, even if it is within a child version","For instance, at present it selects the parent version which has no release date set (cause we don't yet know when we release). Instead, it should select the child version that has a release date."
1,As a user I would like the Rapid Board to be available via Labs,COS: - Labs option under General Configuration to enable/disable the Rapid Board - Rapid Board displayed in Agile drop down when enabled - Feedback button (using Andreas' JIRA plugin) in the header - Manage option when on Rapid Board for current RV
1,As a user I would like the Rapid Board to be available via Labs,COS: - Labs option under General Configuration to enable/disable the Rapid Board - Rapid Board displayed in Agile drop down when enabled - Feedback button (using Andreas' JIRA plugin) in the header - Manage option when on Rapid Board for current RV
1,As a user I would like the past three comments (space permitting) on an issue to be displayed in the detailed card view on the rapid board,"COS: - link to Show next 5 comments (if more than n left) - link to Show all remaining comments (if any left)  - details of total number of comments - v1 to have no tools (∞ + edit), no index, no twixy - expanded by default)"
1,As a user I would like the past three comments (space permitting) on an issue to be displayed in the detailed card view on the rapid board,"COS: - link to Show next 5 comments (if more than n left) - link to Show all remaining comments (if any left)  - details of total number of comments - v1 to have no tools (∞ + edit), no index, no twixy - expanded by default)"
1,As a rapid board user I would like to return to my last viewed Rapid View when I click on the Agile tab,"COS - If I am a Rapid Board user and have a remembered Rapid View (from my last visit) then I would like to be taken to that when I click on Agile, instead of being taken to the Planning Board"
1,As a rapid board user I would like to return to my last viewed Rapid View when I click on the Agile tab,"COS - If I am a Rapid Board user and have a remembered Rapid View (from my last visit) then I would like to be taken to that when I click on Agile, instead of being taken to the Planning Board"
1,As a user i would like the ability to manage my rapid views similar to jira managed filters,"List all rapid views, link to edit, delete. View option for non-owners."
1,As a user i would like the ability to manage my rapid views similar to jira managed filters,"List all rapid views, link to edit, delete. View option for non-owners."
1,Configuration of custom and scrum templates lost after upgrade,"If custom templates were created or the scrum template was modified in a version prior to 5.4.2, an upgrade to one of the affected ones will cause the configuration associated with these templates to be lost. This bug is caused as a side effect of fixing http://jira.atlassian.com/browse/GHS-2826.    See http://confluence.atlassian.com/x/fBIaDg for additional information."
1,Configuration of custom and scrum templates lost after upgrade,"If custom templates were created or the scrum template was modified in a version prior to 5.4.2, an upgrade to one of the affected ones will cause the configuration associated with these templates to be lost. This bug is caused as a side effect of fixing http://jira.atlassian.com/browse/GHS-2826.    See http://confluence.atlassian.com/x/fBIaDg for additional information."
0,Editing the labels triggers a page reload - on PlanningBoard,NULL
0,Editing the labels triggers a page reload - on PlanningBoard,NULL
1,As a user I would like the ability to create and release a fix version containing all of the issues in my done column,"COS: - Based on correct permissions (Project Administrator) - Release button on RHS column (normally ""Done"") - Release dialog w/  - new versions   - name the fix version   - set the description   - set the release date (auto populate with today's date, plus date picker) - Notifications that explain what occured"
1,As a user I would like the ability to create and release a fix version containing all of the issues in my done column,"COS: - Based on correct permissions (Project Administrator) - Release button on RHS column (normally ""Done"") - Release dialog w/  - new versions   - name the fix version   - set the description   - set the release date (auto populate with today's date, plus date picker) - Notifications that explain what occured"
1,As a user I would like the ability to add maximum and minimum constraints to a column on my Rapid View based upon total issue count,"COS - Configure this on the Manage Rapid View using inline edit - Default to total issue count (include sub tasks, exclude swimlane duplicates) - Yellow for min constraint busted in header - Red for max constraint busted in header - Show min, current, max at top of column"
1,As a user I would like the ability to add maximum and minimum constraints to a column on my Rapid View based upon total issue count,"COS - Configure this on the Manage Rapid View using inline edit - Default to total issue count (include sub tasks, exclude swimlane duplicates) - Yellow for min constraint busted in header - Red for max constraint busted in header - Show min, current, max at top of column"
1,As a user I would like to be able to edit a Rapid View that I created using someone elses saved filter,"COS: - regardless of who owns the Saved Filter, if I have created a Rapid View based on it I can edit the Rapid View - Other users can not edit my Rapid View - Manage Rapid View has message saying you cant configure and link to owner"
1,As a user I would like to be able to edit a Rapid View that I created using someone elses saved filter,"COS: - regardless of who owns the Saved Filter, if I have created a Rapid View based on it I can edit the Rapid View - Other users can not edit my Rapid View - Manage Rapid View has message saying you cant configure and link to owner"
1,As a user I would like a control chart showing the time an issue was in progress,COS - in progress is defined as those columns which are yellow
1,As a user I would like a control chart showing the time an issue was in progress,COS - in progress is defined as those columns which are yellow
1,"As a user I would like the ability to configure the ""expedite"" swimlane on my Rapid View","COS - Change the name of the swimlane, default to 'Expedite' - one swimlane with JQL ""AND"" - configure by JQL in a free text field - configure on the Manage Rapid View page - no validation, no autocomplete - if there is no JQL the swimlane is not displayed"
1,"As a user I would like the ability to configure the ""expedite"" swimlane on my Rapid View","COS - Change the name of the swimlane, default to 'Expedite' - one swimlane with JQL ""AND"" - configure by JQL in a free text field - configure on the Manage Rapid View page - no validation, no autocomplete - if there is no JQL the swimlane is not displayed"
0,Issues without a priority set should not be given a priority icon on the Rapid Board as it is misleading/incorrect,
0,Issues without a priority set should not be given a priority icon on the Rapid Board as it is misleading/incorrect,
1,As a user I would like to see a cumulative flow diagram on my Rapid Board to show the progression of issues across my board,"COS: - In Flot - On the Reporting tab of the Rapid Board - Based upon the current Rapid View - Similar colour scheme to the gadget (for consistency) - Not configurable based on columns - Go to Control Chart when loading Reporting, have a drop down for chart types where 'Cumulative Flow Diagram' can be found"
1,As a user I would like to see a cumulative flow diagram on my Rapid Board to show the progression of issues across my board,"COS: - In Flot - On the Reporting tab of the Rapid Board - Based upon the current Rapid View - Similar colour scheme to the gadget (for consistency) - Not configurable based on columns - Go to Control Chart when loading Reporting, have a drop down for chart types where 'Cumulative Flow Diagram' can be found"
1,As a user I would like to be able to return an unlimited number of results,"COS: - the query should be limited to the number of issues in Global Configuration, Query Results Limit - if there is no limit set it will return all issues"
1,As a user I would like to be able to return an unlimited number of results,"COS: - the query should be limited to the number of issues in Global Configuration, Query Results Limit - if there is no limit set it will return all issues"
1,As a user I would like to see the mean and the standard deviation on my control chart,"COS: - perhaps a similar style to http://people.iola.dk/olau/flot/examples/percentiles.html - show this as a line on the control chart for the mean, with a highlight to show std dev - no interaction expected - use all chart data - one standard deviation for the highlight"
1,As a user I would like to see the mean and the standard deviation on my control chart,"COS: - perhaps a similar style to http://people.iola.dk/olau/flot/examples/percentiles.html - show this as a line on the control chart for the mean, with a highlight to show std dev - no interaction expected - use all chart data - one standard deviation for the highlight"
1,As a user I would like my everything else swimlane to exclude the issues from the configured swimlane,"COS: - remove the card stubs in everything else - everything else swimlane appends swimlane JQL as ""AND NOT"" - remove unnecessary code (CSS and JS) - change 'default swimlane' to 'Everything Else'"
1,As a user I would like my everything else swimlane to exclude the issues from the configured swimlane,"COS: - remove the card stubs in everything else - everything else swimlane appends swimlane JQL as ""AND NOT"" - remove unnecessary code (CSS and JS) - change 'default swimlane' to 'Everything Else'"
0,As a user I would like the 'New Column' text in a new column to be selected enabling quick update,
0,As a user I would like the 'New Column' text in a new column to be selected enabling quick update,
0,As a user I would like the global statuses to be sorted by name in the Manage Rapid View to assist me when creating a new column,Unmapped column only
0,As a user I would like the global statuses to be sorted by name in the Manage Rapid View to assist me when creating a new column,Unmapped column only
0,As a user I would like a link from my Manage Rapid View screen back to the Rapid Board,
0,As a user I would like a link from my Manage Rapid View screen back to the Rapid Board,
1,"GreenHopper does not display the assignee auto-complete when the ""User Searching by Full Name"" JIRA configuration option has been turned off",See [JST-4804|https://studio.atlassian.com/browse/JST-4804] for more information. Please resolve that when resolving this.
1,"GreenHopper does not display the assignee auto-complete when the ""User Searching by Full Name"" JIRA configuration option has been turned off",See [JST-4804|https://studio.atlassian.com/browse/JST-4804] for more information. Please resolve that when resolving this.
1,URL doesn't select issue on the Planning Board as it should,For [instance|https://jira.atlassian.com/secure/ProjectBoard.jspa?pageType=PlanningBoard&selectedProjectId=12200&viewIssueKey=GHS-1098&subType=ProjectBoard#] on GreenHopper 5.6.3 that does not select the correct page and the issue on the correct page.
1,URL doesn't select issue on the Planning Board as it should,For [instance|https://jira.atlassian.com/secure/ProjectBoard.jspa?pageType=PlanningBoard&selectedProjectId=12200&viewIssueKey=GHS-1098&subType=ProjectBoard#] on GreenHopper 5.6.3 that does not select the correct page and the issue on the correct page.
1,As a user I would like to be able to add another swimlane to my Rapid View,COS:  - Add additional swimlanes  - Ability to delete swimlanes  - Not Sortable (will be another story)  - JIRA Restful Table UI/UX e.g. Manage Versions
1,As a user I would like to be able to add another swimlane to my Rapid View,COS:  - Add additional swimlanes  - Ability to delete swimlanes  - Not Sortable (will be another story)  - JIRA Restful Table UI/UX e.g. Manage Versions
0,When clicking on issue on control chart show issue in detail view,
0,When clicking on issue on control chart show issue in detail view,
0,"As a rapid board owner I would like to be able to have a column constraint for ""Issues, excluding Sub-Tasks""","(!) Testing Notes (!)  Q: What happens in the case with an issue that has a subtask not captured by the filter? A: (/) Nothing - the count is for parent issues and not sub-tasks, so the count will not be affected.  Q: Test to see this works with Stories Swimlane config, as well as Queries. A: (/) Still works, though it is less clear what the issue count refers to given that you do not see the parent issue in the column it is in.  Q: What about negative numbers? A: (/) They continue to not be allowed - same input controls for constraints as before.  Q: Is this customizable for individual constraints, or globally for all constraints? A: (/) For the whole board, not individual columns  Q: What if the last parent is removed from that column but it's subtasks remain? A: (/) The count drops to zero.  Q: What happens if there is no subtask issuetype? A: (/) Nothing - the count should not be affected by issues which do not exist  Q: What happens when there are quick filters active that filter out parent issues? What happens when there are quick filters active that filter out sub-task issues? A: (/) The count should still display the appropriate number of issues - so if column constraint excludes sub tasks then filtering out parents should set all visible counts to 0."
0,"As a user, if I accidentally browse the Rapid Start before logout, I want to be about to go straight to RapidBoard when I login","Currently GreenHopper will remember that you were at the RapidStart when you logout, and when you log back in, clicking the Agile tab brings you back there. We want to change the behavior so that the user returns to the RapidBoard instead. This will reduce annoyance for the customer."
0,make sub projects,"We really need this feature, where a single projects can have its own projects"
0,make sub projects,"We really need this feature, where a single projects can have its own projects"
0,"As a user, I would like to get visual feedback on the progress of Plan Mode's loading","When you have a lot of issues, it can be slow to load and it looks like nothing is happening."
1,"As a user, I would like to be able to quickly filter issues out in Plan mode","Quick filters   Best Effort is fine for now, probably same as quick filters on Work tab (seems consistent)  The marker should still count items above the line even if they are filtered  "
0,"As a user, I would like to just see Rapid Board everywhere Rapid View exists currently",NULL
0,"As a user, I would like to just see Rapid Board everywhere Rapid View exists currently",NULL
0,In Doco: I would like to see Rapid Board everywhere Rapid View is currently,NULL
0,In Doco: I would like to see Rapid Board everywhere Rapid View is currently,NULL
0,"As an OnDemand user, I'd like to be able to provide the Atlassian team with feedback on GH labs features","Need to implement a ""got feedback"" mechanism for OnDemand like we do in BTF."
0,A problem with Rapid Board`s cards display.,"If a Rapid Board includes many columns (> 11 columns)and a screen resolution is resolution - 1280 x 720, then ticket`s key and summary are become unreadable (like on an attached screenshot).     !rapid_board.png|thumbnail!    This becomes problem where user cannot use less amount of columns and we can`t increase a browser window size in some cases (when using a laptop, for example).    Thanks in advance.    SAC ticket: https://support.atlassian.com/browse/GHS-3831  "
1,New features in dashboard for drop down list,New features in dashboard for drop down list a drop down list should be added to the dashboard for easy access
1,New features in dashboard for drop down list,New features in dashboard for drop down list a drop down list should be added to the dashboard for easy access
1,"As a user, I would like to view attachments when elaborating stories in a Sprint planning meeting",#NAME?
1,"As a user, I'd like to see a burn down that includes both the 'feeling' of the burndown as well as the actual facts",NULL
1,"As a user, I'd like to see a burn down that includes both the 'feeling' of the burndown as well as the actual facts",NULL
0,"As a user, I would like to see the issue priority in plan mode",Need some basic design
1,"As a user, I would like sub tasks to have the same Sprint as their parent in all cases ",#NAME?
0,NullPointerException occurs when editing issue - related to 'GreenHopper Released Version History' field,"When attempting to edit an issue in JIRA the edit fails when user clicks the 'update' button, it throws a java.lang.NullPointerException at the top of the edit issue window. If you check the logs the following error appears:    {noformat}  @400000004f45c7530fe9a96c 2012-02-23 01:57:45,265 TP-Processor10 ERROR sysadmin 117x11680x1 evyd4p 59.167.133.99,207.223.247.46 /secure/QuickEditIssue.jspa [jira.bc.issue.DefaultIssueService] Exception occurred editing issue: java.lang.NullPointerException  @400000004f45c7530fe9b13c java.lang.NullPointerException  @400000004f45c7530fe9b524       at java.util.AbstractCollection.addAll(AbstractCollection.java:303)  @400000004f45c7530fe9b524       at com.pyxis.greenhopper.jira.customfields.ReleasedVersionHistoryCFType.updateValue(ReleasedVersionHistoryCFType.java:68)  @400000004f45c7530fed04cc       at com.pyxis.greenhopper.jira.customfields.ReleasedVersionHistoryCFType.updateValue(ReleasedVersionHistoryCFType.java:25)  @400000004f45c7530fed08b4       at com.atlassian.jira.issue.fields.CustomFieldImpl.updateValue(CustomFieldImpl.java:405)  @400000004f45c7530fed08b4       at com.atlassian.jira.issue.fields.CustomFieldImpl.updateValue(CustomFieldImpl.java:366)  @400000004f45c7530fed146c       at com.atlassian.jira.issue.managers.DefaultIssueManager.updateFieldValues(DefaultIssueManager.java:589)  @400000004f45c7530fed146c       at com.atlassian.jira.issue.managers.DefaultIssueManager.updateIssue(DefaultIssueManager.java:554)  @400000004f45c7530fed1854       at com.atlassian.jira.bc.issue.DefaultIssueService.update(DefaultIssueService.java:272)  @400000004f45c7530fed27f4       at com.atlassian.jira.bc.issue.DefaultIssueService.update(DefaultIssueService.java:246)  @400000004f45c7530fed2bdc       at sun.reflect.GeneratedMethodAccessor1929.invoke(Unknown Source)  @400000004f45c7530fed2bdc       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004f45c7530fed2fc4       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004f45c7530fed33ac       at com.atlassian.multitenant.impl.MultiTenantComponentFactoryImpl$AbstractMultiTenantAwareInvocationHandler.invokeInternal(MultiTenantComponentFactoryImpl.java:181)  @400000004f45c7530fed3794       at com.atlassian.multitenant.impl.MultiTenantComponentFactoryImpl$MultiTenantAwareInvocationHandler.invoke(MultiTenantComponentFactoryImpl.java:211)  @400000004f45c7530fed3b7c       at $Proxy298.update(Unknown Source)  @400000004f45c7530fed3b7c       at sun.reflect.GeneratedMethodAccessor1929.invoke(Unknown Source)  @400000004f45c7530fed434c       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004f45c7530fed434c       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004f45c7530fed4734       at com.atlassian.plugin.osgi.hostcomponents.impl.DefaultComponentRegistrar$ContextClassLoaderSettingInvocationHandler.invoke(DefaultComponentRegistrar.java:129)  @400000004f45c7530fed4f04       at $Proxy298.update(Unknown Source)  @400000004f45c7530fed52ec       at sun.reflect.GeneratedMethodAccessor1929.invoke(Unknown Source)  @400000004f45c7530fed52ec       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004f45c7530fed56d4       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004f45c7530fed5abc       at com.atlassian.plugin.osgi.bridge.external.HostComponentFactoryBean$DynamicServiceInvocationHandler.invoke(HostComponentFactoryBean.java:154)  @400000004f45c7530fed5ea4       at $Proxy298.update(Unknown Source)  @400000004f45c7530fed5ea4       at com.atlassian.jira.quickedit.action.QuickEditIssue.doExecute(QuickEditIssue.java:128)  @400000004f45c7530fed628c       at webwork.action.ActionSupport.execute(ActionSupport.java:165)  @400000004f45c7530fed79fc       at com.atlassian.jira.action.JiraActionSupport.execute(JiraActionSupport.java:82)  @400000004f45c7530fed79fc       at webwork.interceptor.DefaultInterceptorChain.proceed(DefaultInterceptorChain.java:39)  @400000004f45c7530fed7de4       at webwork.interceptor.NestedInterceptorChain.proceed(NestedInterceptorChain.java:31)  @400000004f45c7530fed7de4       at webwork.interceptor.ChainedInterceptor.intercept(ChainedInterceptor.java:16)  @400000004f45c7530fed85b4       at webwork.interceptor.DefaultInterceptorChain.proceed(DefaultInterceptorChain.java:35)  @400000004f45c7530fed916c       at webwork.dispatcher.GenericDispatcher.executeAction(GenericDispatcher.java:205)  @400000004f45c7530fed916c       at webwork.dispatcher.GenericDispatcher.executeAction(GenericDispatcher.java:143)  @400000004f45c7530fed9554       at com.atlassian.jira.web.dispatcher.JiraWebworkActionDispatcher.service(JiraWebworkActionDispatcher.java:152)  @400000004f45c7530fed993c       at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)  @400000004f45c7530fed9d24       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)  @400000004f45c7530feda10c       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  @400000004f45c7530feda10c       at com.atlassian.jira.web.filters.steps.ChainedFilterStepRunner.doFilter(ChainedFilterStepRunner.java:78)  @400000004f45c7530feda4f4       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)  @400000004f45c7530feda8dc       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  @400000004f45c7530fedacc4       at com.atlassian.core.filters.HeaderSanitisingFilter.doFilter(HeaderSanitisingFilter.java:44)  @400000004f45c7530fedacc4       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)  @400000004f45c7530fedb87c       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  @400000004f45c7530fedb87c       at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  @400000004f45c7530fedbc64       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)  @400000004f45c7530fedbc64       at com.atlassian.applinks.core.rest.context.ContextFilter.doFilter(ContextFilter.java:25)  @400000004f45c7530fedc81c       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)  @400000004f45c7530fedc81c       at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)  @400000004f45c7530fedcc04       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)  @400000004f45c7530fedcc04       at com.atlassian.agmp.integration.jira.ReprovisionConfigFilter.doFilter(ReprovisionConfigFilter.java:87)  @400000004f45c7530fedd3d4       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)  {noformat}    I've confirmed that this error is only occuring for issues which have a value in a custom field of type 'GreenHopper Released Version History'.    h2. Workaround    This only appears to occur when the 'Release Version History' field has been added to the screen used for editing. If you remove this field from all screens then the issue should be resolved."
0,NullPointerException occurs when editing issue - related to 'GreenHopper Released Version History' field,"When attempting to edit an issue in JIRA the edit fails when user clicks the 'update' button, it throws a java.lang.NullPointerException at the top of the edit issue window. If you check the logs the following error appears:    {noformat}  @400000004f45c7530fe9a96c 2012-02-23 01:57:45,265 TP-Processor10 ERROR sysadmin 117x11680x1 evyd4p 59.167.133.99,207.223.247.46 /secure/QuickEditIssue.jspa [jira.bc.issue.DefaultIssueService] Exception occurred editing issue: java.lang.NullPointerException  @400000004f45c7530fe9b13c java.lang.NullPointerException  @400000004f45c7530fe9b524       at java.util.AbstractCollection.addAll(AbstractCollection.java:303)  @400000004f45c7530fe9b524       at com.pyxis.greenhopper.jira.customfields.ReleasedVersionHistoryCFType.updateValue(ReleasedVersionHistoryCFType.java:68)  @400000004f45c7530fed04cc       at com.pyxis.greenhopper.jira.customfields.ReleasedVersionHistoryCFType.updateValue(ReleasedVersionHistoryCFType.java:25)  @400000004f45c7530fed08b4       at com.atlassian.jira.issue.fields.CustomFieldImpl.updateValue(CustomFieldImpl.java:405)  @400000004f45c7530fed08b4       at com.atlassian.jira.issue.fields.CustomFieldImpl.updateValue(CustomFieldImpl.java:366)  @400000004f45c7530fed146c       at com.atlassian.jira.issue.managers.DefaultIssueManager.updateFieldValues(DefaultIssueManager.java:589)  @400000004f45c7530fed146c       at com.atlassian.jira.issue.managers.DefaultIssueManager.updateIssue(DefaultIssueManager.java:554)  @400000004f45c7530fed1854       at com.atlassian.jira.bc.issue.DefaultIssueService.update(DefaultIssueService.java:272)  @400000004f45c7530fed27f4       at com.atlassian.jira.bc.issue.DefaultIssueService.update(DefaultIssueService.java:246)  @400000004f45c7530fed2bdc       at sun.reflect.GeneratedMethodAccessor1929.invoke(Unknown Source)  @400000004f45c7530fed2bdc       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004f45c7530fed2fc4       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004f45c7530fed33ac       at com.atlassian.multitenant.impl.MultiTenantComponentFactoryImpl$AbstractMultiTenantAwareInvocationHandler.invokeInternal(MultiTenantComponentFactoryImpl.java:181)  @400000004f45c7530fed3794       at com.atlassian.multitenant.impl.MultiTenantComponentFactoryImpl$MultiTenantAwareInvocationHandler.invoke(MultiTenantComponentFactoryImpl.java:211)  @400000004f45c7530fed3b7c       at $Proxy298.update(Unknown Source)  @400000004f45c7530fed3b7c       at sun.reflect.GeneratedMethodAccessor1929.invoke(Unknown Source)  @400000004f45c7530fed434c       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004f45c7530fed434c       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004f45c7530fed4734       at com.atlassian.plugin.osgi.hostcomponents.impl.DefaultComponentRegistrar$ContextClassLoaderSettingInvocationHandler.invoke(DefaultComponentRegistrar.java:129)  @400000004f45c7530fed4f04       at $Proxy298.update(Unknown Source)  @400000004f45c7530fed52ec       at sun.reflect.GeneratedMethodAccessor1929.invoke(Unknown Source)  @400000004f45c7530fed52ec       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  @400000004f45c7530fed56d4       at java.lang.reflect.Method.invoke(Method.java:597)  @400000004f45c7530fed5abc       at com.atlassian.plugin.osgi.bridge.external.HostComponentFactoryBean$DynamicServiceInvocationHandler.invoke(HostComponentFactoryBean.java:154)  @400000004f45c7530fed5ea4       at $Proxy298.update(Unknown Source)  @400000004f45c7530fed5ea4       at com.atlassian.jira.quickedit.action.QuickEditIssue.doExecute(QuickEditIssue.java:128)  @400000004f45c7530fed628c       at webwork.action.ActionSupport.execute(ActionSupport.java:165)  @400000004f45c7530fed79fc       at com.atlassian.jira.action.JiraActionSupport.execute(JiraActionSupport.java:82)  @400000004f45c7530fed79fc       at webwork.interceptor.DefaultInterceptorChain.proceed(DefaultInterceptorChain.java:39)  @400000004f45c7530fed7de4       at webwork.interceptor.NestedInterceptorChain.proceed(NestedInterceptorChain.java:31)  @400000004f45c7530fed7de4       at webwork.interceptor.ChainedInterceptor.intercept(ChainedInterceptor.java:16)  @400000004f45c7530fed85b4       at webwork.interceptor.DefaultInterceptorChain.proceed(DefaultInterceptorChain.java:35)  @400000004f45c7530fed916c       at webwork.dispatcher.GenericDispatcher.executeAction(GenericDispatcher.java:205)  @400000004f45c7530fed916c       at webwork.dispatcher.GenericDispatcher.executeAction(GenericDispatcher.java:143)  @400000004f45c7530fed9554       at com.atlassian.jira.web.dispatcher.JiraWebworkActionDispatcher.service(JiraWebworkActionDispatcher.java:152)  @400000004f45c7530fed993c       at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)  @400000004f45c7530fed9d24       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)  @400000004f45c7530feda10c       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  @400000004f45c7530feda10c       at com.atlassian.jira.web.filters.steps.ChainedFilterStepRunner.doFilter(ChainedFilterStepRunner.java:78)  @400000004f45c7530feda4f4       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)  @400000004f45c7530feda8dc       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  @400000004f45c7530fedacc4       at com.atlassian.core.filters.HeaderSanitisingFilter.doFilter(HeaderSanitisingFilter.java:44)  @400000004f45c7530fedacc4       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)  @400000004f45c7530fedb87c       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  @400000004f45c7530fedb87c       at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:46)  @400000004f45c7530fedbc64       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)  @400000004f45c7530fedbc64       at com.atlassian.applinks.core.rest.context.ContextFilter.doFilter(ContextFilter.java:25)  @400000004f45c7530fedc81c       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)  @400000004f45c7530fedc81c       at com.atlassian.plugin.servlet.filter.IteratingFilterChain.doFilter(IteratingFilterChain.java:42)  @400000004f45c7530fedcc04       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter$1.doFilter(DelegatingPluginFilter.java:66)  @400000004f45c7530fedcc04       at com.atlassian.agmp.integration.jira.ReprovisionConfigFilter.doFilter(ReprovisionConfigFilter.java:87)  @400000004f45c7530fedd3d4       at com.atlassian.plugin.servlet.filter.DelegatingPluginFilter.doFilter(DelegatingPluginFilter.java:74)  {noformat}    I've confirmed that this error is only occuring for issues which have a value in a custom field of type 'GreenHopper Released Version History'.    h2. Workaround    This only appears to occur when the 'Release Version History' field has been added to the screen used for editing. If you remove this field from all screens then the issue should be resolved."
1,As an admin who hasn't seen the analytics question I would like to be asked on every page until I say yes or no,"Remove the X  Getting Started, Rapid Board, Configure Rapid Board, Manage Rapid Board, GreenHopper General Configuration page, License Page"
1,As an admin who hasn't seen the analytics question I would like to be asked on every page until I say yes or no,"Remove the X  Getting Started, Rapid Board, Configure Rapid Board, Manage Rapid Board, GreenHopper General Configuration page, License Page"
0,"As a user, I'd like the issue I just created on a board to be the selected issue by default, or to get a popup indicating it was created if the issue is not visible",To be clear: * Create issue and it is visible -> select the issue * Create issue and it is not visible -> show a popup * Does not happen in Report mode (only Plan and Work)
1,"As a user, I would like to see the issue count on the Sprint marker ",NULL
1,MVR: Issue operation to jump to a rapid board from an Issue,"This is a complex one, so let's break it down.    * Providing an issue operation for an Issue to be able to ""go to"" a Rapid Board for this issue.  * The boards available to go to will be calculated only once the user initiates the action.  * Valid boards are calculated based on which boards the user has permission to see, as well as if the issue matches the filter of the board;  ** In the case of Kanban boards, the issue must also match the Work mode sub filter. It should *not* match if the issue only appears on Report mode.  ** In the case of Scrum boards, the issue may appear in any mode (Plan, Work or Report).  * If there are no valid boards, the user will be shown a message;  ** _Optionally, the user will be prompted to create a new board based on the project of the issue_.  * If there are multiple valid boards, the user will be prompted to choose one.  * If there is only one valid board, or if the user has chosen a board, the user will be redirected instantly.  * The user will be redirected to the precise location of the issue on the chosen board.  ** The issue will be selected if the location is Plan or Work mode.  ** In the case of a Scrum board, if the location is the Report mode, and the issue belongs to more than one sprint, the most recent sprint's retrospective report will be chosen."
1,"MVR: Report page showing Sprints (with a selector), include table of issues, link to issue nav",NULL
1,MVR: Sprint Report to include a burndown for the Sprint itself,#NAME?
2,MVR: Make the burndown chart understand when an issue was added/removed from Sprint (i.e understand scope change),"- Should handle change of Story Points value   https://extranet.atlassian.com/display/GHDEV/Estimation+and+Tracking+in+the+Rapid+Board#EstimationandTrackingintheRapidBoard-Tracking - The graph begins at the start time of the Sprint, it logically ends on the specified end date of the Sprint (which is marked on the graph) but continues until the time the Sprint was actually closed - Allow the Sprint to be specified but default to currently active Sprint  - The start value of the graph does not compensate for changes after the start of the sprint to the value  - Items burn down as they reach the done column  - Items that are added to the Sprint after its beginning are shown as scope change  - Items that are removed from the Sprint after its beginning are shown as scope change  - Items that have their estimate changed after the beginning of the Sprint are shown as scope change  - Scope changes (i.e anything other than a burndown) are shown with some sort of visual signal and information on hovering the inflection point "
2,MVR: Search (filter) for Rapid Board Plan mode - (Search by issue key or by summary text),Client side as a first attempt  Straight direct match of substring is minimum (no need for multiple keywords)  
1,MVR: Add a cog to access issue actions from detail view ,NULL
2,"MVR: Allow plan mode to show subtasks, include a default filter for Scrum boards which hide subtasks",NULL
2,"MVR: Allow plan mode to show subtasks, include a default filter for Scrum boards which hide subtasks",NULL
3,MVR: Implement issue swimlane strategy,Java remodelling
2,MVR: Implement visualisation of swimlanes as parents ,Needs drop down in the config (refer to design) Parents always present except if themselves and all of their children are filtered  Swimlane configuration retains user defined filters even if you switch to parent strategy  
0,"As a user, I would like to see the status of an issue in the detail view (in work and plan)",Not including the Status icon as this may be 'gone' in JIRA soon anyway...
0,As a user I would like to rank above the marker even when there is one issue in scope,NULL
1,MVR: Prompt the user to resolve the parent when the subtasks are all done ,Just for parent swimlane strategy
1,MVR: Prompt at the end of a Sprint if unfinished parent stories with all complete subtasks exist,#NAME?
1,MV?: When all of the subtasks of a story are complete but the parent is not show an affordance that can be clicked by the user to advance the parent (and try to resolve the issue),Clarification: Summary states that the affordance only needs to appear when the parent status is out of sync.    Should the affordance just show the status of the parent  Minimum required
3,MV?: Implement the skinny detail view,Moves as you traverse with n and p   Fixed on RHS with all columns visible) (was Shifts to the right of the currently selected column 
0,As a user I would like to go to GH as my default JIRa home page,This story involves implementing Trevs MyHome page feature.    Currently this is a per user dark feature.    We could add a -button- affordance to make this available on the board page
1,Upgrade task to unify Board/Sprint data into AO instead of using PropertySets,NULL
1, MVR: Agile issue web panel to display Scrum information of an issue,"This is a complex one, so let's break it down.    * Providing a web panel on the View Issue page to display Scrum information.  * Information displayed will be a list of the sprints of which this issue is a part of.  * The sprints will be broken down into two sections: Active and Completed  * Sprints will provide a link to ""go to the sprint"". This means going to a Rapid Board for this sprint.  * The boards available to go to will be calculated only once the user initiates the action.  * Valid boards are calculated based on which boards the user has permission to see, as well as if the issue matches the filter of the board;  ** Only scrum boards will be considered.  * If there are no valid boards, the user will be shown a message;  * If there are multiple valid boards, the user will be prompted to choose one.  * If there is only one valid board, or if the user has chosen a board, the user will be redirected instantly.  * The user will be redirected to the precise location of the sprint on the chosen board;  ** If the sprint is active, the location is Work mode of the board.  ** If the sprint is completed, the location is the retrospective report of that sprint on that board."
0,"As a user, I'd like to avoid confusion when hitting enter in the description field and having that save by saving on blur",NULL
1,Enhance Sprint Report ,"- Include scheduled start/end and actual end (triggered when hit complete button)  - Divide into issues completed during Sprint and those not, include the status of the item at the end of the Sprint   - Include the estimate statistic for each item (ideally from time the item was added to the Sprint but current if not easy) - include the sum of the statistic (i.e the velocity)  - -Call out issues that were added and removed-  "
1,Implement Sprint Burndown chart (for Numeric Fields),- For the specified estimation statistic field (assuming the 'time tracking' option has not been specified for tracking)
3,Make the Sprint Burndown chart use remaining estimate when the board has been configured that way,#NAME?
1,Add a subtasks tab to the detail view on plan mode ,"- Add a new 'Subtasks' tab with an add button  - For each sub task: Show summary, key, avatar, status, remaining estimate, edit button (JIRA Edit Dialog), delete button"
2,Allow the backlog item estimation statistic to be selected as any numeric field in the issue,"- Allow the Backlog item estimation statistic for a *Scrum* Rapid Board to be selected, the default should be the Story Point field but it should also be possible to set  any other Numeric field   - Show the Backlog item estimation statistic on the Plan mode in the lozenge  - Replaces existing use of lozenge (in detail view & card view)  - Label unit as 'Estimate', on hover show unit"
1,Implement the Velocity chart,"- The Velocity chart will show amount of the Backlog item estimation statistic from Sprint to Sprint  - The graph is bucketed by Sprint  - For each Sprint the velocity is equal to the sum of the backlog item estimate for all issues that are in the last column at the end of the Sprint - Would also show the sum of estimation statistic committed to during the Sprint (i.e the ones that did not get done) - The PBI estimate used is from the time the issue was added to the Sprint (i.e at the start of the Sprint if the item was in at Sprint start or at the time it was added in the middle of the Sprint) - The graph is followed by a table that describes the values shown in the table - For each Sprint a row is shown along with the sum of completed and non completed PBIs, hyperlink to the Sprint report - A link is provided to the detailed Sprint burndown for that Sprint  - For now hardcode to last 7"
2,Allow the Backlog item estimation statistic for a Rapid Board to be set to Original Estimate  ,Allow the Backlog item estimation statistic for a Rapid Board to be selected to be set to Original Estimate   
1,Warn on start sprint if primary items do not have the backlog item estimate set,#NAME?
1,Allow a numeric field to be specified Sprint tracking statistic for the Rapid Board,#NAME?
1,Allow a numeric field to be specified Sprint tracking statistic for the Rapid Board,#NAME?
1,Add a detail view cog menu item to remove an issue from a Sprint (DOES cause scope change),#NAME?
1,Implement Data Table for Burndown chart,- The graph is followed by a table that describes the exact timeline shown in the graph:     * At each point in time the issue that changed and the values that changed are shown     * At time 0 the table lists all issues that were present at the beginning of the graph     * At time <end> the table lists all issues that were remaining
1,Go to best Sprint in retro report,- If the Sprint Retro report is opened and I've not opened it before I should  go latest finished Sprint  - If I was on the 'latest finished sprint' when I go back to the retro report it should always show me the 'latest finished sprint'
1,Implement Original Estimate burndown,NULL
3,Allow a user to configure tracking to use Remaining Estimate,"- In configuration describe this as ""This option will show the Remaining Estimate on the detail view so you can edit it""  - Make card and detail view operate as expected - If the estimate statistic is set to ""Original Time Estimate"", then when you enter an Estimate for the first time, it should also update the Remaining Time Estimate for the issue *iff* there is no value set for it already."
1,Make TEAMCAL integration work,NULL
0,Incoporate the changes to My JIRA home into GH next,"The JIRA team have tweaked (changed) the My JIRA home in 5.1      Martin Meinhold 12:53:07  the link element in the web-item only contains the target url, like:    <link linkId=""set_my_jira_home_issuenav"">/secure/IssueNavigator.jspa</link>    The actual update url is provided by JIRA itself  so no: /secure/MyJiraHome!Set.jspa?target= 12:54:01      So we need to put in 2 web items (since its not on 5.0.x) and have a condition on them so that only one turns up as appropriate."
0,"As a user, following Sprint completion I'd like to see the report of everything that happened automatically",NULL
1,Figure out what to do with issues which come into a sprint with no statistic value and then change half way through,"- Put all items in to the data table at the bottom (i.e when they're added or removed from the Sprint) even if they do not have an estimate and therefore don't affect the burndown  Add issue to sprint with no estimate, then change estimate, only shows as statistic value change in table. This problem is particularly bad when you then later remove the issue from the sprint - as the issue addition and removal is completely hidden from the table and is not shown as scope change."
1,Figure out what to do with issues which come into a sprint with no statistic value and then change half way through,"- Put all items in to the data table at the bottom (i.e when they're added or removed from the Sprint) even if they do not have an estimate and therefore don't affect the burndown  Add issue to sprint with no estimate, then change estimate, only shows as statistic value change in table. This problem is particularly bad when you then later remove the issue from the sprint - as the issue addition and removal is completely hidden from the table and is not shown as scope change."
1,As a planner I want to be able to show the sum of all issues' remaining estimate in an upcoming sprint,"As a RapidBoardUser I want to be able to show the sum of all issues remainingestimate, so I can see which workload I have.  COS: - Not show if equal - Only when tracking is enabled for a board"
1,As an admin I would like to know what greenhopper upgrade tasks are up to,Currently GH does some things to protect itself in upgrade tasks but not enough    We need to track    * when a upgrade tasks runs  * what upgrade task  * what version of GH at the time    
1,Rearrange the Agile menu,"- Labs configuration option used to enable new Agile menu - 5 most recently used boards at the top, 'Other...' takes you to manage  - 'Classic...' at the bottom, takes me to Planning version board initially then I can choose another one using the dropdown menu  - Click on Agile should work as currently (i.e most recent classic or Rapid Board) "
1,MVR: When I click 'Classic...' in the new menu structure give me a way back,"- If I go to Classic I need a notification saying I probably don't want to be here and a link to go back to the Getting Started page - This message always appears until I say don't show again - Shaun will write the copy - Only with new menu structure  ""The Classic boards are no longer being actively improved and are not recommended for use in new projects.{0}Learn more about {1}Getting Started{2} with the new boards."" "
1,MVR: Change keyboard shortcuts (Go to Planning Board) etc based on the new Agile menu structure,- Make 'g' and 'h' - Go to Agile  - Make 'g' and 'a' - Go to Agile - Invisible - Make 'g' and 't' - Go to Agile - Invisible
0,MVR: Rename issue operations (or remove not used ones) when new Agile menu structure is enabled,- Remove: Go to Planning/Task - Rename 'Rapid Board' to 'Agile Board'
1,Translate URLs according to new terminology,NULL
1,Translate URLs according to new terminology,NULL
1,"When starting a Sprint with Time Remaining set to the Sprint tracking statistic, warn if Remaining Estimate is unset (or uninherited from children) for any item",NULL
1,"As a rapid board user with buggy browser extensions, I don't want exceptions thrown from extensions to break GreenHopper","GreenHopper appears to use a global exception handler that reacts to any uncaught exception by showing an error on the rapid board.  We've seen a couple of support requests from users with browser extensions that throw exceptions. In some cases, the errors may be benign and GreenHopper could continue to function normally, but the errors alarm people, and as a result they raise support requests.  This could potentially be caused by any JavaScript that runs within the context of the page: browser extensions, GreaseMonkey scripts, even JIRA plugins or Speakeasy extensions."
1,Remove Scrum Rapid Board from labs,#NAME?
0,Add option to exclude weekends from cycle time calculations,"When viewing cycle time charts, weekends are included in the calculations. There should be an option to exclude weekends, if desired.  (!) Testing Notes (!)  Q: What if the user's timezone does not match the server timezone? A: (x) Not Answered  Q: Is this setting remembered between page views? A: (x) Not Answered  Q: Is it clearly specified when this option is on/off? A: (x) Not Answered  Q: What happens to any events that happen in these excluded windows? Will they simply not be displayed/will they be calculated? A: (x) Not Answered  Q: What if there are large outliers in the excluded period? Will the chart adjust properly? A: (x) Not Answered"
2,Make CFD in RB Sprint sensitive,Add the option to refine the CFD using a sprint
2,Make CFD in RB Sprint sensitive,Add the option to refine the CFD using a sprint
0,Put add comment button the comment tab ,NULL
0,Put add attachment button on attachment tab,NULL
0,Show sub tasks tab on Work mode in Rapid Board,NULL
1,Sprint Report should visually call out added and removed stories,#NAME?
0,"Sort status objects in column configuration to show non zero alphabetical, divider then zero alphabetical",NULL
0,When story moved to done column don't scroll to it,Stay selected  
1,"When a Rapid Board is shown, add the projects shown in issues to the most recently used list in JIRA","COS: * Pool/Backlog data will now return as part of the payload a list of projects contained in the issues. This data will be expanded upon in the future. * This story will incorporate GHS-5217 * In the case where no issues are found in a backlog/pool, we determine recent projects by the query of the board ** If the query is not project-based, then we do not modify the project history * Be smart about where projects get inserted into the history * When viewing a Rapid Board via a gadget, this should not have an effect on the recent projects for the user   To be smart about history, consider the following example.  Viewing a board with issues from projects A, B, X and Y.  The issues are seen by the board (ordered by Rank), in the order X, A, B, Y.  The user's current history contains the projects in the following order (most recent first): D, B, A, F, G.  Because we do not want to upset the order of projects that were already in the history and that we have just seen, the desired new ordering of projects is as follows: B, A, X, Y, D, F, G."
1,"As a user, I'd like the Rapid Board to remember which board I'm on regardless of which computer I'm on",COS:   - Keep the last 5 boards used on the server side  - Logged in only
1,"As a user, I'd like the Rapid Board to remember which board I'm on regardless of which computer I'm on",COS:   - Keep the last 5 boards used on the server side  - Logged in only
1,Add the ability to specify multiple board administrators so other users can change the configuration (delegated admin),Currently only the owner or a jira-admin can do this. I think it would be good to allow this so you can have other team members adjust the settings.    Currently at my company there are multiple producers on a team that all use the same board and they would like to continue to have one board for the whole team.
1,Add the ability to specify multiple board administrators so other users can change the configuration (delegated admin),Currently only the owner or a jira-admin can do this. I think it would be good to allow this so you can have other team members adjust the settings.    Currently at my company there are multiple producers on a team that all use the same board and they would like to continue to have one board for the whole team.
1,"As a user I want to see all issue on the statistic burndown table, even if some don't have a statistic value",NULL
1,Perform work modes initial XHR calls in parallel,"At the moment we perform three XHR calls in order to render a rapid board in series:  First we get the rapid board selector data. Then we get the rapid board config data. Finally we get the issues data (allData).  For the 99% case we should be able to perform all three of these calls in parallel (kick them all off at the same time), and just perform the rendering in series.  This will kill off the Rapid Board selection drop down  This means we will have to perform an additional two rest calls in the unlikely case where someone attempts to load a rapid board that has been deleted and gets punted to the default board."
0,"As a user, I'd like the manage Rapid Board screen to have a clear 'Create new board' button directly on the screen rather than being hidden in a submenu",NULL
0,"As a user, I would like a board that is not ordered by rank and therefore not draggable on PLAN mode to not show the ""hand"" icon or drag handles so I don't get confused",NULL
1,"As a user of the PLAN mode, I would like a board that is not draggable to give me a warning when I attempt a drag like action on it and explain what the problem is","COS:  - Show a warning box ""Ranking is not possible because the filter for this board does not order by a rank""  - Include a link to the filter section of the configuration - Discuss testing"
1,Plan|Work|Report links should support opening in another tab with middle click (cmd click),"If I middle click them or Ctrl click them, they should do nothing and fallthrough to the native browser handling.    We have code for this in Stash, but since we only use it with live events, we don't handle checking for left click (we probably should):    EDIT: Here's with which checking as well.  {code}  exports.openInSameTab = function (e) {          return (e.which == null || e.which === 1) &&                !(e.metaKey || e.ctrlKey || e.shiftKey || (e.altKey && !$.browser.msie));      };  {code}    Called like   {code}  $(document).on('click', 'a', function(e) {      if (!openInSameTab(e)) return;        ... do stuff ...        e.preventDefault();  });  {code}    If you want to handle left click only, I believe you can check e.which === 1, but you should test that first.  "
1,"As a user of the WORK mode, I would like a board that is not rankable to give me a warning when I attempt a rank like action on it and explain what the problem is",COS: - Discuss testing
0,Expand / Collapse All Swimlanes on the Rapid Board,"Rather than just having the individual swimlane expand / collapse function, it will be convenient to have the expand / collapse all the swimlanes in one go.    COS:   - Menu items will appear at the bottom of the Tools menu  - Include a keyboard shortcut for toggling expand/collapse all   - Decide on a keyboard shortcut (Current choice is {{MINUS SIGN}})"
0,"As support, I need an itemized list of the remaining time/items at the end of the Sprint in the burndown",- With 45 issues doing this manually is very painful (particularly when you're trying to reconcile history) 
1,"As a multi-team one backlog user, I'd like to be able to start multiple sprints at once","(!) Testing Notes (!)  Q: How many sprints can be open at once? A: No limit  Q: Is there any impact on reporting when multiple sprints are active? A: (x) Not answered  Q: How do we know which report to jump to when juggling sprints in work mode? A: (x) Not answered  Q: What happens when an active sprint is closed by some other user while you are working on it? A: (x) Not answered  Q: What if i want to view different swimlane strategies in each of my active sprints? A: Raise a feature request  Q: What happens when all issues are removed from an active sprint? A: Existing problem, iirc there's an open bug to better handle this  Q: What if all issues from one sprint are moved to another active sprint? A: As above  Q: Do we still jump to reports if an active sprint is still open? A: (x) Not answered "
1,"As a multi-team one backlog user, I'd like to be able to pick which sprint I'm looking at on work mode (possibly quickfilters?)",https://extranet.atlassian.com/display/GHDEV/GHS-5543+Sprint+Picker
1,"As a multi-team one backlog user, I'd like to be able to pick which sprint to finish when I close one","COS:  - Display inline dialog with name, start and end date - Button to finish"
1,"As a forward planner, I'd like to be able to name future sprints",NULL
1,Store sprint created date and show on the burndown chart data table as the first line,COS: - For historical sprints do not show this entry in the burndown
1,Store sprint created date and show on the burndown chart data table as the first line,COS: - For historical sprints do not show this entry in the burndown
0,"Allow more than one plugin to register to have a tab in the detail view, use icons in the tab header",NULL
1,"As a user, I'd like the board to poll for staleness and provide a notification if there are changes at a sane interval","COS:   - Detect when page loses focus and stop doing this so regularly     (!) Testing Notes (!)    Q: How often does this poll? Can the user define this polling interval?  A: 30 sec. No, user can't define interval    Q: What if there was a change, then another change to undo that change? aka there is no effective delta?  A: The client side will receive the updated data and then decide its already up to date, thus ignore it    Q: What if the user has been logged out?  A: Thanks to Jason a separate logic automatically redirects to the login page when an ajax request fails due to being logged out. This works for all requests, be it filter change, detail view, or update poller. The question here probably is whether the update poller should really trigger this redirect...    Q: What if the user does not have permission to see the board? Do they still get notifications?  A: Why would the board open if they had no permission    Q: Any conflict if there was an error on the page as well?  A: Not handled    Q: Is there a way for a user to disable these notifications?  A: No    Q: Any conflict if I have multiple boards open in multiple tabs?  A: Should not. Each board will separately poll for updates though    "
1,"As a user, I would like to be able use the work mode while an update is being performed (kill the spinner)",COS: - Allow re-rank  - Allow quick filter
1,"As a user, I would like to be able use the work mode while an update is being performed (kill the spinner)",COS: - Allow re-rank  - Allow quick filter
1,"As a user, I'd like to have a recent version of JIM automatically installed for me",NULL
1,"As a supporter, I'd like exceptions to be collapsed by default, to allow users to continue to use the app without having a large error box on the screen","COS:  - A link to ""Read More"" will expand the container to the full height. - The box can be collapsed again."
0,As a user I want to be able to see the Remaining Estimate on Rapid Board cards instead of the Original Time Estimate,"You would normally only look at a sprint board once the sprint has started, and when a sprint has started the only valu ethat is relevant for any given sprint task is the remaining estimate. Therefore this should be visible directly on the rapid board cards in stead of the Original Time Estimate.   This will give a far better overview of the sprint, when viewing the entire Rapid Board.   As a bonus it would be very nice if the value is editable directly on the Rapid Board cards.  COS:  - On work mode the lozenge will show remaining estimate - For parent stores this will not include sub tasks - Only if time tracking is enabled"
0,"As a user, I'd like to see the count of visible (i.e. unfiltered) issues in the column header",(!) Testing Notes (!)  Q: Any limit on the number of digits? A: (/) Issue count should be unlimited (constraints currently limited to 999).  Q: Takes into account when multiple filters are on? A: (/) Yes  Q: Properly hides issues that are hidden for a particular user? A: (/)  for browse permissions  Q: Looks good in all browsers? A: (/) Yes  Q: Looks good in wallboard mode? A: (/) yes
0,As a user I want to see median time on scrum/kanban charts,This would allow people to find the median time that issues take to be fixed or to be QAed or to pass through arbitrary stages of the issue lifecycle. The median is arguably a more useful time than the mean as it is less influenced by outliers.
1,"As a user, during the create board process I want to select a board type first then either a filter or projects as a second step",NULL
1,"As a user, during the create board process I want to select a board type first then either a filter or projects as a second step",NULL
1,"As a user, I'd like to be able to inline edit the components, fix version and affects version fields in the detail view",COS: - Use the Frother control for entering input (i.e. same experience as View Issue)?
1,"As a user, I'd like to be able to inline edit the components, fix version and affects version fields in the detail view",COS: - Use the Frother control for entering input (i.e. same experience as View Issue)?
1,"As a user, I'd like to know in the filter configuration tab that some issues that match my filter are not appearing on my screen because they are not in a column","COS:  - Something in the filter tab  - Click through to the column tab  (!) Testing Notes (!)  Q: Alert? Dialog? Dismissable? A: Warning, shown in Filter tab. Not dismissable.   Q: Count of issues not displayed? A: Nope  Q: Do you get this message as soon as there exists an issue which is unmapped? A: Whenever you view the filters tab and there is an issue unmapped (current user). Other users on page refresh/load of configuration.  Q: Is this only for statuses that currently have issues in them, or any unmapped status? A: Only unmapped statuses that have issues in them.   Q: Does the message immediately disappear when the last applicable status becomes mapped? A: Yes, once you fix the last unmapped status with issues, then view the filters tab, it no longer appears (current user). Other users on page refresh/load of configuration. "
0,"When automatically choosing a name for the next sprint, increment the number at the end even when there are word characters or punctuation",Some examples * Sprint 1 -> Sprint 2 * Stash M1 -> Stash M2 * JIRA 5.0.1 -> JIRA 5.0.2
2,"As a rapid board user, I'd like to be able add/remove text custom fields to the bottom of the detail section of the detail view","COS:   - Configuration per board  -- New tab in config page  - Any text custom field in the system  -- Double check that fields for Estimation are not filtered based on issues in board  - Text custom field will be read only on detail view  - Add and remove fields  -- If possible - reordering of fields  - Fields are displayed at the *bottom* of the *top* section on the *Details* tab i.e. underneath Fix Version  - Read only display for now    (!) Testing Notes (!)    _(Answers by mtokar)_    Q: How many text fields will we allow?  A: Unlimited - as many as there are in the system to be added    Q: Which text fields will be allowed, and are there any that are prohibited?  A: Only Custom Fields which are of a ""text"" type - in the standard custom field types this means Text Area, Small Text and URL    Q: What happens if the name of the attached field is modified?  A: Nothing should happen - field is assigned by ID    Q: Who has permission to add these fields?  A: Board owner and admins    Q: Are these fields added per board, or per project, or global?  A: Per board    Q: What happens when we remove then re-add a text field?  A: It will be in a different position to before, but otherwise nothing.    Q: What if a user does not have permission to see the attached field?  A: There are no user permissions for fields. But if the field is not applicable in the scope of the selected issue (i.e. it is not configured for that issue type or project), then it will not be displayed on the detail view.  Regardless of scope, all text custom fields are available to be added (if they have not already been added).    Q: If a field is added while a user has the detail view open, do we require a full page reload or just a detail view refresh to see it?  A: Just reload detail view.    Q: Can users add GreenHopper fields to the details tab? This could lead to odd behavior in the case of Story Points.  A: No special GreenHopper fields are currently allowed to be added.    Q: Will we only allow text fields, or number fields as well?  A: At the moment only text fields    Q: Will these fields be inline editable in the detail view?  A: At the moment read-only.    Q: Will these fields be XSS safe?  A: They should be, yes.      "
1,"As a user, I want to link scipe to stories",COS:  - New link type - Need service to link/unlink  - Only valid between an Epic issue type and a Story issue type
0,XML view for issue contains empty value for field SPRINT,"If the SPRINT field contains value and is correctly displayed on the issue screen, click Views->XML. The xml contains information about the SPRINT customfield but its value is empty.    This bug causes following issue with IDE Connectors:  * user opens issue in the IDE Connector (it is based on the issue XML view and contains information that Sprint value is empty)  * user makes changes and submit issue with the Sprint value empty  * the Sprint value is erased on the JIRA side (ups!)"
0,Ability to use the Upcoming Sprint value or name in JQL searches,"I'd like to have the ability to use the notion of Upcoming sprint in JQL searches.    For instance, I have planned 2 sprints ahead of the current open sprint. I'd like to be able to filter separately the issues planned in upcoming sprints 1 & 2, using the advanced search. I would use this search query to create swimlanes in Kanban boards or create structures within the Structure plugin.    Note that future sprints don't have a date so autocomplete needs to be revisited.    h3. Workaround via database:    https://confluence.atlassian.com/display/JIRAKB/How+to+find+issues+from+non-started+sprints"
1,"As a user, I would like to be able to configure the due date or any other date field on to the detail view",COS:  - Due Date  - Resolution Date  - Custom fields of date type - Second restful table for dates  - Go in to the dates section  (!) Testing Notes (!)  Q: Be sure to test with Greek and Korean dates A: (x) Not Answered  Q: What if the selected day is a non-working day? A: (x) Not Answered  Q: What happens when you change the JIRA date format with the field in place? A: (x) Not Answered  Q: How many dates fields can you add to the detail view? A: (x) Not Answered
1,"As a user, I would like to be able to configure the business value (or any other numeric field) to be shown on the detail view","COS: - Business Value - Custom fields of type numeric  - If the same as the estimate statistic do not offer it to add -- If already added, show a warning in the configuration page, and do not show in Detail View on board  Testing: - Kanban vs Scrum boards - story points are handled differently  (!) Testing Notes (!)  Q: What if you add business value when it is already the selected estimate field? A: The currently selected estimate field should not appear in the dropdown list. The only time it does if if you change the estimate field then click Issue Detail tab without refreshing the page. In this case, it does nothing when you try to add it.   Q: What if you added business value before setting it as the estimate statistic? A: It does not appear in the issue detail. When you view the configuration, there's a message against the field.   Q: If we allow both, do they update properly when either of them is updated? A: (x) Not Answered (don't understand this one)  Q: How many fields can we add? Are there any limitations? A: No  Q: Any issues when the user cannot edit the field? A: No  Q: Can you add duplicate fields? A: No  Q: What happens when a number field is changed to another field type? A: I don't think this is even possible.  Q: What happens if the field is deleted after it was added? A: It should show up as not valid with a message on the config page and does not show up in issue detail.   Q: Any limit to the length of input to the   ...?"
0,"As a user, I would like to be able to configure the resolution, environment and security level fields to be shown on the detail view","COS:  - Resolution   - Environment   - Security level     (!) Testing Notes (!)    Only plays for this type of story include:  * deleting the field when assigned  -> this field is a system field so we can't delete it. I tried to hide it but we don't honour those settings (see https://jira.atlassian.com/browse/GHS-5831?focusedCommentId=412545&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-412545)     * restricting permission to particular users  -> the user will see an error message saying that it is not possible to access the issue after security level configuration change and on board reload the issue will not be visible in the list    * checking behavior when naming fields as system-wide objects (Unassigned) and duplicates  -> we can create a custom field with the same name as a system field (like resolution, environment and security level). It will be selectable in the field list in the Issue Detail View tab of the configuration page.  "
1,"As a user, I would like to configure label, select, check, radio and version fields to be shown on the detail view","COS: - Label custom fields - Select & Multi select custom fields - Check & multi check - Radio - Version custom field   (!) Testing Notes (!)  Q: How many of these fields can you add? A: (/) As many as you like  Q: Will these be selectable in the detail view? Or only the current state of the field? A: (/) Read-only  Q: How will we display a field with multiple selections? A: (/) Comma-separated   Q: Will there be any problem adding fields that have duplicate names? A: (/) No, field configuration is stored by id. It would be hard to know which one is which though  "
1,"As a user, I would like to configure user or group custom fields to be shown on the detail view","COS:  - User custom field type  - Group custom field type  - Another RESTful table for people section    (!) Development Notes (!)    Q: How many users and groups can we add to the detail view? Any limit?  A: (/) No actual users or groups are added, only defined custom field containing users / groups. There is no limit on the amount of custom fields displayed    Q: Where is the people section located?  A: (/) In the rapid board config page, under the issue detail view tab. In the third table.    Q: Are users/groups notified when they are added to a board?  A: (/) They aren't actually added to the board. The issue detail view will display custom fields of a type related to people (user picker, group picker, ...).     Q: What if a person/group added to the board does not have permission to see it (aka no permission to see the filter)?  A: (/) The person or group does not know that he is being displayed in the issue's detail view on the rapid board.     Q: Will this be a new tab on the vertical jump nav?  A: (/) No, the people custom field will be displayed in the details tab of an issue detail view, under the people section (under assignee and reporter)    Q: What if a user or group is deleted while displayed on the board?  A: (/) The link to the person / group is displayed. If the person or group is deleted then following the link will show an appropriate error page.    Q: What if a user/group is renamed?  A: (/) The link will still work. The changes will be reflected on the next refresh of the data    Q: Do any of these questions surprise you? If so, what hadn't you previously considered?  A: (x) Not Answered"
0,As on ondemand user I want to be able to use sample data,NULL
1,As a use GH should create OD compliant projects. From the welcome mat it does not respect all-app-access requirement in ondemand,    Code to be copied from https://bitbucket.org/wseliga/jira-ondemand-jim-bridge-plugin    See https://studio.atlassian.com/browse/JIM-691 for more details    
1,"As a user, I'd like basic flagging of issues","This is just the first step in flagging  COS:  - Flag is set / cleared from the cog menu in detail view & context menu - Flag icon takes place of issue priority - background color change when issue flagged (also specific color when selected) - Appear in work mode and plan mode - use the flag field introduced in classic mode : -- not empty -> flagged -- empty -> un flagged - in case of mixed selection propose ""flag"" action, else propose the opposite action of the selection - in case of multiple issues flagging with error : -- flag the one that can be modified -- display a list of info message ""issue key -> error"""
1,"As a user, I'd like the existing fields included by default in the detail view to be configurable like for custom fields","COS:  - Upgrade task  - All the existing fields on the detail tab of the detail view in the same order they are there atm  - Newly created boards should have the default set up    NOTE: We should lock the Rapid View when an upgrade is in progress.    (!) Development Notes (!)    Q: What if one of these fields does not exist?  A: (/) Not possible to delete, but you can hide the field. It is not visible on detail view, but its listed on the detail view configuration.    Q: What if one of these fields has been renamed?  A: (/) No rename option available for system fields    Q: What if there is a custom field named with same name?  A: (/) No special handling in this case - works as expected.    Q: Is there any difference for in-line editable fields vs non-editable?  A: (/) We are only doing this for fields in Detail Section    Q: What happens if you remove a default field after the upgrade task?  A: (/) Its not possible to delete system fields from JIRA    Q: This needs to be tested on all databases.  A: (/) OK    Q: What older versions of the data do we need to be wary of? What about pre- Detail View data?  A: (/) Should be handled automatically by AO staff    Q: What if fields are not in the expected order?  A: (/) The order in this case does not matter -   we will add those fields which are not already present there.      "
1,"Allow JQL for searching for all issues in an Epic via their ""epic label""","* Either JQL function, or change the fundamental clause handling to allow for these labels  * Use the epic label field to look up an epic and then use its ID in the Lucene query  * Don't consider autocomplete    (!) Testing Notes (!)    Q: What if there are epics with duplicate names (either globally or within the search scope)?  A: (/) They will all match so the issues list will be an union of all the issues of those matching epics    Q: What if the epic is closed/resolved?  A: (/) We are agnostic    Q: What if the epic label is empty? (is this even possible?)  A: (/) The epics with empty label won't match"
1,"Simplified Workflow button fails with ""Workflow Migration did not complete successfully"" can be improved","(!) Testing Notes (!) Q: Who has permission to simplify the workflow? Board owner or admin? A: (/) Admin or Sys Admin  Q: If board owner is allowed, what if they don't have permissions to edit the workflow of the project? A: N/A  Q: What if a simplified workflow already exists for this project? A: Dialog prompt is not given.  Q: What about the resolve screen or modal dialogs from the last workflow? A: All screens from previous workflow are removed.  Q: What will happen if the user is logged out/migration is interrupted during migration? A: We now make one call to the server via REST, to both create the new (simplified) workflow scheme and to kick off the migration of the workflow.  Q: What if an issue is being transitioned during migration? A: Whatever happens in JIRA will happen here.  Q: What if the user's security token is expired? A: The XSRF token will no longer be used in this operation.  Q: What if the board filter is changed to include multiple projects when at the workflow migration dialog? A: I'm assuming it will fail with an error.   # Deploying a fresh *JIRA 5.0.7* instance equipped with *GreenHopper 6.0.2*, tried to enable the *Simplify Workflow* button but it failed with the following error: !migrate_failed.JPG!  # After that, authenticate as a user with the Administrator Access: !admin_access.JPG!  # After that, the workflow migration works fine: !migrate_success.JPG!  It would be useful if there is some sort of indication that shows the user does not have the Administrator Access. Perhaps next to the *Simplify Workflow* button.  Workaround to this issue: [Simplified Workflow button fails with ""Workflow Migration did not complete successfully""|https://confluence.atlassian.com/pages/viewpage.action?pageId=301663531]"
1,Allow the detail view to be closed on plan mode,"COS:  - What does this look like without the ""Epics"" column? - The whole plan mode just goes across the screen    Scenarios / Requirements    * Behaviour in plan and work mode should now be more consistent ...   * Plan and Work mode details are independent, i.e. when opening issue A in Plan and issue B in Work, flicking between the two should preserve the open issues.   * Opening and closing details in plan and work mode are independent, i.e. closing detail in plan mode should not close it in work mode and vice versa.  * On initial load, the backlog will fill the screen - the plan mode content block is gone for good.   * Deselecting an issue in plan mode has the same effect as closing it.   * Clicking on an issue in plan mode now just selects it and no longer opens the issue detail. However if it's already opened then it will refresh to the currently selected issue (same behaviour as work mode).  * If a filter is applied and the selected issue is filtered out then issue detail is closed.   * Multi-select - if issue detail is open, display the last selected issue. If it's closed, don't open it.       (!) Testing Notes (!)    Q: What if an inline edit is open?  A: Changes should be saved and then detail closed.     Q: Will it have an XBox™ to close akin to work mode?  A: Oui    Q: What if there is an error in plan mode when you try to close it?  A: It should not close until the error is fixed.   Example: Edit the estimate and put some letters in the field (not allowed) - currently the error appears inline, then on close appears at the top.     Q: Do the cards resize nicely without detail view?  A: They will stretch across the page    Q: Same keyboard shortcut? If so, is there any collision with S + T?  A: Shortcut will be T. ST should rank the issue without showing the issue detail if it's not open. If it is, ST selects the next issue that was below the one that moved so this would then appear in the detail view.     Q: How performant is it with thousands of issues?  A: (x) Not Answered No idea yet."
0,"As a user, I'd like to inline edit sprint name, start and end date at the top of plan mode so that KA styles make sense","COS:  - Dates will split to two edits - Dates will behave like existing inline edit in work mode dialog - When you edit the name you'll just be editing the name inline  (!) Testing Notes (!) Q: What happens when clicking between open inline edits? A: Clicking from a name to a date picker will submit the name (if changed) and activate the edit for the date picker. Clicking between date pickers will change the currently edited date picker, as choosing a new date will automatically stop editing after the new date is saved.  If there is an error in a field, clicking away from the field will not keep the field in edit mode.  Q: Are foreign date formats properly supported? A: Tested with Swedish locale - there is still the AM/PM time code, but that is a JIRA issue.  Q: Inline edit works in all browsers? A: (x) Not Answered"
0,Make error box dismissable,NULL
1,As a scrum planner I would like the ability to reorder sprints,"It would be extremely useful to be able to reorder sprints on the Plan board.  Changing the sprint order would swap the relative order of all of the issues contained in those sprints.    In addition to improving Scrum planning, this would resolve a critical missing feature available in the Classic boards where the Version cards could be re-ordered by giving equivalent functionality in the Rapid Board."
1,As a scrum planner I would like the ability to reorder sprints,"It would be extremely useful to be able to reorder sprints on the Plan board.  Changing the sprint order would swap the relative order of all of the issues contained in those sprints.    In addition to improving Scrum planning, this would resolve a critical missing feature available in the Classic boards where the Version cards could be re-ordered by giving equivalent functionality in the Rapid Board."
1,Restrict editing of the Sprint CF to users with Schedule permission for that issue,"This is something I found randomly and I guess it makes sense for us to check. I tried this on a clean installation and reproduced many times in different ways. In a nutshell, I think the shortest path to demonstrate is the following:    # Let us say I have a project named _ABC_ and a Rapid Board _ABC Board_ containing issues from that project.  # I also have two users:  ## charlie: All permissions on project ABC  ## alpha: Only in jira-users, essentially only able to create issues in ABC (user has been added specifically in this example but it would work for him anyway):  !1-alphagroups.png!  !2-createissues.png!  # Project roles in ABC are like the following:  !3-people.png!  # charlie creates ABC-1:  !4-create.png!  # Then charlie adds ABC-1 to Sprint 1 (just created by him, also):  !5-plan.png!  # The sprint id for that sprint is 1 (the address bar is from the sprint report):  !6-sprintId.png!  # The Sprint field is also visible on all screens:  !7-sprintfield.png!  # Now user _alpha_ tries to create an issue in JIRA. Remember that he cannot even browse project ABC but he can edit the Sprint Field and add the sprint id (1, in this case):  !8-alphacreatesissue.png!  # ... so when he tries to view ABC-2 he cannot:  !9-permissionviolation.png!  # However, ABC-2 has been added to the sprint:  !10-issueadded.png!    Should this be possible? Am I missing something, perhaps?"
0,Add Epic Label to the Create Issue Screen in JIRA config so it will show up in the Epic Quick Create dialog,"(!) Testing Notes (!) Q: What happens if this field is deleted/removed from the screen? A: It won't appear on the screen.  Q: What if it is renamed? A: In theory, it should just display with the new name but atm, GH blows up and disables itself. See subtask GHS-6162.   Q: What if the field's permissions are modified? A: It doesn't show up.   Further testing notes: - the field is added to the screen for the create issue operation for the epic issue type - it's not added to screens within an issue type screen scheme that is not associated to any projects - if there is a specific screen scheme for epics, it's only added to those screens - otherwise screens within the default scheme"
1,Create labs feature to enable Epic support,"COS:  - Add flag - Wrap existing prototype code with a check for the flag - Convert the creation of Epic related fields to be on the switch of this flag as opposed to the current update task approach  (!) Testing Notes (!) Q: What happens to Epics fields if Labs is disabled after enabling? A: The fields remain in the system, as we don't delete fields  Q: What if those fields are deleted after Labs is disabled? A: The field won't be recreated as it isn't used anymore  Q: And what if a user again re-enables Labs? A: All missing fields are created/recreated upon first usage  Q:If Epic Labs is disabled, can I still interact with previously created Epics? A: Yes, but they show up as normal issues inside the backlog "
0,Plan mode layout for 3 panels (to bring in the closable detail view and allow closable Epic panel),"(!) Testing Notes (!)  Q: Need to test all states: Matrix of Epics open/populated, issues in backlog exist/don't exist, and Detail view open/closed/help text A: (x) Not Answered  Q: Check above at minimum resolution A: (x) Not Answered  Q: Look for overlapping panels, and try with long but reasonable length data in relevantly displayed fields. A: (x) Not Answered "
0,Temporary tools menu option to hide/show Epics (only on plan mode),"(!) Testing Notes (!)  Q: What does temporary mean and how will a user know that? If they get used to it, and it disappears, they will be >:/  A: (/) that's why it's Labs™    Q: Does this interact with the Epics sidebar twixie?  A: (x) The toggle menu state reflects the current state of the Epic Panel (see GHS-6179)    Q: If so, do they play nicely together?  A: (x) Yes    Q: Does hide/show mean the entire panel and twixie go away, or does it actually mean expand/collapse?  A: (x) Expand/Collapse    Q: Must test with all states of pan mode: Epics expanded/collapsed/populated/un-populated, backlog empty/filled, detail view open/closed  A: (x) Not Answered    Q: What happens when jumping to Work and Report mode and back?  A: (x) Persists    Q: Is this persisted across logins/users?  A: (x) I think each user should own his/her own state    Q: Who has permission to do this? Can non-admin or anonymous users do this?  A: (x) Anyone"
0,Make sure that Epics do not get included in to the sprint (at the moment they are in the backlog and get put in to a sprint),"COS:   - Epic should not be in the backlog list  - When starting a sprint Epics are NOT included    (!) Testing Notes (!)  Q: Check that Epics are not draggable into the backlog area.  A: if the epics feature is enabled, no epic is available in the backlog list and then you can not drag and drop it. If the feature is disabled then you can drag and drop the epics like any issue type (bug, story etc.)"
0,Drop down on Epic with View in JIRA link,COS:  - Implement drop down menu - Include View in JIRA link in there - Middle/Cmd click open in new tab works  (!) Testing Notes (!) Q: Make sure open drop down handles  (x) Epics panel closing (x) dragging issues within backlog and to epics (x) quick/instant filter (x) switching modes (x) open/closing of detail view (x) inline edits (x) any of the other dropdowns opening
0,Allow a story to be unassigned from any Epic,"Minimum Criteria:   - Cross on booger  - No warning on removal    -Goal is to work with design to actually design this before implementation-    (!) Testing Notes (!)  Q: Who can remove from an epic?  A: (/) Only the admin    Q: What if the epic itself is deleted? Are stories automatically unassigned from it?  A: (/) Yes    Q: What if the issue is not currently visible due to filter/column mapping?  A: (/) If an issue is not visible, you have no mechanism to unassigne it"
0,Implement estimated vs unestimated stories progress bar,"COS: - Coloured section of bar is the number of issues that have an estimate applied, uncoloured (line) section of the bar is the count of issues with no estimate applied  - Tooltip shows both issue counts and %'s - Put in expanded Epic information panel unestimated and estimated  (!) Testing Notes (!) Q: How will this handle filtered issues? A: Ignores filtering  Q: What if there are no stories in the epic, or only 1? A: The bar will not show for zero, will be small if % is small  Q: What if no estimate statistic is selected for the board? A: One is always selected  Q: What if stories are removed from the sprint? A: (x) Not Answered  Q: What happens when the last one gets removed? A: (x) Not Answered  Q: And another one added after that? A: (x) Not Answered"
1,Implement the estimate done vs estimate undone progress bar,"COS: - Green is the sum of the estimate value for the completed stories, the line (unfilled section) is the sum of the estimate value for the incompleted stories - Tooltip shows both estimate sums and %'s - Put in expanded Epic information panel   (!) Testing Notes (!) Q: How will this handle filtered issues? A: (x) Doesn't change   Q: What happens to the bar if there is no estimation statistic for the board? A: (x) There always is  Q: What if a non-story points estimate is chosen? A: (x) The bar should represent the completed % of whatever estimate statistic is used.   Q: What if there are no stories to estimate, or there's only 1? A: (x) If there are no stories it shows as completed (nothing left outstanding right?) If there are no stories estimated it shows as completed unless the estimation statistic is issue count. If there's one story with an estimate incompleted it shows as empty.   Q: What about the estimates on subtasks? Are those summed as well? A: (x) No. These are used only for remaining estimate.   Q: Does the bar update if an invalid estimate is entered? A: (x) No  Q: Does the bar update on estimate change of a story in the sprint? A: (x) You can't change an estimate of a story in a sprint once it's started. If it hasn't started then yes.  Q: Does the bar update on drag of a story into the sprint?  A: (x) No, only on stories being completed or added into an epic. "
0,Make expanded status of Epic sticky,COS:  - Not included in URL - Does store expanded state of multiple Epics
1,Make selected status of Epic remembered,"COS:  - Include in the URL - In addition to the selected status of an issue  Discussion: Selection states across tiered items: parent/child -  Mimic some common approaches like mail clients - folder can be selected, when child item is selected, folder has a ""secondary selection"" that shows it is still active,  but not the active selection.  Bringing up details -  Thinking about this further in light of showing the Epic ""details"" on the right, I don't think we want to do this on Select of the Epic.  That should be a primary action that is to filter the backlog list.  It may be worthwhile to add a ""details"" affordance though to manually display the issue details for an epic (just not on epic selection).   I'll add that to the next iteration of the design. "
0,Multiple select for Epic filtering,"This may be an additional story, but I had shown some designs around giving users an additional way of knowing the backlog was filtered by Epic - originally I had put the epic label on the headers of backlog containers (sprints and backlog) but that might be problematic.  Martin had suggested re-using the active quickfilters area, which has merit.  Will add design task to explore."
1,Allow editing of the colour swatch for an Epic,COS:  - Fixed palette to start with  - Requires edit issue permission  - Use the colour custom field we have created    (!) Testing Notes (!)    Q: How does an open colour picker interact with the following:  Clicking other inline edits (x)  Closing the epics or detail panel (x)  Switching modes (x)  Drag and Drop (x)  Create/Delete sprints (x)  Reordering epics (x)  Multi-select (x)  Opening the tools menu (x)  Applying quick/instant filters(x)  Creating a new issue (x)  Create a new epic (x)  Anything I'm missing (?)    Q: What if the exact same colour is chosen as another epic?  A: (x) Not Answered    Q: Are default colours still rotated?  A: (x) Not Answered    Q: 
0,Show Epic label in detail view for story,"COS: - Show in both plan and work mode - Don't show it on sub tasks  (!) Testing Notes (!) Q: Make sure detail view updated after label update/removal A: (x) Not Answered  Q: Will tasks or improvements get labels too? A: (x) Not Answered  Q: Make sure color is updated correctly too A: (x) Don't need to worry about colour for now, this is a text only representation."
0,Add 'Enable Epic Labs' box on to Getting Started screen (like it was for Scrum support),"(!) Testing Notes (!) (x) Be sure to check the visibility for this with admin, non-admin, and anonymous permissions - Non-admin and anon should be the same  (x) Be sure to check multiple on-off  "
1,"Have checkboxes in board configuration for Burndown to specify days that are not working days, shade that area of the graph and have the guideline be level at that point","- Default on new boards is not to show sat / sunday in the timezone of the server - Affects burndown chart only (and mini burndown in sprint report) - Default to timezone of server but allow to be changed - Display timezone being used - New reports tab in configuration - Allow additional days to be specified - If JIRA has a weekend setting use that for the default (rather than just Sat/Sun)  (!) Development Notes (!)  Q: Must test when there is a switch-over from standard time to daylight savings. A: (x) Not Tested  Q: What if the non-working days are not valid? A: (x) Not Answered  Q: What if all days are marked as non-working days? A: (x) Not Tested  Q: What if the sprint starts or ends on a non-working day? A: (x) Not Tested  Q: Is there support for defining which days are your weekend? aka the Saudi case A: (x) Not Answered  Q: Is the chart displayed in the client's timezone, the user's timezone (from profile), or server? Chart should line up with the sprint dates. A: (x) Not Answered  Q: What if there are datapoints in a non-working day time period (shaded area)? Are they treated the same or excluded? A: (x) Not Tested  Q: What if 2 non-working periods overlap? A: (x) Not Answered  Q: Do any of these questions surprise you? If so, what hadn't you previously considered? A: (x) Not Answered"
1,"As a GH developer, I would like for the custom fields that GH creates to appear as ""managed"" in JIRA's administration interface","FINAL BEHAVIOR:  1. For fields with options (epic status): If everything is 1:1 like the original configuration, then lock the field, otherwise don't lock the field  1a. Unless there is all projects/appropriate issues field scheme with ZERO options in there. In that case we will just add the options.  1b. Unless there are no schemes, in which case we introduce our own.  2. For fields without option fix up the contexts then lock the field  3. do NOT fix up field names/descriptions for any of the fields  3a. But fix the searcher.  4. for the Rank field, change managed level to ""admin"" (field creation) plus a message that the field should normally not be created as GH creates the field  5. The default rank field should be fixed up in context and then locked.    In JIRA 5.2 there is a new API around Managed Configuration Items. This allows a plugin to specify that instances of custom fields, and custom field types, are ""managed"" by the plugin and should not be modified/created by administrators.  Fields to be restricted (no edit, no create): All Issue Types/Global Context: - Epic Link(show) - Sprint(show) - Rank(show)  Epics only/Global Context - Epic Status(show) - Epic Name(show/require/renderer - default text) - Epic Color(show)  We must also ensure that existing fields are in the correct configuration before locking them down.  (!) Development Notes (!)  Q: Is fixVersions one of these since we rely on it for release planning? A: (/) No - it is a system field provided by JIRA and therefore should be locked by definition.  Q: Rank field? A: (/) Also fix/lock the default rank field"
1,"As a user, I would like to add issues in an active sprint to an epic","COS:   - Allow dragging of the story but make sure that the droppable area in the backlog does not indicate anything that might make the user think they were dropping the story from the sprint    (!) Testing Notes (!)    Q: Can you multi-select issues and drag them into an epic from the active sprint?  A: (/) Yes    Q: What if you multi-select issues from both the active sprint and from the backlog and drag to an epic?  A: (x) We can't do that, should we ?    Q: What if the epics panel is collapsed and you drag to it?  A: (/) Works the same way it would if it was open    Q: What happens when you drag to the edges of the screen?  A: (/) If there is non visible space it scrolls, nothing else    Q: What if the issues from the active sprint are already in another epic when you drag to a new one?  A: (/) The issue is affected to the new epic    Q: What if a quick/instant filter is on while dragging?  A: (/) Issues are filtered out and not selectable    Q: Any difference if the Epic is expanded or collapsed?  A: (/) No"
1,Allow inline edit of Epic label,"(!) Testing Notes (!)  Q: Make sure to test opening the inline edit, making a change, and the following actions: Clicking other inline edits (x) Closing the epics or detail panel (x) Switching modes (x) Drag and Drop (x) Create/Delete sprints (x) Reordering epics (x) Multi-select (x) Opening the tools menu (x) Applying quick/instant filters(x) Creating a new issue (x) Create a new epic (x) Anything I'm missing (?)  Q: Test that foreign character's work A: (/) UTF-8 characters work; no XSS  Q: Try long but reasonable input A: (/) Input is limited to a reasonable length  Q: Does tab submit as well? A: (/) Yes but does not initiate inline edit of next epic, which I think is okay"
0,"As a user, I would like to see some feedback after creating an epic on Plan Mode","* Show a success message with a link to the issue view  Must be applied both when ""Create epic"" button is used as well as regular Quick Create / C keyboard shortcut.  (!) Testing Notes (!)  Q: What if the epic was added to another project not on the board? A: The message is displayed  Q: What if the board is multi-project and an epic is added to one of these projects? A: The message is shown regardless of whether the epic is on the screen or not"
1,Improve epic linking code to not perform multiple re-indexes,"Once JRA-30144 is completed, code changes are required in order to opt-out if indexing on createIssueLink and removeIssueLink, collect the issues that require re-indexing and perform a bulk re-index on all those issues at the end of the operation."
0,Restore the feedback link in the ROTP header,COS:  - Put at bottom like 'Give Feedback' in JIRA - Continue saying 'Give GreenHopper Feedback' - Apply same treatment to BTF 
1,Implement 'Jump Nav' (vertical icons for tabs which jump to the part on the continuous page),"COS:   - Implement icon bar on the left  - Continuous page   - Headings for sections that didn't have headings before (i.e. a heading for each tab)  - Selection status for visible section  - Clicking on the icon scrolls to that section  - Remembered state of the Detail View when navigating through issues or modes (current functionality - don't break it!)      (!) Development Notes (!)    Q: Make sure this plays well with all existing functionality, such as keyboard shortcuts and moving tabs.  A: (/) We need to remember the currently selected 'tab' when moving to the next issue or returning to the mode.    Q: How do third party icons affect this layout?  A: (/) The icons will occupy the same space which is a maximum of 16px x 16px    Q: How will resizing of large inline edit fields like the Description field play with the new continuous page and jump navs?  A: (/) It will move the content below relative to the change in size.    Q: Will users be able to switch back if they don't like it?  A: (/) They'll all like it ;-) There is no option for legacy versions, only for continuous improvement.    Q: Do the jump navs work ok when zoomed and at all supported window sizes?  A: (/) Window size should not make a difference, though resizing will be taken into account for the size of the detail view content.    Q: Does tabbing through the tabs still work?  A: (/) They won't be tabs, but they'll be accessible using the tab key as before.    Q: What if some fields are hidden from the default screen (like Description)?  A: (x) They should be hidden.  R: Please test this.    Q: Are the icons always hidden when the detail view is closed? Should they be displayed?  A: (/) They will be hidden since they are within the container that is removed.    Q: What happens when the item counts exceed the available space?  A: (/) Unlikely with expected usage but expected behaviour is to overflow into content area. Clipping or ellipsis would not help much. We can fit 5 digit numbers before this occurs.    Q: What happens when the nav items exceed the available space?  A: (/) Unlikely with expected usage but possible as there's a plugin point there. After a point they would be clipped and unavailable to click. Content can be accessed by scrolling.    Q: Does the Detail View render ok when no issue is selected?  A: (/) Yes, note the DV has a single column in this state for Plan mode. In Work mode it closes."
1,"As a user, I want improved performance of epic issue statistics updates because I want up-to-date data ","Currently on GDOG, dragging 4 issues into an epic takes ~10 seconds for the issue count in the epic to update.  (?) This will need to update *all* statistics shown for a given epic -- see comments.  (!) Testing Notes (!)  Q: We should write a soke test for this and try out the difference before and after the changes with varying sizes of data. A: (x) Not Answered  Q: What calculations will be made on the client side? A: (x) Not Answered  Q: Will these calculations impact other client side operations, or otherwise affect responsiveness? A: (x) Not Answered  Q: Will need to be checked in all browsers. A: (x) Not Answered"
1,Allow the use of Sprint name in JQL to find issues in a sprint,"COS:   - Must autocomplete  -- Autocomplete options are rendered with *Sprint Name - Start Date (ID)*  -- When choosing an option from autocomplete, it boils down to the sprint ID  - Must accept 'sprint = ""name""'  (even if they are not unique) and 'sprint = id'  "
1,"Epic Label requireness must apply to all relevant Field Configurations, not just the Default","Currently, the ""Epic Label"" field is only made ""Required"" in the Default Field Configuration. This logic needs to change to include any Field Configuration that is used for the Epic issue type in a project.    (!) Testing Notes (!)    Q: Does this logic also extend to recreation of the Epic Label field after it is deleted?  A: (/) Yes. When the field is re-created it is again made Required.    Q: What if there is another field/scheme with a duplicate name? Are we referencing both by ID?  A: (/) Field configurations are referenced by ID."
0,XSRF in com.pyxis.greenhopper.jira.actions.VersionBoardAction,NULL
0,XSRF com.pyxis.greenhopper.jira.actions.TaskBoardAction,NULL
1,"As a user, I would like for ""Done"" epics to not show up in Plan Mode","This would be based on the value of the Epic Status field, not the workflow status.  See sub-task re: writing an upgrade task.  COS:  - Show a confirmation dialog when a user updates an Epic to done in Plan mode (y) This has been done as part of GHS-6372 - Upgrade task to convert all existing Epics with a resolution set to have Epic Status or Done, any existing Epics with no status set to ToDo - Epic label on an issue (and in detail view) should still show even if the parent is not shown in the Epic panel"
1,"As a user, I would like to easily see and update the Epic Status of an epic in Plan Mode","COS:   - Single menu item to mark as done with confirmation dialog    (!) Testing Notes (!)     Q: Can you reopen the epic via the same menu?  A: (/) No. You must reopen the epic via JIRA View Issue.    Q: Test when Epic status is stale/out of date. Try updating and dragging epic around.  A: (/) Marking an epic as Done when the state is stale will have no ill effect, as the issue still exists. Ranking similarly is okay. Adding an issue to an epic that has since been removed will work but then afterwards the epic will disappear.    Q: What happens when the last issue in a completed epic is deleted/removed?  A: (/) Behaves as normal?    Q: Will updating the status affect quick/instant filters on that epic?  A: (/) If an epic is selected (filtering active) and then marked as done, that filter will be removed. Should not affect any other filters.    Q: Can you change the statuses of the epic in any way? Deleting, renaming the status and messing up the epic?  A: (/) Yes - the ""options"" are configurable from JIRA's Custom Field admin UI (Configure). It will continue to work until there is only one option left. It will pick the ""last"" option as the one to set when marking as Done."
0,"As a user, I would like to display the number of votes in the agile board.","As we use the voting facility of JIRA/Greenhopper to get a guideline when ordering/ranking the backlog in Planning mode in the agile board, we would love to see the number of votes on the agile board (only planning mode).  For this prupose we could go into two directions: * display the number of votes in all cards in the backlog (configurable?) * display the number of votes in the detail view of the agile board.  The latter is acceptable and probably easier to get.  We recommend put this in the People section of the Info area.  We'll link the tally directly to the voters page just as JIRA View Issue does."
0,As a user I would like to see which issues are not linked to an Epic in the plan mode,"It would be awesome to have an entry on the Epic area, let's call it 'Ungrouped', which when clicked shows all the items which are not mapped to an Epic. Found that it is painful to scroll down the entire backlog to find the right issue, click and drag it all the way to top to drop it to the right Epic issue. - When an item is dragged and dropped on it, it will be unassociated from any Epic it's currently connected to  (!) Development Notes (!)  Q: What happens to this item when there are no epics assigned to it? Will there be a way to tell it is empty without selecting it? A: (/) No, it does not have a count.  Q: So clicking the x on an epic link will automatically add that issue to the unlinked epic? A: (/) Correct (v good note)  Q: Will the unlinked epic update with the new addition in the case above if it is currently selected? A: (/) Yes  Q: Will this only include issues within the backlog or also issues in the work mode? A: (/) Epic filtering is for all issues in Plan mode so includes active sprints listed at top  Q: Does the board require a refresh in order to display these newly linked issues? A: (/) Uses same update as epic filtering = not a full page reload "
1,All Epic Progress Report,"Show the status of each Epic in the board, along with the number of stories, complete vs incomplete, estimated vs unestimated   Include done Epics  Link from each Epic to the Single Epic Report for that Epic"
1,All Epic Progress Report,"Show the status of each Epic in the board, along with the number of stories, complete vs incomplete, estimated vs unestimated   Include done Epics  Link from each Epic to the Single Epic Report for that Epic"
1,Single Epic Progress Report,"There will be an Epic Report with three sections:    - 1: Epic progress chart  ** Starts at epic creation time  ** finishes on current date  ** line for issue count  ** line for unestimated issue count  ** line for total amount of estimated work (using selected estimation statistic)  ** line for completed work (using selected estimation statistic)  ** tooltips will be like the sprint burndown  ** show non-working days markings    This means we have two vertical scales (count and estimate)    - 2: Summary of current epic status   ** Looks like the current statistics in the plan mode    - 3: Table showing all issues currently assigned with the current status (done in GHS-6616)    (!) Testing Notes (!)    Q: What happens with stories that were moved/deleted/converted to subtasks and vice versa, and how is this displayed in the table?  A: (x) Not Answered    Q: Are there any maximums to the table length, in terms of issuekey/number of issues/etc? Can you break these limits?  A: (x) Not Answered    Q: What if the Epic itself was closed/done then reopened? Is the report still available and is there any indication about this history?  A: (x) Not Answered    Q: What if editing another issuetype into an epic or vice versa?  A: (x) Not Answered    Q: Are reports generated dynamically?  A: (x) Not Answered    Q: What happens if I visit the report URL for another non-epic?  A: (x) Not Answered"
0,"As a user, I would like a quick link to an epic's Epic Report on the View Issue page","Once the Epic Report has been implemented, have a link in the Agile web panel to open the report for the current epic. Like sprint links, it should open a dialog with the board options if there are multiple boards."
0,"As a user, I would like a quick link to an epic's Epic Report on the View Issue page","Once the Epic Report has been implemented, have a link in the Agile web panel to open the report for the current epic. Like sprint links, it should open a dialog with the board options if there are multiple boards."
1,"The ""View on Board"" link for an epic does not take you to any scrum boards, despite the epic appearing in the Epic Panel in plan mode","Need to modify how this calculation is done so that the operation will offer to take users to Plan Mode.    Q : Would the epic be selected? That could be strange as it would apply filtering.  A : no, the epic is not selected    (!) Testing Notes (!)    Q: Make sure to be searching by an issue id, not epic name to prevent searching on duplicate names.  A: (/) sure, searched by issue id    Q: How performant is this across many boards, like JAC?  A: (/) hard to say, but we basically added only one clause to the query. It shouldn't perform very differently from what is already deployed on JAC. Needs verification tough.    Q: What if the Epics Panel is collapsed?  A: (/) the panel is opened and the epic is selected. It means that issues that are not in the epic are filtered out    Q: What if the user does not have permission to see a board/project that the Epic is involved with?  A: (x) [~pobara], not sure how to produce this case, let's speak together about that"
1,"As a planner with a long backlog, I would like the Epics Panel to stalk the backlog as I scroll down","COS:   - Need separate scrolling for long Epic lists    (!) Testing Notes (!)    Q: What happens if there are error messages in the page as well?  A: (/) Error messages have an absolute position so they are not influenced by the epics pane.    Q: How well does this work with all browsers?  A: (/) Tested on IE8, IE9, last Firefox, last Safari and last Chrome.    Q: What happens when the epics tab is very long and you scroll in it or in the main pane?  A: (/) The epics pane has its own scrollbar, so when the content is longer than the available visible height, a scrollbar appears.    Q: Try a combination of clicks and scrolls with epics/issues selected.  A: (/) Bad behaviour when the view port is smaller than the minimum JIRA supported size, but it is normal.    Q: What happens if the detail view is closed?  A: (/) The epic pane is not influenced by the detail view."
0,"As a planner, I would like a way to create new issues for an epic, so that the issues are automatically assigned to the epic","COS: - Issuetype should be their last issue type unless it was Epic in which case it should default to story - Use Quick Create - If they choose Epic let them create the Epic but don't do association  (!) Testing Notes (!) Q: What happens if there is no Epic issuetype? A: (x) Not Answered  Q: What happens after creating subtasks? A: (x) Not Answered  Q: What happens after a Quick Edit, of an epic or story? A: (x) Not Answered  Q: Is there an alert that will indicate to a user that the issuetype field has changed under them? A: (x) Not Answered  Q: What if the issuetype field is not displayed on that screen? A: (x) Not Answered  Q: What if it is restricted to a particular group? A: (x) Not Answered"
0,Update selected epic treatment,spoke to Tokes about this this morning.  See treatment at: https://extranet.atlassian.com/display/GHDEV/Epics+Selection+Scheme
1,Allow it to be possible to create a project at the same time as creating a board,"COS:  - After Scrum / Kanban choice allow (a) selection of existing projects (b) selection of a filter (c) creation of a new project - Link the Getting Started links in to this new dialog (with the appropriate selection, i.e. create sample kanban project goes to the second step of the dialog with 'Sample Data' selected) - Kill off ""Advanced"""
1,Allow it to be possible to create a project at the same time as creating a board,"COS:  - After Scrum / Kanban choice allow (a) selection of existing projects (b) selection of a filter (c) creation of a new project - Link the Getting Started links in to this new dialog (with the appropriate selection, i.e. create sample kanban project goes to the second step of the dialog with 'Sample Data' selected) - Kill off ""Advanced"""
0,"Restore ""Create Issue"" dialog preferences after Epic is created","COS: - Also remember your 'All' or 'Custom' preference for fields  - None of this applies to the Create Epic dialog  (!) Testing Notes (!)  Q: What happens if you switch modes or go to config before you create an issue? Aka how long do we queue up this memory? A: It's reset when the dialog is hidden. If you refresh the page with the dialog open, it doesn't get reset.   Q: What happens if you edit an issue first, changing the fields that are shown there, then try quick create? A: Quick Edit and Quick Create customisation are independent.  Q: What happens when you open the create dialog but then close it, and reopen again to actually create? A: (/) it's good"
1,"As a product owner, I'd like a days remaining in sprint gadget for the new boards (similar to the days remaining gadget)","# Same defaults when choosing the sprint as in GHS-6467  # Needs to take non-working days into account (we should make it explicit in the description of the gadget)  # Mimic behaviour of classic version of the gadget otherwise  # Wallboard compatible  # Call it ""Days Remaining in Sprint"" gadget    (!) Development Notes (!)  Q: What happens if a user doesn't have permission to access a project in the gadget?  A: (/) No permission error message?    Q: Will the days remaining be calculated in the server timezone or adjusted to user timezone? (I think it would have to follow the server timezone to be accurate for remote teams)  A: (/) User's timezone    Q: Do the days remaining honor the time of day that the sprint ends?  A: (/) Yes    Q: What is displayed if there is no active sprint?  A: (/) No active sprint error message    Q: What is displayed if the sprint has been closed?  A: (/) 0 Days Remaining"
1,"As a product owner, I'd like a burndown gadget for the new boards (similar to the existing Classic Burndown gadget)","- this is to mimic the behaviour of the burndown chart of the board    # The sprint value can be chosen by the user. The default value is auto (the one that ends first)  # No configuration to change estimation statistic  # Needs to be wallboard compatible  # Call it ""Sprint Burndown"" gadget    (!) Development Notes (!)    Q: Will Wallboard Mode be compatible for all supported browsers (IE specifically)?  A: (/) Works but with some adjustments on IE8 (because of quirks mode)    Q: What if a user does not have permission to see a project configured in the gadget? (either a single project or one out of multi-project board)  A: (/) Will get an error message saying that the board can not be find    Q: What if the sprint has run over its end-date?  A: (/) Same as burndown chart    Q: Does the y-axis adjust if there is scope addition? Do the tickmarks readjust?  A: (/) Yes, thanks to flot    Q: What if there is no active sprint?  A: (/)  Displays a message like ""could not find an active sprint""    Q: What if the active sprint has only issues that are in a project that the user does not have permission for?  A: (/) Will get an error message saying that the board can not be find    Q: Can the user hover over data points in the chart?  A: (/) The user can't for now"
0,"As a user, I would like a confirmation when I delete a future sprint so that I don't accidentally lose all my work","The team I'm in had our 5 next sprints all planned out, which is something the Greenhopper planning UI somewhat encourages by saying ""Plan some sprints"".    Somebody accidentally deleted the second one by hitting the subtle grey (x) in the top corner of the sprint.    Re-adding it again is impossible.  We have to rename all the sprints and shuffle the work down each one.  Due to a single mis-click.    If adding sprints to the middle is impossible, I suggest that deleting them from the middle also should be.  "
1,"As a PO, I'd like to see the stories of an Epic in JIRA view issue","COS: - 'Issues in Epic' panel - Immediately beneath the sub tasks panel - Looks like the sub tasks panel -- No ""Plus"" button to create new issues -- No dropdown to filter issues -- No numbered list -- Prepend summary with issue key -- Fields: done-ness icon, issue key, summary, issue type icon, status, assignee -- Include issue operations cog per issue -- Order by ""default"" rank field - Don't show if there are no linked issues  (!) Development Notes (!)  Q: How does this perform with a large number issues? A: should be fine  Q: What if you dont have permission to see some of the issues? Or not logged in? A: Only issues which the user has permission to see will be displayed in the list.  Q: Any support or infrastructure for possible bulk edit functionality? A: No  Q: View in issue nav link? A: No  Q: What if there are non-stories with an epic link? Story was converted to something else? A: Any issue currently linked to the epic via the Epic Link field will be displayed.  Q: What if epics have an epic link? (also converted from story) A: see above"
0,"As a PO, I'd like to see the Epic of a story in JIRA view issue",COS: - Show like the label - Can't remove it  in the agile box  (!) Development Notes (!)  Q: Can you edit the value there? A: (/) No  Q: Can you see this information as an anonymous user? A: (/) Yes
1,"As a user, I would like to be able to associate an issue to an Epic from JIRA",Possible implementations:  * Issue operation dialog  * Field to edit with inline edit or Quick Edit
1,Clean up Epic-Issue link when changing the issue type of an issue,"Epic -> Issue: Remove the epic link for all issues linked to that epic Issue -> Epic: Remove epic link if was previously part of an Epic Issue -> Subtask: Remove epic link to Epic if was linked Subtask -> Issue: If old issue parent had epic then link issue to epic (to be discussed if we really want this), unless the new issue type is Epic  We shouldn't have to clean up epic label and epic color fields as these are only applicable to epics and therefore will automatically be cleared.  (!) Testing Notes (!)  Q: Make sure this plays well with the change to Epic Name field. A: The two are completely unrelated  Q: What if subtasks are disabled either before or after trying to convert? A: (/) Can't disable sub-tasks while there are sub-tasks in the system.  Q: Don't forget to add the Epic Link to the subtask if the parent has one. A: Epics are always only linked to parent issues, not to subtasks"
1,Be able to query Epic assignment history of all issues ever assigned with an Epic,"COS:  - For a given epic return the history of all issues added and removed (with time) from the Epic - Will need to inform admins that they need to re-index - This does not involve the addition of JQL to assist in this querying  (!) Development Notes (!)  Q: I'm assuming this will only be seen on the view issue page? A: It won't be seen in the UI anywhere. It will only be seen by accessing the URL directly.   Q: What if the epic has been deleted? A: You get an error message: issue does not exist.   Q: What if the epic was converted to an issue/subtask? A: It doesn't check. If you changed it in JIRA and there are still epic child links, you'll still see the data.  Q: What if the linked issue was moved/converted/deleted? A: It should still show up in the appropriate group (incomplete, complete, punted)  Q: From what version forward will we be tracking this history? aka if an epic link already existed before we introduce this change? A: previous epic links should be tracked  Q: Do any of these questions surprise you? If so, what hadn't you previously considered? A: the first one, because it's not a UI story  Q: What else can you think of? A: (x) Not Answered"
1,"Show table at bottom of single Epic report that shows the estimate (current) for Done, Undone - Estimated, and Undone - Not Estimated stories","COS:  - Similar to existing Sprint Report - Include key, summary and current estimate  (!) Testing Notes (!)  Q: What if issues are removed and readded to an epic? A: the report represents current state only  Q: What happens to the table if there are negative estimates? A: same thing as sprint report  Q: What if the epic is visible on the board, does not contain any issues on that board, but contains issues from another project? What will be displayed in the table? A: no issues - report is dependent on the board query  Q: What if the estimation config is removed/modified from issues on the board? A: report represents current estimation config as per sprint report"
1,Remove Epics from Labs,COS:  - Remove admin option - Welcome mat changes (remove the banner) - Create Epic fields on startup  (!) Testing Notes (!)  Q: Make sure THE UPGRADE works both when Epics are PREVIOUSLY not enabled and are enabled. A: Tested empty upgrade with different JIRA versions as well as upgrading on different JIRA versions  Q: Make sure Epics that are in the backlog of plan mode move properly to the sidebar and colored lozenges populate properly. A: The Epics move as expected but the colours + Epic Name will be empty as not yet set
1,"As a user, I'd like to see change history for my Epic in the history tab for the Epic","COS:  - Show change history every time a new issue is attached or detached from the Epic - When multiple issues are attached to an epic at once log just one change history entry - When multiple issues are detached from an epic at once log just one change - Can't use the Epic Link field (because that's always from Child to Parent)  (!) Testing Notes (!)  Q: Which events are tracked in the history? Are there any special events for issues that are either not tracked or different for epics? A: Just adding / removing issues, no special events.  Q: Can you backdate changes? A: No  Q: What happens to the change history for completing an epic and reopening it? A: History of added / removed issues shouldn't be affected.  Q: What happens to the change history when estimating stories in that epic? A: Nothing"
0,"Put 'Filtered' icon at top of backlog and in each sprint header, clicking it clears all filters","For active sprints, upcoming sprints and the backlog we currently we only show a message inside the container when its contents are affected by a filter and no issues remain. The filter can be one or more of Instant Filter, Quick Filter(s), Epic Filter. For the sprints there are counts showing visible issues e.g. 3/11 Visible Issues, but not for the backlog.  To help users realise there are filters affecting the contents of each container and assist with the removal of any filtering we will add an icon to any filtered container. The title attribute (tooltip) will contain a message in the format ""32 Issues filtered. Click to clear all filters."" Clicking the icon will reset all filters on the board including all Quick Filters, Epic Filters and the Instant Filter. If the Instant Filter was active then its contents would be removed as it is reset.  *Stretch* Additionally while we are making the issue count available to the backlog container  we will show the current issue count on the backlog header.  See linked EAC page for sketch.  ---  (!) Testing Notes (!)  Q: What happens when you apply instant filters? Is the field cleared as well? A: (x) Needs to clear value  Q: What happens if you have issues selected? A: (x) Should not change selection  Q: What happens to any open dropdowns when this button is clicked? A: (x) Not Answered  Q: Do these buttons only clear filters for 1 sprint, or all? If not all, how would we display which filters are active for which sprints? A: (x) Needs to be for all  Q: What if Epic filters are on but the panel is closed? A: (x) Needs to clear epic filter "
0,Change Epic Label field to Epic Name,"COS: - Rename any existing instances of the field to Epic Name - Rename the type of the field to Epic Name - Ignore any change items that were previously created against this field - Search for any references to ""Epic Label"" and replace them with ""Epic Name"" -- including documentation (cc: [~rosie@atlassian.com])  (!) Testing Notes (!)  Q: What if there is a duplicate named field? A: (x) This could cause unexpected results but [~miruflin] states we can't do much about this. Deemed unlikely and an extreme edge case.  Q: What if there is a field type for this already in the system? A: (x) This could cause unexpected results but [~miruflin] states we can't do much about this. Deemed unlikely and an extreme edge case.  Q: What about existing quick filters or swimlanes (any JQL) that reference the field name? A: (x) JQL will become invalid, same as any JQL made on a field where the name is changed. Swimlanes/QF would be using Epic Link though since the parent's value is not available."
0,"As a user, I would like to be able to find out more information about an issue's epic when the epic is ""Done""","With the ability to mark epics as Done, it is now possible that users will see an Epic Name lozenge in the Backlog and not be able to see the epic in the panel that this name refers to.  The lozenge on the Issue Detail View should contain a link to the epic's Epic Report."
1,As a Classic user I would like my epic-issue relationships migrated for use on the new boards,"COS:  - manually triggered for a whole project  - show the number of issues to be updated before you hit go  - make it clear that this only migrates data that exists right now - it does not synchronise the relationships over time. also make it clear that it can be re-run at a later time if necessary.  - put in the global config in a new tab -- *Classic Epic Migration*  - explain what is about to happen when they proceed:  -- find issues with an issue key in the 'Epic / Theme' field then convert this to an Epic link. If a story is already linked with Epic Link field, ignore it.  -- server-side logging  - notify the Admin on confirmation page they should remove the Epic / Theme field (if it is no longer being used)  - modify description of Epic/Theme custom field so that it references Classic Boards    (!) Development Notes (!)    Q: Need to try with data from epics still in labs  A: (x) Not Answered    Q: What happens when editing the old epic links?  A: (/) The old epic links can be updated but are not synced to the new style of epic link. Another migration is needed to port over the updated classic epic link (this will only work if the target issue has not yet been linked to a new style epic link)    Q: If you update classic epic links, will new ones be created as well from now on?  A: (/) No, there is no synchronization between old and new epic links    Q: What if there is old epic label data from when epics was still in labs (aka field is named something else)?  A: (/) Upgrade tasks will have migrated it. The migration uses the configuration of classic mode to find the classic epic link custom field    Q: Are there any differences with JIRA/AO versions we need to be wary of?  A: (/) Nope"
0,On plan mode rank newly created items immediately below the currently selected item,"If issues are selected: - Ranked after the last selected issue  If no issues are selected: - Ranked before the top visible issue, where ""visible"" means the section of the Backlog visible in the viewport (including upcoming sprints but not the current sprint) - If the current sprint is taking up the entire viewport, then the top of the backlog/future sprints is used  If multiple issues are created at once: - Ranked in order of creation (as normal), all issues appear in the appropriate position based on the above rules.  Filtering of any kind (Quick, Instant, Epic) should not impact on where issues are placed - the rules of visibility still apply.  (!) Testing Notes (!)  Q: What if the view is stale? A: We will place the issue according to the currently visible issues, so if they are out of date the issue might appear at the wrong place  Q: What if the viewport is huge or very very small? A: Same result as if the viewport is normally sized.  Q: What if the detail view is closed/open? A: doesn't affect the logic  Q: What if multiple issues are selected? A: We'll place the issue after the last selected issue   Q: What if an epic is selected? A: We currently don't have epic selection  Q: What if a quick/instant filter is applied? A: See first answer, we'll place it before the first visible issue. In case the selected issue is instant filtered, we still place it after the selected issue  Q: What if your last selected issue is the last issue but it was moved/deleted? A: moved: it will be ranked according to the new location. deleted: bad luck, it will be ranked last  Q: Can you fool GH by setting the URL without loading it and creating at the new selection? A: no, GH doesn't look at the url, it looks at the internal state (which might come from the url when you hit return in the address bar). Even if it did, this wouldn't be the end of the world.  Q: What if you don't have schedule permission? A: We'll silently ignore rank errors, so the issue will end up at the end of the backlog  Q: What if the selection is within an active sprint? A: The issue would be added at the top of the backlog  Answers provided by Michael R and Nikolay"
0,Align project creation dialog to JIRA style,"There are some differences in our project creation dialog to the JIRA one.  * Remove ""Project"" from ""Name"" and ""Key"" field labels * Standardise on field descriptions for Name and Key (maximum length) ** Enforce maximum lengths on those fields  "
0,Align project creation dialog to JIRA style,"There are some differences in our project creation dialog to the JIRA one.  * Remove ""Project"" from ""Name"" and ""Key"" field labels * Standardise on field descriptions for Name and Key (maximum length) ** Enforce maximum lengths on those fields  "
1,GH code fixes required for latest 6.0-breakit version,"In 6.0-breakit-SNAPSHOT since Jan 3rd, last unaffected version is 6.0-m04, ActionUtil, ActionNames, possibly ActionDispatcher and other related code has been removed.    GreenHopper uses this code in classic board fields to update field values, e.g. set the assignee, component, version etc.    To fix it, we'll have to replace all update code to use proper JIRA api. See the deprecated messages in ActionNames on what to replace the code with."
1,"As Patrick, I want to see list of versions when I'm looking at the backlog","Considered versions: - versions which are BOTH: -- part of projects included in the board, AND -- unreleased -- not archived  Ordering: - version sequence for versions within the same project (oldest first) - overall projects are ordered by project name  Display: - version name along with project key (name on mouse hover) -- version name wraps around project key, if one or both are long - if the board only has one project, omit project name  If there are no release versions you get an empty column. In the filters story we'll add an ""all releases"" button so it doesn't look so empty.  (!) Development Notes (!)  Q: What if there are duplicate version names? A: They look the same.   Q: What if the version names are long? A: They wrap.   Q: What if a version has been renamed / deleted in the system? A: It doesn't show up (archived versions dont' show up).  Q: When reordering versions, are they bound to their container? A: The order can't be changed on this page.   Q: What can I edit about the versions? A: Nothing. Nada. Rien. Nope.  Q: What does the detail view display if a version is selected but no issues are selected? A: There is no selection of a version yet, that's part of the filter story.   Q: What if the fixVersions field doesn't exist? A: (x) Not Answered  Q: Is there a way to select another version field to be displayed here? A: (x) Not Answered  Q: What if the version is ""Archived"" in JIRA? A: It isn't included in the list. "
1,"As Patrick, I want to be able to see more details about the version when looking at the Versions Panel",#NAME?
1,"As Patrick, I want to be able to create a new version","# Dialog is shown  # fields we need to populate: Project, Name, Description, Release Date  # Single Select (with autocomplete aka Frother) Control for project selection  # Description field - text field  # Apply sensible length limits to these input fields (check what limits JIRA has in place if any or just enforce a limit of 255 chars on the front end)  # create version affordance is only displayed if the user has project admin permissions to at least one project (it is the same as can manage sprint logic)  # Analytics to track usage of dialog  #* Also capture length of version names    (!) Development Notes (!)    Q: XSS safe?  A: (/) yes    Q: What if the version name already exists?  A: (/) We show an error message"
1,"As Patrick, I want to be able to delete a version",#NAME?
1,"As Patrick, I want to be able to delete a version",#NAME?
1,"As Patrick, I want to be able to edit details of a version",https://extranet.atlassian.com/display/GHDEV/GHS-7163+Versions+Inline+Edit
1,"As Patrick, I want to choose a colour for a version so I can identify it easier in the backlog",#NAME?
1,"As Patrick, I want to choose a colour for a version so I can identify it easier in the backlog",#NAME?
1,"As Patrick, I want to assign issues to a version","- permissions  - only one version can be associated with an issue (using this feature): fixVersion is replaced  - can assign multiple issues to a single version easily  - what happens when multiple issues from multiple projects are dragged?  -- don't allow drop on version panel  -- possibly explore additional message or change cursor  - what happens when an issue from PROJA is dropped onto PROJB?  -- don't allow drop on version panel  -- show message  - able to drag issues from active sprints or future sprints or backlog  -- refer to comments below re: dialog when destructively updating    Note:  * IE8 experience may not be as good due to bad drag and drop performance  * consider messaging for invalid drop targets    (!) Development Notes (!)    Q: What if the version list is stale and a version doesn't exist anymore/was archived?  A: (/) We show a ""Issue doesn't exist"" message and we reload the issues list    Q: What if an issue was converted to a subtask or epic?  A: (/) We attach it anyway    Q: What if the issue is already part of a sprint?  A: (/) We allow it    Q: Need to make sure the issue is not reranked if drop on version fails.  A: (/) We always cancel ranking after drop on versions even if it fails    Q: What if a group of issues is dropped but one or more fail to be added? What happens to the rest and how do we notify the user?  A: (/) We don't update anything and we show a message saying that the user doesn't have the permission to update the issue(s)    Q: Are the dropzones properly hidden when the version tab is hidden?  A: (/) yes    Q: Does drag and drop work well if quick and/or instant filters are on?  A: (/) yes, no direct influence"
0,"As Patrick, I want to see the version assigned to an issue in the list view so I can quickly identify which issues belong to which versions","* Estimate column spacing still honoured * If there is no epic associated, version lozenge will be right-justified next to estimate * All version lozenges use the same colour, which is not a colour available to epics * Truncation assessed by the width (similar to epic lozenges) * What happens when an issue is associated with multiple fix versions through JIRA? ** Show somehow that there's more than one version on the lozenge ** The primary version is the first unreleased version in the sequence * Released versions are displayed * Archived versions are not  (!) Development Notes (!)  Q: What if the version name is very long? A: it's truncated (via CSS - martin to review)  Q: Shouldn't the primary version in a multi-version be the soonest version, rather than the alphabetical one? A: (x) it's currently ordered alphabetically, please talk to Tokes / Tom if you think it should be different  Q: How does this play with the epics / DV being open / closed? A: not affected  Q: How do you remove an issue from a single version if it has multiple? A: you can't remove a fix version at all  Q: What if one of the versions is / has been released? A: it still shows up  Q: What if the version is ""Archived"" in JIRA? A: (x) Not Answered"
1,"As Patrick, I want to be able to filter my backlog to a specific version so I can easily see which stories will be included","* Includes adding ""All issues"" option for versions AND epics  * Includes adding ""Issues without version"" option  * Single version selection  * Selected version for filter should be remembered in the URL  * Epic numbers metadata will not be updated when filtering is active  * If an epic filter is active, and then a version filter is activated which excludes that epic, the epic filter will revert back to ""All issues"" option    (!) Development Notes (!)    Q: So shift or control click do not work for multiple select?  A: (x) Not Answered    Q: What if a quick filter applies a version filter?  A: (x) Not Answered    Q: What if an instant filter is active?  A: (x) Not Answered    Q: Do we want to link the order of versions to the order of issues in the backlog?  A: (x) Not Answered    Q: What if an epic is selected and a version which is empty is selected? Do we still drop all the issues from the filter the same way if issues in the epic aren't in the version?  A: (x) Not Answered    Q: Does the instant filter only filter on the subfiltered issues, or on the entire backlog?  A: (x) Not Answered    Q: What is the hierarchy about how filters are applied now?  A: (x) Not Answered"
1,"As Patrick, I want to be able to filter my backlog to a select few versions so I can see which stories will be included in them",NULL
1,"As Patrick, I want to be able to filter my backlog to a select few versions so I can see which stories will be included in them",NULL
1,"As Patrick, I want to understand the progress towards a version with a single report","- the chart needs to show the complete history of a version (incl scope changes) - it's a burnup chart showing total estimate vs complete estimate, total vs unestimated stories - also display count of unestimated stories - start date is defined as the day when first story from that version was added to the version (to use startDate once we have it) - also display the list of stories split into complete, incomplete, incomplete unestimated (we may want to consider adding full history later on); - each story on the list should display the epic it belongs to  (!) Development Notes (!)  Q: How does the chart look if it has been archived then unarchived again? A: the same  Q: What if an issue is a part of multiple versions? Are there any implications for which version it is displayed in/ permission issues if this issue is in another project our user has no permissions to see? A: it's displayed in all of them if the user has permissions to see it  Q: What if epics are in a version, how are they counted and displayed? A: They are added to the issue count and will show up in the table.  Q: What happens to the chart when the first issue added to the version is deleted/moved/converted? A: it should show issue removed"
2,"As Patrick, I want to adjust release report projections to simulate and better understand what can be achieved within a version",- allow the percentage calculation to be adjusted (default to 10%) 
2,"As Patrick, I want to adjust release report projections to simulate and better understand what can be achieved within a version",- allow the percentage calculation to be adjusted (default to 10%) 
1,"As a user, I would like to create an issue inside an epic when viewing an epic in JIRA","The sub-task panel has a 'Create' button to create sub-tasks quickly for an issue. It would be good to have a similar mechanism when viewing an epic, so that you can create issues for the epic.    (!) Development Notes (!)    Q: How does this overwrite the current project and issuetype selection of the create issue dialog?  A: (/) No overriding is done for the create issue dialog. It behaves identical to the normal dialog.     Q: What happens if you open the create issue for epic and create issue dialogs one after the other?  A: (/) Multiple issue creation is supported. After all issues are created and dialog is closed, then the created issues are associated with the epic    Q: What if the epic is closed?  A: (/) Issues will still be able to be associated to the epic.    Q: What if the epic doesn't exist anymore?  A: (/) An error message is shown saying the issue does not exist    Q: What if the epic has been changed to another issuetype?  A: (/) If the user is on the epic's view issue page and the epic's type is changed on another page, then creating issues in the epic work, but the association will fail with an error message saying the Issue should be of type Epic    Q: What if you have inline edits open in jira before clicking the button?  A: (/) They are closed before the dialog opens"
1,Add Atlassian Analytics events to GH,"All of the values below will need to be grouped by instance:    - how many instances are actually activated (measurement: for every instance with GH enabled, make sure we store first visit to an instance - most likely to be /secure/Dashboard.jspa, which is in JIRA)     - of these instances, how many find Rapid Boards (measurement: first visit to /secure/RapidStart.jspa in an instance) + how many of the ones without GH at first had them (this necessary to have the numbers right for the following points)   -- Event is ""gh.welcome.show""    - how many attempt to create a project? (measurement: opening of the dialogue to create a project)   -- Event: ""gh.welcome.create.project.start"" with type in ""label"" field    - how many create a project + how many create a project of each type (scrum, kanban, diy) + how many create sample/non-sample projects? (measurement: store project_created as an event, with type associated to it, as well as if it was a sample project).   -- Event: ""gh.welcome.create.project.sampleData"" with type in ""label"" field preceding the next event in case of simple data is used   -- Event: ""gh.welcome.create.project.complete"" with type in ""label"" field    - how many non-sample projects create a single issue (measurement: store issue_created event)   -- Event: ""gh.issue.created"" with keys array stringified in ""keys"" field    - of scrum non-sample users, how many create a sprint (measurement: sprint_created event)   -- Event: ""gh.rapidboard.sprint.backlog.addmarker""    - of scrum non-sample users, how many start a sprint (measurement: sprint_started event)   -- Event: ""gh.start.sprint.dialog.change.start"" when attempt to start the sprint   -- Event: ""gh.start.sprint.dialog.change.complete"" when they actually start it    +    - how many users use rapid boards exclusively, how many use a mix, how many use classic exclusively - the first and last is what we cannot determine at the moment (measurement: store requests to RapidBoard.jspa vs the other two classic boards, then count these per week grouping by instance). Note that this is actual usage grouped instance, as opposed to usage overall (so larger instances having more influence) or binary on/off ""has ever used a classic/rapid board""   -- Event: ""gh.rapidboard.show"" on view of rapid board   -- Event: ""gh.classic.show"" on view of classic board      From Jay:    - How many epics appear on a board   -- Event ""gh.rapidboard.epics"" with ""value"" field as count    - How many versions appear on a board   -- TBD    (!) Development Notes (!)    Q: What if 1 action triggers more than 1 event?  A: (/) It could and probably will - its up to the post-processing to effectively filter, order & group the events."
0,"As an administrator, I would like to be able to turn on/off the Release Planning feature as a Labs Feature",* Surfaced in the GH Labs Configuration tab * Controls the visibility of any Release Planning feature/story 
0,"As a user, I would like for the Release Planning labs feature to be promoted on the Getting Started page","For administrators: this includes a button to enable the feature For non-admins: if not enabled, show a message saying it is available and they have to ask an administrator to enable it.  Once enabled: show users a Feedback button. Admins also see a link to disable the feature.  Jay to provide new icon."
0,As Patrick I'd like to be able to hide the versions panel,"* State persisted in URL * Default view is Versions panel is collapsed * When both versions and epics panels are collapsed, show affordances that allow the user to differentiate between the two  (!) Development Notes (!)  Q: What if the detail view is not visible either? A: (x) Not Answered"
0,Store startDate for fixVersion,"As Patrick, I want to be able to specify a startDate for my fixVersion when creating it so that my charts can be displayed in a reasonable way.  # allow to specify startDate when creating a version within GH # store as date with UTC timezone"
0,document compatibility of GH with JIRA 6.0 (NB. dependent on Plugins 3),NULL
1,"As a user, I want to be able to create a GH board straight from the JIRA create project dialog so it's easier for me","This is about plugging into the plugin point exposed by JIRA and  - specify the title - hijack the project post creation and redirect to GH - separate buttons for Scrum + Kanban - Simplified workflow by default without option to select  (!) Development Notes (!)  Q: Is there differing implementation between btf and OD that we need to consider? Application permissions? A: (/) I don't believe there should be any, these would relate to installation of GH when it's not present and will be dealt with in subsequent stories  Q: What board configuration do we allow from this dialog? A: (/) This story only covers the minimum - the user will be able to specify project name, key and project lead (all of that provided by JIRA already), and we need to create a board for that project, assuming simplified workflow  Q: What happens to our old create project+board dialog? A: (/) Nothing "
1,"As a BTF user, I want to be able to install GH via UPM when creating a project","This is to allow us to launch the UPM and install GH as the current dashboard process does. Further customisation of the process will be dealt with in a further story.  Banner Text: ""Please install GreenHopper""  "
1,"As Patrick, I want to be able to see a projection of possible end date of a version","This is to be calculated using -an average time spent per completed story point (or any other estimation statistic)- average story points per day in a given release so far, plus optimistic and pessimistic variants by adding or subtracting -one standard deviation- 10% from it. (note this was changed because we already return data for story points completed by date but not time per story, and a standard deviation was too big of a difference in most cases and didn't seem to make sense)    # The projections must be displayed somewhere outside of the chart as well.  # Released versions do not need to show projections.  # Show the intended release date on the chart (midnight end of the date specified).  # Do not show calculations of velocity in the page.  # Take NWD into account with the calculation of mean and SD.  # Tooltips for potential targets show ""type"" (optimistic, pessimistic) and date.  # At least 10% of total estimated work for the Version must be completed before projections are shown.    (!) Development Notes (!)  Q: What if users are using negative story points?  A: If the predicted release date is in the past, we shouldn't display a prediction at all.    Q: What if any of the rates approach infinity?  A: We probably need to discuss with [~jrogers] [~mjopson] [~tkotecki] what is a good point to cut off. At the moment, if the predicted date is infinity, we don't display a prediction. If the most pessimistic date is infinity, we display a flat line and cut the chart off after the predicted date. But as long as it's not infinity, we display it.     Q: What distinguishes a rate that approaches vertical from the scope change events that ARE vertical?  A: (x) Not Answered (Don't really understand this question)    Q: What happens when users change the estimation of their board, or remove it completely?  A: The board will reflect the current estimation.     Q: What if the estimation uses time tracking? Will we calculate deviation in minutes, hours, days? How will this be reflected on the axis?  A: We will calculate units (of estimation) completed per day and use this to project. The axis won't change from what is displayed now: X is time and Y is estimation units.    Q: What if the user is tracking work with Logged Work? Should this rate also be displayed?  A: (x) Not Answered (do you mean time tracking?)    Q: What do we display if a projected line goes significantly into the future before it touches the scope line? Potentially this could be years into the future, but this isn't realistic to display.  A: The chart expands to display the most pessimistic date, unless it's infinity, in which case it expands to show the middle prediction. It could be years.     Q: What happens to the lines if the version has been released?  A: If the version is marked as released or if total scope = total completed scope then we shouldn't display a prediction.     Q: What happens to the lines if the ""predicted"" release date is moved to the past?  A: This should never happen.    Q: What happens if the release date is moved into the future?  A: Marking a version as released is separate from displaying a release date. Regardless of whether a release date is assigned, we will try to predict a date if total scope > completed scope and the version is not marked as released in JIRA.    Q: What if the release date is removed?  A: See above.    Q: What happens to the projected lines if issues are removed from the version (resulting in scope change) or deleted entirely (resulting in chart change without history)?  A: The scope line stays horizontal so if scope is removed then it moves down and in theory the release date should move forward. If the story was completed before it was removed then it's used in the calculation of average points completed per day."
1,"As Patrick, I want the list of epics to reflect the current version filter","* When a version filter is active, the epics which are not part of that version (by way of issues) are shown as ""faded out""  ** These faded out epics will still be interactive (drag and drop, filter toggling)  * Epic numbers metadata may be updated when filtering is active  * If an epic filter is active, and then a version filter is activated which excludes that epic, the epic filter will revert back to ""All issues"" option  * If a version filter is active, and a epic filtered for a an epic which has no issues in the currently selected version is selected, both filters are active and no results are shown    (!) Development Notes (!)  Q: Are there still drop zones for faded epics?  A: (/) Yes    Q: Will the epic unfade immediately if an issue is added?  A: (/) Yes    Q: Will the epic fade immediately when an issue is removed from it?  A: (/) Yes    Q: What happens if a quickfilter filters out all issues that are part of an epic?  A: (/) There will be no impact on the fading of the epics    Q: What if the instant filter is used?  A: (/) There will be no impact on the fading of the epics    Q: What happens when you ""Create an issue in epic"" for a faded epic?  A: (/) If the created issue has the fix version of the version being filtered, the epic will be unfaded because now it contains an issue with the correct version    Q: What happens if you create a new epic with a version selected? Is it born faded?  A: (/) It will be fade-born yes    Q: Do all faded epics unfade if you click the funnel?  A: (/) Yes    Q: Can you still modify faded epics through their dropdowns?  A: (/) Yes    Q: For all the files being touched, which other dependencies might be affected by changes?  A: (/) No impact on other files/components"
1,Bring context menu to work mode,"- allow for multiple select (with current limitations)  actions:  - bulk change - open in issue nav - remove from sprint (& show the same warning as elsewhere)  (!) Development Notes (!)  (x) Check Interoperability with other actions on the board (while menu open, try ranking, transition, cog menu, etc)  (x) Check permissions to remove from spint, bulkchange (non-admin, user without delete permission, etc)  (x) Check right-click does not trigger where it shouldn't (issue in done then open detail view)  (x) Scrolling - should disappear when context changes  (x) Stale state - what if an issue was already removed from sprint? (need 2 users)  (x) Check with various board configurations (swimlanes, kanban with releasing)"
1,Swimlane by epic,"As Patrick, I want to use epics for swimlanes so I can see progress of each epic within a sprint more easily.  # Both stories and subtasks for the epics should display. # Only an option for Scrum boards # The swimlane presentation is similar to ""story task"" swimlane: Epic Name, possibly issue key # No detail view shown for epic; clicking on swimlane selects first issue in swimlane (same as Assignee) # Issues without epics shows at the bottom"
0,Display a invalid drop target icon on the version list when trying to assign story for a different project to a version,"Use the browser default cursor ""not allowed"""
1,Integrate GH install via JIRA project template,"The work for this is on a branch of the JIRA plugin (jira-project-templates-plugin)    (!) Development Notes (!)    Q: For each of the files you are modifying, what dependencies rely on or share the code you're changing?  A: (x) Not Answered    Q: What safeguards exist if the GH install fails or is incomplete?  A: (/) That is handled by UPM, if GH is already installed, the installer shouldn't show because GH is providing it's own project templates    Q: Do we have a dependency on UPM version for this to work? If so, do we have a check in place?  A: (/) Dependency on JIRA 6.0+ and the UPM it uses.    Q: How does licensing work? What if GH is unlicensed? Or if a license cannot be obtained from MAC?  A: (/) If no evaluator license can be obtained, the installer will show an error and GH will be unlicensed.    Q: What if a very old version of GH is already installed on the instance?  A: (x) Not Answered"
1,ADG GH: Resizable Detail View,"- For this story, Plan Mode only - Idea is that we have a draggable left side for when people need more space, similar to other products (e.g. Confluence). - This drag would be remembered per client (local storage) in terms of percentage of window size - Consider what happens when DV is dragged too far - do other columns collapse or resize - Consider whether we do two-line issue representation for backlog column  Testing notes below"
1,Enable GH on OD instances from the Create Project dialog,"COS  The user will go through these steps: # Click to create a project # Select Scrum / Kanban # If GreenHopper is NOT installed, display an installation message tailored to the type of instance (BTF/OD) # For OnDemand, click a link to go to MAC and get the license # Once license is purchased, go to welcome page for GH with create project dialog open and scrum/kanban selected  # If user cancels, return to previous page (can create project dialog be open?) # Add analytics events for every step as much as possible  This should use the existing API as provided by the Internal Systems team (meaning we need to open a new page with MAC and they'll need to log in).  # Copy to be provided. Note to self: make sure to include a warning that you need to be the account owner to enable it. Also that the trial only works if you haven't installed GH before (otherwise you'll be charged). Copy: https://extranet.atlassian.com/display/GHDEV/GHS-7876+OD+Create+Project+Sequence   (!) Development Notes (!)  Q: What if MAC is inaccessible? A: (x) Not Answered - I guess the user will get a 404 when they click the link.   Q: What if a GH jar is present but not enabled? A: (x) Not Answered - if GH is not licensed, the message will display.   "
1,Edit startDate of a fixVersion in JIRA,"As Patrick, I want to be able to edit startDate for a fixVersion in JIRA so that the experience between GH and JIRA is consistent and not confusing.    h4. Approach for JIRA 6.0    JIRA code base:  * Modify JIRA's {{version}} table to include a start date column  * Make the above schema change on JIRA 5.2.x branch so that downgrades continue to work.  * Augment the RESTful table in JIRA's Project Admin page for Versions to show this start date and allow editing of it  * Augment the REST representation of versions to include start date  * Add the start date field to the JIRA API for a Version object    GH code base:  * Make GH aware of the capability of storing start date in either JIRA or GH, depending on the JIRA version.  * Migrate data stored in GH for start date to JIRA storage if available.    (!) Development Notes (!)    Q: How will adding a startDate to a previously released version affect the issues that apply to it?  A: (/) If a previously released version has a release date, the new start date will be validated against the release date.    Q: How will adding a startDate affect issues that have been Archived?  A: (/) Changes to version data do not affect issues.    Q: Which timezone setting will the startDate follow, user's or system's?  A: (/) Start date and release date are both stored in the JVM system time zone.    Q: Can you remove the startDate after adding it?  A: (/) Yes.    Q: What is one way to get the startDate to come after the releaseDate?  A: (/) Not possible.    Q: What happens when you add a startDate to a version that has been released but has no releaseDate?  A: (/) Start date can be added without release date.    Q: Is there a special place in the database this column/row will go? Depending on the database used, is there a way for this location to be already taken, and/or do we recycle a location/field ID? We need to protect against overwrites.  A: (/) The field {{startdate}} did not exist in the table {{projectversion}} before now.    Q: How is the data synchronised between GH AO tables and JIRA?  A: (/) When the Version meta data is asked for in GH, it will check to see if it exists in JIRA as well. If so, it will be migrated over."
0,Display a trendline for version chart,"As Patrick, I want to see a trendline on the version chart so I can understand how the projection works.  A/C:  # display the trendline connecting projection to start of the version # if some stories have been removed from the version *after* they have been completed, then the line won't match the projection and that's fine. Otherwise the trendline is a straight line."
0,Adjust the shading for epic & version report,"# Use the same shades as Jay indicated in ""Sexy charts"" doc: https://extranet.atlassian.com/display/GHDEV/GHS-7825+Sexy+Chart  # don't display the lines on the edge of the projection"
0,Use the startDate in version report calculations,"# include the startDate for calculations in Version Report - both in the description on the top as well as on the timeline (i.e. timeline starts on the startDate)  # versions without startDate use the previous calculations  # if some stories were completed before the startDate, the ""completed"" line is adjusted at start, but are not included in the trendline calculations, nor do we display time before startDate.     Notes:  - we might not be happy with the result and it may end up with the code staying on a branch + extra stories being created to improve it"
0,As a user I want to be able to see the Epic Name as a column in Issue Navigator searches,"Currently, using the Epic Link column only shows the epic's issue key."
1,Update the open/close epic/versions panel(s),"COS:  Went through some variations on design before Martin suggested a 2-story-point fix for the story that is a slight deviation from the current design, but that  a) pulls in hover-X to close version or epic panel b) when a panel is open, there is no left-side sideways text "
0,Promote Release Planning out of labs,COS:  # Remove the advertisement from Getting Started page # Remove labs flags in code 
1,Excel export - plan mode,"As Patrick, I want to be able to export selected stories to excel straight from GreenHopper so I can easily manipulate them the way I want.    Notes:  - this should use JIRA's ""Export to Excel"" feature, and respect its configuration  - it should be available from the context menu"
1,diagram of GreenHopper data / how it all works,Suggested by Matt Doar:  https://confluence.atlassian.com/display/GH/GreenHopper+JIRA+Configuration?focusedCommentId=335479270#comment-335479270    e.g. see diagram on this page:  https://confluence.atlassian.com/display/JIRA/Configuring+Fields+and+Screens
1,"In order to be able to select report options easily, I want a better control to use","When opening a dropdown menu with a large amount of options, the menu doesn't force the page to scroll.  CoS:  # Four reports are affected: Version Report, Sprint Report, Sprint Burndown, Epic Report # Control uses auto-complete, single select style # No grouping of select options  (!) Dev Notes (!) Q: Is the frother case-insensitive? A: yes  Q: Does it search on spaces or any single char? A: Spaces no. Any character yes.   Q: What happens when an invalid string is typed in? Can we do other actions? A: No Matches message, if you try to submit an invalid value it should revert to the previous selection  Q: What happens when you search on closed/archived sprints/versions? A: same options show up as we had previously  Q: Can you search as an anonymous user? A: same options show up as we had previously  Q: Is the input XSS-safe? A: yes   Q: How long can the input be? A: no limit, if it doesn't match then we revert to the selection  Q: Are there tests to update? A: Epic and Version Report qunit tests are updated to reflect the changes"
0,GH updates for JIRA 6.0 ADG changes (mainly Admin nav?) plus Gadgets,See https://extranet.atlassian.com/display/TECHWRITING/ADG+Changes+for+JIRA+6.0  Main functionality changes: * admin nav?  GH docs to be updated: * *TO DO* https://confluence.atlassian.com/display/GH/GreenHopper+Administrator%27s+Guide *(CHILDREN)* * *TO DO*  https://confluence.atlassian.com/display/GH/GreenHopper+Installation+and+Upgrade+Guide * *TO DO (WILL INHERIT MINOR PAGE CHANGES IN JIRA 6.0)* Gadgets - screenshots
0,"Change ""hide nav bar"" shortcut","Currently it's 'z' to cycle between normal, projector and no-nav mode. Let's separate it so 'z' toggles projector mode only (as it used to), and Ctrl+Shift+F toggles the ""hide nav bar"" mode.    Rationale: this is consistent with how Confluence does it, and also Google Docs."
1,Gather information about potential sprint conflicts,What we want to get out of it:    - 1 for having gh  - amount of rapid boards  - amount of kanban boards  - amount of scrum boards    - amount of sprints  - amount of issues in scrum boards    - amount of issues which belong to multiple sprints  - amount of sprints which contain said issues  - 1 if this instance had problem
0,Write soke tests to measure UI performance on the plan mode,Scenarios to cover:  - rerank within the backlog - add to an active sprint - add to an epic - add to a version - render the context menu (both for an issue that's already selected as well as one that's deselected)
2,Assign a story to future sprint,"As Patrick I want to be able to assign a story to a future sprint so that it's query-able (via sprint id).  A/C:  1. I am able to create future sprints which I can assign issues to 2. I'm able to remove future sprints and the issues are removed from them 3. I'm able to move issues between future sprints and out of future sprints and the db is updated accordingly 4. The backlog shows all future sprints, regardless of whether they are empty or not  Notes:  - as part of this, the necessary DB work to enable future sprints needs to happen  (!) Dev Notes (!)  h4. Functionality that changes: - Send to Top (from JIRA) - Sprint dragging will not include filtered issues  Q: What happens if a sprint becomes orphaned aka all assigned boards are deleted? A: (x) Not Answered  Q: What if the sprint's owning board was deleted but it was visible on a second board? A: (x) Not Answered  Q: What if a non-visible orphaned sprint was included in the filter of a new board? Is it reassigned? A: (x) Not Answered  Q: What if there are multiple rank fields? A: (x) Not Answered  Q: Do sprints need their own rank field? If so, is it locked? What if it is deleted/modified in JIRA 5.1? A: (x) Not Answered  Q: What are the conditions for the migration task? A: (x) Not Answered  Q: What happens to sprints for which all issues have been deleted? A: (x) Not Answered  Q: What if an issue is assigned to more than 1 future sprint via different boards? A: (x) Not Answered"
1,Autocomplete for Epic Link in JQL,"As Patrick, I want to be able to see autocomplete when querying Epic Link in JQL so that it's easy to write queries without making spelling mistakes."
0,Add an issue to an Epic from JIRA when no other issues have been added to it yet,"As Patrick, I want to be able to add an issue to an empty epic when in JIRA so that I can fill it in easily.  Notes:  This can be achieved either via issue operation or always showing the existing ""issues in epic"" panel"
1,Allow to set resolution in Simplified Workflow,"As Patrick, I want to be able to specify which statuses transitions to which trigger the resolution field to be set to done so that my issues have any resolution at all.    Solution    1. Allow for a tickbox per status, which makes the global transition to that status set the resolution to ""Done"".  2. Unmapped statuses will also display this information (not necessarily the option to change it)  3. During the conversion to simplified workflow, the user will be presented with the default choice of statuses from the ""Done"" column selected for the resolution but should be able to override it before finishing the conversion.    Note that this will help people who are migrating as well as ones who have already migrated to simplified workflow.    (!) Dev Notes (!)  Q: Is this changed in GreenHopper Configuration?  A: (/) Its board specific so its in board configuration.    Q: How do we display the valid options as a part of the simplify dialog?  A: (/) With a multi select combo box    Q: How many statuses can you tick?  A: (/) No Limit    Q: What happens if the user moves a status to another column?  A: (/) The column is moved, the tick is not changed    Q: What if the user deletes a column that has a ticked box?  A: (/) Just move all statuses to unmapped column    Q: What if a status is dragged to unmapped?  A: (/) Should be disabled but visible"
0,Expose GH defaults as an API,"Currently other plugins can't access GH defaults such as Epic Issue Type or default custom field ids, and as a result can't do stuff where this is information is needed.    Add a new API that exposes this information to be consumable in java"
0,Expose GH defaults as an API,"Currently other plugins can't access GH defaults such as Epic Issue Type or default custom field ids, and as a result can't do stuff where this is information is needed.    Add a new API that exposes this information to be consumable in java"
0,GH further ADG updates,* *DONE* https://confluence.atlassian.com/display/GH/Configuring+a+Board *PLUS CHILDREN* * *DONE* https://confluence.atlassian.com/display/GH/Creating+a+Board * *POSTPONE UNTIL JIRA 6.0* https://confluence.atlassian.com/display/GH/GreenHopper+Administrator%27s+Guide *(CHILDREN)* * *POSTPONE UNTIL JIRA 6.0*  https://confluence.atlassian.com/display/GH/GreenHopper+Installation+and+Upgrade+Guide
0,Flagging on stories as swimlanes in work mode,"This needs to cover wallboards too.  CoS:  # Right click context menu should be available for parent issue swimlane ## The menu only contains one option which is to Add or Remove Flag (depending on state) ## Menu is only opened when right-clicking not on a control inside the swimlane (i.e. not on the issue key, not on the ""transition"", not on the twixie) # The option should be available via Cog menu (already present) # Flagged state needs to be visible in Wallboard Gadget  (!) Dev Notes (!) Q: What if the JQL swimlane is defined as a single issue? A: You can't flag it  Q: Does it work if the swimlane is collapsed/expanded? A: It still appears as flagged and you can still flag it.   Q: Does it work in all supported browsers? A: yes"
1,Wizard-like Create Board dialog,"As Patrick, I want to be guided when creating a board so I don't get lost in the process.    CoS:    - The dialog consists of three parts:  -- Board type (Scrum, Kanban)  -- Project/Filter (create new project, choose existing, specify saved filter)  -- leftovers: sample data on/off, workflow (JIRA vs GH Simplified)  - this dialog replaces the existing ones present in:  -- create new scrum board  -- create new kanban board  -- create new DIY board  -- the dialog available from the cog menu (incl Manage Boards)  - the ""DIY"" option is removed from the ""Getting Started"" screen as it is effectively obsolete  - if Scrum or Kanban mode is chosen from the ""Getting Started"" screen, then the wizard should start at step 2 (i.e. we already know if it's scrum or kanban)  - the text inviting you to create new project or choose existing on ""Getting Started"" needs rewording so it simply opens the dialog -- remove the two ""1"" and ""2"" things and replace with just ""1"".  - the link to create a sample project should be rewired to use the new wizard and take you directly to creating the project with sample data and workflow options already selected.    (!) Dev Notes (!)    Q: What if there are no existing projects to select?  A: Skip the step to choose and go straight to create new project    Q: What if there are no filters to select from?  A: No option to choose to create from a filter    Q: What if the user has no permissions to create a project?  A: menu link or button disabled; getting started text changes and does not show option to create    Q: For OnDemand, what if the user doesn't have all application permissions to create a project?  A: menu link or button disabled; getting started text changes and does not show option to create    Q: What if there is a large number of existing projects?  A: Drop down is really long    Q: What if there are a large number of filters?  A: Drop down is really long    Q: What if the user does not have permission to see some/all filters?  A: They won't appear in the list    Q: Are selections persisted between board creations?  A: No    Q: Does scrolling through all the panes persist selections?  A: Back / Next through pages should persist selections    Q: Is there proper XSS escaping for filters and projects?  A: Yes    Q: What if the window is resized?  A: Dialog stays the same size    Q: Does the create button disable/spinner after pressed?  A: There is a spinner.    Q: What happens if the operation is cancelled after the create button is pressed?  A: Within the process the dialog just goes away and no board is created. After the final button is pressed the page will reload and go to board, no cancel button.     Q: Has this been checked from plan, work, and report? Manage Boards, Board Configuration, and Getting Started all the same?  A: Link will be in menu for Manage Boards, Plan, Work and Report.     Q: Will the dialog not render properly in some browsers?  A: No, it will always be beautiful.   A2: In JIRA 5.1, 5.2 the dialog has some differences as expected."
0,Update close icon in Detail View to be a font icon,NULL
0,"In order to be better aware of the sprint markers migration, as an administrator I want to see a banner when GH 6.3 is installed","CoS:    * Simple text banner communicating that this release contains migration and that administrators need to be aware of what's happening.  * Link to Upgrade Notes/Migration docs  * Appears on every JIRA page once only  * Appears for every administrator user.  * Needs to stack well with other potential banners that JIRA provides.  * If the user installs GH for the first time and has no boards, or all the boards are already migrated, then we don't want to see this message.    (!) Dev Notes (!)  Q: Does the banner show up for every user, or just admins?  A: (/) Just system et regular admins  Q: Does it show only once, or does it persist?  A: (/) Once    Q: Does it still display if GreenHopper is unlicensed?  A: (/) No    Q: Does it display even with the redirect to Getting Started?  A: (/) Yes    Q: Does it display for anonymous users?  A: (/) No"
0,Change visual treatment on issue count in swimlane header to not use AUI badge,"See attached screenshot. Should be 12px ""Small text"" from ADG:  https://developer.atlassian.com/design/typography.html  edit: currently there is no filtering treatment"
1,"Quick filters should collapse into a ""... Show more"" reveal pattern when they will wrap to more than one line","We should use a pattern similar to the Reveal text pattern from the ADG.    * When there are more Quick Filters than there is horizontal screen space (eg. the screen is not wide enough, and they will wrap), we hide the remaining ones and display a ""... Show more"" link as the last Quick Filter.  * Clicking ""... Show more"" will reveal the rest of the Quick Filters  * ""... Show more"" will now change to say ""Show less"" instead  * The default state for all users is the collapsed state  * Whether the Filters are collapsed or all shown is persistent on a user level  * Reordering the Quick Filters stays the same as it is today  * When the header is collapsed (z on the keyboard), the behaviour stays the same.    "
1,"Quick filters should collapse into a ""... Show more"" reveal pattern when they will wrap to more than one line","We should use a pattern similar to the Reveal text pattern from the ADG.    * When there are more Quick Filters than there is horizontal screen space (eg. the screen is not wide enough, and they will wrap), we hide the remaining ones and display a ""... Show more"" link as the last Quick Filter.  * Clicking ""... Show more"" will reveal the rest of the Quick Filters  * ""... Show more"" will now change to say ""Show less"" instead  * The default state for all users is the collapsed state  * Whether the Filters are collapsed or all shown is persistent on a user level  * Reordering the Quick Filters stays the same as it is today  * When the header is collapsed (z on the keyboard), the behaviour stays the same.    "
1,Change activity stream documentation regarding repository commits,It is not clear in [the documentation for activity stream|https://confluence.atlassian.com/display/JIRA/Adding+the+Activity+Stream+Gadget] which commits from repositories in DVCS accounts are shown.    It seems that only ones with smart commits linking to JIRA issues appear in activity stream.
1,As a writer I want to create multiple articles so that they can be published,NULL
1,As a reviewer to be able to review articles and add comments,NULL
1,As a reviewer I want to have an account so that i can review an article,NULL
1,As an Admin I want to be able to assign roles,NULL
1,As an Admin I want to create Writer accounts,NULL
1,As an Admin I want to create Reviewer accounts,NULL
1,As an Admin I want to create MediaManager accounts,NULL